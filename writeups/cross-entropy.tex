\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.8}

\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bmu#1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

% \newcommand{\bmu}{\boldsymbol{\mu}}
% \newcommand{\V}{\mathcal V}
% \newcommand{\N}{\mathcal N}
% \newcommand{\Ed}{\mathcal E}
% \newcommand{\sfM}{\mathsf M}
% \def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bmu#1)}
% \def\Pa{\mathbf{Pa}}


\begin{document}
    \section{Inconsistency + Extra Information}
    We have seen the definition the inconsistency $\zeta$ of a test distribution $p$ with respect to $\sfM$.
    \[
        \zeta(M ; p) := %\inf_{p \in \Delta(W^{\mathcal V})}~
        \!\!\!\!\!\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed }\!\! \mathop{\mathbb E}_{x \sim p_X} \left[\kldiv[\Big]{\bmu_{L}(x) }{ p_Y(\cdot | X \!=\! x) } \right]
    \] 
    which scores $p$ based on its closeness to correctly matching the cpts of $\sfM$. In particular, if $\zeta(M;p) = 0$, then $p \in \bbr{\sfM}\SD$, and higher numbers intuitively require larger modifications to the tables of $\sf M$ in order to match $p$.     
    We also recently defined the \emph{extra information} of a distribution $p$ to be
    \[ \H^{\sfM}(p) := \left(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim p_X}  \H (\bmu_L (x)) \right) - \H(p) \] 
    
    Summing the two, we get 
    \begin{align*}
         \bbr{\sfM}(p) :=& \left(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim p_X}  \left[ \H (\bmu_L (x)) +\kldiv[\Big]{\bmu_{L}(x) }{ p_Y(\cdot | X \!=\! x) }  \right] \right) - \H(p) \\
         =& \sum_{ X \xrightarrow{\!\!L} Y } \sum_{x \in \V(X)} p_{\!_X}\!(x) \sum_{y \in \V(Y)}  \left[ \bmu_L (y\mid x) \Big(\log \frac{1}{\bmu_L(y \mid x)} + \log \frac{\bmu_L(y \mid x)}{p(y \mid x)}\Big)  \right]  - \H(p) \\
         =& \sum_{ X \xrightarrow{\!\!L} Y} \left[ \sum_{x, y }   p_{\!_X}\!(x) \bmu_L (y\mid x) \log \frac{1}{p(y \mid x)}  \right]  - \H(p) \\
         =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{\substack{x \sim p_{\!_X} \\[-0.0em] y \sim \bmu_L(x)}} \log \frac{1}{p(y \mid x)}  \bigg]  - \H(p) \\
         =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{x \sim p_{\!_X}} \underbrace{H_{\bmu_L} (p_{_Y})}_{\text{Cross Entropy}}  \bigg]  - \H(p ) \\
    \end{align*}
    So the first term here is the sum of all of the cross entropies.
    Fitting a distribution $p$ to a PDG $\sfM$ is therefore a logistic regression 
    
    % \[\left[\sum_{x, y }   p_{\!_X}\!(x) \bmu_L (y\mid x) \log \frac{1}{p(y \mid x)}  \right]\] 
    
    \subsection*{Is $\bbr{\sfM}$ convex?} 
    We've shown it to be convex in the specific case where $\sfM$ results from a Bayesian Network.
    
    
    
    \section{}
    
\end{document}
