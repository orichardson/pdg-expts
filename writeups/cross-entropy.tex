\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.8}

\newcommand{\bp}[1][L]{\mat{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bp#1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}

\newcommand{\obrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbrace{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbrace{\color{black}#2}_{#3}}\endgroup}
\newcommand{\obrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbracket{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbracket{\color{black}#2}_{#3}}\endgroup}



\begin{document}
	
	\section{Inconsistency + Extra Information}
			
		We have seen the definition the inconsistency $\zeta$ of a test distribution $\mu$ with respect to $\sfM$.
		\[
			\zeta(M ; \mu) := %\inf_{\mu \in \Delta(W^{\mathcal V})}~
			\!\!\!\!\!\sum_{ X \xrightarrow{\!\!E} Y  \in \Ed }\!\! \mathop{\mathbb E}_{x \sim \mu_X} \left[\kldiv[\Big]{ \mu_Y | X \!=\! x) }{\bp(x) } \right]
		\] 
		which scores $\mu$ based on its closeness to correctly matching the cpts of $\sfM$. In particular, if $\zeta(M;\mu) = 0$, then $\mu \in \bbr{\sfM}\SD$, and higher numbers intuitively require larger modifications to the tables of $\sf M$ in order to match $\mu$.     
		We also recently defined the \emph{extra information} of a distribution $\mu$ to be
		\[ \H^{\sfM}(\mu) := \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_X}  \H (\bp (x)) \Bigg) - \H(\mu) \] 
		
		Summing the two, we get 
	
	\begin{align*}
		\bbr{\sfM}(\mu) :=& \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_{\!_X}}  \left[ \H (\bp (x)) + \beta_L \cdot \kldiv[\Big]{ \mu_Y(\cdot | X \!=\! x) }{\bp(x) }  \right] \Bigg) - \H(\mu) \\
		=& \sum_{ X \xrightarrow{\!\!L} Y } \sum_{x \in \V(X)} \sum_{y \in \V(Y)}  \left[
			\mu(x,y) \log \frac{\mu(y \mid x)}{\bp(y \mid x)}\cdot \beta_L 
			+ \mu(x) \bp(y \mid x) \log \frac{1}{\bp(y \mid x)} \right]  - \H(\mu) \\
%
% \intertext{Using $\omega$ for a complete setting of all vairaibles, and for compactness, writing $x$ and $y$ in place of $\omega_X$, $\omega_Y$, }
%
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \sum\alle \left[
			\log \frac{\mu(\omega_Y \mid \omega_X)^{\beta_L}}{\bp(\omega_Y \mid \omega_X)^{\beta_L}} 
			\right] + \sum\alle \sum_{x,y} \mu(x) \bp(y \mid x) \log \frac{1}{\bp(y \mid x)}   - \H(\mu) \\
%
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \sum\alle \left[
			\log \frac{1}{\bp(\omega_Y \mid \omega_X)^{\beta_L} } - \log \frac{1}{\mu(\omega_Y \mid \omega_X)^{\beta_L}}
			\right] + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) \Big] - \H(\mu) \\
%
	\intertext{
	%Let $\beta$ be a parameter which controls each $\beta_L$ at once (some global parameter that intuitively is something like their average), so that $\beta_L = \beta \cdot \tilde \beta_L$. 
	We now define the factor graph distribution $\phi(w) = \frac{1}{Z_{\phi}}\prod\alle \bp(\omega_Y \mid \omega_X)^{\beta_L}$, where $Z_\phi := \sum_{\omega} \phi(\omega)$ is the normalization constant. We can now represent the equation above as }
%
		\bbr{\sfM}(\mu) =& \sum_{\omega \in \V(\sfM)} \mu(\omega) \log \left[
			\frac{1}{\phi(\omega) \cdot Z_\phi} 
			\right] + \sum\alle \E_{x \sim \mu_{\!_X}} \left[ \H(\bp \mid x) - \mu(y \mid x) \log \frac{1}{\mu(y \mid x)^{\beta_{\!_L}\!}} \right]  - \H(\mu) \\
%
		=& \left(\sum_{\omega \in \V(\sfM)} \mu(\omega) \log \frac{1}{\phi(\omega) }\right) - \log Z_\phi + 
			\sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] -\sum_{\omega \in \V(\sfM)} \mu(w) \log \frac{1}{\mu(w)}  \\
%		
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \log \left[
			\frac{\mu(\omega)}{\phi(\omega)  } 	\right] - \log Z_\phi  + \sum\alle \left( \E_{x \sim \mu_{\!_X}}\!\! \Big[ \H(\bp \mid x)\Big] - \beta_L \ \H_\mu(Y \mid X) \right) \\
		=& \kldiv{\mu}{\phi} - \log Z_\phi + \sum\alle \left( \E_{x \sim \mu_{\!_X}}\!\! \Big[ \H(\bp \mid x)\Big] - \beta_L \ \H_\mu(Y \mid X) \right)
		% =& \ubrace{\sum_{\omega \in \V(\sfM)} \mu(\omega)\bigg[ \log 
		% 	\frac{\mu(\omega) }{\phi(\omega) } 
		% 	 + \sum\alle \beta_L \log (\mu(y \mid x))
		% 	\bigg]}{\text{(1)}}  + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) \Big] - \log Z_\phi  \\
	\end{align*}
	
	
	The first term is a divergence, which pushes $\mu$ towards the factor graph distribution, and the second one is an additive constant. Together, they define the free energy used in the factor graph literature, to score distributions. Therefore, we discover that PDGs and factor graphs give the same free energy only if the remaining terms are zero. Because we can design cpts such that the remaining terms have large magnitudes for the factor graph distribution \todo{walk through this example}, we conclude that in general, the unique distribution given by our semantics is not the same as the one determined by a factor graph, for any setting of the factor parameters $\beta$. 
	
	
	We now turn our attention to the somewhat opaque final term:
	\[ \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] 
		% = \H^\sfM(\mu) - \H^\sfM()
	  \]
	Even if unclear what this term does to our minimization, this has a straightforward meaning: the sum of the expected differences, for each link $L$, between the entropy predicted by the cpt $\bp$ and the actual resulting entropy of $\mu$ when conditioned on the same event.
	The signs imply that $\mu$ causing higher entropy in $\bp(x)$ makes $\mu$'s score worse, and a higher entropy in the target marginal $\mu$ makes its score better; the trade-off is controlled by $\beta_L$.
	
	\subsection{Minimizing This}
	\note{The notes in this section don't actually lead to a point, but I wanted to typeset my derivations so I do not lose them and they are ready to go or review if they one day become useful. For this reason, the rest of the section will be colored Gray.}
	\begingroup\color{gray!50!black}
	
	\def\pz#1{\frac{\partial}{\partial z_v\!}\left[\vphantom{\Big|}#1\right]}
	Let's paramterize the distribution $\mu$ over a finite set $W := \V(\sfM)$ of possible values, by a (possibly unnormalized) vector $z$, so that if $S = \sum_{w} z_w$, we have $\mu(w) = \nicefrac{z_w}{S}$. Assuming that $z$ starts normalized ($S = 1$), we can take the partial derivitives of $\bbr{\sfM}(\mu)$ with respect to a particular component $z_v$ at a world $v$, ultimately in search of the gradient $\nabla_{\mu} \bbr{\sfM}$.
		
	First note that $\pz{S} = \pz{\sum_w z_w} = 1$, so
	\[ \pz{\mu(w)} = \pz{\frac{z_w}{S}} 
		= \frac{1}{S^2} \left(\pz{z_w} {S} - \pz{S}z_w \right) 
		= \frac{1}{S}(\delta_{w,v} - \mu(w))
	\]
	Again, if we are only interested in the instantaneous change, we can assume $S = 1$ without loss of generality.
	Suppose $X$ is a random variable in $\N^\sfM$. We denote the value of $X$ in world $w$ as $X(w)$, which we will also denote $X_w$ to avoid distracting parentheses as things get more complex. For any $x \in \V(X)$, we get a result similar to the above:
	\begin{align*} \pz{\mu(X = x)}
	  	&= \pz{\sum_{w \in \V(\sfM)} \mu(w) \mathbbm1[X(w) = x]}
	 	= \sum_{w \in \V(\sfM)} \pz{\mu(w)} \mathbbm1[X(w) = x] \\
	 	&= \sum_{w \in \V(\sfM)} (\delta_{v,w} - \mu(w)) \mathbbm1[X(w) \!=\! x]
	 	= \mathbbm1[X(v) \!=\! x] - \sum_{w \in \V(\sfM)} \mu(w) \mathbbm1[X(w) \!=\! x] \\
	 	&=  \mathbbm1[X(v) \!=\! x]  - \mu(X = x)
	\end{align*}
	Substituting the joint $XY$ above, and using the more compact notation, we also find that
	\[ \pz{\mu(x,y)} = \mathbbm1[XY(v) \!=\! x,y] - \mu(x,y) \] 
	
	\textbf{Differentiating KL Divergence.}
	\begin{align*}
		\pz{\kldiv{\mu}{\phi}} &= \pz{\sum_w \mu(w) \log \frac{\mu(w)}{\phi(w)}} \\
			&= \sum_w \left[ \pz{\mu(w)} \log \frac{\mu(w)}{\phi(w)} + \pz{\log\frac{\mu(w)}{\phi(w)} }\mu(w) \right] \\
			&=\sum_w \left[ \pz{\mu(w)} \log \frac{\mu(w)}{\phi(w)} + \frac{\phi(w)}{\ln 2\cdot\mu(w)} \pz{\frac{\mu(w)}{\phi(w)}} \mu(w) \right] \\
			&=\sum_w \left[ \pz{\mu(w)} \left(\log \frac{\mu(w)}{\phi(w)} + \frac{1}{\ln 2} \right) \right] \\
			&=\sum_w \left[ (\delta_{w,v} - \mu(w)) \left(\log \frac{\mu(w)}{\phi(w)} + \frac{1}{\ln 2} \right) \right] \\
			&= \log \frac{\mu(v)}{\phi(v)} + \frac{1}{\ln 2} - \sum_w \left[ \mu(w) \left(\log \frac{\mu(w)}{\phi(w)} +  \right) \right] - \frac{1}{\ln 2} \\
			&= \log \frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} \\
	\end{align*}
	
	\textbf{Differentiating Conditional Entropy.}
	For a particular $x \in \V(X)$, $y \in \V(X)$, we have
	\begin{align*}
		\pz{\mu(x,y) \log \frac{\mu(x)}{\mu(x,y)}}
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \pz{\log \frac{\mu(x)}{\mu(x,y)}} \cdot \mu(x,y) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \pz{\frac{\mu(x)}{\mu(x,y)}} \\
%		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \left(\frac{1}{\mu(x,y)^2} \right) \left(\pz{\mu(x)} \mu(x,y) - \mu(x) \pz{\mu(x,y)}\right) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \left(\frac{ \pz{\mu(x)} \mu(x,y) - \mu(x) \pz{\mu(x,y)}}{\mu(x,y)^2} \right) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{1}{\ln 2} \left( \pz{\mu(x)} \frac{\mu(x,y)}{\mu(x)} - \pz{\mu(x,y)} \right) \\
		&= \pz{\mu(x,y)} \left(\log \frac{\mu(x)}{\mu(x,y)} - \frac{1}{\ln 2}\right) -  \frac{1}{\ln 2} \pz{\mu(x)} \mu(y \mid x) \\
	\end{align*}
	
	Therefore, twice using the fact that $\sum_x \mathbbm1[X(v) = x]\varphi(x) = \varphi(X(v))$, we have
	\begin{align*}
		\pz{\H_\mu(Y \mid X)} &= \pz{\sum_{x,y}\mu(x,y) \log \frac{\mu(x)}{\mu(x,y)}} \\
			&= \sum_{x,y} \left[ \Big(\mathbbm1[XY_v \!=\! x,y] - \mu(x,y)\Big)\left(\log \frac{1}{\mu(y \mid x)} - \frac{1}{\ln 2}\right) -  \frac{1}{\ln 2} \Big(\mathbbm1[X_v \!=\! x] - \mu(x)\Big) \mu(y \mid x) \right] \\
			= \Bigg(\log& \frac{1}{\mu(Y_v | X_v)} - \frac{1}{\ln 2}\Bigg)  + \frac{1}{\ln 2} \sum_{y} \mu(y \mid X_v) - \sum_{x,y} \left[\mu(x,y) \log\frac{1}{\mu(y\mid x)} + \frac{1}{\ln2} (\mu(x)\mu(y\mid x) - \mu(x,y)) \right] \\
			&= \log \frac{1}{\mu(Y_v | X_v)} - \sum_{x,y} \left[\mu(x,y) \log\frac{1}{\mu(y\mid x)} \right] \\
			&= \log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X)
	\end{align*}
	
	Also, for a link $L$, since $\H(\bp(x))$ does not depend on $\mu$, we find immediately that 
	\[\pz{ \E_{x \sim \mu_X} \H(\bp(x)) }  = \H(\bp(X_v)) - \E_{x \sim \mu_X} \H(\bp(x))  \] 
	
	\textbf{The Total Gradient.}\\
	We can now compute this partial derivative for the entire link. Putting it all together, we have:
	\begin{equation}
	\begin{aligned}
		\pz{\kldiv{\mu}{\phi} + \sum\alle\left\{ \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x)\Big] - \beta_L \ \H_\mu(Y \mid X)\right\}  - \log Z_\phi }
			&= \\
			\left(\log\frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} \right) + \sum\alle\Bigg[ \H(\bp(X_v)) - \!\! \E_{x \sim \mu_{\!_X}}\!\! \Big[ \H(\bp \mid x)\Big]  - \beta_L & \bigg(\log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X) \bigg) \Bigg] 
	\end{aligned}\label{eq:totgrad}
	\end{equation}
	Note that the last half of each sub-expression is a copy of the original without the constant. If we let
	\[ J_\sfM(\mu) := \kldiv{\mu}{\phi} + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x)\Big] - \beta_L \ \H_\mu(Y \mid X) \]
	then $\bbr{\sfM}(\mu) = J_\sfM(\mu) - \log Z_\phi$, and share a gradient. We therefore examine just the behavior of $J_\sfM$, whose partial derivative with respect to $z_v$ is the first half of each sub-expression \eqref{eq:totgrad}
	\[ \pz{J_\sfM(\mu)} = \log\frac{\mu(v)}{\phi(v)} + \sum_L\left[ \H(\bp(X_v)) - \beta_L \log \frac{1}{\mu(Y_v | X_v)} \right] - J_\sfM(\mu) \] 
	
	
	Since $J_\sfM$ and each of these partial derivatives is continuous, the only way it can achieve a minimum on the interior of its set is if each $\nicefrac{\partial}{\partial z_v} J_\sfM = 0$, for every $v$. Because $J_\sfM(\mu)$ does not depend on the identity of the coordinate $v$, we have $|\V(\sfM)|$ equations constraining $\mu$, for every coordinate $v$, the function of $v$ on the left must equal the constant (of $v$), $J_\sfM(\mu)$ on the right of the equation below.
		
	\[  \log\frac{\mu(v)}{\phi(v)} + \sum_L\left[ \H(\bp(X_v)) - \beta_L \log \frac{1}{\mu(Y_v | X_v)} \right] = J_\sfM(\mu) \]
	
	In particular, consider the world $v'$, for which $N(v') = N(v)$ for every variable $N \in \N \setminus \{A\}$---that is, $v'$ agrees with $v$ on all variables except $A$. Then,
	
	\endgroup
	\subsection{What about the other way around?}
	
	The most important feature of the inconsistency $\zeta(\sfM ; \mu)$ we've used is that it is a divergence: equal to zero if and only if the two conditional marginals agree. It is still necessary to use the candidate distribution $\mu$ to weight them, as $\sfM$ may not itself have any data about the liklihoods of a variable $A$ despite stating something about an edge $A \to B$.
		
	We now explore what would happened if the divergence were reversed.

	\begin{align*}
		 \bbr{\sfM}'(\mu) :=& \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_X}  \left[ \H (\bp (x)) +\kldiv[\Big]{\bp(x) }{ \mu_Y(\cdot | X \!=\! x) }  \right] \Bigg) - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y } \sum_{x \in \V(X)} \mu_{\!_X}\!(x) \sum_{y \in \V(Y)}  \left[ \bp (y\mid x) \Big(\log \frac{1}{\bp(y \mid x)} + \log \frac{\bp(y \mid x)}{\mu(y \mid x)}\Big)  \right]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \left[ \sum_{x, y }   \mu_{\!_X}\!(x) \bp (y\mid x) \log \frac{1}{\mu(y \mid x)}  \right]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{\substack{x \sim \mu_{\!_X} \\[-0.0em] y \sim \bp(x)}} \log \frac{1}{\mu(y \mid x)}  \bigg]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{x \sim \mu_{\!_X}} \underbrace{ \H_{\bp} (\mu_{_Y} \mid X = x)}_{\text{Cross Entropy}}  \bigg]  - \H(\mu ) \\
	\end{align*}
	So the first term here is a sum of $\mu$-expectations of entropies, from $\H_{\mu_L(x)}$ to $\mu_Y$. 
	This is effecively is a hybrid expectation of surprise of $\mu$, where $x$ is drawn from $\mu$, $y$ is drawn from the agent's corresponding belief $\bp(x)$ of $Y$ given $X$ according to $L$. This is related to the log likelihood from a logistic regression, and a common loss for training neural networks
	\todo{Walk through this connection as well.}  
	% Fitting a distribution $p$ to a PDG $\sfM$ is therefore a logistic regression 
	% \[\left[\sum_{x, y }   p_{\!_X}\!(x) \bp (y\mid x) \log \frac{1}{p(y \mid x)}  \right]\] 
	
	
	\todo{In this formulation, you get an additive average of distributions, rather than a multiplicative one. Show this example.}
	
	
	
	\subsubsection*{Questions}
	\begin{description}
		\item [Is $\bbr{\sfM}$ convex?] We've shown it to be convex in the specific case where $\sfM$ results from a Bayesian Network.
	\end{description}
			
	% Now, if the product $\phi$ of the cpts normalizes to 1, and each cpt has the same entropy on every row, then both of the last terms are zero, and so $\bbr{\sfM}'(\mu) = \kldiv{\mu}{\phi}$, and is uniquely minimized at the product of the factors. Therefore, for an under-constrained network PDG such as one arising BN, 
	

	
\end{document}
