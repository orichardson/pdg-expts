\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.8}

\newcommand{\bp}[1][L]{\mat{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bp#1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}

\newcommand{\obrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbrace{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbrace{\color{black}#2}_{#3}}\endgroup}
\newcommand{\obrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbracket{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbracket{\color{black}#2}_{#3}}\endgroup}



\begin{document}
	\section{Inconsistency + Extra Information}
	We have seen the definition the inconsistency $\zeta$ of a test distribution $\mu$ with respect to $\sfM$.
	\[
		\zeta(M ; \mu) := %\inf_{\mu \in \Delta(W^{\mathcal V})}~
		\!\!\!\!\!\sum_{ X \xrightarrow{\!\!E} Y  \in \Ed }\!\! \mathop{\mathbb E}_{x \sim \mu_X} \left[\kldiv[\Big]{\bp(x) }{ \mu_Y(\cdot | X \!=\! x) } \right]
	\] 
	which scores $\mu$ based on its closeness to correctly matching the cpts of $\sfM$. In particular, if $\zeta(M;\mu) = 0$, then $\mu \in \bbr{\sfM}\SD$, and higher numbers intuitively require larger modifications to the tables of $\sf M$ in order to match $\mu$.     
	We also recently defined the \emph{extra information} of a distribution $\mu$ to be
	\[ \H^{\sfM}(\mu) := \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_X}  \H (\bp (x)) \Bigg) - \H(\mu) \] 
	
	Summing the two, we get 
	\begin{align*}
		 \bbr{\sfM}(\mu) :=& \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_X}  \left[ \H (\bp (x)) +\kldiv[\Big]{\bp(x) }{ \mu_Y(\cdot | X \!=\! x) }  \right] \Bigg) - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y } \sum_{x \in \V(X)} \mu_{\!_X}\!(x) \sum_{y \in \V(Y)}  \left[ \bp (y\mid x) \Big(\log \frac{1}{\bp(y \mid x)} + \log \frac{\bp(y \mid x)}{\mu(y \mid x)}\Big)  \right]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \left[ \sum_{x, y }   \mu_{\!_X}\!(x) \bp (y\mid x) \log \frac{1}{\mu(y \mid x)}  \right]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{\substack{x \sim \mu_{\!_X} \\[-0.0em] y \sim \bp(x)}} \log \frac{1}{\mu(y \mid x)}  \bigg]  - \H(\mu) \\
		 =& \sum_{ X \xrightarrow{\!\!L} Y} \bigg[ \E_{x \sim \mu_{\!_X}} \underbrace{ \H_{\bp} (\mu_{_Y} \mid X = x)}_{\text{Cross Entropy}}  \bigg]  - \H(\mu ) \\
	\end{align*}
	So the first term here is a sum of $\mu$-expectations of entropies, from $\H_{\mu_L(x)}$ to $\mu_Y$. 
	This is effecively is a hybrid expectation of surprise of $\mu$, where $x$ is drawn from $\mu$, $y$ is drawn from the agent's corresponding belief $\bp(x)$ of $Y$ given $X$ according to $L$, and  
	% Fitting a distribution $p$ to a PDG $\sfM$ is therefore a logistic regression 
	
	% \[\left[\sum_{x, y }   p_{\!_X}\!(x) \bp (y\mid x) \log \frac{1}{p(y \mid x)}  \right]\] 
	
	
	\subsubsection*{Questions}
	\begin{description}
		\item [Is $\bbr{\sfM}$ convex?] We've shown it to be convex in the specific case where $\sfM$ results from a Bayesian Network.

	\end{description}
	
	\section{What about the other way around?}
	The important feature of the inconsistency $\zeta(\sfM ; \mu)$ we've used is that it is a divergence: equal to zero if and only if the two conditional marginals agree. It is necessary to use the test distribution $\mu$ to weight them, as $\sfM$ may not itself have any data about the liklihoods of a variable $A$ despite stating something about an edge $A \to B$.
	
	We now explore what would happend if the divergence were reversed.
	
	\begin{align*}
		\bbr{\sfM}'(\mu) :=& \Bigg(\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_{x \sim \mu_{\!_X}}  \left[ \H (\bp (x)) +\kldiv[\Big]{ \mu_Y(\cdot | X \!=\! x) }{\bp(x) }  \right] \Bigg) - \H(\mu) \\
		=& \sum_{ X \xrightarrow{\!\!L} Y } \sum_{x \in \V(X)} \sum_{y \in \V(Y)}  \left[
			\mu(x,y) \log \frac{\mu(y \mid x)}{\bp(y \mid x)}\cdot \beta_L 
			+ \mu(x) \bp(y \mid x) \log \frac{1}{\bp(y \mid x)} \right]  - \H(\mu) \\
%
\intertext{Using $\omega$ for a complete setting of all vairaibles, and for compactness, writing $x$ and $y$ in place of $X(\omega)$, $Y(\omega)$, }
%
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \sum\alle \left[
			\log \frac{\mu(y \mid x)^{\beta_L}}{\bp(y \mid x)^{\beta_L}} 
			\right] + \sum\alle \sum_{x} \mu(x) \bp(y \mid x) \log \frac{1}{\bp(y \mid x)}   - \H(\mu) \\
%
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \sum\alle \left[
			\log \frac{1}{\bp(y \mid x)^{\beta_L} } - \log \frac{1}{\mu(y \mid x)^{\beta_L}}
			\right] + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) \Big] - \H(\mu) \\
%
	\intertext{Let $\phi(w) = \frac{1}{Z_{\phi}}\prod\alle \bp(\omega_Y \mid \omega_X)^{\beta_L}$, where $Z_\phi := \sum_{\omega} \phi(\omega)$ is the normalization constant. Continuing,}
%
		\bbr{\sfM}'(\mu) =& \sum_{\omega \in \V(\sfM)} \mu(\omega) \log \left[
			\frac{1}{\phi(\omega) \cdot Z_\phi} 
			\right] + \sum\alle \E_{x \sim \mu_{\!_X}} \left[ \H(\bp \mid x) - \mu(y \mid x) \log \frac{1}{\mu(y \mid x)^{\beta_{\!_L}\!}} \right]  - \H(\mu) \\
%
		=& \left(\sum_{\omega \in \V(\sfM)} \mu(\omega) \log \frac{1}{\phi(\omega) }\right) - \log Z_\phi + 
			\sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] -\sum_{\omega \in \V(\sfM)} \mu(w) \log \frac{1}{\mu(w)}  \\
%		
		=& \sum_{\omega \in \V(\sfM)} \mu(\omega) \log \left[
			\frac{\mu(\omega)}{\phi(\omega)  } 
			\right]  + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] ~~ - \log Z_\phi \\
		=& \kldiv{\mu}{\phi} + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] ~~ - \log Z_\phi 
		% =& \ubrace{\sum_{\omega \in \V(\sfM)} \mu(\omega)\bigg[ \log 
		% 	\frac{\mu(\omega) }{\phi(\omega) } 
		% 	 + \sum\alle \beta_L \log (\mu(y \mid x))
		% 	\bigg]}{\text{(1)}}  + \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) \Big] - \log Z_\phi  \\
	\end{align*}
	
	
	The first term is a divergence, which pushes $\mu$ towards the factor graph distribution, and the last one is an irrelevant additive constant. We therefore turn our attention to the somewhat opaque middle term:
	\[ \sum\alle \E_{x \sim \mu_{\!_X}} \Big[ \H(\bp \mid x) - \beta_L \ \H(\mu_{_Y}\! \mid x) \Big] \]
	This has a straightforward meaning: the sum of the expected differences, for each link $L$, between the entropy predicted by the cpt $\bp$ and the actual resulting entropy of $\mu$ when conditioned on the same event.
	The signs imply that $\mu$ causing higher entropy in $\bp(x)$ is bad (from the perspective of $\mu$ being a `good fit' to $\sfM$), and a higher entropy of $\mu$ is good; the trade-off is controlled by $\beta_L$. 
	
	% Now, if the product $\phi$ of the cpts normalizes to 1, and each cpt has the same entropy on every row, then both of the last terms are zero, and so $\bbr{\sfM}'(\mu) = \kldiv{\mu}{\phi}$, and is uniquely minimized at the product of the factors. Therefore, for an under-constrained network PDG such as one arising BN, 
	

	
\end{document}
