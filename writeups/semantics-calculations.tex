\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.8}
\definecolor{expand}{rgb}{0.1,0.8,0.2}
\definecolor{eliminate}{rgb}{0.9,0,0}
\definecolor{t1}{rgb}{0.5,0.5,0}
\definecolor{t2}{rgb}{0.3,0,0.3}
\definecolor{t3}{rgb}{0,0.3,0.3}

\newcommand{\bp}[1][L]{\mat{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bp#1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}

\newcommand{\obrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbrace{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbrace{\color{black}#2}_{#3}}\endgroup}
\newcommand{\obrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbracket{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbracket{\color{black}#2}_{#3}}\endgroup}



\begin{document}
	
	\section{Inconsistency + Extra Information}
			
	We have seen the definition the inconsistency $\zeta$ of a test distribution $\mu$ with respect to $\sfM$.
	\[
		\zeta(M ; \mu) := %\inf_{\mu \in \Delta(W^{\mathcal V})}~
		\!\!\!\!\!\sum_{ X \xrightarrow{\!\!E} Y  \in \Ed }\!\! \beta_L \! \mathop{\mathbb E}_{x \sim \mu_X} \left[\kldiv[\Big]{ \mu_Y | X \!=\! x) }{\bp(x) } \right]
	\] 
	which scores $\mu$ based on its closeness to correctly matching the cpts of $\sfM$. In particular, if $\zeta(M;\mu) = 0$, then $\mu \in \bbr{\sfM}\SD$, and higher numbers intuitively require larger modifications to the tables of $\sf M$ in order to match $\mu$.     
	We also recently defined the \emph{extra information} of a distribution $\mu$ to be
	\[ \H^{\sfM}(\mu) := \Bigg[\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \alpha_L \E_{x \sim \mu_X}  \H (\bp (x)) \Bigg] - \H(\mu) \] 
	
	In the previous document, which was similarly structured, we simply sum the two. Here, we do something slightly different: as the extra information was defined for distributions who matched the constraints of the PDG, we can replace it with a simpler surrogate rather than using $\E_\mu\H(\bp(x))$ we can use $- \E_\mu \log \mu(y \mid x) = \H_\mu(Y \mid X)$ (and it's not clear to me as of yet which is the better general definition%
		\footnote{With $\H(\bp(x))$, we get the intuition of ``information beyond the specification''. In the general case, it pushes towards $x$ that have low entropy entries in tables, to offset the global max-entropy push (without changing information capacity of the channel, only the input distribution). On the other hand, using $\H(Y \mid X)$, we get a benefit either from making resulting effectively more deterministic by changing the prior distribution, OR by changing the channel itself to be more deterministic. }%
	). Using this definition instead, and expanding out all of the coefficients, we get:
	
	\begin{align*}
		\bbr{\sfM}(\mu) :=&\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_\mu  \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} - \beta_L \lambda \log \frac{1}{\mu(y \mid x)} + \alpha_L \gamma \log \frac{1}{\mu(y\mid x)}  \right] - \gamma \H(\mu) \\
			=&\E_\mu \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} + (\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(w)} \Bigg\} 
	\end{align*}

	We now have some options. Following the path we did before, we can combine the first and last terms.  Define the factor graph distribution, to be $\phi(w) = \frac{1}{Z_{\phi}}\prod\alle \bp(\omega_Y \mid \omega_X)^{\nicefrac{\beta_L}{\gamma}}$, where $Z_\phi(\beta, \gamma) := \sum_{\omega} \phi(\omega)$ is the normalization constant, we can rewrite as:
	
	\begin{align*}
		\bbr{\sfM}(\mu) &= \E_\mu \Bigg\{ \log \frac{1}{\prod\alle \bp(y\mid x)^{\beta_L}}
			- \gamma \log \frac{1}{\mu(w)} 
		 + \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \left[
		(\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] \Bigg\} \\
			&= \E_\mu \Bigg\{ \log \frac{\mu(w)^\gamma}{\prod\alle \bp(y\mid x)^{\beta_L}} 
		+ \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \left[
		(\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] \Bigg\} \\
			&= \E_\mu \Bigg\{ \gamma \log \frac{\mu(w)}{\phi(w) Z_\phi(\beta,\gamma)} 
		+ \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \left[
		(\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] \Bigg\} \\
		&=   \gamma \kldiv{\mu}{\phi} - \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed }\E_\mu \left[
		(\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log Z_\phi(\beta,\gamma) \\
	\end{align*}
	Now, so long as $\gamma\vec\alpha = \lambda \vec\beta$ (for instance, if we set $\lambda$ and each $\alpha_L$ to zero), then we obtain the free energy determined by the factor graph distribution.
	
	Alternatively, we can re-express it in its original form:
	\begin{align*}
		\bbr{\sfM}(\mu) &=\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_\mu  \left[
		\beta_L \log \frac{1}{\bp(y\mid x)} - \beta_L \lambda \log \frac{1}{\mu(y \mid x)} + \alpha_L \gamma \log \frac{1}{\mu(y\mid x)}  \right] - \gamma \H(\mu) \\
		&= \sum_{ X \xrightarrow{\!\!L} Y } \E_\mu  \left[
		\beta_L \log \frac{\mu(y \mid x)^\lambda}{\bp(y\mid x)} \right] + \gamma \Bigg[  - \H(\mu) + \sum_{ X \xrightarrow{\!\!L} Y } \E_\mu \alpha_L  \log \frac{1}{\mu(y\mid x)} \Bigg] \\
		&= \lambda \sum_{ X \xrightarrow{\!\!L} Y } \E_\mu  \left[
		\beta_L \log \frac{\mu(y \mid x)}{\bp(y\mid x)^{\nicefrac{1}{\lambda}}} \right] + \gamma \Bigg[  - \H(\mu) + \sum_{ X \xrightarrow{\!\!L} Y } \E_\mu \alpha_L  \log \frac{1}{\mu(y\mid x)} \Bigg] \\
		&= \lambda \sum_{ X \xrightarrow{\!\!L} Y }\beta_L\ \E_{x \sim \mu}  \kldiv[\Big]{\mu(Y\mid x)}{\bp(x)^{\nicefrac{1}{\lambda}}} + \gamma \Bigg[  - \H(\mu) + \sum_{ X \xrightarrow{\!\!L} Y } \alpha_L \cdot\H(Y \mid X)\Bigg] \\
	\end{align*}
	For $\lambda = 1$, this is clearly $\zeta_\sfM(\mu) + \gamma \tilde \H^\sfM(\mu)$, with our surrogate. 
	
	For those worried about the useless extra parameter, 
	The original coefficient on $H(Y \mid X)$ is over-parameterized, and we can eliminate $\lambda$, by using the coefficient $\tilde \alpha_L := \alpha_L + (1-\lambda) \nicefrac{\beta_L}{\gamma}$ as a replacement for $\alpha_L$. We do not present it this way because the meaning of $\tilde \alpha$ is much less clear, and we may now require it to be greater than one.

	
	\subsection{Minimizing This}
	We now take the gradient of this expression; we will have fewer terms than in the previous version of this document. The stuff which I repeat there will stay in gray.
	
	\def\pz#1{\frac{\partial}{\partial z_v\!}\left[\vphantom{\Big|}#1\right]}
	\begingroup\color{gray!50!black}
	Let's paramterize the distribution $\mu$ over a finite set $W := \V(\sfM)$ of possible values, by a (possibly unnormalized) vector $z$, so that if $S = \sum_{w} z_w$, we have $\mu(w) = \nicefrac{z_w}{S}$. Assuming that $z$ starts normalized ($S = 1$), we can take the partial derivitives of $\bbr{\sfM}(\mu)$ with respect to a particular component $z_v$ at a world $v$, ultimately in search of the gradient $\nabla_{\mu} \bbr{\sfM}$.
		
	First note that $\pz{S} = \pz{\sum_w z_w} = 1$, so
	\[ \pz{\mu(w)} = \pz{\frac{z_w}{S}} 
		= \frac{1}{S^2} \left(\pz{z_w} {S} - \pz{S}z_w \right) 
		= \frac{1}{S}(\delta_{w,v} - \mu(w))
	\]
	Again, if we are only interested in the instantaneous change, we can assume $S = 1$ without loss of generality.
	Suppose $X$ is a random variable in $\N^\sfM$. We denote the value of $X$ in world $w$ as $X(w)$, which we will also denote $X_w$ to avoid distracting parentheses as things get more complex. For any $x \in \V(X)$, we get a result similar to the above:
	\begin{align*} \pz{\mu(X = x)}
	  	&= \pz{\sum_{w \in \V(\sfM)} \mu(w) \mathbbm1[X(w) = x]}
	 	= \sum_{w \in \V(\sfM)} \pz{\mu(w)} \mathbbm1[X(w) = x] \\
	 	&= \sum_{w \in \V(\sfM)} (\delta_{v,w} - \mu(w)) \mathbbm1[X(w) \!=\! x]
	 	= \mathbbm1[X(v) \!=\! x] - \sum_{w \in \V(\sfM)} \mu(w) \mathbbm1[X(w) \!=\! x] \\
	 	&=  \mathbbm1[X(v) \!=\! x]  - \mu(X = x)
	\end{align*}
	Substituting the joint $XY$ above, and using the more compact notation, we also find that
	\[ \pz{\mu(x,y)} = \mathbbm1[XY(v) \!=\! x,y] - \mu(x,y) \] 
	
	\textbf{Differentiating KL Divergence.}
	\begin{align*}
		\pz{\kldiv{\mu}{\phi}} &= \pz{\sum_w \mu(w) \log \frac{\mu(w)}{\phi(w)}} \\
			&= \sum_w \left[ \pz{\mu(w)} \log \frac{\mu(w)}{\phi(w)} + \pz{\log\frac{\mu(w)}{\phi(w)} }\mu(w) \right] \\
			&=\sum_w \left[ \pz{\mu(w)} \log \frac{\mu(w)}{\phi(w)} + \frac{\phi(w)}{\ln 2\cdot\color{eliminate}\mu(w)} {\color{expand}\pz{\frac{\mu(w)}{\phi(w)}}} {\color{eliminate}\mu(w)} \right] \\
			&=\sum_w \left[ \pz{\mu(w)} \log \frac{\mu(w)}{\phi(w)} + \frac{\color{eliminate}\phi(w)}{\ln 2} 	\left(\frac{{\color{eliminate}\phi(w)} \pz{\mu(w)} - {\color{eliminate}\mu(w) \pz{\phi(w)}}}{\color{eliminate}\phi(w)^2}\right)  \right] \\
			&=\sum_w \left[ \pz{\mu(w)} \left(\log \frac{\mu(w)}{\phi(w)} + \frac{1}{\ln 2} \right) \right] & \text{since $\pz{\phi(w)} = 0$}\\
			&=\sum_w \left[ (\delta_{w,v} - \mu(w)) \left(\log \frac{\mu(w)}{\phi(w)} + \frac{1}{\ln 2} \right) \right] \\
			&= \log \frac{\mu(v)}{\phi(v)} + \frac{1}{\ln 2} - \sum_w \left[ \mu(w) \left(\log \frac{\mu(w)}{\phi(w)}  \right) \right] - \frac{1}{\ln 2} 
				&\text{as $\ln 2$ cancels}\\
			&= \log \frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} \\
	\end{align*}
	
	\textbf{Differentiating Conditional Entropy.}
	For a particular $x \in \V(X)$, $y \in \V(X)$, we have
	\begin{align*}
		\pz{\mu(x,y) \log \frac{\mu(x)}{\mu(x,y)}}
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \pz{\log \frac{\mu(x)}{\mu(x,y)}} \cdot \mu(x,y) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \pz{\frac{\mu(x)}{\mu(x,y)}} \\
%		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \left(\frac{1}{\mu(x,y)^2} \right) \left(\pz{\mu(x)} \mu(x,y) - \mu(x) \pz{\mu(x,y)}\right) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{\mu(x,y)^2}{(\ln 2)\ \mu(x) } \left(\frac{ \pz{\mu(x)} \mu(x,y) - \mu(x) \pz{\mu(x,y)}}{\mu(x,y)^2} \right) \\
		&= \pz{\mu(x,y)} \log \frac{\mu(x)}{\mu(x,y)} + \frac{1}{\ln 2} \left( \pz{\mu(x)} \frac{\mu(x,y)}{\mu(x)} - \pz{\mu(x,y)} \right) \\
		&= \pz{\mu(x,y)} \left(\log \frac{\mu(x)}{\mu(x,y)} - \frac{1}{\ln 2}\right) +  \frac{1}{\ln 2} \pz{\mu(x)} \mu(y \mid x) \\
	\end{align*}
	
	This is not quite the conditional entropy $\H(Y\mid X)$; we have to also add the summation. Substituting that in, together with the fact that $\pz{\mu(\mathbf X)} = \Big(\mathbbm1[\mathbf X_v \!=\! \mathbf x] - \mu(\mathbf x)\Big)$ for any set of variables $\mathbf X$, we can factor the terms multiplied by a $\mathbbm1$ out of the sum:
	\begin{align*}
		\pz{\H_\mu(Y \mid X)} &= \pz{\sum_{x,y}\mu(x,y) \log \frac{\mu(x)}{\mu(x,y)}} \\
			&= \sum_{x,y} \left[ {\color{t1}\Big(\mathbbm1[XY_v \!=\! x,y] - \mu(x,y)\Big)\left(\log \frac{1}{\mu(y \mid x)} - \frac{1}{\ln 2}\right)} + {\color{t2}\frac{1}{\ln 2} \Big(\mathbbm1[X_v \!=\! x] - \mu(x)\Big) \mu(y \mid x)} \right] \\
\intertext{
	twice using the fact that $\sum_x \mathbbm1[X(v) = x]\varphi(x) = \varphi(X(v))$, we have
}
			= \color{t1}\Bigg(\log&{\color{t1} \frac{1}{\mu(Y_v | X_v)} - \frac{1}{\ln 2}\Bigg)}
			 +{ \color{t2}\frac{1}{\ln 2} \sum_{y} \mu(y \mid X_v)} - \sum_{x,y} \left[ {\color{t1}\mu(x,y) \log\frac{1}{\mu(y\mid x)} -\frac{1}{\ln2} \mu(x,y)} + {\color{t2}\frac{1}{\ln2} \mu(x)\mu(y\mid x)} \right] \\
			&= \log \frac{1}{\mu(Y_v | X_v)} - \sum_{x,y} \left[\mu(x,y) \log\frac{1}{\mu(y\mid x)} \right] \\
			&= \log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X)
	\end{align*}
	
	Also, for a link $L$, since $\H(\bp(x))$ does not depend on $\mu$, we find immediately that 
	\[\pz{ \E_{x \sim \mu_X} \H(\bp(x)) }  = \H(\bp(X_v)) - \E_{x \sim \mu_X} \H(\bp(x))  \] 
	\endgroup
	
	\textbf{The Total Gradient.}\\
	We can now compute this partial derivative for the entire link. Putting it all together, we have:
	
	\begin{align*}
		\pz{\bbr{\sfM}(\mu)} &= \pz{ \sum_{ X \xrightarrow{\!\!L} Y} \sum_{x,y}\mu(x,y)  \left[
		\beta_L \log \frac{1}{\bp(y\mid x)} + (\alpha_L\gamma - \beta_L) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \H(\mu)} \\
		&= \sum_{ X \xrightarrow{\!\!L} Y} \left[ \pz{  \sum_{x,y}\mu(x,y)\;
		\beta_L \log \frac{1}{\bp(y\mid x)}} +  \pz{\sum_{x,y}\mu(x,y) (\alpha_L\gamma - \beta_L) \log \frac{1}{\mu(y \mid x)} } \right] - \gamma \pz{\H(\mu)} \\
%		&= \sum_{ X \xrightarrow{\!\!L} Y}  \left[  \sum_{x,y}\pz{\mu(x,y)} \beta_L \log \frac{1}{\bp(y\mid x)} +  (\alpha_L\gamma - \beta_L) \pz{\sum_{x,y}\mu(x,y)    \log \frac{1}{\mu(y \mid x)} } \right] - \gamma \pz{\H(\mu)} \\
		&= \sum_{ X \xrightarrow{\!\!L} Y}  \left[   \beta_L\sum_{x,y}\pz{\mu(x,y)}  \log \frac{1}{\bp(y\mid x)} +  (\alpha_L\gamma - \beta_L) \pz{\H(Y\mid X) } \right] - \gamma \pz{\H(\mu)} \\
		= \sum_{ X \xrightarrow{\!\!L} Y} & \left[  \beta_L\sum_{x,y} \Big[\mathbbm1[X\!Y_v \!=\! xy] - \mu(xy)\Big]  \log \!\!\frac{1}{\bp(y\mid x)} +  (\alpha_L\gamma - \beta_L) \left[ \log\!\! \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X)\right] \! \right] \!- \gamma\left[\log\!\!\frac{1}{\mu(v)}- \H(\mu)\right] \\
		 &= \sum_{ X \xrightarrow{\!\!L} Y}\left[  \beta_L \log \frac{1}{\bp(Y_v \mid X_v)} +  (\alpha_L\gamma - \beta_L) \log \frac{1}{\mu(Y_v | X_v)} \right] \!- \gamma\log\frac{1}{\mu(v)} - \bbr{\sfM}(\mu) \\
	\end{align*}
	\def\MM{\mathbf{M}}
	So if we write $\MM^\mu$ for the random variable indicating the appropriate amount of surprise (given distribution $\mu$), so that $\bbr{\sfM}(\mu) = \E_{w \sim \mu} \MM^\mu(w)$, then 
	\[ \pz{\bbr{\sfM}(\mu)} = \MM^\mu(v) - \E_{\mu}\Big[ \MM^\mu \Big] \]
	and therefore
	\[ \nabla_\mu{\bbr{\sfM}(\mu)} = \MM^\mu - \E_{\mu}\Big[ \MM^\mu \Big]
		= \MM^\mu - \bbr{\sf M}(\mu) \]
%	\mathbbm1[XY(v) \!=\! x,y] - \mu(x,y)
	
	\clearpage
	\section*{OLD}
	
	\begin{equation}
	\begin{aligned}
		\pz{\bbr{\sfM}(\mu)} &= 
		\pz{\kldiv{\mu}{\phi} + \sum\alle (\alpha_L \gamma - \beta_L \lambda ) \H_\mu(Y \mid X) - \log Z_\phi }\\
			&= 
			\gamma \left(\log\frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} \right) + \sum\alle\Bigg[ (\alpha_L \gamma - \beta_L \lambda ) \bigg(\log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X) \bigg) \Bigg] 
	\end{aligned}\label{eq:totgrad}
	\end{equation}
	We are looking for zeros, which are unaffected by dividing through by the positive constant $\gamma$:
	\[\ 0 = \log\frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} + \sum\alle\Bigg[ \left(\alpha_L  - \beta_L \frac{\lambda}{\gamma} \right) \bigg(\log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X) \bigg) \Bigg]  \]
	Recall that $\phi(w) = \frac{1}{Z_{\phi}}\prod\alle \bp(\omega_Y \mid \omega_X)^{\nicefrac{\beta_L}{\gamma}}$. Substituting this back in, we have:
	
	\[\ 0 = \log\frac{\mu(v)}{\phi(v)} - \kldiv{\mu}{\phi} + \sum\alle\Bigg[ \left(\alpha_L  - \beta_L \frac{\lambda}{\gamma} \right) \bigg(\log \frac{1}{\mu(Y_v | X_v)} - \H_\mu(Y \mid X) \bigg) \Bigg]  \]
	
	\subsubsection*{(previous attempt)}

	Note that the last half of each sub-expression is a copy of the original without the constant. If we let
	\[ J_\sfM(\mu) := \kldiv{\mu}{\phi} + \sum\alle (\alpha_L  - \beta_L \lambda / \gamma ) \ \H_\mu(Y \mid X) \]
	then $\bbr{\sfM}(\mu) = J_\sfM(\mu) - \log Z_\phi$, and share a gradient. We therefore examine just the behavior of $J_\sfM$, whose partial derivative with respect to $z_v$ is the first half of each sub-expression in \eqref{eq:totgrad}:
	\[ \pz{J_\sfM(\mu)} = \log\frac{\mu(v)}{\phi(v)} + \sum_L\left[ (\alpha_L  - \beta_L \lambda / \gamma ) \log \frac{1}{\mu(Y_v | X_v)} \right] - J_\sfM(\mu) \] 
	
	
	Since $J_\sfM$ and each of these partial derivatives is continuous, the only way it can achieve a minimum on the interior of its set is if each $\nicefrac{\partial}{\partial z_v} J_\sfM = 0$, for every $v$. 
	
	\[ \pz{J_\sfM(\mu)} = \log\frac{\mu(v)}{\phi(v)} + \sum_L\left[ (\alpha_L  - \beta_L \lambda / \gamma ) \log \frac{1}{\mu(Y_v | X_v)} \right] - J_\sfM(\mu) \] 
	
	
	Because $J_\sfM(\mu)$ does not depend on the identity of the coordinate $v$, we have $|\V(\sfM)|$ equations constraining $\mu$, for every coordinate $v$, the function of $v$ on the left must equal the constant (of $v$), $J_\sfM(\mu)$ on the right of the equation below.
		
	\[  \log\frac{\mu(v)}{\phi(v)} + \sum_L\left[ (\alpha_L \gamma - \beta_L \lambda ) \log \frac{1}{\mu(Y_v | X_v)} \right] = J_\sfM(\mu) \]
	
	In particular, consider the world $v'$, for which $N(v') = N(v)$ for every variable $N \in \N \setminus \{A\}$---that is, $v'$ agrees with $v$ on all variables except $A$. Then,
	
	\section{}
	\begin{prop}
		If $\lambda \leq 1$, and
		$\displaystyle \forall Y \in \N. ~\sum_{ \xrightarrow{\!\!L} Y  \in \Ed } \alpha_L\leq 1$, then
		$\bbr{M}(\mu)$ is convex.
	\end{prop}
	\begin{proof}
		\begin{align*}
			\bbr{\sfM}(\mu) &= \sum_{ X \xrightarrow{\!\!L} Y} \E_\mu  \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} - (\alpha_L\gamma + \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \H(\mu) \\
		\end{align*}
		
	\end{proof}
	
	

	
\end{document}
