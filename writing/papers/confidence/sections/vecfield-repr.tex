

% Suppose that the space $\Theta$ is actually a differentiable manifold.
% In this case, we might want want $F$ to be compatible with the differentiable structure.
% \begin{CFaxioms}
%     \item $\Theta$ is a differentiable manifold.
%         For fixed $\theta$ and $\phi$, the function $\beta \mapsto F^{\beta}_\phi(\theta)$
%         is continuously differentiable.
%             \hfill \textbf{(differentiability)} \label{ax:diffble2}
% \end{CFaxioms}
% If $\Theta$ is a differentiable manifold and 


Recall that the number of training iterations $n$ in \cref{ex:classifier}
and Shafer's weight of evidence $w$ in \cref{ex:shafer}
are measurements of confidence that do not lie in $[0,1]$, but
 rather in $[0,\infty]$. 
% In this case, 
In such cases,
% we must instead use an analogous function of the form 
the appropriate analogue is a function
\begin{equation}
	F : \Phi \times [0, \infty] \times \Theta \to \Theta
	\label{eq:zero-inf-form}
\end{equation}
satisfying modified versions of
\cref{ax:zero,ax:idemp,ax:cont,ax:diffble,ax:seq-for-more,ax:nopause}
% identical except with $\infty$ in place of 1. 
that use $\infty$ in place of 1 for the upper limit of confidence. 
% We abuse terminology by stating that $F$ satisfies these axioms.
One reason to prefer this scale is that it 
allows us to represent degree of confidence in a way that combines additively. 
Nearly all measurable quantities used in science 
and everyday life can be described additively:
if you have six (feet/meters/galons/joules/people/votes/dollars), 
and then you find seven additional (distinct) ones, then you have thirteen altogether. 
We would like a measure of confidence that also works this way.

\begin{CFaxioms}
	\item For all
		% $\beta_1, \beta_2 \in \Rplus$,~
		% $\beta_1, \beta_2 \in [0,\infty]$,~
		$\chi_1, \chi_2 \in [0,\infty]$,~
		% $F^{c_1}_\phi \circ F^{c_2}_\phi = F^{c_1 \oplus c_2}_\phi$
		% $F^{\beta_1}_\phi \circ F^{\beta_2}_\phi = F^{\beta_1 + \beta_2}_\phi$.
		$F^{\chi_1}_\phi \circ F^{\chi_2}_\phi = F^{\chi_1 + \chi_2}_\phi$.
		% \hfill \textbf{(additivity)} \label{ax:additivity}
		% \hfill \textbf{(additivity)}
		\label{ax:additivity}
\end{CFaxioms}

% Recall that \cref{ax:seq-for-more} implies the behavior of updates is generated by low-confidence updates.
Recall how \cref{ax:seq-for-more} allows us to decompose high-confidence updates into sequences of low-confidence ones;
\cref{ax:additivity},
which implies \cref{ax:seq-for-more}, describes a particularly convenient
way that the decomposition might work. 

% In fact, there is a unique additive 
\begin{defn}
	% A function satisfying
	A \emph{flow update function}
	is a function
	$F : \Phi \times[0,\infty] \times \Theta\to \Theta$
	satisfying the appropriate analogues of
	% \cref{ax:zero,ax:idemp,ax:cont,ax:seq-for-more,ax:diffble}
	\cref{ax:zero,ax:idemp,ax:cont,ax:diffble}
	and \cref{ax:additivity}.
\end{defn}

To some,
% \cref{ax:additivity} might be pallatable already,
\cref{ax:additivity} might already be pallatable,
% but it looks like it might be an assumption that significantly restricts the expressiveness of our framework.
% but it also might look like it might significantly restrict the expressiveness .
but it is clearly a nontrivial assumption, and looks like it might severely restrict the expressiveness of our update formalism.
% This is not the case.
Fortunately, this is not the case.
While \cref{ax:additivity} does significantly pin down how confidence 
can be measured, it has no effect on what confidence can express.  
% More concretely, for every confidence function that does not satsify \cref{ax:additivity}, we can 


\begin{linked}{theorem}{add-reparam}
	If $F$ satisfies \cref{%
		ax:funcform,%
		ax:zero,ax:idemp,ax:cont,ax:seq-for-more,ax:diffble,ax:nopause},
	then there is a unique 
	flow update rule
	 $^+\!F$
	that behaves like $F$ for low confience updates
	(and is also additive: \cref{ax:additivity}). 
	%
	% Moreover,
	Furthermore, there exists a function 
	% $g$, increasing in $\chi$ such that,
	$g$ such that,
	for all $\theta,\phi,$ and $\chi$,
	\[
		F( \phi, 
		% g(\phi,\beta,\theta),
			\chi,
		 \theta )
		 =
		{^+}\!F(\phi,~ 
		% \beta,
		g(\phi,\chi,\theta),~
		 \theta).
	\]
\end{linked}
Thus, updates performed with $F$ are equivalent
to updates performed with ${^+}\!F$, except that
the degree of confidence needs to be translated appropriately (via $g$).


% Moreover, 

% Recall that \cref{ax:seq-for-more} impilies that the behavior of updates
% is generated by low-confidence updates; we saw a particularly nice
% way of doing that in \cref{ax:additivity},
% which has the feature that confidence behaves the same way no matter what your initial beliefs are. 


% Even restricting to , additivity is a particularly natural. 
% 
% \begin{prop}
% 	If $F$ is a differentiable \cofunc\ with confidence domain $\Rplus$,then there is a unique update rule $G$ with the same confidence domain, that behaves approximately like $F$ for small increments of confidence, and is also additive (\cref{ax:additivity}).
% \end{prop}


\subsection{Vector Field Representations}
\label{sec:vecrep}

We now turn to another representation of additive 
update functions, as vector fields. 
This representation allows us to extend the set $\Phi$ of possible observations to 
% to a set $\ext\Phi \supseteq \Phi$ with some algebraic operations.  
to a larger set $\ext\Phi \supseteq \Phi$ with some algebraic operations.  

In \cref{ax:diffble}, we assumed that $\Theta$ has a differentiable 
structure; thus, it makes sense to talk about its tangent space
$T\Theta$, which consists of pairs $(\theta, \mat v)$, where
$\theta \in \Theta$, and $\mat v$,
% is intuitively an infinitessial direction rooted at $\theta$.
intuitively, is a direction that one can travel in $\Theta$ beginning at $\theta$
% tangent to $\Theta$
% rooted at $\theta$
\parencite[\S3]{lee2013smooth}.
% structure; for ease of presentation, suppose that it is an $n$-dimensional manifold \parencite{lee2013smooth}. 
% For a smooth manifold $M$ (such as the space $\Delta \X$ of distributions over $\X$),
% and a point $p \in M$, we follow convention by writing $T_p M$ for the tangent space to $M$ at point $p$ \parencite{lee2013smooth}, and % $TM := \sqcup_{p \in M} (p, T_p M)$
% $TM := \sum_{p \in M} T_p M$ for the full tangent bundle over $M$.
%
% A vector field over $M$ is a smooth map $\mat v : M \to T M$ assigning a tangent vector $\mat v(p) \in T_p M$, to every point $p \in M
%
A vector field $X$ over $\Theta$ is then
a smooth map $X : \Theta \to T \Theta$ 
assigning a tangent vector $X(\theta) = (\theta, \mat v) \in TM$ 
to every $\theta \in \Theta$.
The set of all vector fields over $\Theta$ is denoted $\mathfrak X(\Theta)$
 % and is closed under linear combination
 and forms a vector space.
\parencite[\S8]{lee2013smooth}
\unskip.

There is a close relationship between additive confidence and such vector fields.
% A vector field is called \emph{complete} if it generates a global flow.
% , or equivalently, a smooth section of the projection map $\pi : T M \to M$, where $\pi((p, v)) = p$.
Given a flow update function $F$, and observation $\phi$, the
differential of $F$
intuitively represents an update with infinitessimal confidence, 
and is a vector field
\begin{equation}
	F'_\phi 
	:= \frac{\partial}{\partial \chi} F_{\theta}^{\chi} \Big|_{\chi=0}
	\in  \mathfrak X(\Theta).
	\label{eq:f-field}
\end{equation}
Moreover, we can recover $F_\phi$ as the integral curves of $F'_\phi$.
% In other words, if we knew only the vector field $F'_\phi$, 
% we could 
% because $F_\phi$ is the unique function satsifying \eqref{eq:f-field}.

\begin{fact}[{\citeauthor[Thm 9.12]{lee2013smooth}}]
	If $X \in \mathfrak X(\Theta)$, 
	there is at most one
	 % function 
	$f : [0,\infty) \times \Theta \to \Theta$
	such that for all $\theta \in \Theta$ and $a,b\ge 0$,
	\[
	f(a, f(b, \theta)) = f(a+b,\theta)
		~~\text{and}~~
	\frac{\partial}{\partial \chi}
	 	f(\chi,\theta)
		% \underset{\chi=0}|
		\Big|_{\chi{=}0}
		\!\!= X(\theta)
		.
	\]
	\label{fact:unique-integral-curves}
\end{fact}
\begin{coro}
	% Suppose $X_\phi \in \mathfrak X(\Theta)$ be a vector field.
	% Let $F$ be a flow-update rule. 
	% Suppose $X_\phi \in \mathfrak X(\Theta)$ be a vector field.
	% Then there is at most one flow-update function satisfying \eqref{eq:f-field}.
	% Fix the vector field $X := F'_{\phi}$.
	% $F$ is the only flow-update rule satisfying \eqref{eq:f-field}.
	Let $F$ be a flow update function, and fix the vector field $F'_{\phi}$.
	$F_\phi$ is the only flow update satisfying \eqref{eq:f-field}.
	% If $F$ is the uni
	\label{fact:unique-flow-for-vfield}
\end{coro}

% Therefore, \cref{theorem:add-reparam}
Thus, every flow update function can be equivalently represented
by its differential.
% \begin{prop}
% 	% Let $F$ be a flow-update rule.
% 	% Then, there is a bijective correspondence between 
% 	There is a biective correspondence between
% 	flow-update rules 
% 	% $F : \Phi \times[0,\infty] \times \Theta \to \Theta$.
% 	and
% 	$\Phi$-indexed families of vector fields $X : \Phi \to \mathfrak X(\Theta)$.
% % Every update rule $F : \Phi \times \mathbb R \to (\Theta  \to \Theta)$
% % satisfying \cref{ax:zero,ax:additivity,ax:diffble} corresponds to a unique
% % $\Phi$-indexed collection of vector fields
% %     $F' : \Phi \times \Theta \to T\Theta$
% \[
% 	X()
% \]
% \end{prop}
% \begin{coro}\label{thm:vecrep}
%     There is a natural bijection between
%     % update rules $F : \Phi \times \mathbb R \to \Delta \X \to \Delta \X$
%     update rules $F : \Phi \times \mathbb R \to (\Theta  \to \Theta)$
%         satisfying \cref{ax:zero,ax:additivity,ax:diffble},
%     and $\Phi$-indexed collections of complete vector fields
%         % $\{ F'_\phi : \Delta X \to T \Delta X \}_{\phi \in \Phi}$%
%         % $\{ F'_\phi : \Theta \to T \Theta \}_{\phi \in \Phi}$%
%         $ F' :  \Phi \times \Theta \to T \Theta$%
%         % $F' : \Phi \to \Delta\X \to T\Delta \X$%
%     .
% \end{coro}
% In the language of 
% 
% Not all vector fields can be integrated to get an update function
% 
% \begin{coro}\label{thm:vecrep}
% There is a bijective correspondence between udpate rules satisfying \cref{ax:zero,ax:additivity,ax:diffble} and $\Phi$-indexed collections of \textbf{complete} vector fields.
% \end{coro}
% We call $F'$ the \emph{vector field representation} of an update function $F$. 
% This vector field representation of an update function 
It may seem counter-intuitive that $F'_\phi$,
which no longer mentions confidence at all,
can capture confidence. In a sense, it does so by specifying 
everything about the update \emph{except} for the degree of confidence.
Moreover, this representation will turn out to be quite useful, 
in two respects: at a mathematical level, it will allows us to
describe and classify update functions on $\Theta$, 
and at a practical level, it will give us a natural extension of $\Phi$
that allows us to talk about ``mixtures'' of observations. 
% Having separated the confidence from the mechanics of the update,
% this vector field representation allows us to describe and
% classify update functions on $\Theta$

% \subsection{Orderless Combination of Observations}
% \textbf{Orderless Combination of Observations.}
One defining feature of vector fields is that they
form a vector space, and so
can be linearly combined to form new vector fields.
Therefore flow update functions, which are in natural correspondance
with differentiable update functions, also inherrit this structure.

% The first way of combining 
From scalar mutliplication, we get a way of rescaling
inputs $\phi$ by a ``relative confidence'' $k \in [0,\infty)$.
% \begin{prop}
	% Suppose $F$ is a flow udpate rule.
% For $\phi \in \Phi$, we can extend $F$ 
Concretely,
 % given $k$ and $\phi$,
define a new observation 
% $k\cdot\phi \in \ext\Phi$
$k\cdot\phi$
and extend $F$ to handle it by:
\[
	F^{\chi}_{k\cdot\phi}(\theta) := F^{k\chi}_{\phi}(\theta)
	,\quad\text{or equivalently,}\quad
	F'_{k\cdot \phi} := k F'_{\phi}
	.
\]
% \end{prop}
Note that if $k > 0$, the rescaled input
$k\cdot \phi$ behaves the same way that $\phi$
does for extreme values of confidence,
since $k 0 = 0$ and $k\infty = \infty$. 

% In this way, the set $\Phi$ inherits 
% the additivity of the update rule in the form of scalar multiplication.
% It turns out more is possible: updates inherit the entire vector space structure.


% The second way of combining propositions is to ``observe them concurrently''.
% The second way we 
% Making use of vector field addition, we can also 
From vector field addition, we get a natural way to combine observations.
Up to now, we have only been able to combine observations provided they are
both of the same input $\phi$ (e.g., via \cref{ax:additivity}).
The vector field representation allows us to do this for distinct inputs. 
% This can be quite powerful.
% In particular, given \cofunc s $F, G : \mathbb R \to \Theta$, we can define
% $F \oplus G$ via the vector field $(F \oplus G)' = F' + G'$.
% \begin{defn}
% 	For $\phi_1, \phi_2 \in \Phi$, we extend $F$ to 
% 	$\phi_1 \oplus \phi_2$
% \end{defn}
Concretely,
given $\phi_1, \phi_2 \in \Phi$, we can form a new input 
% $\phi_1 \oplus \phi_2 \in \ext \Phi$, 
$\phi_1 \oplus \phi_2$
and extend $F$ to handle it by taking 
% $F'_{\phi_1 \oplus \phi_2} := F'_{\phi_1} + F'_{\phi_2}$.
its vector field 
$F'_{\phi_1 \oplus \phi_2}$
to be the sum $F'_{\phi_1} + F'_{\phi_2}$ of the vector fields of $\phi_1 $ and $\phi_2$.
Unlike before, there is no easy way to describe the flow update function
$F_{\phi_1\oplus\phi_2}$ directly,
but \cref{fact:unique-integral-curves} implies that there's a unique
such function, if it exists.
We now prove that it does.
% limits to a point, 
% allowing it to be continously extended to $\infty$. 

\begin{prop}
	If $F$ is a flow update function, and $\phi_1, \phi_2 \in \Phi$, 
	then there exists a function 
	$F_{\phi_1 \oplus \phi_2} : [0, \infty] \times \Theta \to \Theta$
	such that
	% to handle 	input $\phi_1 \oplus \phi_2$ such that $F'_{}$
\end{prop}


Note that by definition, $\phi_1 \oplus \phi_2 = \phi_2 \oplus \phi_1$,
so this is a way of combining observations orderlessly, even in cases
where $\phi_1$ and $\phi_2$ do not commute. And when $\phi_1$ and $\phi_2$
already do not depend on order, $\phi_1\oplus \phi_2$ has the same effect
as $\phi_1$ followed by $\phi_2$.

\begin{prop}
	% If $\phi_1$ and $\phi_2$ commute
	% (i.e., $F^{\chi}_{\phi_1} \circ F^{\chi}_{\phi_2} =
	%  	F^{\chi}_{\phi_2} \circ F^{\chi}_{\phi_1}$ for all $\chi$)
	% \unskip, then both are equal to $F^{\chi}_{\phi_1 \oplus \phi_2}$ 
	% for all $\chi
	%  % \in [0,\infty]
	%  $.
	If $F^{\chi}_{\phi_1} \circ F^{\chi}_{\phi_2} =
	 	F^{\chi}_{\phi_2} \circ F^{\chi}_{\phi_1}$,
	then both updates are equal to $F^{\chi}_{\phi_1 \oplus \phi_2}$.
	\commentout{That is,
	\[
		F^{\chi}_{\phi_1}( F^{\chi}_{\phi_2}(\theta))
		=
		F^{\chi}_{\phi_2 \oplus \phi_1} (\theta)
		=
		F^{\chi}_{\phi_1 \oplus \phi_2} (\theta)
		=
		F^{\chi}_{\phi_1}( F^{\chi}_{\phi_2}(\theta)) 
		.
	\]}
\end{prop}

Intuitively, $\phi_1 \oplus \phi_2$ is a ``mixture observation'' containing
one part $\phi_1$ and one part $\phi_2$. This intuition is made
precise by the following proposition,
which shows that 

\begin{prop}
	Let $\phi_1, \phi_2 \in \Phi$ be inputs, and for $t > 0$, let
	$u_t := F_{\phi_1}^t \circ F_{\phi_2}^t : \Theta \to \Theta$
 	be shorthand for 
	a confidence-$t$ update $\phi_1$ followed
	by a confidence-$t$ update of $\phi_2$. 
	Then,
	\[
		F_{\phi_1 \oplus \phi_2}^\chi = 
		\lim_{n \to \infty}~~
		\overbrace{u_{\nf \chi n}\circ u_{\nf \chi n} \circ\cdots\circ
			u_{\nf \chi n}}^{\text{$n$ times}}
			.
		%  (F^{\frac\chi n}_{\phi_1} \circ F^{\frac\chi n}_{\phi_2})
		%  \circ
		%  (F^{\frac\chi n}_{\phi_1} \circ F^{\frac\chi n}_{\phi_2})
		%  \circ
		%  \cdots
		%  \circ
		%  (F^{\frac\chi n}_{\phi_1} \circ F^{\frac\chi n}_{\phi_2})
	\]
\end{prop}

\begin{example}
	ML example: dataset = orderless combination.
	Rescaling = changing learning rate.

	\TODO
	
\end{example}


\begin{defn}
For an assertion language $\Phi$, let $\ext\Phi$ denote
the space of weighted formal sums of elements of $\Phi$.
% the space $\mathbb R^{\Phi}_{\mathrm{fin}}$ of finitely supported vectors over $\Phi$,
\end{defn}

% If $\Phi$
\begin{prop}
Every  update rule $F$ on $\Phi$, can be naturally extended to an update rule
$\bar F$ on $\ext\Phi$
% $\mathbb R^{\Phi}$,
via the total vector field
\[
    % \bar F'_{\vec{x}}( \theta ) := \sum_{\phi} F'_\phi(\theta) x_\phi.
    \bar F'_{\textstyle\sum_i a_i \phi_i} ( \theta ) := \sum_{i} a_i F'_{\phi_i}(\theta).
\]
% \end{prop}
%
\end{prop}

If $\Phi$ is itself a measurable space, we can extend this further:
Every  update rule $F$ on $\Phi$, can be naturally extended to an update rule $\bar F$ on the space $\mathcal M(\Phi)$ of measures over $\Phi$, via
\[
% \bar F'(\theta) := \sum_{} \beta_x \
\bar F'_{\beta(\Phi)}( \theta ) := \int_{\Phi} F'_\phi(\theta) \mathrm d\beta.
\]



% \subsection{Commutative Update Rules}
%
% All differentiable update rules are ``locally'' commutative, in the sense that the difference between
% $F_{\phi_1}^\epsilon \circ F_{\phi_2}^\epsilon$ and
% $F_{\phi_2}^\epsilon \circ F_{\phi_1}^\epsilon$ goes to zero as $\epsilon \to 0$.
% This is an immediate consequence of differentiability and the fact that they share a limit point (the identity function).
%
% If we fix a commutative and differentiable update rule $F$, and an initial point $\theta_0$, then the space $\mathbb R^\Phi$ of real-valued vectors over $\Phi$,
% serves as a coordinate system for $\Theta$.

%
% Not all update rules of interest are commutative, even if otherwise well-behaved.
%
% \begin{example}
%     The inconsistency-reduction update rule, $\tau$, is not commutative, but it is differentiable, additive, invertable, and even conservative.
% \end{example}
\subsection{Linear Update Rules}

% In some sense, ALL update rules are linear in $\bar\Phi$ by definition.

There are many definitions of linear update rules:
\begin{defn}\label{ax:linear}
Let $F$ be a differentiable update rule on $\Theta$. We say that $F$ is \textellipsis
\begin{itemize}
\item \emph{linear} if $\Theta$ is a vector space over $\mathbb R$, and the
vector field $F'_\phi$ is a linear operator, i.e., for all $a, b \in \mathbb R$, we have that
\[ F'_\phi(a \theta_1 + b \theta_2) = a F'_\phi(\theta_1) + b F'_\phi(\theta_2). \]

\item \emph{cvx-linear} if $\Theta \subset \mathbb R^n$ is a convex set, and, for all $a \in [0,1]$, we have that
\[ F'_\phi(a \theta_1 + (1-a) \theta_2) = a F'_\phi(\theta_1) + (1-a) F'_\phi(\theta_2). \]

\item \emph{$\mathcal L$-cvx-linear} if $\Theta \subset \mathbb R^n$ and $F$ is an optimizing update rule with a loss representation $\mathcal L$ linear in its first argument, i.e.,
\[
    \mathcal L(a \theta_1 + (1-a) \theta_2, \varphi) = a \mathcal L(\theta_1, \varphi) + (1-a) \mathcal L(\theta_2, \theta).
\]
\end{itemize}
% $F'_\phi(\theta) = \mathrm{V}_\phi \theta$ for some linear operator $V_\phi \in \mathbb R^{n \times n}$.
% $F'_\phi(\theta) = \mathrm{V}_\phi \theta$ for some linear operator $V_\phi$.
\end{defn}

\begin{prop}
If $F$ is a $\mathcal L$-cvx-linear, then it is also cvx-linear.
\end{prop}

In fact, the first condition is much stronger;
\begin{prop}
if $F$ is a nontrivial $\mathcal L$-cvx-linear optimizing UR, then $\Theta$ equals cone generated by  the rays $\{ F'_\varphi\theta : \theta \in \Theta, \varphi \in \Phi \}$. In particular, if there is some $\theta$ such that $0$ is in the interior of the convex hull $\mathrm{conv}(\{F'_\phi\theta\}_{\phi \in \Phi})$, then $\Theta = \mathbb R^n$.
\end{prop}

% Implicit in this definition is the supposition that the integral curves generated by the differential equations, started at any point $\theta \in \Theta$, are

\begin{prop}
% If $F$ is a  differ
Every linear update rule is of the form
$
    F^{\beta}_\phi(\theta) =  \theta^{T} \exp(\beta V)
$,
where $\exp(\beta V)$ is the matrix exponential.%
    \footnote{Concretely, if $V = U^T \mathrm{Diag}(\lambda_1, \ldots \lambda_n) U$ is an eigendecomposition of $V$, then $\exp(V) = U^T \mathrm{Diag}(e^{\beta\lambda_1}, \ldots e^{\beta\lambda_n}) U$.}
\end{prop}

\begin{prop}
A linear update rule $F$ is commutative iff, for every pair of statements  $\phi, \phi' \in \Phi$, the
matrices $V_\phi$ and $V_{\phi'}$ commute.
\end{prop}
