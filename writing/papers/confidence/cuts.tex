%%%%%%%%%%%%% INTRO %%%%%%%%%%%

High confidence is in many ways like high probability: if we really trust a statement \stmt, we should fully incorporate it into our beliefs, and thereby come to believe it with high probability.
Similarly, it only makes sense to be extremely confident in a \stmt\ if you believe that \stmt\ is extremely likely to be true.
Low confidence, on the other hand, is quite different from low probability.
If we have little trust in \stmt, we should \emph{ignore} \stmt, rather than coming to believe that \stmt\ is unlikely.
% To say \stmt\ has low probabilty is  high confidence in the $\lnot$\stmt.
For example, if an adversary tells you something that you happen to already believe,
% is likely to be true
you might say you have low confidence in their statement, but nevertheless ascribe it high probability.



For example, we might trust a surprising piece of information despite thinking it unlikely.
Of course, after updating, we would find that same information likely (precisely because we took it seriously in the update).
Thus, high confidence implies high posterior likelihood, making it easy to confuse the two.
The converse, however, is not true:
we should ascribe high (prior \emph{and} posterior) likelihood, but low confidence, to information from an untrusted source which we happen to believe already.


%% in ML example
It is not so clear that a fraction of the way to completion (say 0.7) is as meaningful here as it was before (and it would be difficult to recognize that point in any case),
but we now have a different way to quantify confidence: the number of training iterations. 


%% GEOMETRY
Note that the relevant geometry in \cref{ex:classifier} is not just the geometry of the weight space, but also that induced by the architecture and the loss function.



% Likelihood and trust often tightly coupled, but they are different.
% in an updating context: it's how seriously you take an input
% What we have in mind applies in an updating or learning setting. 
% We take confidence (in an input $\phi$) to be a measure of how seriously we take $\phi$ in updating our beliefs.
% Like probability, confidence lies in continuum between two extremes, but it ranges from untrusted to trusted, rather than from unlikely to likely.
% More concretely, confidence interpolates between prior beliefs and the beliefs one would obtain by fully incorporating $\phi$, so as to obtain posterior beliefs somewhere in between. 
More concretely, confidence interpolates between prior beliefs and those obtained by fully trusting $\phi$, so as to obtain posterior beliefs somewhere in between. 
Consequently, the notion is heavily dependent on the geometry of beliefs.
% This already suffices to characterize our simplest running example:
To take a simple example,
if our prior belief is a probability measure $p$ over a finite set $X$, then to update on event $A$ with confidence $\chi \in [0,1]$ might be to arrive at the posterior belief $p'(X) = (1-\chi) p(X) + (\chi)\, p(X|A)$. 
% In this case, $\chi$ can be viewed as a ``fraction of incorporation''.
Note that 
an update with no confidence ($\chi{\,=\,}0$) ignores new information (since $p' \!=\! p$),
while a
% more customary
full confidence update ($\chi{\,=\,}1$) takes it as seriously as possible,
% (since then $p'(A) \!=\! 1$).  
since afterwards $p'(A) \!=\! 1$ and there is no way to further incorporate $A$.
%
%
% It is less clear that the intermediate points are the ``right'' ones, or that this 
% is the ``appropriate'' parameterization of the path
The particular choice of path, as well as its parameterization,
are open for debate, but are natural consequences of the geometry when we think of probabilities as points in a simplex.




\textbf{Certainty Axioms.}
    % The simplest thing to do
    At the other extreme, we want to describe learning a certainty as the limit of updating with high confidence, so we require that
    \begin{CFaxioms}
        % \item[Cert1]
        \item
            % $\lim_{c \to \infty} F^{c}_\phi \in \bar\Theta$ exists, where $\bar\Theta$ is the closure of $\Theta$
            $\displaystyle\lim_{c \to \infty} F^{c}_\phi : \Theta \to \bar\Theta$
            exists, so we may abbreviate it as $F^\infty_\phi$
            \label{ax:cert-exists}
    \end{CFaxioms}

    It follows from \Cref{ax:additivity,ax:cert-exists} that
    % learning a certainty
    $F^\infty$
    is a ``projection'', in that is an idempotent operation, since,
    for all $\theta \in \Theta$,
    \[
        F^\infty_\phi (\theta)
            = \lim_{c \to \infty} F^{c}_\phi (\theta)
            % = \lim_{c_1 \to \infty} \lim_{c_2 \to \infty} F^{c_1 + c_2}_\phi (\theta)
            = \lim_{\substack{c_1 \to \infty \\ c_2 \to \infty}}
                F^{c_1 + c_2}_\phi (\theta)
            % = \lim_{c_1 \to \infty} \lim_{c_2 \to \infty}
            = \lim_{\substack{c_1 \to \infty \\ c_2 \to \infty}}
                F^{c_1}_\phi ( F^{c_2}_\phi (\theta))
            % = \lim_{c_2 \to \infty} F^{\infty}_\phi (F^{c_2}_\phi(\theta))
            = \lim_{c_1 \to \infty} F^{c_1}_\phi (F^{\infty}_\phi(\theta))
            % = \lim_{c \to \infty} F^{c}_\phi \,\circ\, F^{c}_\phi %(\theta)
            % F^\infty_\phi
            % = F^{\infty}_\phi (F^{\infty}_\phi(\theta))
            = F^\infty_\phi \,\circ\, F^\infty_\phi  (\theta)
            .
    \]

    Let us return to the setting where $\Theta$ parameterizes a family of probability distributions over $\X = (X, \mathcal A)$, via a function $\Pr : \Theta \to \Delta\X$,
    and suppose that for some assertions $\phi \in \Phi$ represent events $A \in \mathcal A$.
    The result of learning such an assertion with certainty should correspond to conditioning.

    \begin{CFaxioms}
        \item If $\phi$ represents $A \in \mathcal A$, and $\Pr_{\theta}(A) > 0$, then
            $\displaystyle \lim_{\beta\to\infty} \Pr_{ F^\beta_\phi\theta } = \Pr_\theta \mid A$.
            % \hfill \textbf{(absolute certainty)} \label{ax:certainty}
            \hfill \textbf{(conditioning)} \label{ax:conditioning}

        % \item
        % % $F^\beta_A$ and $F_{\bar A}^\beta$ are inverses.
        % $F^\beta_A \circ F_{\bar A}^\beta = 1_{\Delta\X}$.
        %     \hfill \textbf{(complementation)} \label{ax:comp}
    \end{CFaxioms}


    We can also consider the weaker variant of \Cref{ax:conditioning}:
    \begin{CFaxioms}
        % \item[U3$'$.]  \textbf{(effectiveness)~} $\supp F^\infty_A (\Pr) \subset A $
        % customlabel
        \item[\Cref*{ax:conditioning}$'$]
        % \item[\customlabel{ax:effectiveness}{\textbf{\Cref*{ax:conditioning}$'$.}}]
        If $\phi \in \Phi$ represents $A \in \mathcal A$ and $\Pr(A) > 0$, then
            % $F^\infty_A (\Pr)(A) = 1$
            % $\displaystyle \lim_{\beta\to\infty} F^\beta(\Pr)(A) = 1$
            $\displaystyle \lim_{\beta\to\infty} \Pr_{F^\beta_\phi\theta}(A) = 1$.
            \hfill \textbf{(effectiveness)}
            \label[CFaxiomsi]{ax:effectiveness}
            % \label{ax:effectiveness}
            % \label{ax:effectiveness}
    \end{CFaxioms}
    in which the limit of infinite confidence in $A$ is not required be the same as conditioning the starting point on $A$, but still must have the same effect of ensuring that $A$ occurs with probability 1.
    % \cref{ax:effectiveness}
