%%% ABSTRACT %%


%joe1: I have no idea what you mean by "a PDG inferencd algorithm can
%be used to calibrate a broad class of statistical models."  Since I
%don't think you discuss this issue anywhere in the paper, I just cut it.
%other probabilistic graphical models, and also because a PDG
%    inference algorithm can be used
%    % for ``inconsistency minimization'',
%    % which has been argued to be widely useful.
%    % to resolve inconsistencies, which has  as a generic modeling task.
%    % as a black box to train statistical models in ML.
%    to calibrate a broad class of statistical models.
%joe1: cut paragraph break
%oli4: rewrote.
% The key to our approach is combining
% (1) the observation that inference in PDGs with bounded tree-width can be reduced to a tractable linear optimization problem with exponential cone constraints, with
% (2) a recent interior-point method that can solve such problems efficiently (Dahl \& Anderson, 2022).
%joe5
%The key components are

%it does on random PDGs is nbot enough.  I don't think comparing it
%only to belief propagation is enough either.
%    We provide a concrete implementation and empirical evaluation.
%joe1: I wouldn't worry about hte other approaches now.  There are
%more important issues hou have to deal with first.
%    In addition, we prove auxiliary results about complexity of this
%    problem, and discuss other approaches to it.
% We also characterize the complexity of various components of the
% inference problem.

%%%%%%%%%%% INTRODUCTION: PARAGRAPH ON WHAT INFERENCE MEANS %%%%%%%%%%%%%




%%% Oli's version of Joe's two paragraphs.
\voli{
    %oli5:
    % What does it mean to answer probabilistic queries \emph{correctly} for a PDG?
    Before discussing our algorithm, we must discuss what it even means
    % to answer such probabilistic queries with respect to a PDG. 
    to answer a probabilistic query with a PDG.
    % How to answer conditional probaility queries based on a PDG? 
    % $?(Y|X)$?
    The situation is more subtle that it is for other graphical models, because a PDG may
    not just represent one probability distribution.
    % It is true that PDGs
    % As \authorcite{pdg-aaai} point out,
    %joe4*: I'm afraid I don't liker "observatiosl surrogate: either.
    %There's hothing that relates it to obervation (at least, not in the
    %intro).  Why not just call it the "representative distribution", if
    %you must give it a anme, or perhaps "the observational semantics (of
    %PDG M)".
    %oli4*: let's workshop this. I think it needs a name, in part because it's not the *only* representative distribution. I recall you suggesting "observational semantics", which was my starting point, but I think we can do better. What about "observational representative?" Some thoughts:
    % (1) "Semantics" is usually contrasted with "syntax", which sometimes helpful, but often everything in sight will be semantics.
    % (2) It might be helpful if the head noun could somehow emphasize that it is a joint distribution over all variables, or better yet, convey that this is the distribution that best incorporates all probabilistic observations.
    % (3) "Semantics" is an even more technical word than "limit", and I think buys us less--- it's helpful to keep in mind that it's one end of a spectrum.
    % In any case, I think we agree that the more important aspect of the the name needs to have something that specifies that it deals with \beta and Inc rather than \alpha and IDef.  "Observational-structural" seem to be our working consensus for the dichotomy. Is it really necessary to relate the term to the notion of observation in the intro? That seems difficult. In my opinion, we already do a reasonable job explaining where "observational" comes from once we dive into PDGs and define it properly.
    %oli4: explaining the intuition further
    % Nevertheless, under very mild assumptions 
    % If one takes an extreme empirical view, 
    %oli5:
    % From an extreme empirical viewpoint,
    % From an extreme empirical viewpoint,
    At one extreme,
    % At one extreme, which might well be called the ``emperical view'', 
    %joe5: I don't know what makes this "empirical"; I think it's a
    %mistake to focus on this, becuase we don't have good motivation for it.
    %oli5: I disagree: this is the very definition of what means to be an empiricist: you value observational data above all else. 
    a PDG does single out a particular distribution which we call its ``\obslimit'';
    %oli4: changing away from "primary"
    % Our primary goal
    %joe5: I now disagree
    %oli5: you disagreee with it being "one of our main goals"??? 
    one of our main goals 
    is to answer probabilistic queries with respect to this distribution.
    %
    %joe4*: isn't it actually a limit, rather than an "endpoint".  I don't
    % know what "endpoint" means.
    %%oli4: we can complete the open interval with its limit point, and in this sense it is an endpoint. I wrote it this way because you told me to hide the fact that it's a limit.  I agree---the fact that it's a limit is not relevant yet.
    %joe5: your usage of the word endpoint here (like many of your other
    %usages) is idiosyncractic.  The fact that you intend to complete an
    %open interval with its endpoint is not something that anyone else
    %will understand.   Your usage of "empirical" (not "empirical") is
    %equally idiosyncratic.  
    %joe4*: After rewriting this sentence, I decided
    % that we should cut it altogether.  How does knowing about all these
    % distributions that also, in a sense, represent the PDG help our
    % story?  To my mind, it just clutters it up.
    %oli4: the point of this paragraph is to address the question 'what is inference for a PDG?' at a high level, and illustrate the subtlety.  We cannot simply leave the paragraph at "Our primary goal is to answer queries w.r.t. this dist" because the observational representative, while special in some important ways, may not be the only one we care about. And it's not the only one our paper addresses.
    %joe5: I agree with your comment above.  I completely removed the
    %emphasis on what your calling \obslimit, and rewrote the story.
    However, 
    this distribution is just 
    % the \obslimit\ is just 
    % a special endpoint
    one end 
    of a continuous family of distributions that a PDG represents,
    %oli5: adding
    depending on the relative importance one lends to 
        causal information as opposed to empirical observation.
    % which might be of interest.
    % Under stricter assumptions,
    Our techniques enable inference with respect to 
        many of these other distributions as well.
    % For the PDG that corresponds to a BN, all of these distributions are identical, and coincide with the one that the BN represents.
    %oli5: rewording this bit and expanding
    We remark that all of these distributions are identical
    when there is no conflict between structure and observation;
    for the PDG corresponding to a BN, for instance, each one
    % and coincide with the one that the BN represents.
    coincides with the distribution represented by the BN.
    }

%% %oli5: one reason to name this:  I don't have to describe which end is zero here; 
% perhaps I prefer 
% \unskip, which we call the \emph{$0^+$-semantics}.
% \unskip, which we call the \emph{observational semantics}.


%%%%%%%%%%% DEFINITION OF IDEF %%%%%%%%%%%%%

% By contrast, there is also a ``qualitative'' term, which  measures
%oli2: I don't want to go here yet because we haven't worked out the details.
%in particular, I don't like "how far" analogy so well here, because IDef can
%be negative. I tried to rewrite it
% There is also another aspect of inconsistency of a disribution $\mu$
% with respect to a PDG ${\dg M}$: how far $\mu$ is from modeling the
% treating the edges in ${\dg M}$ as describing independent mechanisms
% that determine the target given the source.  This is captured by the
% \emph{information deficiency}, given by
% The second way in which we evalute a distribution $\mu$ is
% treating the edges in ${\dg M}$ as describing independent mechanisms
% that determine the target given the source.
%
%joe3*: this is *not* captured by the information deficiency.  IDef is
%a function of \mu, so at best it's capturing some relationship
%between \mu and the PDG.   You need to describe what that
%relationship is.  Although IDef can be negative (which is part of why I have
%0 intuition for it), I think what I wrote is far more accurate that
%what you wrote
%oli3*: updated
% The second way in which we score a distribution $\mu$,
% There is also a second aspect of how well a distribution $\mu$ fits $\dg M$:
% There is a second kind of discrepency between $\mu$ and $\dg M$, of a more structural flavor:


%joe2*: the weights are a bit of a red herring.  once we have a clear
%intuition for IDef (and I think I now see how the intuition should
%go)  then we're just multiplying by the confidence, because the
%confidence is indicating the probabiility that the edge is there.  We
%need to make clear the basic intuition without \alpha.
%oli2*: I feel like you're missing the point I was trying to make.
% It happens that the presence or absence of edges can be encoded
% with ones and zeros in the weights. I'm not emphasizing the continuum
% aspect of the weights. I'm emphasizing that it depends only on the (degree of)
% presence or absence of an edge, which is the weight \alpha --- and not on the
% cpds or the nature of the variables involved.


%joe4: I *really* don't like this.  I don't know what it means for \mu
%to "fail to arise", and talking about units of \alpha is horrible.
%oli4: I concede that "fails to arise" is a little clunky. But I actually really liked the description in terms of units of \alpha. It's simultaneously one of the shortest and also the most accurate ways I've written this down---and we've been rewriting this bit for years now.  What's wrong with "units of alpha"? 

% $\mu$ fails to arise in a situation in which each unit of $\alpha_a$ corresponds to an
% $\mu$ enforces the fact a hyperarc from $\Src$ to $\tgt$ respreseents an
%  independent causal mechanism that determines $\Tgt a$ based on $\Src a$.
%oli4: your replacement (above) very squishy. What does it mean for $\mu$ to enforce that a hyperarc represents something?  It sounds like giberish to me. 
%oli4: trying to rewrite again
% $\mu$ cannot arise from
%joe5: For those of us who think of \alpha as a measure of confidence,
%and a number in [0,1], it makes absolutely no sense to talk about
%units of alpha
%oli5*: sure it does! half a unit of \alpha means the edge is present with 50% probability. If you have \alpha is either 0 or 1 like in a causal model, then this is exactly right, and "half of a unit" does exactly what you might expect. 
%oli5: I would bet Chris doesn't mind this wording. But more importantly, it is accurate, unlike the version you have. 

%joe5*: I still really object to "deficiency".  Please change it to incompatibility.  I'm extremely uncomfortable with it being negative.  I have no intuition for that, and you've never given me any.
%oli5*: There's not a simple enough solution for the negativity that we can 
% "fix" here; the fact that your intuitions for incompatibility don't
% work when it's negative is a strong indication, to me, that we
% shouldn't call it that.  Based on our conversation, it seems 
% you would now like me to rename it name to  "structural score"
% or "structural loss". After all, we've been using a notion close to
% "structural incompatibility" to refer to something else. 
%oli5: I'm still looking for a term here that we can agree on; I don't imagine what I have here has resolved the issue yet. 
%oli5: my current proposal of "deficit" is a close synonym for "loss". Some other benefits over other terms, in my view: 
% (1) Like "loss" but unlike "incompatibility", intuitions extend readily to negative numbers.
% (2) Unlike "loss" it has a "relative to baseline" connotation which plays an important role in its construction. 
% (3) if possible, I'd like to keep "loss" separate for the context of ML, where it refers to the inconsistency.
% (4) it concords with a strong thermodynamic intuition about being like a difference of bond energies (or more accurately, entropies); the it's the change in structural energy going from something that shaped like a joint distribution, to something shaped like the hypergraph. 
% (5) it has the same ending as IDef (for deficieincy), for consistency with other presentations.
%joe6*: I would still argue strongly for structural incompatibility by
%analogy to observational incompatiblity. This analogy will help the reader,
%who will typically not share any of your concerns.  It's more
%important to be helpful to the reader than it is to satisfy your sensibilities.

%%%%%%% COMBINING INTO SCORING FNS %%%%%%%
%joe2*: First of all, this is not one semantics, but a family of
%semantics, indexed by \gamma.  Second, you need to give INTUITION for
%\gamma.
%oli2: OK, I've reworded it, although personally I don't think it's important to make such a distinction; it can be a single semantics that is a map from pairs (\mu, \gamma) to extended reals, just as easily  as it can be a family of semantics, indexed by \gamma, each mapping \mu to extended reals.



%joe2*: We need INTUITION.  why do we care about what happens as
%\gamma -> 0.  If we can't motivate this well, there's no reason for a
%reader to be interested in the rest of the paper, so this is critical.
% $\gamma$ controls the trade-off
% between matching quantitative beliefs and qualitative ones.
%
%joe4
% The notation 



%%% observational limit
% $\epsilon$-inference, on the other hand, is different.
% One case of particular interest is the limit as $\gamma \to 0$,
% which corresponds to a fully empirical approach: matching quantitative observations is the primary concern, and causal information is used only to break ties.
% in which observational data dominates, and
% structural information is used only to break ties.
% observational data is strictly more important than raw structural information.
% raw structural information is used only to break ties.
% When $\gamma$ is small enough.
% So long as $\bbeta > \mat 0$,
% So long as $\bbeta \gg \balpha$,
% It also has an interesting property.
% It is also of practical value, because under


% reading of $\dg M$, in which concrete observations and data trump causal structure.
% primary objective of this paper is to do inference with respect to this distribution.
% Why this limit in particular?
%
%joe2*: We need INTUITION.  why do we care about what happens as
%\gamma -> 0.  If we can't motivate this well, there's no reason for a
%reader to be interested in the rest of the paper, so this is critical.
%
% \TODO[ In my opinion is way too much intuition for $\gamma$, but I'll put it all here so it can be pared down. ]
%
% \begin{enumerate}[nosep]
%     \item %1.
% This quantitative limit is what is used to generate nearly all of the loss functions in \textcite{one-true-loss}, which are largely empirical in nature.
%     \item %2.
% Optimizing inconsistency in this limit guarantees a calibrated model, which is one of the biggest advantages PDGs have over factor graphs
%          \parencite[Example 5]{pdg-aaai}.
%
%     \item %3.
% When $\balpha = \mat 0$, this is the principle of maximum entropy; for other values of $\balpha$ it is a causally sensitive variant which accounts for the fact that cpd constraints themselves carry different amounts of entropy depending on the settings of the variables.
%     \item %4.
% Another reason to focus on the quantitative limit is pragmatic: there is a unique optimal joint distribution, $\bbr{\dg M}^*$ (at least if $\bbeta > \mat 0$).
%  In any case, this is the way in which PDGs define a unique joint distribution, and hence may be thought of as a graphial model.
% \end{enumerate}
% This distribution uniquely achieves the smallest information deficiency among those distributions maximally compatible with $\dg M$.

%%% inconsistency
%joe2: There is not a unique "inconsistency" of a PDG.  Again, you're
%giving a family of inconsistencies, indexed by \gamma.  So, at best,
%you can talk about the inconsistency relative to \gamma (which you
%must MOTIVATE).



%%%%%%%%%%%%%%%%%%%%% PDG INFERENCE AS CVX PROGRAM %%%%%%%%%%%%%%%%%%%%%%%%%%%
%Is it a convex program?
% More importantly, is it a \emph{disciplined} convex program,
%     which would mean it can be solved in polynomial time?
% That depends on $\gamma$.
%
%
%joe1
%We now present the central finding of our paper:  observation that the
%oli1: it's the central finding in that it's the lynchpin, but this is not the main result; nobody should care about it by itself. It's more like the key lemma.
% We now prove our main result: that the
% We now present the key technical finding of our paper: that the
% PDG scoring function $\bbr{\dg M}_\gamma$
% \eqref{eqn:scoring-fn}
% can be written as an exponental conic program, or sequence thereof.
% This requires different approaches, depending on the value of $\gamma$.
