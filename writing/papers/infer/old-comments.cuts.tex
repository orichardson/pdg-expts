
%%%%%%%%%%% INTRODUCTION: PARAGRAPH ON WHAT INFERENCE MEANS %%%%%%%%%%%%%




%%% Oli's version of Joe's two paragraphs.
\voli{
%oli5:
% What does it mean to answer probabilistic queries \emph{correctly} for a PDG?
Before discussing our algorithm, we must discuss what it even means
% to answer such probabilistic queries with respect to a PDG. 
to answer a probabilistic query with a PDG.
% How to answer conditional probaility queries based on a PDG? 
% $?(Y|X)$?
The situation is more subtle that it is for other graphical models, because a PDG may
not just represent one probability distribution.
% It is true that PDGs
% As \authorcite{pdg-aaai} point out,
%joe4*: I'm afraid I don't liker "observatiosl surrogate: either.
%There's hothing that relates it to obervation (at least, not in the
%intro).  Why not just call it the "representative distribution", if
%you must give it a anme, or perhaps "the observational semantics (of
%PDG M)".
%oli4*: let's workshop this. I think it needs a name, in part because it's not the *only* representative distribution. I recall you suggesting "observational semantics", which was my starting point, but I think we can do better. What about "observational representative?" Some thoughts:
% (1) "Semantics" is usually contrasted with "syntax", which sometimes helpful, but often everything in sight will be semantics.
% (2) It might be helpful if the head noun could somehow emphasize that it is a joint distribution over all variables, or better yet, convey that this is the distribution that best incorporates all probabilistic observations.
% (3) "Semantics" is an even more technical word than "limit", and I think buys us less--- it's helpful to keep in mind that it's one end of a spectrum.
% In any case, I think we agree that the more important aspect of the the name needs to have something that specifies that it deals with \beta and Inc rather than \alpha and IDef.  "Observational-structural" seem to be our working consensus for the dichotomy. Is it really necessary to relate the term to the notion of observation in the intro? That seems difficult. In my opinion, we already do a reasonable job explaining where "observational" comes from once we dive into PDGs and define it properly.
%oli4: explaining the intuition further
% Nevertheless, under very mild assumptions 
% If one takes an extreme empirical view, 
%oli5:
% From an extreme empirical viewpoint,
% From an extreme empirical viewpoint,
At one extreme,
% At one extreme, which might well be called the ``emperical view'', 
%joe5: I don't know what makes this "empirical"; I think it's a
%mistake to focus on this, becuase we don't have good motivation for it.
%oli5: I disagree: this is the very definition of what means to be an empiricist: you value observational data above all else. 
a PDG does single out a particular distribution which we call its ``\obslimit'';
%oli4: changing away from "primary"
% Our primary goal
%joe5: I now disagree
%oli5: you disagreee with it being "one of our main goals"??? 
one of our main goals 
is to answer probabilistic queries with respect to this distribution.
%
%joe4*: isn't it actually a limit, rather than an "endpoint".  I don't
% know what "endpoint" means.
%%oli4: we can complete the open interval with its limit point, and in this sense it is an endpoint. I wrote it this way because you told me to hide the fact that it's a limit.  I agree---the fact that it's a limit is not relevant yet.
%joe5: your usage of the word endpoint here (like many of your other
%usages) is idiosyncractic.  The fact that you intend to complete an
%open interval with its endpoint is not something that anyone else
%will understand.   Your usage of "empirical" (not "empirical") is
%equally idiosyncratic.  
%joe4*: After rewriting this sentence, I decided
% that we should cut it altogether.  How does knowing about all these
% distributions that also, in a sense, represent the PDG help our
% story?  To my mind, it just clutters it up.
%oli4: the point of this paragraph is to address the question 'what is inference for a PDG?' at a high level, and illustrate the subtlety.  We cannot simply leave the paragraph at "Our primary goal is to answer queries w.r.t. this dist" because the observational representative, while special in some important ways, may not be the only one we care about. And it's not the only one our paper addresses.
%joe5: I agree with your comment above.  I completely removed the
%emphasis on what your calling \obslimit, and rewrote the story.
However, 
this distribution is just 
% the \obslimit\ is just 
% a special endpoint
one end 
of a continuous family of distributions that a PDG represents,
%oli5: adding
depending on the relative importance one lends to 
    causal information as opposed to empirical observation.
% which might be of interest.
% Under stricter assumptions,
Our techniques enable inference with respect to 
    many of these other distributions as well.
% For the PDG that corresponds to a BN, all of these distributions are identical, and coincide with the one that the BN represents.
%oli5: rewording this bit and expanding
We remark that all of these distributions are identical
when there is no conflict between structure and observation;
for the PDG corresponding to a BN, for instance, each one
% and coincide with the one that the BN represents.
coincides with the distribution represented by the BN.
}


%%%%%%%%%%% DEFINITION OF IDEF %%%%%%%%%%%%%

% By contrast, there is also a ``qualitative'' term, which  measures
%oli2: I don't want to go here yet because we haven't worked out the details.
%in particular, I don't like "how far" analogy so well here, because IDef can
%be negative. I tried to rewrite it
% There is also another aspect of inconsistency of a disribution $\mu$
% with respect to a PDG ${\dg M}$: how far $\mu$ is from modeling the
% treating the edges in ${\dg M}$ as describing independent mechanisms
% that determine the target given the source.  This is captured by the
% \emph{information deficiency}, given by
% The second way in which we evalute a distribution $\mu$ is
% treating the edges in ${\dg M}$ as describing independent mechanisms
% that determine the target given the source.
%
%joe3*: this is *not* captured by the information deficiency.  IDef is
%a function of \mu, so at best it's capturing some relationship
%between \mu and the PDG.   You need to describe what that
%relationship is.  Although IDef can be negative (which is part of why I have
%0 intuition for it), I think what I wrote is far more accurate that
%what you wrote
%oli3*: updated
% The second way in which we score a distribution $\mu$,
% There is also a second aspect of how well a distribution $\mu$ fits $\dg M$:
% There is a second kind of discrepency between $\mu$ and $\dg M$, of a more structural flavor:


%joe2*: the weights are a bit of a red herring.  once we have a clear
%intuition for IDef (and I think I now see how the intuition should
%go)  then we're just multiplying by the confidence, because the
%confidence is indicating the probabiility that the edge is there.  We
%need to make clear the basic intuition without \alpha.
%oli2*: I feel like you're missing the point I was trying to make.
% It happens that the presence or absence of edges can be encoded
% with ones and zeros in the weights. I'm not emphasizing the continuum
% aspect of the weights. I'm emphasizing that it depends only on the (degree of)
% presence or absence of an edge, which is the weight \alpha --- and not on the
% cpds or the nature of the variables involved.


%%%%%%% COMBINING INTO SCORING FNS %%%%%%%
%joe2*: First of all, this is not one semantics, but a family of
%semantics, indexed by \gamma.  Second, you need to give INTUITION for
%\gamma.
%oli2: OK, I've reworded it, although personally I don't think it's important to make such a distinction; it can be a single semantics that is a map from pairs (\mu, \gamma) to extended reals, just as easily  as it can be a family of semantics, indexed by \gamma, each mapping \mu to extended reals.



%joe2*: We need INTUITION.  why do we care about what happens as
%\gamma -> 0.  If we can't motivate this well, there's no reason for a
%reader to be interested in the rest of the paper, so this is critical.
% $\gamma$ controls the trade-off
% between matching quantitative beliefs and qualitative ones.
%
%joe4
% The notation 



%%% observational limit
% $\epsilon$-inference, on the other hand, is different.
% One case of particular interest is the limit as $\gamma \to 0$,
% which corresponds to a fully empirical approach: matching quantitative observations is the primary concern, and causal information is used only to break ties.
% in which observational data dominates, and
% structural information is used only to break ties.
% observational data is strictly more important than raw structural information.
% raw structural information is used only to break ties.
% When $\gamma$ is small enough.
% So long as $\bbeta > \mat 0$,
% So long as $\bbeta \gg \balpha$,
% It also has an interesting property.
% It is also of practical value, because under


% reading of $\dg M$, in which concrete observations and data trump causal structure.
% primary objective of this paper is to do inference with respect to this distribution.
% Why this limit in particular?
%
%joe2*: We need INTUITION.  why do we care about what happens as
%\gamma -> 0.  If we can't motivate this well, there's no reason for a
%reader to be interested in the rest of the paper, so this is critical.
%
% \TODO[ In my opinion is way too much intuition for $\gamma$, but I'll put it all here so it can be pared down. ]
%
% \begin{enumerate}[nosep]
%     \item %1.
% This quantitative limit is what is used to generate nearly all of the loss functions in \textcite{one-true-loss}, which are largely empirical in nature.
%     \item %2.
% Optimizing inconsistency in this limit guarantees a calibrated model, which is one of the biggest advantages PDGs have over factor graphs
%          \parencite[Example 5]{pdg-aaai}.
%
%     \item %3.
% When $\balpha = \mat 0$, this is the principle of maximum entropy; for other values of $\balpha$ it is a causally sensitive variant which accounts for the fact that cpd constraints themselves carry different amounts of entropy depending on the settings of the variables.
%     \item %4.
% Another reason to focus on the quantitative limit is pragmatic: there is a unique optimal joint distribution, $\bbr{\dg M}^*$ (at least if $\bbeta > \mat 0$).
%  In any case, this is the way in which PDGs define a unique joint distribution, and hence may be thought of as a graphial model.
% \end{enumerate}
% This distribution uniquely achieves the smallest information deficiency among those distributions maximally compatible with $\dg M$.

%%% inconsistency
%joe2: There is not a unique "inconsistency" of a PDG.  Again, you're
%giving a family of inconsistencies, indexed by \gamma.  So, at best,
%you can talk about the inconsistency relative to \gamma (which you
%must MOTIVATE).



%%%%%%%%%%%%%%%%%%%%% PDG INFERENCE AS CVX PROGRAM %%%%%%%%%%%%%%%%%%%%%%%%%%%
%Is it a convex program?
% More importantly, is it a \emph{disciplined} convex program,
%     which would mean it can be solved in polynomial time?
% That depends on $\gamma$.
%
%
%joe1
%We now present the central finding of our paper:  observation that the
%oli1: it's the central finding in that it's the lynchpin, but this is not the main result; nobody should care about it by itself. It's more like the key lemma.
% We now prove our main result: that the
% We now present the key technical finding of our paper: that the
% PDG scoring function $\bbr{\dg M}_\gamma$
% \eqref{eqn:scoring-fn}
% can be written as an exponental conic program, or sequence thereof.
% This requires different approaches, depending on the value of $\gamma$.
