\documentclass{article}
\newif\ifmarginprooflinks
	\marginprooflinkstrue
	% \marginprooflinksfalse


% \input{../model-commands}
\usepackage[margin=1in, inner margin=0.7in, outer margin=1.3in]{geometry}
\usepackage{mathtools,amssymb,amsfonts}
\usepackage{parskip}

\usepackage{tikz}
	\usetikzlibrary{positioning,fit,calc, decorations, arrows, shapes, shapes.geometric}
	\usetikzlibrary{cd}
	\pgfdeclaredecoration{arrows}{draw}{
		\state{draw}[width=\pgfdecoratedinputsegmentlength]{%
			\path [every arrow subpath/.try] \pgfextra{%
				\pgfpathmoveto{\pgfpointdecoratedinputsegmentfirst}%
				\pgfpathlineto{\pgfpointdecoratedinputsegmentlast}%
			};
	}}
	%%%%%%%%%%%%
	\tikzset{AmpRep/.style={ampersand replacement=\&}}
	\tikzset{center base/.style={baseline={([yshift=-.8ex]current bounding box.center)}}}
	\tikzset{paperfig/.style={center base,scale=0.9, every node/.style={transform shape}}}

	% Node Stylings
	\tikzset{dpadded/.style={rounded corners=2, inner sep=0.7em, draw, outer sep=0.3em, fill={black!50}, fill opacity=0.08, text opacity=1}}
	\tikzset{dpad0/.style={outer sep=0.05em, inner sep=0.3em, draw=gray!75, rounded corners=2, fill=black!08, fill opacity=1}}
	\tikzset{dpad1/.style={outer sep=0.1em, inner sep=0.4em, draw=gray!75, rounded corners=2, fill=black!08, fill opacity=1}}
	% \tikzset{dpad/.style args={#1}{every matrix/.append style={nodes={dpadded, #1}}}}
	\tikzset{light pad/.style={outer sep=0.2em, inner sep=0.5em, draw=gray!50}}

	\tikzset{arr/.style={draw, ->, thick, shorten <=3pt, shorten >=3pt}}
	\tikzset{arr0/.style={draw, ->, thick, shorten <=0pt, shorten >=0pt}}
	\tikzset{arr1/.style={draw, ->, thick, shorten <=1pt, shorten >=1pt}}
	\tikzset{arr2/.style={draw, ->, thick, shorten <=2pt, shorten >=2pt}}
	\tikzset{archain/.style args={#1}{arr, every arrow subpath/.style={draw,arr, #1}, decoration=arrows, decorate}}


	\tikzset{fgnode/.style={dpadded,inner sep=0.6em, circle},
	factor/.style={light pad, fill=black}}

	\newcommand\cmergearr[4]{
		\draw[arr,-] (#1) -- (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\mergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1,#2=1,#3=1.2);
		\cmergearr{#1}{#2}{#3}{center-#1#2#3}
	}
	\newcommand\cunmergearr[4]{
		\draw[arr,-, , shorten >=0] (#1) -- (#4);
		\draw[arr, shorten <=0] (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\unmergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1.2,#2=1,#3=1);
		\cunmergearr{#1}{#2}{#3}{center-#1#2#3}
	}

	\usetikzlibrary{matrix}
	\tikzset{toprule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1]
	            (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east);%
	        }
	    },
	    bottomrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    leftrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west);%
	        }
	    },
	    rightrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    table with head/.style={
		    matrix of nodes,
		    row sep=-\pgflinewidth,
		    column sep=-\pgflinewidth,
		    nodes={rectangle,minimum width=2.5em, outer sep=0pt},
		    row 1/.style={toprule=thick, bottomrule},
  	    }
	}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}		%also loads amsmath
\usepackage{scalerel}
\usepackage{amssymb, bbm}
\usepackage{relsize}
\usepackage{color}
\usepackage{faktor}
%\usepackage{stmaryrd}
\usepackage{hyperref} % Load before theorems...
\hypersetup{colorlinks=true, linkcolor=blue!75!black, urlcolor=magenta, citecolor=deepgreen}
\usepackage{amsthm,thmtools}

	\theoremstyle{plain}
	\newtheorem{theorem}{Theorem}[section]
	\newtheorem{coro}{Corollary}[theorem]
	\newtheorem{prop}[theorem]{Prop}
	\newtheorem{claim}[theorem]{Claim}% maybe don't use this.
	\newtheorem{lemma}[theorem]{Lemma}
	\newtheorem{fact}[theorem]{fact}
	\newtheorem{conj}[theorem]{Conjecture}
	\newtheorem{question}[theorem]{Question}

	% \newcommand{\regugitatethmname}{}
	% \newtheorem{regurgitatethm}[theorem]{\regugitatethmname}% to be defined later
	% \newtheorem*{regurgitatethm*}{\regugitatethmname}% to be defined later

	\declaretheorem[name={}]%[numbered=unless unique,name=#1]
		{namedtheoremINNER}
	\newenvironment{namedthm}[1]
	  {\renewcommand{\thenamedtheoremINNER}{#1}\namedtheoremINNER}
	  {\endnamedtheoremINNER}

	\newcounter{proofcntr}
	\newenvironment{lproof}{\begin{proof}\refstepcounter{proofcntr}}{\end{proof}}
	\declaretheorem[numberwithin=theorem,name=Claim]{iclaim}

	\theoremstyle{definition}
	\declaretheorem[name=Definition,qed=$\square$,numberwithin=section]{defn} %
	\declaretheorem[name=Definition,qed=$\square$,numberwithin=theorem]{idefn} %
	\declaretheorem[name=Construction,qed=$\square$,sibling=defn]{constr}
	\declaretheorem[qed=$\square$]{example}

	\theoremstyle{remark}
	\newtheorem*{remark}{Remark}


\usepackage{xstring}
\usepackage{enumitem}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\usepackage[noabbrev,nameinlink,capitalize]{cleveref}
	\crefname{example}{Example}{Examples}
	\crefname{defn}{Definition}{Definitions}
	\crefname{prop}{Prop}{Propositions}
	\crefname{claim}{Claim}{Claims}
	\crefname{iclaim}{Claim}{Claims}
	\crefname{constr}{Construction}{Constructions}
	\crefname{conj}{Conjecture}{Conjectures}
	\crefname{fact}{Fact}{Facts}


\relax %%%%%%%%%    WRITING TOOLS  %%%%%%%%
	% see also: wip and storytellingnote environments defined with mdframed.
	\newcommand\commentout[1]{}
	\newcommand{\TODO}[1][INCOMPLETE]{{\color{red}\hangindent=0.7cm\rightskip=1.5cm$\smash{\Big\langle}$~\texttt{#1}~\raisebox{-0.3ex}{${\Big\rangle}$}\hspace{-1.5cm}\par}}

\relax %%%%%%%%% GENERAL MACROS  %%%%%%%%
	\let\Horig\H
	\let\H\relax
	\DeclareMathOperator{\H}{\mathrm{H}} % Entropy
	\DeclareMathOperator{\I}{\mathrm{I}} % Information
	\DeclareMathOperator*{\Ex}{\mathbb{E}} % Expectation
	\DeclareMathOperator{\supp}{\mathrm{supp}} % support
	\DeclareMathOperator*{\argmin}{arg\,min}
	\newcommand\mat[1]{\mathbf{#1}}
	\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
		#1\;\delimsize\|\;#2%
	}
	\newcommand{\thickD}{I\mkern-8muD}
	\newcommand{\kldiv}{\thickD\infdivx}

	\newcommand{\grad}{\vec\nabla}
	\newcommand{\tto}{\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}

	\newcommand{\datadist}[1]{\Pr\nolimits_{#1}}
	
	\newcommand\poly{\mathsf{Poly}}

	\relax % Short arrows.
		\newcommand{\veryshortarrow}[1][3pt]{\mathrel{%
		\vcenter{\hbox{\rule[-.5\fontdimen8\textfont3]{#1}{\fontdimen8\textfont3}}}%
		\mkern-4mu\hbox{\usefont{U}{lasy}{m}{n}\symbol{41}}}}
		\makeatletter
		\setbox0\hbox{$\xdef\scriptratio{\strip@pt\dimexpr
		\numexpr(\sf@size*65536)/\f@size sp}$}
		\newcommand{\scriptveryshortarrow}[1][3pt]{\mathrel{%
		\vcenter{\hbox{\rule[-.5\fontdimen8\scriptfont3]
		{\scriptratio\dimexpr#1\relax}{\fontdimen8\scriptfont3}}}%
		\mkern-4mu\hbox{\let\f@size\sf@size\usefont{U}{lasy}{m}{n}\symbol{41}}}}
		\newcommand{\sto}{\scriptveryshortarrow}
		\makeatother
\relax %%%%%%%%%   PDG  MACROS   %%%%%%%%
	\newcommand{\bp}[1][L]{\mat{p}_{\!_{#1}\!}}
	\newcommand{\V}{\mathcal V}
	\newcommand{\N}{\mathcal N}
	\newcommand{\Ed}{\mathcal E}

	\DeclareMathAlphabet{\mathdcal}{U}{dutchcal}{m}{n}
	\DeclareMathAlphabet{\mathbdcal}{U}{dutchcal}{b}{n}
	\newcommand{\dg}[1]{\mathbdcal{#1}}

	\DeclareMathOperator\src{\mathbf{src}}
	\DeclareMathOperator\tgt{\mathbf{tgt}}

	\newcommand\smid{\!\mid\!}
	\newcommand\MAP{\mathrm{MAP}}
	% \newcommand{\bundle}{\mathbin{\sqcup}}
	% \newcommand{\bundle}{\mathbin{\oplus}}
	\newcommand{\bundle}{\mathbin{+}}
	\newcommand\ado{\mathrm{do}}



	\newcommand{\pdgunit}{\mathrlap{\mathit 1} \mspace{2.3mu}\mathit 1}

	\newcommand\Pa{\mathbf{Pa}}
	\newcommand{\IDef}[1]{\mathit{IDef}_{\!#1}}
	\newcommand\Inc{\mathit{Inc}}
	\newcommand{\PDGof}[1]{{\dg M}_{#1}}
	\newcommand{\UPDGof}[1]{{\dg N}_{#1}}
	\newcommand{\WFGof}[1]{\Psi_{{#1}}}
	\newcommand{\FGof}[1]{\Phi_{{#1}}}
	\newcommand{\Gr}{\mathcal G}
	\newcommand\GFE{\mathit{G\mkern-4mu F\mkern-4.5mu E}}
	% \newcommand\aar[1]{\langle\mskip}

	\newcommand{\ed}[3]{%
		\mathchoice%
		{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{${#1}$}}}{\xrightarrow{\hphantom{\scriptstyle {#1}}}} #3} %display style
		{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptstyle {#1}}}} #3}% text style
		{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptscriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptscriptstyle {#1}}}} #3} %script style
		{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptscriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptscriptstyle {#1}}}} #3}} %scriptscriptstyle


	\newcommand{\alle}[1][L]{_{\ed {#1}XY}}


	% need some magic to do the starred versions;
	% taken from https://tex.stackexchange.com/questions/426375/explain-this-code-to-me-doubled-delimiters.
	\newcommand{\nhphantom}[2]{\sbox0{\kern-2\nulldelimiterspace$\left.\delimsize#1\vphantom{#2}\right.$}\hspace{-.97\wd0}}

	%automating it.
	\newcommand\DeclareDoubleDelim[5]{
	\DeclarePairedDelimiterX{#1}[1]{#2}{#5}
		{\nhphantom{#3}{##1}\hspace{1.2pt}\delimsize#3\mathopen{}##1\mathclose{}\delimsize#4\hspace{1.2pt}\nhphantom{#4}{##1}}
	}
	\newcommand\DeclareDoubleDelimPP[7]{
		\DeclarePairedDelimiterXPP{#1}[1]{#2}{#3}{#6}{#7}
			{\nhphantom{#4}{##1}\hspace{1.5pt}\delimsize#4\mathopen{}##1\mathclose{}\delimsize#5\hspace{1.5pt}\nhphantom{#5}{##1}}
	}


	% \DeclarePairedDelimiterX{\bbr}[1]{\lbrack}{\rbrack}{
	%     \nhphantom{\lbrack}{#1} \delimsize\lbrack \mathopen{} #1 \mathclose{} \delimsize\rbrack \nhphantom{\rbrack}{#1}
	% }
	\DeclareDoubleDelim
		\SD\{\{\}\}
	\DeclareDoubleDelim
		\bbr[[]]
	\DeclareDoubleDelim
		\aar\langle\langle\rangle\rangle
	\DeclareDoubleDelimPP
		\aarz{}\langle\langle\rangle\rangle{_{\!_0}} %\downarrow


%% Theorem Restatement
\usepackage{xpatch}
	\makeatletter
	\xpatchcmd{\thmt@restatable}% Edit \thmt@restatable
	   {\csname #2\@xa\endcsname\ifx\@nx#1\@nx\else[{#1}]\fi}% Replace this code
	   % {\ifthmt@thisistheone\csname #2\@xa\endcsname\typeout{oiii[#1;#2\@xa;#3;\csname thmt@stored@#3\endcsname]}\ifx\@nx#1\@nx\else[#1]\fi\else\csname #2\@xa\endcsname\fi}% with this code
	   {\ifthmt@thisistheone\csname #2\@xa\endcsname\ifx\@nx#1\@nx\else[{#1}]\fi
	   \else\fi}
	   {}{\typeout{FIRST PATCH TO THM RESTATE FAILED}} % execute on success/failure
	\xpatchcmd{\thmt@restatable}% A second edit to \thmt@restatable
	   {\csname end#2\endcsname}
	   {\ifthmt@thisistheone\csname end#2\endcsname\else\fi}
	   {}{\typeout{FAILED SECOND THMT RESTATE PATCH}}

	% \def\onlyaftercolon#1:#2{#2}
	\newcommand{\recall}[1]{\medskip\par\noindent{\bf \expandarg\Cref{thmt@@#1}.} \begingroup\em \noindent
	   \expandafter\csname#1\endcsname* \endgroup\par\smallskip}
	% \newenvironment{linked}[3][]{%
	% 	\def\linkedproof{#3}%
	% 	\def\linkedtype{#2}%
	% 	\restatable[#1]{#2}{#2:#3}\label{#2:#3}}
	% 	{\endrestatable%
	% 	\marginpar{%
	% 	% \vspace{-0.5em}
	% 	% \hspace{2em}
	% 		\raggedright
	% 		\hyperref[proof:\linkedproof]{%
	% 		\color{blue!50!white}
	% 		$\Big[$\,{\small\tt\begin{tabular}{@{}l@{}} proof of \\~\cref*{\linkedtype:\linkedproof}\end{tabular}}\,$\Big]$}
	% 		}%
	% 	}
	%%
   	\setlength\marginparwidth{1.55cm}
	\setlength\marginparpush{2ex}
	\let\oldmarginpar\marginpar
	\renewcommand{\marginpar}[1]{%
		\leavevmode%
		\oldmarginpar{#1}%
		\ignorespacesafterend\ignorespaces}
	\newsavebox\marginprooflinkbox
	\newenvironment{linked}[3][]{%
			\def\linkedproof{#3}%
			\def\linkedtype{#2}%
			\ifmarginprooflinks%
			\sbox\marginprooflinkbox{%
				\centering%
				\hyperref[proof:\linkedproof]{%
				\color{blue!30!white}%
				\scaleleftright{$\Big[$}{\,\mbox{\footnotesize\centering\tt\begin{tabular}{@{}c@{}}
					link to\\[-0.15em]
					proof
				\end{tabular}}\,}{$\Big]$}}%
				}
			\fi
			\restatable[#1]{#2}{#2:#3}\label{#2:#3}%
			\ifmarginprooflinks\marginpar{\vspace{-1ex}\usebox\marginprooflinkbox}\fi%
		}%
		{\sbox\marginprooflinkbox{}\endrestatable}
	%
	% \newenvironment{linked}[3][]{%
	% 	\def\linkedproof{#3}%
	% 	\def\linkedtype{#2}%
	% 	\sbox\marginprooflinkbox{%
	% 				\centering%
	% 				\hyperref[proof:\linkedproof]{%
	% 				\color{blue!30!white}%
	% 	\restatable[#1]{#2}{#2:#3}\label{#2:#3}}
	% 	{\endrestatable%
	% 	\marginpar{%
	% 	% \vspace{-0.5em}
	% 	% \hspace{2em}
	% 		\raggedright
	% 		\hyperref[proof:\linkedproof]{%
	% 		\color{blue!50!white}
	% 		$\Big[$\,{\small\tt\begin{tabular}{@{}l@{}} proof of \\~\cref*{\linkedtype:\linkedproof}\end{tabular}}\,$\Big]$}
	% 		}%
	% 	}
	\makeatother

	\newcommand{\begthm}[3][]{\begin{#2}[{name=#1},restate=#3,label=#3]}


\usepackage[framemethod=TikZ]{mdframed}

	\surroundwithmdframed[ % lproof
		   topline=false,
		   linewidth=3pt,
		   linecolor=gray!20!white,
		   rightline=false,
		   bottomline=false,
		   leftmargin=0pt,
		   % innerleftmargin=5pt,
		   skipabove=\medskipamount,
		   skipbelow=\medskipamount
		]{lproof}
	\surroundwithmdframed[ % example
		   topline=false,
		   linewidth=2pt,
		   linecolor=gray!20!white,
		   rightline=false,
		   bottomline=false,
		   leftmargin=0pt,
		   % innerleftmargin=5pt,
		   skipabove=\medskipamount,
		   skipbelow=\medskipamount
		]{example}
	\newmdenv[ % wip
			roundcorner=5pt, backgroundcolor=gray!20!white,
			frametitle={$\langle$under construction $\rangle$},
			frametitlerule=false,
			innertopmargin=2pt,
			frametitlebelowskip=3pt,
			frametitleaboveskip=2pt,
			frametitlebackgroundcolor=gray!70!white,
			skipabove=1em,skipbelow=1em,
			frametitlefont={\normalfont\itshape},
			leftmargin=-10pt,
			rightmargin=-10pt]
		{wip}
	\newmdenv[ % storynote
			roundcorner=5pt,
		 	backgroundcolor=violet!20!white,
			frametitle={$\langle$storytelling note$\rangle$},
			frametitlerule=false,
			innertopmargin=2pt,
			frametitlebelowskip=3pt,
			frametitleaboveskip=2pt,
			frametitlebackgroundcolor=violet!50!white,
			skipabove=1em,
			skipbelow=1em,
			frametitlefont={\normalfont\itshape},
			leftmargin=10pt,
			rightmargin=10pt]
		{storynote}

\usepackage{subfiles}

\allowdisplaybreaks
% \setlength\delimitershortfall{-1pt}

\begin{document}
\begin{center}
	% {\bfseries\Large Inference, Dynamics, and Inconsistency in PDGs}
	{\bfseries\Large Inference, Dynamics, and Inconsistency in PDGs}
\end{center}

\section{Introduction}
% In our first paper, we introduced PDGs, their semantics, and some examples.
% PDGs are able to represent an inconsistent collection of local beliefs; we have argued that doing so allows us to represent important states of knowledge, and hinted that it may allow us to make better decisions and
% save computation.

In \cite{}, we introduced Probabilistic Dependency Graphs (PDGs), argued that they are a particularly natural modeling tool, and proved that their semantics subsumes that of other other common models (Bayesian Networks and factor graphs), but are strictly more expressive.
% Syntactically, PDGs are essentially arbitrary weighted collections of conditional probablity distributions (cpds), and they  have semantics in terms of a scoring function on joint distributions over all variables.
A model has limited usefulness without a query mechanism, and in \cite{} we only gesture briefly at how to draw inferences with them.
In the present paper, we give a more complete picture of how to do inference in a PDG, as we explore the rich connection between the inconsistency of a PDG, and standard inference algorithms in other settings.


% Some queries will be easier to answer than others.
% Nevertheless, all PDGs have semantics in terms of probability distributions, and
% in particular, pick out a \emph{best} distribution---%
% % (we actually have more: a
% % (small set of) distributions for every $\gamma \ge 0$, which is guaranteed to
% % be unique small $\gamma$ and converge in the limit as $\gamma\to 0$).
% a feature we have argued allows them to stand in for other graphical models.
% But without also providing algorithms to answer the queries that
% these other graphical models support.
%
%
% To that end, we start with two of the most important kinds:
% 	conditional probability queries of the form $\Pr(Y \smid X)$,
% 	% maximum likelihood queries like $\MAP(Y \mid X)$,
% 	and causal probabilistic queries, such as $\Pr(Y \smid \ado(X))$.



%	 "I would like to ask you, as a first step, to write up carefully the three
%	 results that you claimed in the AAAI conclusion: how we compute M*(X|Y),
%	 how we do updating, and variational autoencoders (which you’ll have to
%	 review, since I’m not familiar with them)."   -- Joe



\subsection*{Outline}
We will begin by defining the inconsistency of a PDG (\cref{sec:inc-definition}), and proving that it has nice properties which make it amenable to gradient descent (\cref{sec:inc-properties}). We then show that many tasks of interest (belief updating and model inference) can be done efficiently with access to an inconsitency oracle (\crefrange{prop:optimalYgivenX}{prop:jeffrey-by-inc}), although computing the inconsistency is hard (\cref{prop:consistent-NP-hard,prop:sharp-p-hard}).
In the remainder of the paper, we lay the foundations for tractable approximations, where we find analogues of belief propagation, variational inference, and particle-based approaches.


% \section{Preliminaries}
\subsection*{Notation and PDGs}


\begin{defn}
	
\end{defn}


Before we get to the central material, we first inroduce some new notation and conventions. We now write $\SD{\dg M}$ to denote
the set of distributions consistent with a PDG $\dg M$.%
	\footnote{In \cite{richardson2020probabilistic}, this set of consistent distributions was denoted $\bbr{\dg M}_{\textrm{sd}}$.}
We view a cpd $p(Y \mid X)$ as a special case of a PDG with
a single edge $X \to Y$ annotated with the cpd $p$, and taking
default weights $\alpha_p = 0$, and $\beta_p = 1$. In cases where we wish
to indicate other weights, we indicate the modifications with a parameter
assignment. For instance, to give a PDG consisting of the cpd $p$, attached to
a value of $\alpha_p$ equal to $\alpha_0$, and
a value of $\beta_p$ equal to $\beta_0$, we might write
$ p^{(\alpha:\alpha_0;\beta : \beta_0)} $, or in abbrevaited form as $p^{(\alpha_0;\beta_0)}$.
% If only one parameter is given, it is \beta.
Often we will want to describe limiting PDGs with infinite quantitative confidence $\beta$; we will use an explanation point to convey this briefly, so that $p!$ is shorthand for
$\displaystyle\lim_{t \to \infty} p^{(\beta:t)}$.
%

We call two PDGs $\dg A$ and $\dg B$ \emph{value-compatable} if they agree
on the values of any variables they have in common---that is, if
$\V^{\dg A}(X) = \V^{\dg B}(X)$ for all $X \in \N^{\dg A} \cap \N^{\dg B}$.
%
If $\dg A$ and $\dg B$ are value-compatible PDGs, we write $\dg A \bundle \dg B$ to indicate the PDG with the union of the variables and the \emph{disjoint} union of all edges, where each edge $L$ retains its associated parameters $\bp$, $\alpha_L$, and $\beta_L$.
% Similarly, we write $\dg A \cap \dg B$ for the PDG with the intersection of the variables

\section{Inference and Dynamics via Inconsistency}
% \subsection*{Inconsistency}
We start by introducing a repackaged version of PDG semantics,
	% \footnote{in fact, it is roughly equivalent as far as
	% expressive power by \cref{prop:sementics-via-inconsistency}}
in terms of its degree of ``inconsistency''.

\begin{defn}
	If $\dg M$ is a PDG, let $\aar{\dg M}_\gamma$ denote the \emph{degree of
	inconsistency} of $\dg M$ at $\gamma$, and be given by
	$$ \aar{\dg M}_\gamma := \inf_{\mu \in \Delta[\V(\dg M)]}
		\bbr{\dg M}_\gamma(\mu)
		% % \sum\alle \beta_L \;\kldiv{\mu_{Y|X}}{\bp} + \alpha_L\;\H_\mu(Y\mid X)
		% ,\qquad\text{which we write as}\qquad
		% \aarz{\dg M} := \lim_{\gamma\to 0^+} \aar{\dg M}_\gamma
		.
	$$
	% in the limiting case for small $\gamma$.
\end{defn}

So now, for each $\gamma \in [0, \infty)$, $\aar{\dg M}_\gamma$ is a real number.
First, we have already given a sensible notion of what it means
for a PDG to be (in)consistent, given by the semantics $\SD{-}$. Fortunately,
this definition of inconsistency is compatible with it.

\begin{prop}
	$\dg M$ is a \emph{consistent} PDG, in that $\SD{\dg M} \ne \emptyset$,
	if and only if $\aarz{\dg M} = 0$.
\end{prop}

% \begin{proof}
% 	(an immediate corolary of \cref{prop:inc-is-inconsistency})
% \end{proof}


In the full paper, we defined
$\Inc(\dg M) := \inf_\mu \Inc_{\dg M}(\mu)$, also a property of
a PDG that is motivated to capture inconsistency.
Sice $\Inc(\dg M)$ is simply $\aar{\dg M}$ without the qualitative term,
it should come at no surprise that $\aar{-}$ reduces to $\Inc(-)$ in the limit
of small $\gamma$.

% \begthm{prop}{prop:inc-is-inconsistency}
\begin{linked}{prop}{inc-is-inconsistency}%\label{prop:inc-is-inconsistency}
	$\displaystyle \lim_{\gamma\to 0}\aar{\dg M}_\gamma = \Inc (\dg M) = \aarz{\dg M}
		% \inf_{\mu % \in \Delta[\V(\dg M)]
		% } \Inc{\dg M}(\mu)
	$.
\end{linked}
% \end{prop}
% We might therefore say that $\Inc(\dg M)$ is in some sense the
% ``quantitative end'' of $\aar{\dg M}$.

What about for fixed values of $\gamma$, or the limit as $\gamma$ becomes large?
These are also worth considering, and correspond to different weightings of the
qualitative information (the set of edges and parameters $\alpha$, indicating the qualitative dependency structure), against the quantitative information (the
cpds $\bp{}$ and the confidences $\beta$ in them).
We will write $\aar{\dg M}$ without a subscript to indicate the function
$\gamma \mapsto \aar{\dg M}_\gamma$, which we will use when a result does not
depend on $\gamma$.

Although we defined $\aar{-}$ in terms of $\bbr{-}$,
we note that it is also possible to do the reverse,
defining $\bbr{-}$ in terms of $\aar{-}$.
This is done by adding $\mu$ to $\dg M$ with large $\beta$.
Intuitively, as our confidence that $\mu$ is the right
joint distribution becomes large, the best distribution
gets closer to $\mu$.

\begin{prop} \label{prop:sementics-via-inconsistency}
	$\displaystyle
		\bbr{\dg M}_\gamma(\mu)
			=  \aar*{\dg M \bundle \mu!}_\gamma
			\qquad\Big(~= \lim_{t \to \infty} \aar[\Big]{\dg M \bundle \overset{(\beta:t)}\mu}_{\!\!\gamma}
				~\Big)
	% \qquad\text{and}\qquad
	% \bbr{\dg M}(\\)
	$
\end{prop}

As a result, $\aar{-}$  may be taken as primitive, and so PDG
semantics can be couched in terms of inconcistency. For instance, the
PDG $\PDGof{\mathcal B}$ for a Bayesian Network $\mathcal B$ is always
consistent ($\Pr_{\mathcal B} \in \SD{\PDGof{\mathcal B}}$, and $\aar{\PDGof{\mathcal B}} = 0$)
by construction, because $\mathcal B$ represents a distribution
$\Pr_{\mathcal B}$ that has the appropriate conditional marginals and
(in)dependence structure, and more explicitly, one might \emph{define}
the semantics of a PDG to be the distribution $\mu$, such that, if combined
with the data of $\mathcal B$, results in a maximally consistent PDG.


\subsection{Inference by Gradiant Descent}

We now return to the task at hand.
The most important and standard query is a conditional probability
query: given a PDG $\dg M$, how do you compute the probability of $Y$ given $X$?
We use a similar approach as we did in giving PDGs semantics in the first place
---rather than giving probabilistic information directly, we instead give a
measure the quality of a candidate answer $p(Y\smid X)$.
Intuitively, a cpd $p(Y\smid X)$ makes for a good answer to the query
if it is consistent with the other cpds in $\dg M$, and so we propose
$\aar{\dg M \bundle p}$ as a measure of (dis)quality of the inference $p$.
This is simply a definition, but we now verify that it has nice properties,
which we might expect from such a measure of inference quality.

Perhaps most importantly, the best cpd(s) according to this measure
are the conditional marginals $\mu(Y\mid X)$ of the best distributions $\mu$ for $\dg M$.

\begin{linked}{prop}{optimalYgivenX}
	% \label{prop:optimalYgivenX}
	For all $\dg M$, $X,Y\in\N^{\dg M}$, and $\gamma > 0$, we have that
	$\displaystyle
		\argmin_{p : X \to \Delta Y} \aar{\dg M \bundle p}_\gamma =
		\Big\{ \mu(Y \smid X) :  \mu \in \bbr{\dg M}_\gamma^* \Big\}
	$.
\end{linked}
In the limit, of small $\gamma$, since there is only one such distribution,
the expression beomes simpler.
\begin{linked}{coro}{smallgammaopt}
	$\displaystyle
		\bbr{\dg M}^*(Y \smid X)
	$ uniquely minimizes $p(Y\mid X) \mapsto \aarz{\dg M \bundle p}$.
\end{linked}


So $p \mapsto \aar{\dg M \bundle p}$ gives the best scores to the marginals of
distributions that the scoring semantics of $\dg M$ view as best. Even supposing
we had oracle access to $\aar{-}$,
the prospect of having to enumerate all possible conditional probability
distributions $p$ to find the best one sound prohibitively expensive.
Fortunately, it is not necessary, as the function $p \mapsto \aar{\dg M \bundle
p}$ has properties which make for efficient search,
given oracle access to $\aar{\dg M}$.
Most importantly, it is smooth and
strictly convex, ensuring that gradient descent converges quickly.

\begin{linked}{prop}{smooth-and-strictly-cvx}
	The function $p \mapsto \aar{\dg M \bundle p}_\gamma$ is smooth and
		strictly convex for small enough $\gamma$%
	% (concretely: all $\gamma$ less than $\min (\{1\}\cup\{ \beta^{\dg M}_L : L \in \Ed^{\dg M}\})$
	.
\end{linked}
\begin{conj}
	For cpds $p \in \Delta(Y\mid X)$, the function
	$\aar{\dg M \bundle p}_\gamma$ is Lipshitz in $p$, on any compact region in the interior of $\Delta(Y \mid X)$
\end{conj}

The lower bound on the convexity of the inconsistency, however, is controlled by $\gamma$.
% So this process will converge more quickly for large $\gamma$.

\begin{conj}
	$\aar{\dg M}_\gamma$ is continuous as a function of $\gamma$, and converges as $\gamma\to 0$.
\end{conj}

As a result, progress towards an optimum of an objective for $\gamma = \gamma_0$ will still be useful for the optimization problem at $(1-\epsilon)\gamma_0$, for small enough $\epsilon$.
This suggests using a variant of gradient descent in which
in which $\gamma$ decays to zero during the optimization process,


% {\centering\Large Further Claims \par}


\commentout{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{defn}[degree of inconsistency]
		If $\dg M$ is a PDG, let $\aar{\dg M}_\gamma$ denote the \emph{degree of
		inconsistency} of $\dg M$ at $\gamma$, and be given by
		$$ \aar{\dg M}_\gamma := \inf_{\mu \in \Delta[\V(\dg M)]}
			% \bbr{\dg M}_\gamma(\mu)
			\sum\alle \beta_L \;\kldiv{\mu_{Y|X}}{\bp} + \alpha_L\;\H_\mu(Y\mid X)
		$$
		and let $\aar[\Big]{\dg M} = \lim_{\gamma\to 0^+} \aar{\dg M}_\gamma$.
	\end{defn}

	\begin{claim}
		$$\bbr{\dg M}_\gamma(\mu) = $$
	\end{claim}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% There is an intuitive story to be told here: simply add a guess to the PDG.
% Sort it out if you have time.
% Do not sort it out otherwise.


\subsection{Updating via Inconsistency}
	\label{sec:inference-by-gd}

%%%%%%%%%%%%%%%% VESTIBULE %%%%%%%%%%%%%x
\commentout{
	If, rather than fixing $\dg M$ and optimizing $p$, we fix $p$ and optimize $\dg M$,
	this same proces corresponds to an update, rather than an inference. For instance, in
	the case where $X = \pdgunit$, an observation $Y\!=\!y$  can be added to
	$\dg M$ in the form of an edge $\ed{\delta_y}{\pdgunit}Y$, getting the
	(possibly inconsistent) PDG $\dg M'$. The distribution $\bbr{M'}^*$ turns out
	to be the result of conditioning $\bbr{\dg M}^*$ on $Y\!=\!y$.
}
If, rather than fixing $\dg M$ and optimizing $p$, we fix $p$ and optimize $\dg M$,
this same proces corresponds to updating rather than inference. For instance, in
the case where $X = \pdgunit$, an observation $Y\!=\!y$  can be added to
$\dg M$ in the form of an edge $\ed{\delta_y}{\pdgunit}Y$, getting the
(possibly inconsistent) PDG $\dg M'$. One might hope that the distribution
$\bbr{\dg M'}^*$ is the result of conditioning the best distribution $\bbr{\dg M}^*$ on $Y\!=\!y$, as is true for factor graphs, but this is always the case.

\begin{example}
	Consider a PDG $\dg M$ containing $p(X)$ with confidence $r$, and $q(Y|X)$ with confidence $s$. 
	Now condition on $Y=y$ by adding that event to the PDG, to get 
	\[
		\dg M := 
		\begin{tikzpicture}[center base]
			\node[dpad0] (X) at (0,0) {$X$};
			\node[dpad0] (Y) at (1.5,0) {$Y$};
			%
			\draw[arr2,->] (X) to
				node[above, pos=0.4,inner sep=2pt]{$q$}
				node[below, pos=0.4, inner sep=2pt]{${\color{gray}\scriptstyle(s)}$}
				(Y);
			\draw[arr2, <-] (X) to
				node[above, pos=0.6,inner sep=2pt]{$p$}
				node[below, pos=0.6, inner sep=2pt]
					{${\color{gray}\scriptstyle(r)}$}
				+(-1.1, 0);
			% \draw[arr2, <<-] (Y) to
			% 	node[above, inner sep=2pt, pos=0.7]
			% 		{$y$}
			% 	+(0.9,0);
		\end{tikzpicture}
	\qquad\text{and}\qquad
		\dg M' := 
		\begin{tikzpicture}[center base]
			\node[dpad0] (X) at (0,0) {$X$};
			\node[dpad0] (Y) at (1.5,0) {$Y$};
            %
			\draw[arr2,->] (X) to
				node[above, pos=0.4,inner sep=2pt]{$q$}
				node[below, pos=0.4, inner sep=2pt]{${\color{gray}\scriptstyle(s)}$}
				(Y);
			\draw[arr2, <-] (X) to
				node[above, pos=0.6,inner sep=2pt]{$p$}
				node[below, pos=0.6, inner sep=2pt]
					{${\color{gray}\scriptstyle(r)}$}
				+(-1.1, 0);
			\draw[arr2, <<-] (Y) to
				node[above, inner sep=2pt, pos=0.7]
					{$y$}
				+(0.9,0);
		\end{tikzpicture}.
	\]
	We can easily see that $\bbr{\dg M}^* = p(X)q(Y|X)$ doesn't depend on the confidences, since we can simultaneously satisfy $p$ and $q$. 
	After conditioning on $Y=y$, we get a distribution $\bbr{\dg M}^* | Y\!\!=\!y$ proportional to $p(X) q(y|X)$. 
	But some quick calculation reveals that $\bbr{\dg M'}^*$ is proportional to
	$p(X) q(y|X)^{r/s}$, with an additional exponent $r/s$ that was not present before.  
	
	So, if $r = s$, so that the data of $\dg M$ effectively picks out a probability distribution with uniform confidence, we get probabilistic conditioning, and otherwise there is distortion, as the lower-confidence is bent further, absorbing more of the inconsistency. 
\end{example}

So, even though adding the event $Y \!\!=\!y$ to the PDG is much like probabilistic conditioning, in that afterwards we only consider distributions $\mu$ for which $\mu(Y\!\!=\!y) = 1$, it is not always equivalent to probabilistic conditioning.
%
 % set of finite-scoring distributions $\{ \mu : \bbr{\dg M^{+y}}(\mu) < \infty \}$ such that  
%
We now argue that the answer given by the PD is more reasonable.
 which is lost when one throws away the data of the PDG and works with probability distributions directly, 
Suppose we have a good reasonto trust $q(Y|X)$ much more than $p(X)$. Perhaps $q(Y|X)$ describes a probabilistic program that we wrote ourselves and simply adds a small amount of noise, while $p(X)$ describes the distribution described by a low-quality simulation. Then when we observe $Y\!\!=\!y$, we should not 



What if we only wanted to know $\Pr(Y \smid X=x)$, for a specific value of $X$, rather than the full cpd $\Pr(Y\smid X)$?
% (including the data for every $X$), in the context of a PDG $\dg M$
To answer this question, recall that we have identified the event $X=x$ with the degenerate distribution $\delta_x(X)$, so it can be included in a PDG as an edge
% \begin{center}
\begin{tikzpicture}[scale=1.3, center base]
	\node[dpad0] (X) at (1, 0) {$X$};
	\draw[arr2,<<-] (X) -- node[fill=white,inner sep=1pt,pos=0.6]{$x$} +(-1,0)
	 	% node[left,dpad0]{$\pdgunit$}
		;
\end{tikzpicture}.
% \end{center}
% where $\delta_x$ is the degenerate distribution on $X$ that places all mass on the value $x$.
% \[ \begin{tikzpicture}[scale=1.3]
% 	\node[dpad1] (O) at (0,0) {$\Omega$};
% 	\node[dpad1] (X) at (1, 0) {$X$};
% 	\draw[arr2,->>] (O) -- (X);
% 	\draw[arr2,<<-] (X) -- node[above]{$\delta_x$} +(1,0);
% \end{tikzpicture}\]

If $\dg M$ is the data of a joint distribution, then this is simply


\subsection{Complexity of Computing Inconsistency}
	\label{sec:complexity}

Taken together, the results in \Cref{sec:inference-by-gd} show that an oracle for the degree of inconsistency is sufficient to efficiently do learn and draw inferences.
Unfortunately this turns out to be a lot to ask for, in general.

\begin{linked}{prop}{consistent-NP-hard}
	Deciding if $\dg M$ is consistent is NP-hard.
\end{linked}


\begin{linked}{prop}{sharp-p-hard}
	Computing $\aar{\dg M}_\gamma$ is \#P-hard, for $\gamma > 0$.
\end{linked}


This is just a lower bound on the complexity of estimating $\aar{\dg M}$.

What shall we make of these results? First, the asymptotic compelxity of exact inference
is not great, but no worse than for Bayesian Networks or factor graphs.


There is still hope for approximating it, given that it has such nice properties --- is convex (albeit in an exponentially large space), smooth, and monotonic (additional edges only increase $\aar{\dg M}$).

\TODO


\section{Bounded Tree-Width}

% \begin{theorem}
\begin{conj}
	% The class of PDGs 
	There is an algorithm for inference on PDGs that runs in polynomial time, for the class of PDGs with bounded tree-width. 
\end{conj}
% \end{theorem}

\begin{linked}[Marov Property for PDGs]{prop}{markov-property}
	% Suppose $\dg M_1$ and $\dg M_2$ are compatible PDGs, and let $\mathbf X$ denote the variables they have in common.
	% Then for all $\gamma > 0$, we have that
	% \[
	%  	\bbr{\dg M_1 \bundle \dg M_2}^*_\gamma
	% 		% \subset
	% 		~\models~
	% 	% \mathrm I( \N_1 ; \N_2 \mid \mathbf X)
	% 	\N_1 \mathbin{\bot\!\!\!\bot} \N_2 \mid \mat X
	% \]
	% That is: in every optimizing distribution, for any value of $\gamma$, the variables of $\dg M_1$ and the variables of $\dg M_2$ are conditionally independent given their shared variables $\mat X$.
	Suppose $\dg M_1$ and $\dg M_2$ are value-compatible PDGs,
	with respective sets of nodes $\mat X_1 := \N^{\dg M_1}$ and
	$\mat X_2 := \N^{\dg M_2}$.
	 % and let $\mathbf X$ denote the variables they have in common.
	Then for all $\gamma > 0$, we have that
	\[
	 	\bbr{\dg M_1 \bundle \dg M_2}^*_\gamma
			% \subset
			~\models~
		% \mathrm I( \N_1 ; \N_2 \mid \mathbf X)
		% \N_1 \mathbin{\bot\!\!\!\bot} \N_2 \mid \mat X
		% \N^{\dg M_1} \mathbin{\bot\!\!\!\bot} \N^{\dg M_2} \mid \mat X
		\mat X_1 \mathbin{\bot\!\!\!\bot} \mat X_2 \mid \mat X_1 \cap \mat X_2
	\]
	That is, in every optimal distribution $\mu^* \in \bbr{\dg M_1 \bundle \dg M_2}^*_\gamma$ for some $\gamma>0$, the variables of $\dg M_1$ and the variables of $\dg M_2$ are conditionally independent given the variables they have in common.
\end{linked}


In the bounded tree-width setting, to specify a distribution with the Markov Property over the tree-decomposition, it suffices to specify a distribution over each component, subject to the constraint that the distributions have the same marginals. 
%
Concretely, a tree decomposition of a PDG $\dg M$ is a set of subsets of variables $\mathcal C \in 2^{2^{\N}}$ called components, such that:

\begin{enumerate}[nosep]
\item Every variable is a member of at least one component.
\item Each edge lies entirely within some component. So for every $\ed LXY \in \Ed^{\dg M}$, there is some $C_L \in \mathcal C$ such that $X,Y \in C$.
\item The (undirected) graph $\Gr(\mathcal C) = (\mathcal C, \Ed(\mathcal C))$, whose nodes are the components $\mathcal C$, and which has an edge between $C_1$ and $C_2$ (i.e., $\{C_1, C_2\} \in \Ed$) iff $C_1 \cap C_2 \ne \emptyset$, is a tree.
\end{enumerate}
Now, because of the Markov property (\Cref{prop:markov-property}), any optimal distribution $\mu^* \in \bigcup_{\gamma>0}\bbr{\dg M}^*_\gamma$ for $\dg M$, must factor over the tree structure, meaning that
 % it can be written in `junction tree form'.
it can be specified by a collection of marginal probability distributions $\boldsymbol\nu = \{\nu_{C}(C) : C \in \mathcal C\}$ that is locally consistent (i.e., overlapping components agree on their shared variables: $C_1 \cap C_2 \ne \emptyset \implies \nu_{C_1}(C_1\cap C_2) = \nu_{C_2}(C_1 \cap C_2)$). 
Given such a set of marginals $\boldsymbol\nu$, there is precisely one joint distribution that both matches those marginals and also has the independencies required by the Markov property; that distribution is given by 
\[
 	\Pr_{\boldsymbol\nu}(\cup\, \mathcal C) =
	 	\dfrac{\displaystyle
			% \prod_{C \in \mathcal C} \mu_C(\mathbf X_C)
			\prod_{C \in \mathcal C} \nu_C(C)
		}{\displaystyle
			% \prod_{\{C_1, C_2\} \in \Ed(\mathcal C)} \mu_C(\mathbf X_{C_1} \cap \mathbf X_{C_2})
			\prod_{\mathclap{\{C_1, C_2\} \in \Ed(\mathcal C)}}
			 	\nu_{C_1}(C_1 \cap C_2)
		}.
\]

Furtheremore, the entropy $\H(\Pr_{\boldsymbol\nu})$ of such distributions (which include $\mu^*$), can be analogously computed in terms of the marginals $\boldsymbol\nu$, via the Bethe entropy
\[
	\H(\Pr_{\boldsymbol \nu}) = \sum_{C \in \mathcal C} \H(\nu_C) 
		- \sum_{\mathclap{\{C_1, C_2\} \in \Ed(\mathcal C)}} \H(\nu_{C_1}(C_1 \cap C_2)).
\]

For a fixed tree decomposition $\cal C$, note that searching over distributions $\boldsymbol \nu$ is a constrained optimization problem, over a strictly (and $\gamma$-strongly) convex function, for small enough $\gamma$. 


\TODO[WORRY: smooth, compact, and $\gamma$-strongly convex might not be enough: How can we get complexity bounds for this minimization? All bounds I see require Lipshitz assumptions, which don't hold for relative entropy.]

\subsection{Self Concordance.}
\begin{defn}
	A function $f : K \subset \mathbb R^n \to \mathbb R$ is $\kappa$-\emph{self-concordant} if 
	\[
		\nabla^3 f(x) [h,h,h] \le \kappa (\nabla^2 f(x) [h,h])^{ 3/2}
	\]
\end{defn}

\begin{algorithm}
	\begin{algorithmic}
		\Require An initial distribution $\mu_0$ an
		\Statex

		% \Procedure{Newton}{$f, \mu_0, \epsilon$}
		\Repeat
			\State Let $\delta\! x := - (\nabla^2 f(x))^{-1} \nabla f(x)$
			\State Set $t := 1$;
			% \While{$f(x + t\, \delta\! x) > f(x) + $}
		\Until{ $\nabla f(x) \cdot \delta x \le 2 \epsilon$}
		\State
		\textbf{return} $\dg B$.
		% \EndProcedure
		% \While{not converged}
		% 	\State Choose a sub-PDG $\dg L \subset \dg M \bundle \dg B$
		% 	\For{$\in \dg L$}
		% 		\State Update $\dg L \cap \dg B \gets \dg L \cap \dg B - \grad_{\dg L \cap \dg B} \aar*{\dg L}_\gamma$
		% 	\EndFor
		% \EndWhile
	\end{algorithmic}
	\caption{Newton's Algorithm}
	\label{algo:newton}
\end{algorithm}
\begin{theorem}[Nesterov and Nemirovski]
	%https://class.ece.uw.edu/546/2014spr/lectures/unconstrained.pdf
	%https://web.stanford.edu/class/ee364b/lectures/newton_self-concordance.pdf
	% slide 23/25.
	Newton's method, when started at a point $\mu_0$, converges to an $\epsilon$-approximate point in 
	$O\Big( (\Inc_{\dg M}(\mu_0) - \aar{\dg M}) + \log \log \frac1\epsilon\Big)$
	steps.
\end{theorem}


\TODO[\normalsize QUESTION: What about when $\gamma$ is not small? Now the problem could be non-convex. Can we still say anything about it then?]

% \TODO[This doesn't seem "fixed-parameter tractable", given that the number of parameters grows with the side of the tree\ldots ]


\subsection{The Quantitative Limit (small \texorpdfstring{$\gamma$}{\textbackslash gamma})}

Our approach relies on the following key fact. 

\begin{prop}\label{prop:marginonly}
	For any PDG $\dg M$, 
	the highest-compatibility distributions (the minimizers $\bbr{\dg M}_0^*$ of $\Inc_{\dg M}$) all have the same conditional probabilities along the edges of $\dg M$.   
	That is to say, if there is an edge $\ed LXY \in \Ed^{\dg M}$, and $\mu_1, \mu_2 \in \bbr{\dg M}_0^*$ are quantitatively optimal distributions, then $\mu_1(Y|X) = \mu_2(Y|X)$.  
\end{prop}
\begin{proof}
	\TODO[TODO: add proof via zeros of the Hessian]
\end{proof}


Of course, the converse is true as well---$\Inc_{\dg M}(\mu)$ depends only on the conditional marginals $p(Y|X)$ along edges $\ed LXY \in \Ed^{\dg M}$. 
As a result, the set of distributions $\bbr{\dg M}_0^*$ is characterized precisely by the set of linear constraints enforcing that the conditional marginals are the appropriate optimal cpds.
And to find these optimal cpds, it suffices to find any qualitatively optimal distribution $\mu_0 \in \bbr{\dg M}_0^*$, say by gradient descent as above.

Once we have a solution $\hat{\boldsymbol\nu}$ whose corresponding distribution $\Pr_{\hat{\boldsymbol\nu}}$ minimizes $\Inc_{\dg M}$, we can perform a second, different minimization with further linear constraints: for each edge $\ed LXY \in \Ed^{\dg M}$, we add $|\V(XY)|$ constraints ensuring that
\[
	\nu_{C_L}(Y|X) = \nu^0_{C_L}(Y|X) 
\qquad\iff\qquad
	\sum_{\mathclap{z \in \V(C_L\setminus\{X,Y\})}} \nu_{C_L}(X,Y,z) = \hat \nu_{C_L}(Y|X) \bigg(\sum_{\substack{~\\\mathrlap{z,y \in \V(C_L\setminus\{X\})}}} \nu_{C_L}(X,y,z) \bigg),
\]
for all possible values of $X$ and $Y$. By \cref{prop:marginonly}, the distributions that correspond to feasible points are precisely the minimizers of $\Inc_{\dg M}$. We now perform a secondary optimization problem, with these additional constraints, to minimize $\IDef{\dg M}$, which can also be computed precisely in terms of the marginals $\boldsymbol\mu$.

\textbf{Quick account of complexity}.
If every variable can take at most $V$ different values, there are at most $E$ edges,  the maximum number of variables in any component is $K$ (i.e, the bound on the tree width), and the number of components is $M$, then we now have a constrained optimization problem with
\begin{itemize}[nosep]
	\item at most $M V^{2K}$ paramters, and
	\item at most $(M-1) V^{2K} + E V^2$ constraints:
	\begin{itemize}[nosep, label=\textbullet]
		\item at most $(M-1) V^{2 K}$ marginal constraints to enforce the local polytope\\
		 	(one for each of the $M-1$ junctions),
		\item and precisely $E V^2$ additional constraints in the second stage to ensure an optimizer of $\Inc_{\dg M}$ when searching over $\IDef{\dg M}$.
	\end{itemize}
\end{itemize}
% \TODO[Still don't know how to get a bound for complexity of minimization]
\TODO[Unlike $\Inc_{\dg M}$, we should be able to get a Lipshitz constant for $\IDef{\dg M}$, to get a bound on complexity of minimization, but this would require more work.]


\section{Variational Inference for PDGs}
	\label{sec:inference-from-variation}

Variational inference is a standard approximate inference technique for probabilistic models $p$, in which we use an (arbitrary) auxilary distribution $q$ to directly estimate a feature of $p$ which might be quite difficult to optimize directly, and in the process construct a worst-case bound on the quality of $p$.
Inference can then by performed simultaneously optimizing $p$ to do better with respect  to this worst-case bound generated by $q$, and also optimizing e$q$ so that it more closely matches $p$.
This approach has historically been very fruitful, enabling axpproximations useful approximatate models of physical systems \cite{}, and approximate inference in latent variable models \cite{weight_uncertainty_nns}, such as variational autoencoders \cite{}.
% Mean-field theory and the EM algorithm are two particularly well-known instances that enjoy broad adoption outside of deep learning.

There is a deep bidirectional connection between PDG semantics and variational techniques.
In parallel work \cite{PDG:universal_loss}, we argue that PDGs offer a concise visual account of variational inference more generally, that their semantics automate the algebra behind the variational bounds, and we can embue the approach with more intuition by doing so.
In the present document, we focus on a simpler aspect of this connection: how this variational approach can be applied to draw inferences in PDGs.

In our context, the approach boils down to a very simple observation: the incompatibility $\bbr{\dg M}_\gamma(\mu)$ of a PDG $\dg M$ with a \emph{specific} distribution $\mu$ is at least as large as the smallest possible incompatibility of $\dg M$ with \emph{any} distribution, which is the inconsistency $\aar{\dg M}_\gamma$.

So if we choose a nice class of test distributions $\mathcal M \subset \Delta(\dg M)$, the variational approch suggests the following procedure:
\begin{center}
	\begin{enumerate}[nosep]
		\item optimize the cpds of $\dg M$ so to reduce the incompatibility with $\mu$;
		\item optimize $\mu$ to reduce its incompatibility with $\dg M$;
		\item repeat.
	\end{enumerate}
\end{center}

Moreover, since $\bbr{\dg M}_\gamma(\mu)$ is strictly convex in $\dg M$ and $\gamma$-strongly convex in $\mu$, this procedure gives a family of inference algorithms for PDGs in line with standard algorithms used to train neural networks.

% To give a taste of the full power of the connection,
% Because of the
From \Cref{prop:sementics-via-inconsistency}, it follows that the procedure outline above can be seen as minimizing the inconsistency of a single PDG $\aar{\dg M \bundle \mu! }$, where $\mu$ has very high confidence.
Thus, this approach to inference may be summarized as follows.

\begin{quotation} \it
	\noindent Adding an edge to a PDG makes it no less inconsistent, but may make the degree of inconsistency easier to calculate --- which in turn makes more efficient to minimize, ultimately allowing us to to draw inferences quickly.
\end{quotation}


\subsection{A Nice Class of Distributions}

Recall the Markov Property:
\recall{prop:markov-property}

Moreover, we have a stronger result. Given a dependency graph $\Gr = (\N, \Ed, \V)$, let $\mathcal P_{{\dg M}[\Gr]}$ denote the set of distributions obtainable as optimizers $\mu^* \in \arg\min \bbr{\dg M}_\gamma^*$, for some PDG $\dg M = (\Gr, \bp[], \boldsymbol\alpha, \boldsymbol\beta)$ based on $\Gr$, by supplying some cpds $\bp[\,]$, weights $\boldsymbol\alpha$ and $\boldsymbol\beta$, and $\gamma > 0$. 
Analogously, let $\mathcal P_{\Phi[\Gr]}$ be the set of distributions represented by some factor graph whose factors touch the same variables as $\Gr$. 

\begin{prop}
 	$\mathcal P_{\dg M[\Gr]} = \mathcal P_{\Phi[\Gr]}$. 
	% let $\Delta^*{\dg Q} := \cup_{\gamma > 0, \V, \bp[]}$
	% \[
	% 	\Delta^*{\dg Q} = \mathcal P_{\Phi}
	% \]
\end{prop}
\begin{proof}
	\TODO[TODO: fill in Chris's proof from email]
\end{proof}

Thus, to find for optimal distributions, it suffices to search over the space of factor graphs. 






\subsection{Experiments}
	\label{sec:experiment-variational}

The major drawback of variational inference is that its effectiveness is fundamentally tied to the set of test distributions $\mathcal M$ that we consider.
A poor choice of $\dg M$ makes it impossible not to be extremely inconsistent, resulting in unavoidably poor-quality inferencees.
For this reason, a variational approach requires emperical validation. In the rest of this section, we preseent some toy simulations that suggest that indeed the approach can be useful for a broad class of PDGs that are otherwise difficult to analyze.

\TODO

\section{The Local Inconsistency Reduction (LIR) Algorithm}
% \subfile{belief-prop}

In \cref{sec:complexity}, we saw that the cost of computing the inconsistency of a PDG scales poorly with the size of the PDG.
% becomes expensive as PDG becomes large. as we add more data to the PDG
But do we really need to know the inconsistency of the \emph{whole} PDG, to effectively estimate the marginal on a variable on the fringe of the graph? For particularly simple (tree-structured) topologies, the answer is no. More generally, might computing the inconsistency of a small neighborhood be enough to do inference?

More concretely, starting with a base pdg $\dg M$, we perform an iterative updating procedure, where at each step $t$, we look at some local context $\dg C \subset \dg M$, and tweak some subset of the cpds $\dg A \subset \dg C$ that hold our attention, to reduce the (local) inconsistency of $\dg C$. This is done with a gradient descent step, with step size $\eta$.

% Let $\dg C$ represent the gradient step
 % $A$ is a subgraph of $G$ (written $A \leqslant C$)

 This is the idea behind Local Inconsistency Reduction (\cref{algo:LIR}).

\begin{algorithm}
	\begin{algorithmic}
		\Require A pdg $\dg M$
		\Require A pdg $\dg B$ of inferred beliefs
		\Require A sequence $(A_1 \subset C_1), (A_2 \subset C_2), \ldots $ of subgraphs of $\Gr({\dg M \bundle \dg B})$, corresponding to
		\Require A tradoff parameter $\gamma \ge 0$
		% \Require A step size $\eta > 0$

		\Statex

		\Procedure{LIR}{$\dg M, \dg B, \mathcal R, \gamma$}
		\For{$t = 1, 2, \ldots$}
			\State Let $\dg L^{(t)} := (\dg M \bundle \dg B) |_{\Gr_t}$ be the restriction of $\dg M \bundle \dg B$ to the subgraph $R_t$
			% \While{ not converged }
				% \State Update $\dg L \cap \dg B \gets \dg L \cap \dg B ~ - ~\eta\grad_{\dg L \cap \dg B} \aar*{\dg L}_\gamma$
			\ForAll{edges $L \in \Ed^{\dg B |_{R_t}}$}
				% \State Update $\bp[L] \gets \bp[L] ~ - ~\eta\grad_{\bp[L]} \aar*{\dg L}_\gamma$

			 	\State Update $\bp[L] \gets \bp[L] ~ - ~\eta\grad_{\bp[L]} \aar*{\dg L}_\gamma$
			\EndFor
			% \EndWhile
		\EndFor
		\State \textbf{return} $\dg B$.
		\EndProcedure
		% \While{not converged}
		% 	\State Choose a sub-PDG $\dg L \subset \dg M \bundle \dg B$
		% 	\For{$\in \dg L$}
		% 		\State Update $\dg L \cap \dg B \gets \dg L \cap \dg B - \grad_{\dg L \cap \dg B} \aar*{\dg L}_\gamma$
		% 	\EndFor
		% \EndWhile
	\end{algorithmic}
	\caption{Local Inconsistency Reduction (LIR) algorithm}
	\label{algo:LIR}
\end{algorithm}

The specification of the local contexts and attention




\begin{wip}
\begin{defn}[belief propagation]
    The messages passed are given by:
    \begin{align}
        n_{i \sto a}(x_i) &:= \prod_{c \in \partial a\setminus i} m_{c\sto i} (x_i)
            \qquad &
        m_{a \sto i}(x) &:= \sum_{\mat x_{a} \setminus x_i} f_a(\mat x_a) \prod_{j \in \partial a \setminus i} n_{j \sto a} (x_j)
    \end{align}

    \begin{quotation}\it
        the message sent from a vertex $v$ on an edge $e$ is the product of the local function at $v$, with all messages recieved at $v$ (other than from $e$),
        summarized for $e$'s variable node. \cite{kschischang2001sumproduct}
    \end{quotation}

    The variable beliefs are defined as a function of the messages
    \[
		b_X(x) \propto \prod_f m_{f\sto x}(x),
    \]
	so that
\end{defn}
\end{wip}


In previous work \cite{pdgs}, we showed that $\gamma=1$ essentially regards an unweighted PDG $\dg M$ (i.e, where $\alpha = \beta$) as a factor graph.
We now show that for $\gamma = 1$, the LIR algorithm performs belief propagation.
To that end, we now review the presentation of belief propagation given by \cite{kollerfriedman}.

\begin{defn}[cluster graph]
	A cluster graph $\mathcal U = (V,E,\mat C, \mat S)$ for a colelction of factors $\Phi = \{ \phi_J : \mat X_J \to \mathbb R \}_{J \in \cal J}$ over variables $\mat X$ is
	\begin{itemize}[nosep]
		\item an undirected graph $(V,E)$, where the vertex set $V$ is a partition of $\cal J$, so that there is a unique vertex $V(J) \in V$ for each $J \in \cal J$;
		\item a subset $\mat C_v \subset \mat X$ of variables (a ``cluster'') for each vertex $v \in V$,
				such that $\mat X_J \subset \mat C_{V(J)}$~;
		\item a subset $\mat S_{(u,v)} \subset \mat C_u \cap \mat C_v$ of variables (a ``sepset'') for each edge $(u,v) \in E$.
	\end{itemize}
\end{defn}

Two extremal cluster graphs for $\Phi = \{\phi\}_{\cal J}$ include:
\begin{description}
	\item[{~$\mathcal U_0 :=$}] the one-node (no-edge) cluster graph  whose lone cluster contains all factors;
	\item[{~$\mathcal U_{\mathrm{Bethe}} :=$~}] the cluster graph where each factor has its own vertex (i.e., with $V = \cal J$ and $\mat C_J = \mat X_J$) containing every edge between any two vertices sharing a variable, so that with $S_{(J,K)} = \mat X_J \cap \mat X_K$.
\end{description}


\begin{defn}
	% The \emph{Bethe PDG} $\dg B_{\text{Bethe}}$ is a
\end{defn}

\clearpage
\begin{prop}
	If $\Phi$ is a set of factors, $\mathcal U$ is a cluster graph for $\Phi$, and $E_1, E_2, \ldots$ is a sequence of edges of $\mathcal U$, then LIR with parameters:
	\begin{equation*}
		\dg M = \PDGof{\Phi},
		\qquad \dg B = \Big\{ \overset{(\beta :  )}{\bp[X]} \propto 1 ~\Big|~ X \in \N^{\dg M} \Big\} \bundle
			\Big\{ \bp[E] \propto \prod_{\phi : v_\phi = v} \phi  ~\Big|~ v \in V^{\mathcal U} \Big\},
		\qquad \dg L_i = E_i,
		\qquad \gamma = 1
	\end{equation*}
	is equivalent to belief propagation, in the sense that the cpd $\bp[E]$ of $\dg B$ at timestep $t$ is equal to the cluster belief $b^{(t)}_{E}$ in cluster-graph belief propagation, for every timestep $t$.
	% $ = \beta^i_E$ for all $i = 1,2,\ldots$ and $E \in \mathcal U$, where $\tilde\beta_E^{(i)}$ is the belief on cluster $E$ at the $i^\text{th}$ step.
\end{prop}
\begin{proof}

\end{proof}


\begin{prop}
	If $\dg N$ is an unweighted PDG, then
	$
		\textsc{LIR}^*(\dg N, \mathcal U_{\mathrm{Bethe}})
			 = \textsc{BP}( \FGof{\dg N}, \mathcal U_{\mathrm{Bethe}} )
	$
\end{prop}



\begin{wip}
\begin{prop}
	If $\Phi$ is a factor graph (as a bipartite graph), and $(u_1, v_1), (u_2, v_2), \ldots$ is a sequence of oriented edges of $\Phi$, then
	\begin{align*}
		\dg M &= \PDGof{\Phi},
		\qquad \dg B = \Big\{ \bp[X] \propto 1 ~\Big|~ X \in \N^{\dg M} \Big\} \bundle
			\Big\{ \bp[\phi] \propto \phi ~\Big|~ \phi \in \Phi \Big\},
		\qquad \dg L_i = ,
		\qquad \gamma = 1
	\end{align*}
	is equivalent to belief propagation, in the sense that the cpd $\bp[E]$ of $\dg B$ at timestep $t$ is equal to the cluster belief $b^{(t)}_{E}$ in cluster-graph belief propagation, for every timestep $t$.
	% $ = \beta^i_E$ for all $i = 1,2,\ldots$ and $E \in \mathcal U$, where $\tilde\beta_E^{(i)}$ is the belief on cluster $E$ at the $i^\text{th}$ step.
\end{prop}
\end{wip}


\begin{prop}

\end{prop}


\textbf{Dampening.}

\subsection{Message Passing and Divergences}
In \cite{}, Minsky shows that many message-passing algorithms may be veiwed as local divergence minimization. We now show how this picture fits int ours.

Choose your favorite class of distributions $\mathcal Q \subset \Delta \dg M$, from which you can sample, and for which you can compute the relevant conditional entropies $\H_q(Y \mid X)$ for every edge $\ed LXY$ in your pdg $\dg M$.

Initially, start with some $q^0 \in \mathcal Q$. As before, we can compute the score of $q^0$ by
\[
	\bbr{\dg M}_\gamma(q^0)
	= \Ex_{q^0} \left[ \sum_{\ed LXY}
	 		\beta_L \log \frac{q^0(Y\mid X)}{\bp(Y \mid X)} \right]
		+ \left[ \sum_{\ed LXY} \alpha_L \H_{q^0}(Y \mid X) \right]
		- \H(q^0)
\]

We can define a transformation $\mathrm T : \mathcal Q \to \mathcal Q$ by
\[
	\mathrm T(q) := \arg\min_{q' \in \mathcal Q}
	% \Ex_{q'} \left[ \sum_{\ed LXY}
	% 		\beta_L \log \frac{q(Y\mid X)}{\bp(Y \mid X)} \right]
	% 	+ \left[ \sum_{\ed LXY} \alpha_L \H_{q}(Y \mid X) \right]
	% 	- \H(q').
%
\]
That is, we assume that it has some of the information-theoretic properties that make

\subsection{Generalized Belief Propagation}

\begin{defn}
	A PDG is said to be \emph{qualitatively exact} if the sum of the qualitative weights into each variable is equal to 1.
	That is, if
	\[ \forall Y \in \N.~~ \sum\alle \alpha_{L} = 1  \]
\end{defn}

\begin{prop}
	A
\end{prop}

\section{Axiom Systems}

\subsection{MAC axiom system}
We begin with three simple inference rules, central to probabilistic reasoning: \textbf Marginalization, \textbf Application, and \textbf Conditioning.

\textbf{M.} Given $\Pr(X,Y) = p$, infer $\Pr(X) = \sum_y p(X,Y)$. \\
\textbf{A.} Given $\Pr(Y \mid X) = p$ and $\Pr(X) = q$, infer $\Pr(X, Y) = p(Y \mid X) q(X)$. \\
\textbf{C.} Given $\Pr(X,Y) = p$ and $\Pr(X) = q$, infer $\Pr(Y \mid X) = p(X,Y) / q(X)$.

Each axiom corresponds to a transformer on unweighted PDGs. For instance, axiom \textbf{M} corresponds to the function
\begin{align*}
	\mathit{infer}^{\textbf{M}}_{X,Y} \Big(p(X,Y) \bundle q(X) \bundle \dg R\mathit{est} \Big)
	:=
	p \bundle q \bundle  (X \xrightarrow{\nicefrac pq} Y)~\bundle \dg R\mathit{est}, \\
	%
	% \begin{tikzpicture}[center base]
	% \end{tikzpicture}
	% &\xmapsto{X}
	% \begin{tikzpicture}[center base]
	% \end{tikzpicture}
\end{align*}
	
which takes a PDG containing data about a joint distribution $p(X,Y)$ and a marginal $q(X)$, and adds a new edge with cpd $\Pr(Y \mid X) = p(X,Y)/ q(X)$.

\begin{prop}
	The BU algorithm is is repeated application of \textbf{M},
\end{prop}

\subsection{Constraint Agglomeration}


\section{Discussion}

% Recall the questions that motivated these results.
%
% How can we use PDGs to make predictions? What kinds of queries can we ask a PDG? What do these answers have to do with the scoring rule?
%
% Although we have givne some answers to them, these questions still have some subtleties, because a PDG is somewhat further from being a representation of a probability distribution. (Two extreme cases: it may contain no data at all, or have multiple conflicting distributions on every variable).
One objective of the PDG formalism is to model the internal state of a bounded agent, and such an agent could easily have absorbed more information than they have been able to process.
Thus, we view PDG inference algorithms not as ways of getting ``the one true answer'' out of a modeling tool, but rather as descriptions of what an agent can do in order to:
\begin{enumerate}[nosep]
	\item identify and rank inconsistencies in their beliefs by graveness.
	\item make progress towards resolving such inconsistencies; and
	\item respond to questions without needing to first attain perfect epistemic clarity.
\end{enumerate}



\clearpage
\appendix
\section{Proofs and Lemmas}
%%!!! This is false !!!!!!!
\commentout{
	\begin{lemma} \label{lem:cvx1}
		If $f(x,y)$ is bounded below and convex in both $x$ and $y$, then it is jointly convex in
		$x\otimes y$.
	\end{lemma}

	\begin{lproof}
		By definition, for $f$ to be convex in either variable separately, we have, for all $\lambda \in [0,1]$ and all $x_0, x_1, x_2, y_0, y_1, y_2$, that
		\begin{align*}
			f(\lambda x_1 + (1-\lambda) x_2, y_0) &\le \lambda f(x_1, y_0) + (1-\lambda) f(x_2, y_0) \\
			\text{and}\qquad
			f(x_0, \lambda y_1 + (1-\lambda) y_2) &\le \lambda f(x_0, y_1) + (1-\lambda) f(x_0, y_2). \\
		\end{align*}
		In particular, for $y_0 = \lambda y_1 + (1-\lambda) y_2$ and $x_0 = \lambda x_1 + (1-\lambda) x_2$, we have
		\begin{align*}
			f\Big(\lambda x_1 + (1-\lambda) x_2,  \lambda y_1 + (1-\lambda) y_2 \Big)
				&\le \lambda f(x_1, \lambda y_1 + (1-\lambda) y_2 ) + (1-\lambda) f(x_2, \lambda y_1 + (1-\lambda) y_2 ) \\
				&\le \lambda \Big( \lambda f(x_1, y_1) + (1-\lambda) f(x_1, y_2) \Big) + (1-\lambda) \Big( \lambda f(x_2, y_1) + (1-\lambda) f(x_2, y_2) \Big) \\
				&=  \lambda^2 f(x_1, y_1) + \lambda(1-\lambda) \big[ f(x_1, y_2) +  f(x_2,y_1) \big] + (1-\lambda)^2 f(x_2, y_2).
		\end{align*}
		Now, supposing that $f$ is positive, then $\lambda^2 f(a,b) \le \lambda f(a,b)$, and so
		\begin{align*}
			f\Big(\lambda x_1 + (1-\lambda) x_2,  \lambda y_1 + (1-\lambda) y_2 \Big)
				&\le  \lambda^2 f(x_1, y_1) + \lambda(1-\lambda) \big[ f(x_1, y_2) +  f(x_2,y_1) \big] + (1-\lambda)^2 f(x_2, y_2) \\
				&\le  \lambda^2 f(x_1, y_1) + (1-\lambda)^2 f(x_2, y_2) \\
				&\le  \lambda f(x_1, y_1) + (1-\lambda) f(x_2, y_2),
		\end{align*}
		as desired. More generally, if $f$ is not positive but merely bounded below, then $f(a,b) = g(a,b) + C$ for some constant $C$ and positive function $g$, which is convex in its arguments. Our previous argument shows that $g$ is jointly convex in $x,y$, and because a constant shift does not alter the convexity of a function, $f$ must be jointly convex in $x,y$ also.
	\end{lproof}
	\begin{lemma} \label{lem:cvx2}
		If $f(x,y)$ is $m$-strongly convex in $x$ and $y$, then $y\mapsto \inf_x f(x,y)$ is an $m$-strongly convex function of $y$.
	\end{lemma}
}


\begin{lemma} \label{lem:Dstrongcvx}
	$\kldiv{\mu}{\nu}$ is a $k_\mu$-strongly in $\nu$, for fixed $\mu$, where $k_\mu >0$ is a constant that depends on $\mu$.
\end{lemma}
\begin{lproof}
	\begin{align*}
		\kldiv{\mu}{\nu} &= \Ex_{x\sim\mu} \log \frac{\mu(x)}{\nu(x)} \\
			&= \Ex_{x \sim \mu}\log \mu(x) +  \Ex_{x\sim\mu}\log \frac{1}{\nu(x)}
	\end{align*}
	As the first term depends only on $\mu$, it suffices to consider the behavior
	of the second term, as a function of $\nu$. Let $F(\nu) = - \Ex_{x \sim \mu} \log \nu(x)$.
	Then $\nabla F(\nu) = x\mapsto \frac{\mu(x)}{\nu(x)}$.
	Let $k^{\mu}_{\nu_1, \nu_2} := \inf_x \frac{\mu(x)}{\nu_1(x)\nu_2(x)}$.
	Expanding the inner product of the difference in between $\nu_1$ and $\nu_2$, we have
	\begin{align*}
		(\nabla F(\nu_1) - \nabla F(\nu_2) ) \cdot (\nu_1 - \nu_2)
			&= \sum_{x} \left( \frac{\mu(x)}{\nu_1(x)} - \frac{\mu(x)}{\nu_2(x)} \right)
				(\nu_2(x) - \nu_1(x)) \\
			&= \sum_{x} \mu(x) \left( \frac{\nu_2(x)}{\nu_1(x)\nu_2(x)} -
				\frac{\nu_1(x)}{\nu_1(x) \nu_2(x)} \right) (\nu_2(x) - \nu_1(x)) \\
			&= \sum_{x} \mu(x) \left( \frac{1}{\nu_1(x)\nu_2(x)} \right)
			 (\nu_2(x) - \nu_1(x))^2
			% &\ge \sum_{x} \mu(x)
 			%  (\nu_2(x) - \nu_1(x))^2 \\
		 	% &\le \left(\sum_{x} \frac{\mu(x)}{\nu_1(x)\nu_2(x)} \right)
			%    \left(\sum_x (\nu_2(x) - \nu_1(x))^2  \right) & \text{[Cauchy-Schwarz]}\\
	\end{align*}
\end{lproof}


\recall{prop:optimalYgivenX}
\begin{lproof}\label{proof:optimalYgivenX}
	Because $\alpha_p = 0$, the new cpd $p$ gives the resulting cpd one
	additional term in its scoring function, equal to the expected
	divergence from $p$ to the appropriate marginal of $\mu$, giving us
	$$
		\bbr{\dg M \bundle p}_\gamma(\mu) = \bbr{\dg M }_\gamma(\mu)
			+ \Ex_{x\sim\mu_{\!_X}} \kldiv[\Big]{\mu(Y\mid x)}{p(Y\mid x)}
	$$
	Gibbs inequality tells us that the second term is non-negative, and zero
	if and only if $\mu(Y \mid X) = p(Y \mid X)$.
	%Since the first term is independent of $p$,

	If $\mu$ minimizes the first term, (i.e., $\mu \in \bbr{\dg M}^*_\gamma$), then by definition we have $\bbr{\dg M}_\gamma(\mu) = \aar{\dg M}_\gamma$ and so by choosing $p := \mu(Y \mid X)$, we get
	$$	\bbr{\dg M \bundle p}_\gamma(\mu) =
		\bbr{\dg M \bundle \mu(Y \smid X)}_\gamma(\mu) = \bbr{\dg M}_\gamma(\mu)
		= \aar{\dg M}_\gamma$$
	but $\aar{\dg M}_\gamma \leq \aar{\dg M \bundle p}_\gamma$ for all $p$, so
	$\inf_p \aar{\dg M \bundle p}_\gamma = \aar{\dg M}_\gamma$,
	and $\mu(Y\smid X)$ minimizes $\aar{\dg M \bundle p}_\gamma$.
	This shows that $\big\{\mu(Y\smid X) : \mu \in \bbr{\dg M}_\gamma^* \big\} \subseteq \argmin_p \aar{\dg M \bundle p}$.

	% Conversely, if $p_0 \in \argmin_p \aar{\dg M \bundle p}_\gamma$, then
	Conversely, suppose $p(Y\mid X)$ cannot be expressed as a conditional marginal
	$\mu_0(Y\mid X)$ for any $\mu_0 \in \bbr{\dg M}_\gamma^*$.
	Now, one the one hand, for all $\mu$ in $\bbr{\dg M}_\gamma^*$, we have
	$$
		\bbr{\dg M \bundle p}_\gamma(\mu) = \aar{\dg M}_\gamma + \Ex_{x \sim
		\mu_{\!_X}} \kldiv[\big]{\mu(Y \mid x)}{p(Y \mid x)} > \aar{\dg
		M}_\gamma
		%= \inf_p \aar{\dg M \bundle p}_\gamma
		,
	$$
	where the strict inequality follows from the fact that $p(Y\mid X) \ne \mu(Y\mid X)$ and Gibbs' inequality.
	But on the other hand, if $\mu \notin \bbr{\dg M}_\gamma^*$, then $\bbr{\dg M}_\gamma(\mu) > \aar{\dg M}_\gamma$, so we have
	% \notin \bbr{\dg M}^*_\gamma$, then
	$$
		\bbr{\dg M \bundle p}_\gamma(\mu) \ge \bbr{\dg M}_\gamma(\mu) > \aar{\dg M}_\gamma.
		% = \inf_p \aar{\dg M \bundle p}_\gamma.
	$$
	Putting the two cases together, we find that for every joint distribution $\mu$,
	$\bbr{\dg M \bundle p}_\gamma(\mu) > \aar{\dg M}_\gamma$,
	and since $\aar{\dg M}_\gamma = \inf_p \aar{\dg M \bundle p}_\gamma$,
	we know that $p$ does not minimize $\aar{\dg M \bundle p}_\gamma$.

	Summarizing the argument, we have shown that
	\[ p \notin \argmin_p \aar{\dg M \bundle p} \implies p \notin \argmin_{p'} \aar{\dg M \bundle p'}. \]
	Taking the contrapositive gives the desired inclusion $\big\{\mu(Y\smid X) : \mu \in \bbr{\dg M}_\gamma^* \big\} \supseteq \argmin_p \aar{\dg M \bundle p}$.

\end{lproof}

\recall{prop:smallgammaopt}
\begin{lproof}\label{proof:smallgammaopt}
	The argument used in \hyperref[proof:optimalYgivenX]{the proof} of \autoref{prop:optimalYgivenX} works uniformly for all $\gamma$, and so the limit
	factors through it (more concretely, the proof remains valid if we insert a limit as $\gamma \to 0$ in front of every $\aar{-}_\gamma$ or $\bbr{-}_\gamma$). Since $\bbr{\dg M}^*$ the unique element of $\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$, $\bbr{\dg M}_\gamma^*(Y\smid X)$
	is the unique element of $\argmin_p \aarz{\dg M \bundle p}$.
	% $$ \bbr{\dg M}_\gamma(\mu) $$
\end{lproof}

We will make use of the implicit function theorem in the next result.

\begin{namedthm}{Implicit Function Theorem}[Dini]
	Suppose $Z \subseteq \mathbb{R}^n \times \mathbb{R}^m$ is open, and has coordinates $(y_1, \ldots, y_n, x_1, \ldots, x_n)$. If $\phi : Z \to \mathbb{R}^{m}$ is a $k$-times continuously differentiable function, and $(\mat b, \mat a) \in Z$ is such that $\phi(\mat b, \mat a) = \mat 0$, and if the Jacobian matrix
	\[ \mat J_{\phi,x}(\mat b, \mat a) = \left[ \frac{\partial \phi_i}{\partial x_j} (\mat b, \mat a) \right]_{i,j} \]
	is an invertible matrix, then there exists an open set $U \subset Y$ containing $\mat b$ such that there is a unique $k$-times differentiable function $g: U \to X$ such that $g(\mat b) = \mat a$, and $\phi(\mat y, g(\mat y)) = 0$ for all $\mat y \in U$.
\end{namedthm}

\recall{prop:smooth-and-strictly-cvx}
\begin{lproof}\label{proof:smooth-and-strictly-cvx}
	% First, we deal with the convexity, for which we make use of \cref{lem:cvx2}.
	\commentout{
		\def\mw#1{{\mat w}_{\!_{#1}}}
		\def\ofmw(#1|#2){(\mw{#1} \smid \mw{#2})}
		\begin{align*}
			\aar{\dg M \bundle p}_\gamma &= \inf_\mu \Big[ \Inc_{\dg M \bundle p}(\mu)
				+ \IDef{\dg M \bundle p}(\mu) \Big] \\
			&=  \inf_{\mu} \Ex_{\mat w \sim \mu}
				\left[\log \mu(\mat w) +
				 	\beta_p \log \frac{\mu\ofmw(Y|X)}{p\ofmw(Y|X)} \; +  \!\sum_{\ed LAB} \beta_L \log \frac{\mu\ofmw(B|A)}{\bp\ofmw(B|A)} + \alpha_L \log \frac{0}{\mu\ofmw(B|A)}\right] \\
			&= f
		\end{align*}
	}
	We start by expanding the definitions, obtaining
	\begin{align*}
		\aar{\dg M \bundle p}_\gamma &= \inf_\mu ~\bbr{\dg M \bundle p}_\gamma(\mu) \\
			&= \inf_\mu \left[ \bbr{\dg M }_\gamma(\mu)
				+ \Ex_{x\sim\mu_{\!_X}} \kldiv[\Big]{\mu(Y\mid x)}{p(Y\mid x)} \right]\\
			&= \inf_\mu \left[ \bbr{\dg M }_\gamma(\mu)
				+  \kldiv[\Big]{\mu(X,Y)}{p(Y \mid X)\, \mu(X)} \right].
	\end{align*}
	% % Choose $\gamma < \min (\{1\}\cup\{ \beta^{\dg M}_L : L \in \Ed^{\dg M}\})$.
	% Since $\bbr{\dg M}_\gamma$ is a $\gamma$-strongly convex function of $\mu$ for all
	% such $\gamma < \min_L \beta_L$, and
	% $\kldiv{\mu_{XY}}{\mu_X \; p_{Y\mid X}}$ is 1-strongly
	% convex in $p$ for fixed $\mu$ (\cref{lem:Dstrongcvx}),
	% % $\thickD$ is convex in both of its arguments,
	% their sum is $\gamma$-strongly convex in $\mu$ and in $p$.
	% By \cref{lem:cvx2} taking an infemum preserves this convexity,
	% and so
	% $
	%  	\inf_\mu \left[ \bbr{\dg M }_\gamma(\mu)
	% 	+  \kldiv[\big]{\mu_{XY}}{p_{Y \mid X}\; \mu_X} \right]
	% $, which equals $\aar{\dg M \bundle p}_\gamma$,
	% is $\gamma$-strongly convex in $p$.
	% % $\aar{\dg M \bundle p}_\gamma$ is smooth
	% % Smoothness.


	% Choose $\gamma < \min (\{1\}\cup\{ \beta^{\dg M}_L : L \in \Ed^{\dg M}\})$.
	Fix $\gamma < \min_L \beta_L$. Then we know that $\bbr{\dg X}_\gamma(\mu)$ is a $\gamma$-strongly convex function for every PDG $\dg X$, and hence there is a unique joint distribution which minimizes it.

	\textbf{Strict Convexity.}
	Suppose $p_1(Y \mid X)$ and $p_2(Y\mid X)$ are two cpds on $Y$ given $X$.
	Fix $\lambda \in [0,1]$, and set $p_\lambda = (1-\lambda) p_1 + \lambda p_2$.
	Let $\mu_1, \mu_2$ and $\mu_\lambda$ be the joint distributions that minimze $\bbr{\dg M \bundle p_1}_\gamma$, $\bbr{\dg M \bundle p_2}_\gamma$ and $\bbr{\dg M \bundle p_\lambda}_\gamma$, respectively.  Then we have
	\begin{equation*}
		\aar{\dg M \bundle p_\lambda}_\gamma
			= \bbr{\dg M}_\gamma(\mu_\lambda) + \kldiv[\Big]{\mu_\lambda(X,Y)}{p_\lambda(Y\mid X) \mu_\lambda( X)}.
	\end{equation*}
	By convexity of $\bbr{\dg M}$ and $\thickD$, we have
	\begin{align}
		\bbr{\dg M}_\gamma(\mu_\lambda)
		 	&\le (\lambda-1)\bbr{\dg M}_\gamma(\mu_1) + \lambda \bbr{\dg M}_\gamma(\mu_2)
			 	\label{eqn:score-cvx}\\
		\text{and}\qquad \kldiv[\Big]{\mu_\lambda(XY)}{p_\lambda(Y\smid X) \mu_\lambda( X)}
			&\le (1-\lambda)\kldiv[\Big]{\mu_1(XY)}{p_1(Y \smid X) \mu_1( X)} \nonumber \\
			&\qquad+ \lambda\;\;\kldiv[\Big]{\mu_2(XY)}{p_2(Y\smid X) \mu_2( X)}.
				\label{eqn:D-cvx}
	\end{align}
	If $\mu_1 \ne \mu_2$ then since $\bbr{\dg M}$ is strictly convex, \eqref{eqn:score-cvx} must
	be a strict inequality. On the other hand, if $\mu_1 = \mu_2$, then since $\mu_\lambda = \mu_1 = \mu_2$ and $\thickD$ is stricly convex in its second argument when its first argument is fixed (\Cref{lem:Dstrongcvx}), \eqref{eqn:D-cvx} must be a strict inequality.
	In either case, the sum of the two inequalities must be strict, giving us
	\begin{align*}
		\aar{\dg M \bundle p_\lambda}_\gamma &=
		\bbr{\dg M}_\gamma(\mu_\lambda) + \kldiv[\Big]{\mu_\lambda(XY)}{p_\lambda(Y\smid X) \mu_\lambda( X)} \\
		&<
		 (\lambda-1) \left[\bbr{\dg M}_\gamma(\mu_1)
			 	+ \kldiv[\Big]{\mu_1(XY)}{p_1(Y \smid X) \mu_1( X)} \right]
			 \\[-0.3em]&\qquad\qquad
			 + \lambda \left[ \bbr{\dg M}_\gamma(\mu_2)
			 	+ \kldiv[\Big]{\mu_2(XY)}{p_2(Y\smid X) \mu_2( X)}
			 	\right] \\
		 &= (\lambda-1) \aar{\dg M \bundle p_1} + \lambda\,\aar{\dg M \bundle p_2},
	\end{align*}
	which shows that $\aar{\dg M \bundle p}$ is \emph{strictly} convex in $p$, as desired.


	\textbf{Smoothness.}
	If $\bbr{\dg M \bundle p}_\gamma^*$ is a positive distribution, then by definition $\bbr{\dg M \bundle p}$ achieves its minimum on the interior of the probability simplex $\Delta \V(\dg M \bundle p)$, and so by \Cref{lem:cvx4}, we immediately find that $\aar{\dg M \bundle p}_\gamma$ is smooth in $p$.

	Now, suppose that $\bbr{\dg M \bundle p}_\gamma^*(\mat w) = 0$,  for some $\mat w \in \V(\dg M \bundle p)$.

	Applying \Cref{lem:cvx4} to the function $f = \bbr{\dg M}_\gamma$

	Now for the second case.

	\TODO

	If $x^*_b \in \partial X$, then we claim that either
	\begin{enumerate}[nosep]
		\item There is a subspace $T \subseteq \mathbb R^{m}$ with
			$\SD{}$
	 	\item There is a subspace $S \subseteq \mathbb R^{n}$ with
			$x^*_b \in S \cap \partial X$ such

	\end{enumerate}

\end{lproof}

\begin{lemma}\label{lem:cvx4}
	Let $X$ and $Y$ be convex sets, and
	$f : X \times Y \to \mathbb R$ be a smooth $(C^\infty)$, convex function.
	If $f$ is strictly convex in $X$, and for some $y_0 \in Y$, $f(x, y_0)$ achieves its infemum on the interior of $X$.
	then $y\mapsto \inf_x f(x, y)$ is smooth $(C^\infty)$ at the point $y_0$.
\end{lemma}

\begin{lproof}%[Proof of \Cref{lem:cvx4}]
	% Let $f_y(x) = f(x,y)$.
	% Since $f$ is smooth and stritly convex, each restriction $f_y$ of $f$ to a
	% particular $y$ is also smooth and strictly convex.
	% As a result, each $f_y$ has a unique minimum $m_y := \inf_{x} f_y(x)$.
	% As $f_y$ is smooth, $m_y$ is either a boundary point, or
	% at a point where $\nabla f_y = 0$.
	%
	% Moreover, it is a constrained optimization problem, so
	% $\nabla_{x,y,\lambda} [ f(x,y) + \lambda (y_0 - y)] = 0$.
	%
	% \TODO
	Let $x_0^* := \arg\min_x f(x,y_0)$, which is achieved by assumption, and is unique because $f(-,y_0)$ is strictly convex.

	We will ultimately apply the implicit function theorem to give us a smooth function which is equal to this infemum, but to do so we must deal with the technicality that it requires an open set; the boundary is the most complicated part of this result.
	Here we have essentially required that the domain be open by fiat for $X$, but for $Y$ (which is a possibly non-open subset of $\mathbb R^m$), we use the Extension Lemma for smooth functions \cite[Lemma 2.26]{Lee.SmoothManifolds}. In our context, it states that
	for every open set $U$ with $\overline{Y} \subseteq U \subseteq \mathbb R^m$,
	there exists a function $\tilde f : X \times \mathbb R^m \to \mathbb R$, such that $\tilde f |_{Y} = f$ (and $\supp \tilde f \subseteq U$).
	We only need a small fraction of this power: that we can smoothly extend $f$ to \emph{some} open set of $\mathbb R^m$, which we fix and call $\tilde Y$.

	% Similarly, for other $y \in Y$, let $x^*_y$ be the unique value of $x$ which minimizes $f(x,y)$.

	% \textbf{Smoothness.}
	% By assumption, $x^*_b$ is not a boundary point of $X$.
	%
	We claim that now all conditions for the Implicit Function Theorem are met if invoked with
		$\phi(y,x) := \vec\nabla_x \tilde f(x,y)$ and $(\mat b,\mat a) = (y_0, x^*_0)$.
	Concretely, we have $m = \mathop{dim} X$, $n = \mathop{dim} Y$, and $Z = (\tilde Y \times X)^\circ$, i.e., the interior of $\tilde Y \times X$, which is open and contains $(\mat b, \mat a)$.
	 Becuase $\phi$ is smooth, it is $k$-times differentiable for all $k$. We have $\vec\nabla_x \tilde f (y_0, x^*_0) = \vec 0$ because $x^*_0$ is a local minimum of the smooth function $\tilde f(-, y_0)$ which lies on the interior of $X$.

	Moreover, the Jacobian matrix
	\[ \mat J_{\nabla\!\tilde f, x}(y_0, x_0^*) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j}(x^*_0, y_0) \right]\]
	is the Hessian of the strictly convex funtion $f(-, b)$, and therefore positive definite (and in particular non-singular).
	Therefore, the Implicit Function Theorem guarantees us the existence of a neighborhood $U \subset \tilde Y$ of $y_0$ for which
	there is a unique $k$-times differentiable function $g: U \to X$ such that $g(y_0) = x^*_0$ and $\vec\nabla_x \tilde f(y, g(y)) = 0$ for all $y \in U$. Of course, this implies $g(y) = \argmin_x f(x,y)$ at every such point, and $\inf_x f(x,y) = f(g(y),y)$ is a composition of the smooth function $f$ with the $k$-times differentiable function $g \otimes \mathrm{id}_Y$.
	Therefore, $\inf_x f(x,y)$ is itself $k$-times continuously differentiable at $y_0$ for all $k$, or in other words, $\inf_x f(x,y)$ is smooth at $y=y_0$.
\end{lproof}

\recall{prop:inc-is-inconsistency}
\begin{lproof}\label{proof:inc-is-inconsistency}
	Unwrapping the definitions, we have
	\begin{align*}
		\aarz{\dg M} = \lim_{\gamma\to 0^+} \aar{\dg M}_\gamma
			&= \lim_{\gamma\to 0^+} \inf_{\mu}\;
				\bbr{\dg M}_\gamma(\mu) \\
			&= \lim_{\gamma\to 0^+} \inf_{\mu}\;
				\Big[  \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)  \Big]
		% \inf_{\mu}
	\end{align*}
	Since $\IDef{\dg M}$ is bounded above and below by constants $k \le \IDef{\dg M} \le K$, we have, for all $\gamma$ and $\mu$, that
	$$
		 \Inc_{\dg M}(\mu) + \gamma k \le  \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)
		 	\le \Inc_{\dg M}(\mu) + \gamma K.
	$$
	Since this holds for all $\mu$,  $\Inc_{\dg M}$ and $\IDef{\dg M}$ are
	bounded below, and set of possible distriutions $\mu \in \Delta\V(\dg M$)
	is compact, the infemum $\inf_\mu [\Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)]$
	is achieved by some $\mu^*$, for which
	$$
		\Inc_{\dg M}(\mu^*) + \gamma k \le  \Inc_{\dg M}(\mu^*)
			+ \gamma\;\IDef{\dg M}(\mu^*)
	   		\le \Inc_{\dg M}(\mu^*) + \gamma K
	$$
	and of course, $\Inc_{\dg M}(\mu^*) = \Inc(\dg M)$ by definition of the latter, so
	$$
		\Inc(\dg M) + \gamma k \le \inf_\mu
			\Big[ \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu) \Big]
			\le \Inc(\dg M) + \gamma K	.
	$$

	% 		\lim_{\gamma\to 0^+} \inf_{\mu}\;
	% 			\Big[  \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)  \Big]
	% 	&\geq
	% 		\lim_{\gamma\to 0^+} \Big[	\inf_{\mu}\;  \Inc_{\dg M}(\mu) +
	% 		 	\gamma \inf_{\mu}\;  \IDef{\dg M}(\mu) \Big]\\
	% 	&\geq
	% 		\lim_{\gamma\to 0^+} \Big[	\inf_{\mu}\;  \Inc_{\dg M}(\mu) +
	% 		 	\gamma \inf_{\mu}\; k \Big]
	% \end{align*}
	Taking the limit as $\gamma \to 0$ and using the squeeze theorem, we find that
	$$
		\Inc(\dg M) = \lim_{\gamma\to 0^+} \inf_{\mu}\;
			\Big[  \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)  \Big]
			 = \aarz{\dg M}
			 ,\qquad\text{as desired.}
	$$
\end{lproof}

% \subsection{Conditioning Results}
% \recall{prop:condition-by-inc}



\subsection{}
\recall{prop:markov-property}
\begin{lproof}
	Choose $\mu \in \bbr{\dg M_1 \bundle \dg M_2}^*_\gamma$.
	% Choose $\mu \in \mu^*_\gamma (\dg M_1 \bundle \dg M_2)$.
	Let $\mu' := \mu(\N_1) \mu(\N_2)$
	
	\TODO[Finish Transcribing Proof]
\end{lproof}


\subsection{Hardness Results}

\recall{prop:consistent-NP-hard}
\begin{lproof} \label{proof:consistent-NP-hard}
	We can directly encode SAT problems as PDGs.
	Specifically, let
	$$\varphi := \bigwedge_{j \in \mathcal J} \bigvee_{i \in \mathcal I(j)} (X_{j,i})$$
	be a CNF formula over binary variables $\mat X := \bigcup_{j,i} X_{j,i}$. Let
	$\dg M_\varphi$ be the PDG containing every variable $X \in \mat X$ and a binary
	variable $C_j$ (taking the value 0 or 1) for each clause $j \in \mathcal J$, as well as the following edges, for each $j \in \mathcal J$:
	%\{$``$\varphi(\mat X)$''$\}$ with $\V(\varphi) = \{0,1\}$, and
	\begin{itemize}
		\item a hyper-edge $\{X_{j,i} : i \in \mathcal I(j)\} \tto C_j$, together with a degenerate cpd
			encoding the boolean OR function (i.e., the truth of $C_j$ given $\{X_{j,i}\}$);
		\item an edge $\pdgunit \tto C_j$, together with a cpd asserting $C_j$ be equal to 1.
	\end{itemize}
	% We give each edge $\alpha = 0$ and $\beta = 1$.
	First, note that the number of nodes, edges, and non-zero entries in the cpds are polynomial in the $|\mathcal J|, |\mat X|$, and the total number of parameters in a simple matrix representation of the cpds is also polynomial if $\mathcal I$ is bounded (e.g., if $\varphi$ is a 3-CNF formula).
	A satisfying assignment $\mat x \models \varphi$ of the variables $\mat X$ can be regarded as a degenerate joint distribution $\delta_{\mat X = \mat x}$ on $\mat X$, and extends uniquely to a full joint distribution $\mu_{\mat x} \in \Delta \V(\dg M_\varphi)$ consistent with all of the edges, by
	\[ \mu_{\mat x} = \delta_{\mat x} \otimes \delta_{\{C_j = \vee_i  x_{j,i}\}} \]

 	Conversely, if $\mu$ is a joint distribution consistent with the edges above, then any point $\mat x$ in the support of $\mu(\mat X)$ must be a satisfying assignment, since the two classes of edges respectively ensure that $1 =\mu(C_j\!=\! 1 \mid \mat X \!=\! \mat x) = \bigvee_{i \in \mathcal I(j)} \mat x_{j,i}$ for all $j \in \mathcal J$, and so $\mat x \models \varphi$.

	Thus, $\SD{\dg M_\varphi} \ne \emptyset$ if and only if $\varphi$ is satisfiable, so
	an algorithm for determining if a PDG is consistent can also be adapted (in polynomial space and time) for use as a SAT solver, and so the problem of determining if a PDG consistent is NP-hard.
\end{lproof}

\recall{prop:sharp-p-hard}
\begin{lproof}\label{proof:sharp-p-hard}
	We prove this by reduction to \#SAT. Again, let $\varphi$ be some CNF formula over $\mat X$, and construct
	$\dg M_\varphi$ as in \hyperref[proof:consistent-NP-hard]{the proof} of
	\Cref{prop:consistent-NP-hard}.
	Furthemore, let $\bbr{\varphi} := \{ \mat x : \mat x \models \varphi \}$ be the set of  assingments to $\mat X$ satisfying $\varphi$, and $\#_\varphi := |\bbr{\dg M}|$ denote the number such assignments. We now claim that
	\begin{equation}\label{eqn:number-of-solns}
		\#_\varphi = \exp \left[- \frac1\gamma \aar{ \dg M_\varphi }_\gamma \right].
	\end{equation}
 	If true, we would have a reduced the \#P-hard problem of computing $\#_\varphi$ to the problem of computing $\aar{\dg M}_\gamma$ for fixed $\gamma$. We now proceed with proof \eqref{eqn:number-of-solns}.
	By definition, we have
	\[ \aar{\dg M_\varphi}_\gamma = \inf_\mu \Big[ \Inc_{\dg M_\varphi}(\mu) + \gamma \IDef{\dg M_\varphi}(\mu) \Big]. \]
	We start with a claim about first term.
	% For the particular PDG $\dg M_\varphi$, the

	\begin{iclaim} \label{claim:separate-inc-varphi}
		% $\Inc(\dg M_\varphi)$ is finite if and only if $\varphi$ is statisfiable.
		$\Inc_{\dg M_\varphi}\!(\mu) =
		% \begin{cases}
		% 	0 & \text{if}~  \mat x \models \varphi~\text{and}~\mat c = \mat 1
		% 	 	~\text{for all}~(\mat x, \mat c) \in \supp \mu\\
		% 	\infty & \text{otherwise}
		% \end{cases}
		\begin{cases}
			0 & \text{if}~  \supp \mu \subseteq \bbr{\varphi} \times \{ \mat 1\} \\
			\infty & \text{otherwise}
		\end{cases}$.
	\end{iclaim}
	\vspace{-1em}
	\begin{lproof}
		Writing out the definition explicitly, the first can be written as
		\begin{equation}
			\Inc_{\dg M_\varphi}\!(\mu) = \sum_{j} \left[ \kldiv[\Big]{\mu(C_j)}{\delta_1} +
				\Ex_{\mat x \sim \mu(\mat X_j)} \kldiv[\Big]{\mu(C_j \mid \mat X_j = \mat x)}{\delta_{\lor_i \mat x_{j,i}}} \right], \label{eqn:explicit-INC-Mvarphi}
				% &= \sum_{j} \left[
				% 	\begin{matrix} \mu(C_j\!=\!0) (\infty) \\
				% 	 	+ \mu(C_j \!=\! 1) \log \mu(C_j \!=\! 1)
				% 	\end{matrix} +
				% 	\Ex_{\mat x \sim \mu(\mat X_j)} \kldiv[\Big]{\mu(C_j \mid \mat X_j = \mat x)}{\delta_{\lor_i \mat x_i}} \right],
		\end{equation}
		where $\mat X_j = \{X_{ij} : j \in \mathcal I(j)\}$ is the set of variables that
		appear in clause $j$, and $\delta_{(-)}$ is the probability distribution placing all mass on the point indicated by its subscript.
		As a reminder, the relative entropy is given by
		\[ \kldiv[\Big]{\mu(\Omega)}{\nu(\Omega)} := \Ex_{\omega \sim \mu} \log \frac{\mu(\omega)}{\nu(\omega)},
		\quad\parbox{1.4in}{\centering and in particular, \\ if $\Omega$ is binary,}\quad
			\kldiv[\big]{\mu(\Omega)}{\delta_\omega} = \begin{cases}
				0 &  \text{if}~\mu(\omega) = 1 ; \\
				\infty & \text{otherwise}.
		\end{cases} \]
		Applying this to \eqref{eqn:explicit-INC-Mvarphi}, we find that either:
		\begin{enumerate}[itemsep=0pt]
			\item Every term of \eqref{eqn:explicit-INC-Mvarphi} is finite (and zero) so $\Inc_{\dg M_\varphi}(\mu) = 0$, which happens when $\mu(C_j = 1) = 1$ and $\mu(C_j = \vee_i~ x_{j,i}) = 1$ for all $j$.  In this case, $\mat c = \mat 1 = \{ \vee_i~x_{j,i} \}_j$ so $\mat x \models \varphi$ for every $(\mat{c,x}) \in \supp \mu$;
			\item Some term of \eqref{eqn:explicit-INC-Mvarphi} is infinite, so that $\Inc_{\dg M_\varphi}(\mu) = \infty$, which happens if some $j$, either

			\begin{enumerate}
				\item $\mu(C_j \ne 1) > 0$ --- in which case there is some $(\mat{x,c}) \in \supp \mu$ with $\mat c \ne 1$, or
				\item $\supp \mu(\mat C) = \{\mat 1\}$, but $\mu(C_j \ne \vee_i~ x_{j,i}) > 0$ --- in which case there is some $(\mat{x,1}) \in \supp \mu$ for which $1 = c_j \ne \vee_i~x_{j,i}\;$, and so $\mat x \not\models \varphi$.
			\end{enumerate}
		\end{enumerate}
		Condensing and rearranging slightly, we have shown that
		\[
			\Inc_{\dg M_\varphi}(\mu) =
			\begin{cases}
				0 & \text{if}~  \mat x \models \varphi~\text{and}~\mat c = \mat 1
				 	~\text{for all}~(\mat x, \mat c) \in \supp \mu\\
				\infty & \text{otherwise}
			\end{cases}~.
		\]
		% So if $\mat x \models \varphi$ for all $\mat x \in \supp \mu(X)$,
		%
		% $\Inc_{\dg M_\varphi}(\mu) = 0$
		% The first term is infinite if $\mu(C_j = 1) < 1$, and the second is infinite
		% if $\mu(C_j = \lor_i X_{i,j}) < 1$. Thus, if $\Inc_{\dg M_\varphi}(\mu)$ is finite, then $\mat x \sim \mu(\mat X)$ satisfies $\varphi$ with probability 1, and $\varphi$ must be satisfiable.
		% Conversely,
	\end{lproof}

	% Thus, if $\Inc_{\dg M_\varphi}(\mu)$ is finite, then every $\mat x \in \supp \mu$ is a satisfying assignment of $\varphi$.
	Because $\IDef{}$ is bounded, it follows immediately that
 	$\aar{\dg M_\varphi}_\gamma$, is finite if and only if
	there is some distribution $\mu \in \Delta\V(\mat X,\mat C)$ for which $\Inc_{\dg M_\varphi}(\mu)$ is finite, or equivalently, by \Cref{claim:separate-inc-varphi}, iff there exists some $\mu(\mat X) \in \Delta \V(\mat X)$ for which $\supp \mu(\mat X) \subseteq \bbr{\varphi}$, which in turn is true if and only if $\varphi$ is satisfiable.

	In particular, if $\varphi$ is not satisfiable (i.e., $\#_\varphi = 0$), then $\aar{\dg M_\varphi}_\gamma = +\infty$, and
	\[
		\exp \left[ -\frac1\gamma \aar{\dg M_\varphi}_\gamma \right] =
	 		\exp [ - \infty ] = 0 = \#_\varphi,
	\]
	so in this case \eqref{eqn:number-of-solns} holds as promised. On the other hand, if $\varphi$ \emph{is} satisfiable, then, again by \Cref{claim:separate-inc-varphi}, every $\mu$ minimizing $\bbr{\dg M_\varphi}_\gamma$, (i.e., every $\mu \in \bbr{\dg M_\varphi}_\gamma^*$) must be supported entirely on $\bbr{\varphi}$ and have $\Inc_{\dg M_\varphi}\!(\mu) = 0$.  As a result, we have
	\[
		\aar{\dg M_\varphi}_\gamma =
			\inf\nolimits_{\mu \in \Delta \big[\bbr{\varphi} \times \{\mat 1\}\big]} \gamma\; \IDef{\dg M_\varphi}(\mu) .
	\]
	A priori, by the definition of $\IDef{\dg M_\varphi}$, we have
	\[
		\IDef{\dg M_\varphi}(\mu) =
		 	- \H(\mu) + \sum_{j} \Big[ \alpha_{j,1} \H_\mu(C_j \mid \mat X_j)
						+ \alpha_{j,0} \H_\mu(C_j) \Big],
	\]
	where $\alpha_{j,0}$ and $\alpha_{j,1}$ are values of $\alpha$ for the edges of $\dg M_\varphi$, which we have not specified because they are rendered irrelevant by the fact that their corresponding cpds are deterministic. We now show how this plays out in the present case.
	Any $\mu \in \Delta\big[\bbr{\varphi} \times \{\mat 1\}\big]$ we consider has a degenerate marginal on $\mat C$. Specifcally, for every $j$, we have $\mu(C_j) = \delta_1$, and since entropy is non-negative and never increased by conditioning,
	$$
		0 \le \H_\mu(C_j \mid \mat X_j) \le \H_\mu(C_j) = 0.
	$$
	Therefore, $\IDef{\dg M_\varphi}(\mu)$ reduces to the negative entropy of $\mu$.
	Finally, making use of the fact that the maximum entropy distribution $\mu^*$ supported on a finite set $S$ is the uniform distribution on $S$, and has $\H(\mu^*) = \log | S |$, we have
	\begin{align*}
		\aar{\dg M_\varphi}_\gamma &= \inf\nolimits_{\mu \in \Delta \big[\bbr{\varphi} \times \{\mat 1\}\big]} \gamma\; \IDef{\dg M_\varphi}(\mu) \\
			&= \inf\nolimits_{\mu \in \Delta \big[\bbr{\varphi} \times \{\mat 1\}\big]} -\, \gamma\, \H(\mu) \\
			&= - \gamma\, \sup\nolimits_{\mu \in \Delta \big[\bbr{\varphi} \times \{\mat 1\}\big]}  \H(\mu) \\
			&= - \gamma\, \log (\#_\varphi),
	\end{align*}
	\hspace{1in}giving us
	$$
		\#_\varphi = \exp \left[- \frac1\gamma \aar{ \dg M_\varphi }_\gamma \right],
	$$
	as desired. We have now reduced \#SAT to computing $\aar{\dg M}_\gamma$, for $\gamma \in \mathbb R^{>0}$ and an arbitrary PDG $\dg M$, which is therefore \#P-hard.
\end{lproof}


\recall{prop:fg-inconsistency-is-partition-function}
\begin{lproof}\label{proof:fg-inconsistency-is-partition-function}
	\def\theelt{\mathop{\mathrm{the}}}
	Let $\theelt(\{x\}) := x$ be a function that extracts the unique element singleton set.
	We showed in the orignal paper (Corolary 4.4.1) that
	\[ \theelt \bbr{(\UPDGof{\Phi}, \theta, \theta)}^*_1 = \Pr\nolimits_{\Phi, \theta}(\mat w)
		= \frac{1}{Z_\Psi} \prod_{j} \phi_j(\mat w_j)^{\theta_j}. \]
	Recall the statement of Prop 4.6 from the original paper,
	\begin{equation}\label{eqn:nice-score-repeated}
		\bbr{\dg M}_\gamma(\mu) = \Ex_{\mat w \sim \mu}\! \Bigg\{ \sum_{ X \xrightarrow{\!\!L} Y  } \bigg[\,
		   \!\beta_L \log \frac{1}{\bp(y^{\mat w} |x^{\mat w})} +
		   {\color{red}(\gamma\alpha_L - \beta_L ) \log \frac{1}{\mu(y^{\mat w} |x^{\mat w})}} \bigg] -
		\gamma \log \frac{1}{\mu(\mat w)}  \Bigg\}, \\
	\end{equation}
	but note that since $\gamma = 1$, and $\alpha,\beta$ are both equal to $\theta$ for our PDG
	 (since $\PDGof{\Psi} = \PDGof{(\Phi,\theta)} = (\UPDGof\Phi, \theta,\theta)$), the middle term disappears, yielding the standard variational Gibbs free energy $\GFE(\mu)$.
	Recall also that
	$\aar{\dg M}_\gamma = \inf_\mu \bbr{\dg M}_\gamma(\mu)$ and $\bbr{\dg M}^*_\gamma = \argmin \bbr{\dg M}_\gamma(\mu)$, so (with a minor abuse of notation), $\aar{\dg M}_\gamma = \bbr{\dg M}_\gamma(\bbr{\dg M}_\gamma^*)$. We now compute the value of the inconsistency $\aar{(\UPDGof\Phi, \theta,\theta)}_1$.
	\begin{align*}
		\aar{(\UPDGof{\Phi}, \theta, \theta)}_1
		&= \bbr{(\UPDGof{\Phi}, \theta, \theta)}_1\Big(\Pr\nolimits_{\Phi, \theta}(\mat w) \Big) \\
		%\frac{1}{Z_\Phi} \prod_j \phi_j(\mat w_j)^{\theta_j}
		&=
		 \Ex_{\mat w \sim \mu}\! \Bigg\{ \sum_{X \xrightarrow{\!\!L} Y} \bigg[\,
	      		\!\beta_L \log \frac{1}{\bp(y^{\mat w} |x^{\mat w})}
				% (\alpha_L - \beta_L ) \log \frac{1}{\mu(y^{\mat w} |x^{\mat w})}
				\bigg] - \log \frac{1}{\Pr\nolimits_{\Phi, \theta}(\mat w) }  \Bigg\}
			& \Big[	 ~\text{by \eqref{eqn:nice-score-repeated}}~	\Big]\\
		&=
		 \Ex_{\mat w \sim \mu}\! \Bigg\{ \sum_j \bigg[\,
	      		\!\theta_j \log \frac{1}{\phi_j(\mat w_j)}
				\bigg] - \log \frac{Z_\Psi}{\prod_{j} \phi_j(\mat w_j)^{\theta_j}}  \Bigg\}
			& \Big[ \parbox{1.5in}{\centering%
			 	cpds $\bp$ correspond\\ to factors $\phi_j$}	\Big]\\
		&=
		 \Ex_{\mat w \sim \mu}\! \Bigg\{ \sum_j \bigg[\,
	      		\!\theta_j \log \frac{1}{\phi_j(\mat w_j)}
				\bigg] - \sum_j \left[\theta_j \log \frac{1}{\phi_j(\mat w_j)} \right]
				 - \log Z_\Psi \Bigg\} \\
		&= \Ex_{\mat w \sim \mu} [- \log Z_\Psi] \\
		&= - \log Z_\Psi & \Big[~\text{$Z_\Psi$ is constant in $\mat w$}~\Big]
	\end{align*}
\end{lproof}





\end{document}
