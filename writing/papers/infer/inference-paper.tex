\documentclass[twoside]{article}

\usepackage[accepted]{aistats2023}

% If your paper is accepted, change the options for the package
% aistats2023 as follows:
%
%\usepackage[accepted]{aistats2023}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}
\input{inference-preamble.tex}
\input{pdg-preamble-v2.tex}

% \author{$\{$Oliver E Richardson, Joseph Y Halpern, Christopher De Sa$\}$}

\begin{document}
% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Inference in Probabilistic Dependency Graphs,
    via Exponential Cones and Otherwise}

\aistatsauthor{ Oliver E Richardson \And Joseph Y Halpern \And  Christopher De Sa }

% \aistatsaddress{ Institution 1 \And  Institution 2 \And Institution 3 } 
\aistatsaddress{Cornell University \And Cornell University \And Cornell University}
]

\begin{abstract}
    We provide the first tractable inference algorithm for Probabilistic Dependency Graphs (PDGs) with finite variables, placing PDGs on asymptotically similar footing as other graphical models, such as Bayesian Networks and Factor Graphs.
    This may be surprising, because PDGs are much more expressive than these other models, and also because (as we show) a PDG inference algorithm can be used 
    % for ``inconsistency minimization'', 
    % which has been argued to be widely useful. 
    to resolve inconsistencies, which has been proposed as a generic modeling task. 
    
    The key to our approach is combining 
    (1) our finding that inference in PDGs with bounded tree-width can be reduced to a tractable linear optimization problem with exponential cone constraints,
    with (2) a recent interior point method that can (provably) solve such problems efficiently (Dahl \& Anderson, 2022).
    We provide a concrete implementation and emperical evaluation.
    In addition, we prove auxiliary results about complexity of this problem, and discuss other approaches to it. 
\end{abstract}




% \begin{narrow}
% %%-----------    A FRANK SUMMARY    ---------------
% Measuring / Estimating /  Inconsistency is very useful.
% For instance, (1) propogating it backwards through layers of computation = differentiable learning. 
% 
% Certain localized versions of it can be used to do other algorithms.

% How hard is it? 
% With interior point methods (convex programs with exponential cone constraints) we can do it in $O(n^4 \log n)$ time \& space, worst case for exact inference. So far, this means slightly harder than inference in Graphical models.
% \end{narrow}


% \tableofcontents

\section{INTRODUCTION}

Probabilistic Dependency Graphs \cite{} are a 
very general class of graphical models, 
that has intimate connections with learning problems. 



\section{COMPLEXITY OF INFERENCE}

% \begin{prop}
% \begin
%     	% \label{prop:optimalYgivenX}
%     	% For all $\dg M$, $X,Y\in\N^{\dg M}$, and $\gamma > 0$, we have that
%     	$\displaystyle
%     		\argmin_{p : X \to \Delta Y} \aar{\dg M + p}_\gamma =
%     		\Big\{ \mu(Y | X) :  \mu \in \bbr{\dg M}_\gamma^* \Big\}
%     	$.
% \end{prop}

\begin{linked}{prop}{optimalYgivenX}
	% \label{prop:optimalYgivenX}
	% For all $\dg M$, $X,Y\in\N^{\dg M}$, and $\gamma > 0$, we have that
    For all variables $X,Y$, and $\gamma > 0$, 
	$$\displaystyle
		\argmin_{p : X \to \Delta Y} \aar{\dg M + p}_\gamma =
		\Big\{ \mu(Y | X) :  \mu \in \bbr{\dg M}_\gamma^* \Big\}
	.$$
\end{linked}
In the limit, of small $\gamma$, since there is only one such distribution,
the expression beomes simpler.

\begin{linked}{coro}{smallgammaopt}
	$\displaystyle
		\bbr{\dg M}^*(Y | X)
	$ uniquely minimizes $p(Y\mid X) \mapsto \aar{\dg M + p}$.
\end{linked}

\end{document}
