\documentclass{article}
\input{inference-preamble.tex}

\author{$\{$Oliver E Richardson, Joseph Y Halpern, Christopher De Sa$\}$}

\begin{document}
\begin{abstract}
    We provide the first tractable inference algorithm for Probabilistic Dependency Graphs (PDGs) with finite variables, placing PDGs on asymptotically similar footing as other graphical models, such as Bayesian Networks and Factor Graphs.
    This may be surprising, because PDGs are much more expressive than these other models, and also because (as we show) a PDG inference algorithm can be used for ``inconsistency minimization'', which has been argued to be widely useful. 
    
    The key to our approach is combining 
    (1) our finding that inference in PDGs with bounded tree-width can be reduced to a tractable linear optimization problem with exponential cone constraints,
    with (2) a recent interior point method that can (provably) solve such problems efficiently (Dahl \& Anderson, 2022).
    We provide a concrete implementation and emperical evaluation.
    In addition, we prove auxiliary results about complexity of this problem, and discuss other approaches to it. 
\end{abstract}

\begin{narrow}
%%-----------    A FRANK SUMMARY    ---------------

Measuring / Estimating /  Inconsistency is very useful.
For instance, (1) propogating it backwards through layers of computation = differentiable learning. 

Certain localized versions of it can be used to do other algorithms.



How hard is it? 







With interior point methods (convex programs with exponential cone constraints) we can do it in $O(n^4 \log n)$ time \& space, worst case for exact inference. So far, this means slightly harder than inference in Graphical models.





\end{narrow}







\end{document}
