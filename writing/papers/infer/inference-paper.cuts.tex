We then evaluate our approach, showing 
that this exponential-cone-based approach is more precise
and, often, faster than generic optimization baselines.
While not currently as fast as inference methods such as belief
propagation on the models to which belief propagation can be applied,
we are optimistic that further improvements are possible.
In any case, our results show that inference in PDGs is feasible.

%%%%%%%%%%%==================================================%%%%%%%%%%%%
 
 %joe2*: I think here you need to say something about how you can use
 %inconsistency minimization to do inference.  The paper is about
 %inference, after all, and you haven't made the connection.  This is critical
%oli2: I agree, but I'm a little bit furstrated because it was structurally
% much closer to doing this before I accepted your %joe1 edits. 
% I'm rewriting the paragraph.  
% 
% However, the earlier work on PDGs does not provide any
% computational method for calculating whether a PDG is consistent and,
% if not, its degree of inconsistency.  We provide such methods in this paper.
%
%From a pragmatic point of view, though, PDGs are currently not yet
%very useful.  As it stands, they only have conceptual
%applications---one can use them to justify 
%a choice of loss function analytically, or to derive cute diagrammatic proofs
%of inequalitites you likely already know \parencite{one-true-loss},
%but it is impossible to compute with them.
%% What use is a model without an inference algorithm? 
%Until now, PDGs have been a model without an inference algorithm. 
%
%joe1: where do we do updating?   What inference problems do we
%consider?  You need to slow down here and explain what we do
%We analyze the complexity of inference and updating in pdgs, and show
%joe2: In the previous paragraph you talked about minimizing
%inconsistency.  Here you talk about the complexity of inference.  You
%have to make the connection.  
In more detail,
we analyze the complexity of inference in PDGs, and illustrate
the close relationship it has with inconsistency minimization.
% that it is equivalent to that of inconsistency minimization. 
%joe1*: I have no idea what exponential-cones constraints are.  Unless
%this is a completely standard notion in the AIStats community, you
%*must* give some intuition.  Also, when you talk about reducing the
%problem to a linear program, (a) I don't know which problem you're
%talking about and (b) we usually talk about reducing one problem to
%another, not reducing a problem to a linear program
%
Then, we reduce the problem to a convex optimization problem in standard
form.
This allows us to use powerful interior-point methods
that can solve such problems in polynomial time \parencite{dahl2022primal}. 

%%%%%%%%%%%==================================================%%%%%%%%%%%%
% OLD CCCP SECTION


%joe2*: Why do we care about this?  How does it relate to what you
%called the inference problem on p. 3 (which does *not* involve
%minimizing inconsistency).
%oli2: is this now clearer now that I've connected the two a little better?
\section{INFERENCE WHEN
    \texorpdfstring{$\boldsymbol\gamma \boldsymbol> \mat 0$}{gamma > 0},
    VIA THE CONVEX-CONCAVE PROCEDURE }

We have now given an algorithm that provably finds the distribution $\bbr{\dg M}^*$ in polynomial time. 
What about optimal distributions for fixed $\gamma > 0$.



To do this, we re-use our work in \cref{sec:reductions} employ the convex-concave procedure 
\parencite{yuille2003concave} to find some minimizer $\mu^* \in \bbr{\dg M}^*_\gamma$ (although it may not be unique), for fixed $\gamma > 0$.

The PDG scoring function can be written as \parencite[Proposition 4.6]{pdg-aaai}
\begin{align*}
    \bbr{\dg M}_\gamma(\mu) = 
        -\gamma\H(\mu) + 
            \sum_{L \in \Ed}
                % \left[
                \beta\ssub L\, \Ex_\mu 
                    \log \frac1{p\ssub L(\Tgt L | \Src L)}
                % \right]
                \\
            + \sum_{L \in \Ed}
            (\gamma \alpha \ssub L - \beta\ssub L)
                \Ex_\mu \log \mu(\Tgt L | \Src L)
\end{align*}
The first line is the sum of a linear term and a convex one,
and each individual term on the second line is either convex or concave, depending on the sign of the quantity $\gamma \alpha\ssub L - \beta\ssub L$. 
Once we sort the terms into convex terms $f(\mu)$ and strictly concave terms $g(\mu)$, we can choose an initial guess $\mu_0$, and iteratively use the convex solver to compute
%
\begin{align*}
    \mu_{t+1} &:= \argmin_{\mu} f(\mu) + (\mu - \mu_{t})^{\sf T}
        \nabla g(\mu_t)
\end{align*}

As we will see in \cref{sec:expts}, the aproach presented here section is 
not very fast, but it is guaranteed to make progress, since
\def\tplus1{{t\mskip-2mu+\mskip-2mu1}}
\begin{align*}
    f(\mu_\tplus1) \!+\! g(\mu_\tplus1) &<  f(\mu_\tplus1) \!+\! (\mu_\tplus1 \!-\! \mu_t)^{\sf T} \nabla g(\mu_t) \!+\! g(\mu_t)
        % &\text{(concavity of $g$)}
        \\
    &\le  f(\mu_t) + (\mu_t - \mu_{t})^{\sf T}\nabla g(\mu_t)  + g(\mu_t)
        % &\text{(defn of argmin)}
        \\
    &= f(\mu_t) + g(\mu_t)
\end{align*}
and eventually find an optimum, because the concave terms are bounded.
%
% The slightly trickier part is combining this with the clique tree representation of \cref{sec:clique-tree-expcone}.
% \TODO[ TODO: Show the fancier spanning aborescence trick ]
%
% To combine this tech this with the clique tree representation of 
The only remaining difficulty is to do the convex optimization over
calibrated clique-trees. Fortunately, we already dealt with the 
tricky part (rewriting the joint entropy term as a disciplined convex program) in 
\cref{sec:clique-tree-expcone},
and so we defer the details to the appendix.
    
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROOFS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    

Since $(\C, \cal T)$ is a tree decomposition, every cluster along the unique path from $C_1'$ to $C_2'$ (which must traverse $e$) hence must contain $C_1' \cap C_2'$ (property 2 of a tree decomposition), and, in particular, $Y$.  In particular, $C_1$ and $C_2$ both lie on this path, and therefore $Y \in C_1 \cap C_2$.


    
If either $\mat X$ or $\mat Y$ is contained by $\mat Z$, then $\mat X$ and $\mat Y$ are always trivially independent given $\mat Z$ and we can skip to the end of the proof; otherwise, both $\mat X$ and $\mat Y$ must have elements outside of $\mat Z$.
% In this case, we can choose $X_0 \in \mat X \setminus \mat Z$ and $Y_0 \in \mat Y \setminus \mat Z$.    
In this case, we can choose $X \in \mat X \setminus \mat Z$ and $Y \in \mat Y \setminus \mat Z$.    
% We claim that also there must also exist an edge $e^* = (C_1{-}C_2) \in \cal T$ such that $C_1 \cap \C_2 \subseteq \mat Z$. 
For every such choice, we claim that there must be an edge $e_{(X,Y)}
 = (C^{XY}_1{-}C^{XY}_2)
 \in \cal T$ such that
\begin{enumerate}[nosep]
\item $\mat S^{XY} := C_1^{XY} \cap C_2^{XY} \subseteq \mat Z$, and
\item $e_{(X,Y)}$ intersects every path (in $\cal T$) between a cluster containing $X$, and one containing $Y$. 
\end{enumerate}
% This is because 
Suppose, for contradiction, that this were not true. 
So, for every $e = (C{-}D) \in \cal T$, one of the following two cases must hold:
\begin{enumerate}
\item There is at least one variable $X_e \in (C \cap D) \setminus \mat Z$.
\item There is a path from a cluster containing $X$, to a cluster containing $Y$, that does not use $e$. 
\end{enumerate}
% if $e = (C{-}D) \in \cal T$ intersects every path from a cluster containing $X$ and one containing $Y$, there must be at least one variable $X_e \in (C \cap D) \setminus \mat Z$.
% Then for each edge $e = (C{-}D) \in \cal T$, there would be at least one variable $X_e \in C \cap D \setminus \mat Z$. 
%
% So, if $C_{X_0}$ is a cluster containing $X_0$ and $C_{Y_0}$ is a cluster containing $Y_0$, 
% Every variable is a member of at least one cluster; say $X_0 \in C_{X_0}$ and $Y_0 \in C_{Y_0}$.  
Every variable is a member of at least one cluster; choose clusters $D_{X}, D_Y \in \C$ containing $X$ and $Y$, respectively.
As $\cal T$ is a tree, there is a (unique) path (in $\cal T$) from $D_X$ to $D_Y$; let's label it as follows:
\[
    % C_{X_0}=D_0\overset{e_1}{-}D_1\overset{e_2}{-}D_2 \cdots D_{n-1}\overset{e_n}{-}D_n=C_{Y_0}.
    \begin{tikzcd}[column sep=2em]
        % C_{X_0}=
        D_X=
        &D_0 \ar[r,-,"e_1"]&
        D_1 \ar[r,-,"e_2"]&
        % D_2\ar[r,-,"e_3"]&
          \cdots
        \ar[r,-,"e_{n-1}"]& D_{n-1}
        \ar[r,-,"e_n"] & D_n&
        =D_Y
        % =C_{Y_0}
    \end{tikzcd}.
\] 
% Recall that each $X_{e_i} \in D_i \cap D_{i+1}$ by assumption, for $i \in \{1, \ldots, n-1\}$.
Since every pair of nodes that share a cluster are connected, and 
    $X_{e_i} \in (D_i \cap D_{i+1}) \setminus \mat Z$ by assumption,
we get a corresponding path in $G$, that is disjoint from $\mat Z$:
\[\begin{tikzcd}[column sep=1em]
    X_0 \ar[r,-] \ar[d,sloped,phantom,"\in"]
    & X_{e_1}\ar[r,-] \ar[d,sloped,phantom,"\in"]
    & X_{e_2}\ar[r,-] \ar[d,sloped,phantom,"\in"]
       &\cdots\ar[r,-]
    & X_{e_{n-1}} \ar[r,-] %\ar[d,sloped,phantom,"\in"]
    & X_{e_n}\ar[r,-] \ar[d,sloped,phantom,"\in"]
    & Y_0 \ar[d,sloped,phantom,"\in"]  
        \\
    D_0
    & D_0 \cap D_1
    & D_1 \cap D_2
    &
    & D_{n-2} \cap D_{n-1}
    & D_{n-1} \cap D_n
    & D_n
\end{tikzcd}.\]
This contradicts our earlier assumption that every path in $G$ between a member of $\mat X$ and a member of $\mat Y$ must intersect with $\mat Z$, and so we conclude that indeed there exists some $e^* = (C_1{-}C_2) \in \cal T$ with $C_1 \cap C_2 \subseteq \mat Z$.
     
% Choose a cluster $C \in \C$.
% Choose an edge $e \in \cal T$. 
Since $\cal T$ is a tree, removing $e^*$ disconnects it,
    % , depending on which side of $C$ they were in $\cal T$. 
    partitiong $\C$ in two subsets $\C_1$ and $\C_2$, based on which side of $e^*$ each cluster falls. 
Let $\X_1 := \cup \C_1$ be the union of all clusters in $\C_1$; analogously, define $\X_2 := \cup \C_2$.
% For reasons, we must have either $\mat X \subseteq \X_1$ and $\mat Y \subseteq \X_2$, or $\mat X \subseteq \X_2$ and $\mat Y \subseteq \X_1$
Since $X_0, Y_0 \notin \mat Z  \supseteq C_1 \cap C_2$, both variables must lie 
% Suppose that $\mat X_1, \mat Y \subseteq \X$ are sets of variables, such that 
% Now, choose any subsets of variables $\mat X_1, \mat X_2 \subseteq \X$ such that 
% $\mat X_1 \subseteq \X_1$ and $\mat X_2 \subseteq \X_2$

We claim that $\X_1 \cap \X_2 = C_1 \cap C_2$. 
It's clearly the case that $C_1 \cap C_2 \subset \X_1 \cap \X_2$ since $C_1 \subset \X_1$ and $C_2 \subset \X_2$; we need to show that $\X_1 \cap \X_2$ does not contain anything else.
Choose any variable $Y \in \X_1 \cap \X_2$. 
% Suppose, for contradiction, that it did contain some $Y \notin C_1 \cap C_2$. 
From the definitions of $\X_1$ and $\X_2$, this means $Y$ is a member of some cluster $C_1' \in \C_1$, and also a member of a cluster $C_2' \in \C_2$ in the other partition.  
Since $(\C, \cal T)$ is a tree decomposition, every cluster along the unique path from $C_1'$ to $C_2'$ (which must traverse $e$) hence must contain $C_1' \cap C_2'$ (property 2 of a tree decomposition), and, in particular, $Y$.  In particular, $C_1$ and $C_2$ both lie on this path, and therefore $Y \in C_1 \cap C_2$.


% This is stronger than we need, readily implying that $C_1$ and $C_2$ are independent given $C_1 \cap C_2$ 
Since $\mat X \subset $
(again, for instance, by CIRV2 in Theorem 4.4.4 of \textcite{halpern2017reasoning}).
