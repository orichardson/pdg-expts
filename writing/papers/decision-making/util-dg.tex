\documentclass{article}

\input{pdg-preamble-v1}

\usepackage{rotating,stackengine,scalerel}
\newcommand\atleD{\scalerel*{\stackengine{-1pt}{%
  \rotatebox[origin=c]{30}{\rule{10pt}{.9pt}}\kern-1pt%
  \rotatebox[origin=c]{-30}{\rule{10pt}{1.3pt}}}{%
  \rule{.9pt}{10pt}}{O}{c}{F}{F}{S}}{\Delta}}

\usepackage[margin=0.9in]{geometry}
\usepackage{parskip}
\usepackage{booktabs}

\newcommand\UEd{\mathcal U\!\!\Ed}
\newcommand{\Boltz}[1]{\mathbb G {#1}}

\begin{document}
    \twocolumn
    
    
    \section{}
    \begin{defn}[softmax]
      For an indexed collection $(s_i)_{i \in I}$ of real numbers and a ``temperature'' $T \in \mathbb R$ , let $\oplus$ denote the $T$-\emph{softmax} operation:
      \[ \bigoplus_{i \in I} s_i  := T \log \left[ \sum_{i \in I} \exp \left( \frac{1}{T} s_i \right)\right] 
        \] 
      and take $T=1$ if unspecified.
    \end{defn}
    
    It is called softmax because it is a smooth approximation to a `maximum'. When the temperature goes to zero, it converges uniformly to $\max$. 
    \begin{prop}
      $\displaystyle
        \lim_{T \to 0} \mathop{\overset T\bigoplus}\limits_{i \in I} s_i = \max_{i \in I} s_i
      $
    \end{prop}
  
  
    \begin{prop}
      $\oplus$ is associative and commutative. 
    \end{prop}
    \begin{proof}%
      % \vspace{-3ex}
      Commutativity is immediate due to commutativity of $+$, and to see associativity:
      \begin{align*}
        a \oplus (b \oplus c) &= \log\Big( \exp(a) + \exp( \log(e^b + e^c)) \Big) \\
          &=  \log\Big( e^a + e^b + e^c \Big)   
          \qedhere
      \end{align*}
    \end{proof}
    
    %%% Somewhat difficult
    % \section{THE THERMODYNAMIC PICTURE}
    \section{A Thermodynamic Picture}
    
    \begin{table*}[t]
    \begin{tabular}{ccc}
      \textbf{Economics} & \textbf{Thermodynamics}  & \textbf{Examples from PDG formalism} 
        \\\hline
      coefficient of rationality $\beta$ &  inverse temperature / coldness & $\beta$
        \\
      disutility $U(X)$ & energy landscape $U(X)$ &  $\Inc, \IDef{}, \aar{~\cdot~}$ 
        \\       
      probability & probability & probabilities
    \end{tabular}
    % \medskip
  \end{table*}
    
    
    \textbf{Converting Between Probabilities and Utilities.}
    
    
    \textbf{An Inner Product}
    If $p(X) : \Delta X$ is a probability on $X$, and $u(X) : \mathbb R^{X}$ is a utility function, there's a natural inner product between them:
    \[ 
      \langle p, u \rangle := \Ex_p [u(X)] 
    \]
    
    \begin{prop}
      % If $u = \Bolz p$, 
      $\displaystyle\langle p,  \I p \rangle = \H(p)$.
      % = \sum_x p(x) \log \frac{1}{p(x)} 
    \end{prop}
    % \begin{prop} 
    Similarly, we have
    \[
      \langle \Boltz u, u \rangle
        = \frac{\sum_x \exp(-u(x)) u(x) }{\sum_{x'} \exp (-u(x))}
    \]
    % \end{prop} 
    
    % \textbf{Marginalization and Conditioning for Utilities.}
    \begin{defn}[Marginal and Conditional for Utilities]      
    Given a utility function $U(X,Y)$, define the marginal $U(X) := \bigoplus_y U(X,y)$, 
    and the ``conditional utility'',\\
    $U(Y|X) := U(X,Y) - U(X)$.
    \end{defn}
    
    
    
    These are the ``right analogues'' in the sense that they are preserved by our canonical ways of turning a utility to a probability, and vice versa. 
    
    \begin{prop}
      Conditioning and Marinalization commute with  $\Boltz{}$ and $\I$, which is to say that the following diagrams all commute. 
      
      \begin{center}
        \begin{tikzcd}
          \Delta (X,Y) \ar[r,"|_{\!X}"]  \ar[d, "\I"]
          & \Delta (Y|X) \ar[d, "\I"]
          \\
          \atleD (X,Y) \ar[r,"|_{\!X}"] 
          \ar[u, bend left, "\Boltz{}"]
          & \atleD (Y|X)
          \ar[u, bend left, "\Boltz{}"]
        \end{tikzcd}
        \quad
        \begin{tikzcd}
          \Delta (X,Y) \ar[r,"\Sigma_Y"]  \ar[d, "\I"]
          & \Delta (X) \ar[d, "\I"]
          \\
          \atleD (X,Y) \ar[r,"\oplus_Y"] 
          \ar[u, bend left, "\Boltz{}"]
          & \atleD (X)
          \ar[u, bend left, "\Boltz{}"]
        \end{tikzcd}
      \end{center}
    \end{prop}
    \begin{proof}
       
    \end{proof}
    
    
        
    % \section{A ``Standard'' Approach:\texorpdfstring\\{}
    %     Adding Utilities to PDGs}
    \onecolumn
    \section{Utility Analogues of PDGs}
    
    \begin{defn}
        A \emph{Utility Dependency Graph (UDG)}  $\dg U = (\N, \V, \UEd, U, \lambda)$ is the utility analog of a PDG.  As before, $\N$ is a set of variables, and $\V(X)$ gives the set of possible values each $X \in \N$ can take. 
        Also as before, it has labeled edges $\UEd = \{ \ed {L}XY \} $. But instead of encoding cpds with confidences, they give utilities: for each edge $\ed LXY \in \UEd$, we have 
        \begin{itemize}
            \item  a conditional utility function $U_L(Y|X)$%
                % \footnote{Note: right now, this is just a joint utility function $U(X,Y)$ because of currying, unless we do something clever}%
            ;
            \item and a relative importance $\lambda_L$.  \qedhere
        \end{itemize}
    \end{defn}

    By analogy with the PDG semantics, the next thing we want is a semantics that scores joint utility functions $U : \V(\dg U) \to \mathbb R$, based on how compatible they are with the partial conditional utilities along the edges. 
    
    First, we need a way to measure how different utility functions are. That is,  Suppose $U(X)$ and $V(X)$ are two utility functions on $X$, then we want some function $\thickD(U,V) \ge 0$ such that $\thickD(U,U) = 0$, that assigns higher numbers to pairs of functions that are, in some sense, ``more different''.
    
    Putting aside the precise choice of $\thickD$ we can write the semantics as 
    \begin{equation}
        \bbr{\dg U}(U) =
             % \sum_{\ed LXY \in \UEd} \lambda_L \Big( U(y|x) - \mathcal U \Big)
             % \sum_{\omega \in \V(\N)}\sum_{\ed LXY \in \UEd} \lambda_L\, \thickD\Big( U_L(Y(\omega) |X(\omega)),  U(\omega) \Big)
             \sum_{\ed LXY \in \UEd} \lambda_L\, \sum_{\substack{x \in \V(X)\\z \in \V(\N\setminus XY)}}
             \thickD\Big( U_L(Y | x),  U(x,Y,z) \Big)
    \end{equation}

    Note this implicitly has a uniform measure baked into it. More generally (and compactly) for a fixed measure $\mu$ --- which would be available if we merged the two scoring functions --- we have:
    \begin{equation}
        \bbr{\dg U}(U) =
             \sum_{\ed LXY \in \UEd} \lambda_L\, \Ex_{\substack{x,\mat z \sim \mu(\N \setminus Y)}}
             \thickD\Big( U_L(Y | x),  U(x,Y, \mat z) \Big)
    \end{equation}
    
    Or, if it's not necessary that the utilities be equal for all other values $\mat z$, but only on average, as was the case for PDGS, we get:
    \begin{equation}
        \bbr{\dg U}(U) =
             \sum_{\ed LXY \in \UEd} \lambda_L\, \Ex_{x \sim \mu(X)}
             \thickD\Big( U_L(Y | x),  \Ex_{\mat z \sim \mu(\N\setminus XY|x)}U(x,Y, \mat z) \Big)
    \end{equation}

     
    Now, back to the problem at hand --- what should we use for $\thickD$? Because we only take utility functions seriously up to positive affine transformations, one desirable feature is to be invariant to affine transformations.
    Note something of this flavor is necessary for $U(Y|X)$ to be any different from $U(X|Y)$: unless we take this affine invariance seriously, both are essentially just curried versions of a joint utility function $U(X,Y)$, making the arrow meaningless.
    
    Here are some possibilities for scoring function $\thickD$.
    
    \begin{enumerate}
    \item A silly first approach: try to find an affine transformation, parameterized by $a > 0$, $b \in \mathbb R$, such that $a U + b = V$. 
     \begin{equation*}
        \thickD(U, V) := \inf_{\substack{a > 0\\b \in \mathbb R}}  \frac{1}{|\V(X)|}\sum_{x \in \V(X)} \Big| a U(x) + b - V(x) \Big|^2
        \end{equation*}
        This definition has some problems: most egregiously, it is $V$'s units -- so if V contains large numbers, then this measure will will also be large. So in particular it is not invariant to affine transformations in its second argument. One fix:
        \[
        \thickD(U, V) := \inf_{\substack{a > 0\\b \in \mathbb R}}  \frac{1}{|\V(X)|}\sum_{x \in \V(X)} \Big| a U(x) + b - \frac1a V(x) \Big|^2
        \]
    \item Assume $U$ and $V$ have minimum zero and maximum 1, and take 
    \[ \thickD(U,V) := \big\Vert U - V \big\Vert^2_2 = \Ex_\mu \Big[(U(X) - V(X))^2\Big]  \]
    \item If $U$ is an affine transform of $V$, then the ratio of differences $\frac{U(x) - U(x')}{V(x)- V(x')}$ should be a constant independent of $x,x'$. We can measure its departure from the constant function by:
    \[
        % \thickD(U,V) := \Ex_{\substack{x \sim \mu \\ x' \sim \mu}} \left( 1-  \frac{U(x) - U(x')}{V(x)- V(x')}\right)^2
        \thickD(U,V) := \Ex_{\substack{x' \sim \mu \\ x \sim \mu}} \log \frac{U(x) - U(x')}{V(x)- V(x')} - 
         \log \bigg(\Ex_{\substack{x' \sim \mu \\ x \sim \mu}}\frac{U(x) - U(x')}{V(x)- V(x')} \bigg),
    \]
    which looks complicated, but is essentially the commutator of the expectation and logarithm, which is strictly positive because $\log$ is concave, and analogous to the approach of relative entropy, which can be written 
    \[ \kldiv\mu\nu = \Ex_\mu \log \frac\mu\nu - \underbrace{\log \Ex_\mu \frac{\mu}{\nu}}_{=0}. \]

    \item Again that $U$ and $V$ have min zero and max one. Define the correlation as $\mathrm{Cor}(U,V) := \Ex_{\mu}[U(X)V(X)]$ and take 
    \[
        \thickD(U,V) := - \log \mathrm{Cor}(U,V) = - \log \Ex_\mu [U(X) V(X)]
    \]
    Why the logarithm? Good question. It's not a super principled choice, but it does turn perfect correlation of 1 into zero inconsistency, and correlation less than 1 to positive inconsistency (note: because $U$ and $V$ are positive, this non-centered notion of correlation cannot be negative). 
\end{enumerate}

\end{document}
