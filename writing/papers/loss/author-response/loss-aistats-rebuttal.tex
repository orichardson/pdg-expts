\documentclass{article}

\relax% header
	\usepackage{aistats2022_author_response}
		\usepackage[utf8]{inputenc} % allow utf-8 input
		\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
		\usepackage{hyperref}       % hyperlinks
		\usepackage{url}            % simple URL typesetting
		\usepackage{booktabs}       % professional-quality tables
		\usepackage{amsfonts}       % blackboard math symbols
		% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
		\usepackage{microtype}      % microtypography
		% \usepackage{xcolor}         % define colors in text
		\usepackage{xspace}         % fix spacing around commands

	\relax % tikz
		\usepackage[dvipsnames]{xcolor}
		\usepackage{tikz}
			\usetikzlibrary{positioning,fit,calc, decorations, arrows, shapes, shapes.geometric}
			\usetikzlibrary{cd}

			%%%%%%%%%%%%
			\tikzset{AmpRep/.style={ampersand replacement=\&}}
			\tikzset{center base/.style={baseline={([yshift=-.8ex]current bounding box.center)}}}
			\tikzset{paperfig/.style={center base,scale=0.9, every node/.style={transform shape}}}

			% Node Stylings
			\tikzset{dpadded/.style={rounded corners=2, inner sep=0.7em, draw, outer sep=0.3em, fill={black!50}, fill opacity=0.08, text opacity=1}}
			\tikzset{dpad0/.style={outer sep=0.05em, inner sep=0.3em, draw=gray!75, rounded corners=4, fill=black!08, fill opacity=1, align=center}}
			\tikzset{dpadinline/.style={outer sep=0.05em, inner sep=2.5pt, rounded corners=2.5pt, draw=gray!75, fill=black!08, fill opacity=1, align=center, font=\small}}

		 	\tikzset{dpad/.style args={#1}{every matrix/.append style={nodes={dpadded, #1}}}}
			\tikzset{light pad/.style={outer sep=0.2em, inner sep=0.5em, draw=gray!50}}

			\tikzset{arr/.style={draw, ->, thick, shorten <=3pt, shorten >=3pt}}
			\tikzset{arr0/.style={draw, ->, thick, shorten <=0pt, shorten >=0pt}}
			\tikzset{arr1/.style={draw, ->, thick, shorten <=1pt, shorten >=1pt}}
			\tikzset{arr2/.style={draw, ->, thick, shorten <=2pt, shorten >=2pt}}

			\newcommand\cmergearr[5][]{
				\draw[arr, #1, -] (#2) -- (#5) -- (#3);
				\draw[arr, #1, shorten <=0] (#5) -- (#4);
				}
			\newcommand\mergearr[4][]{
				\coordinate (center-#2#3#4) at (barycentric cs:#2=1,#3=1,#4=1.2);
				\cmergearr[#1]{#2}{#3}{#4}{center-#2#3#4}
				}
			\newcommand\cunmergearr[5][]{
				\draw[arr, #1, -, shorten >=0] (#2) -- (#5);
				\draw[arr, #1, shorten <=0] (#5) -- (#3);
				\draw[arr, #1, shorten <=0] (#5) -- (#4);
				}
			\newcommand\unmergearr[4][]{
				\coordinate (center-#2#3#4) at (barycentric cs:#2=1.2,#3=1,#4=1);
				\cunmergearr[#1]{#2}{#3}{#4}{center-#2#3#4}
				}

	\relax % Most oli packages
	    \usepackage{mathtools}
	    \usepackage{amssymb}
			\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}
	    \usepackage{bbm}
	    \usepackage{graphicx}
	    \usepackage{scalerel}
	    \usepackage{enumitem}
	    \usepackage{nicefrac}\let\nf\nicefrac

	    \usepackage{color}
	    %\usepackage{stmaryrd}
	    \usepackage{hyperref} % Load before theorems...
	        \hypersetup{colorlinks=true, linkcolor=blue!75!black, urlcolor=magenta, citecolor=green!50!black}

	\usepackage{amsthm,thmtools} % Theorem Packages
		\usepackage[noabbrev,nameinlink,capitalize]{cleveref}
	    \theoremstyle{plain}
	    \newtheorem{theorem}{Theorem}
		\newtheorem{coro}{Corollary}[theorem]
	    \newtheorem{prop}[theorem]{Proposition}
	    \newtheorem{claim}{Claim}
	    \newtheorem{remark}{Remark}
	    \newtheorem{lemma}[theorem]{Lemma}
	    \theoremstyle{definition}
	    % \newtheorem{defn}{Definition}
	    % \declaretheorem[name=Definition]{defn}
	    \declaretheorem[name=Definition, qed=$\square$]{defn}

		\crefname{defn}{Definition}{Definitions}
		\crefname{prop}{Proposition}{Propositions}

	\relax %%%%%%%%% GENERAL MACROS %%%%%%%%
	    \let\Horig\H
		\let\H\relax
		\DeclareMathOperator{\H}{\mathrm{H}} % Entropy
		\DeclareMathOperator{\I}{\mathrm{I}} % Information
		\DeclareMathOperator*{\Ex}{\mathbb{E}} % Expectation
		\DeclareMathOperator*{\EX}{\scalebox{1.5}{$\mathbb{E}$}}

	    \newcommand{\mat}[1]{\mathbf{#1}}
	    \DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
			#1\;\delimsize\|\;#2%
		}
		\newcommand{\thickD}{I\mkern-8muD}
		\newcommand{\kldiv}{\thickD\infdivx}
		\newcommand{\tto}{\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}

		\newcommand{\datadist}[1]{\Pr\nolimits_{#1}}
		% \newcommand{\datadist}[1]{p_\text{data}}

		\makeatletter
		\newcommand{\subalign}[1]{%
		  \vcenter{%
		    \Let@ \restore@math@cr \default@tag
		    \baselineskip\fontdimen10 \scriptfont\tw@
		    \advance\baselineskip\fontdimen12 \scriptfont\tw@
		    \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
		    \lineskiplimit\lineskip
		    \ialign{\hfil$\m@th\scriptstyle##$&$\m@th\scriptstyle{}##$\hfil\crcr
		      #1\crcr
		    }%
		  }%
		}
		\makeatother
		\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

	\relax %%%%%%%%%   PDG  MACROS   %%%%%%%%
		\newcommand{\ssub}[1]{_{\!_{#1}\!}}
		% \newcommand{\bp}[1][L]{\mat{p}_{\!_{#1}\!}}
		% \newcommand{\bP}[1][L]{\mat{P}_{\!_{#1}\!}}
		\newcommand{\bp}[1][L]{\mat{p}\ssub{#1}}
		\newcommand{\bP}[1][L]{\mat{P}\ssub{#1}}
		\newcommand{\V}{\mathcal V}
		\newcommand{\N}{\mathcal N}
		\newcommand{\Ed}{\mathcal E}

		\DeclareMathAlphabet{\mathdcal}{U}{dutchcal}{m}{n}
		\DeclareMathAlphabet{\mathbdcal}{U}{dutchcal}{b}{n}
		\newcommand{\dg}[1]{\mathbdcal{#1}}
		\newcommand{\PDGof}[1]{{\dg M}_{#1}}
		\newcommand{\UPDGof}[1]{{\dg N}_{#1}}
		\newcommand\VFE{\mathit{V\mkern-4mu F\mkern-4.5mu E}}

		\newcommand\Inc{\mathit{Inc}}
		\newcommand{\IDef}[1]{\mathit{IDef}_{\!#1}}
		% \newcommand{\ed}[3]{%
		% 	\mathchoice%
		% 	{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{${#1}$}}}{\xrightarrow{\hphantom{\scriptstyle {#1}}}} #3} %display style
		% 	{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptstyle {#1}}}} #3}% text style
		% 	{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptscriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptscriptstyle {#1}}}} #3} %script style
		% 	{#2\overset{\smash{\mskip-5mu\raisebox{-3pt}{$\scriptscriptstyle {#1}$}}}{\xrightarrow{\hphantom{\scriptscriptstyle {#1}}}} #3}} %scriptscriptstyle
		\newcommand{\ed}[3]{#2%
		  \overset{\smash{\mskip-5mu\raisebox{-1pt}{$\scriptscriptstyle
		        #1$}}}{\rightarrow} #3}

	    \newcommand{\nhphantom}[2]{\sbox0{\kern-2%
			\nulldelimiterspace$\left.\delimsize#1\vphantom{#2}\right.$}\hspace{-.97\wd0}}
			% \nulldelimiterspace$\left.\delimsize#1%
			% \vrule depth\dp#2 height \ht#2 width0pt\right.$}\hspace{-.97\wd0}}
		\makeatletter
		\newsavebox{\abcmycontentbox}
		\newcommand\DeclareDoubleDelim[5]{
		    \DeclarePairedDelimiterXPP{#1}[1]%
				{% box must be saved in this pre code
					\sbox{\abcmycontentbox}{\ensuremath{##1}}%
				}{#2}{#5}{}%
			    %%% Correct spacing, but doesn't work with externalize.
				% {\nhphantom{#3}{##1}\hspace{1.2pt}\delimsize#3\mathopen{}##1\mathclose{}\delimsize#4\hspace{1.2pt}\nhphantom{#4}{##1}}
				%%% Fast, but wrong spacing.
				% {\nhphantom{#3}{~}\hspace{1.2pt}\delimsize#3\mathopen{}##1\mathclose{}\delimsize#4\hspace{1.2pt}\nhphantom{#4}{~}}
				%%% with savebox.
			    {%
					\nhphantom{#3}{\usebox\abcmycontentbox}%
					\hspace{1.2pt} \delimsize#3%
					\mathopen{}\usebox{\abcmycontentbox}\mathclose{}%
					\delimsize#4\hspace{1.2pt}%
					\nhphantom{#4}{\usebox\abcmycontentbox}%
				}%
		}
		\makeatother
		\DeclareDoubleDelim
			\SD\{\{\}\}
		\DeclareDoubleDelim
			\bbr[[]]
		% \DeclareDoubleDelim
		% 	\aar\langle\langle\rangle\rangle
		\makeatletter
		\newsavebox{\aar@content}
		\newcommand\aar{\@ifstar\aar@one@star\aar@plain}
		\newcommand\aar@one@star{\@ifstar\aar@resize{\aar@plain*}}
		\newcommand\aar@resize[1]{\sbox{\aar@content}{#1}\scaleleftright[3.8ex]
			{\Biggl\langle\!\!\!\!\Biggl\langle}{\usebox{\aar@content}}
			{\Biggr\rangle\!\!\!\!\Biggr\rangle}}
		\DeclareDoubleDelim
			\aar@plain\langle\langle\rangle\rangle
		\makeatother


\begin{document}
\textbf{Modeling Workflow.}
\textbf{Confidences.}
Reviewers \#5 and \#7 raise an important aspect in which the presentation could be improved: we ought to give more intuition about the confidence parameter $\beta$ (and clarify the auxiliary role of $\alpha$).

Choosing $\beta=0$ for a cpd $p$ means that $p$ is effectively ignored, in the sense that a PDG is semantically equivalent to one without $p$.  On the other hand, setting $\beta$ to a large real number (or $\infty$) indicates high (or absolute) confidence in $p$.  This is particularly appropriate if $p$ is a trusted empirical distribution and the other edges of the cpd are randomly initialized neural networks.   In other cases, different choices of $\beta$ may be appropriate; perhaps one has a very good pre-trained model ($q$) and partially corrupted data ($p$).
In such a case, we might give both $p$ and $q$ the same confidence, and thereby obtain the Bhattacharya distance, a symmetric measure that is less sensitive .
%%%% TODO %%%%
We refer to Figure 1 for a detailed picture of how different choices of $\beta$% impact the choice of loss function for generative models.

Reviewer \#7 asks about our choice of default values of $\beta=1$ and $\alpha=0$.
The default choice of $\beta = 1$ is essentially a convenient choice of units---a default of $\beta =2$ would behave the same way, since doubling every $\beta$ simply doubles the inconsistency.  What’s important are the magnitudes of $\beta$ relative to each other (and to $\gamma>0$, for $\gamma$-inconsistency).


The default choice of $\alpha$ is even \emph{less} consequential, because $\alpha$ does not affect the inconsistency $\aar{\dg M}$ (it only affects the $\gamma$-inconsistency $\aar{\dg M}_\gamma$ for $\gamma > 0$, which we use only where $\alpha$ more carefully in sections 7 and 8).
It follows that all results in Sections 2-6 also hold for other choices of $\alpha$, including ones that better reflect one's belief in the functional dependencies.
We choose to set $\alpha=0$ by default because it is a neutral choice that doesn’t suggest any particular causal model.%
	\footnote{Observe: when $\alpha = 0$, the qualitative scoring function ($\IDef{}$) reduces to negative entropy, and so in the quantitative limit ($\gamma \to 0$) the best distribution will be the one of maximum entropy among those that minimize incompatibility.}
Still, as Reviewer \#7 points out, the modeler might be certain of the functional dependencies, which, confusingly, clashes with this default.  To fix this, we intend to explain more directly that $\gamma$-inconsistency can only depend on $\alpha$ if $\gamma > 0$, and avoid giving a default value.

Reviewer \#7 also asks specifically about the choices of $\beta$ in Prop 11. The short answer is that the choices reflect the rationale for doing variational inference in the first place.  We couldn’t compute the inconsistency of the PDG with only $x$ and $p$, so we guess $q(Z)$ so as to draw samples from $Z$;  but this only helps if we insist that $Z$ is really distributed according to $q$ (i.e., we set $\beta_q := \infty$). At this point, changing $\beta_p$ from 1 to some $k \in \mathbb R$ simply multiplies the inconsistency by $k$, as discussed above; the default value of $\beta=1$ is simply a convenient choice of units.


\textbf{Motivating VAEs.}
Reviewer \#3 points out that a VAE is already associated with a graphical model.
While this is true, that graphical model is only a simple Bayesian Network made up of the prior and decoder; and encodes neither the encoder nor the loss function---it is associated with the VAE for good reason, but falls short of capturing it.  Our observation: simply include the encoder in addition (as is only possible in a PDG), and one also captures the loss function for free.

Reviewer \#3 also pushes back on our claim that the ELBO is difficult to motivate, with reference to the fact that the ELBO is a tractable lower bound on the log likelihood (a simple derivation that we give in Section 6.1).   This is indeed the standard motivation, but it is arguably incomplete---the constant zero satisfies both criteria, but is a poor loss function. Of the many lower bounds on the log likelihood, some of which are both tractable and tight, why use this one? What does the ELBO itself mean? Does it have units? Why is it negative?  Why use the logarithm instead of another concave function?  There are good answers to these questions, but not, in our view, simple ones.

\end{document}
