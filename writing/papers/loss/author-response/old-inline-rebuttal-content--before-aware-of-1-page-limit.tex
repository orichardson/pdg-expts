\begin{center}
    {\Large
    Author Response for Submission 479}\\
    % (Loss as the Inconsistency of a Probabilistic Dependency Graph: Choose your Model, not your Loss Function)
\end{center}
    % \section{Individual Responses}
\section*{Reviewer \#3}

\begin{reviewer says}
    %\color{blue}
    \textbf{[Component 3: Weaknesses]}~
    While I think the contribution of rethinking different losses as inconsistency is a valuable one, I think it is not fair for the paper to say that the ELBO is difficult to motivate. It has a justified explanation of being a tractable lower bound to the likelihood objective (and the derivation is fairly straightforward).
\end{reviewer says}

% The fact that the ELBO is a tractable lower bound is only part of the story. For instance, the constant function 0 satisfies both criteria, but is an extremely poor loss function.
% We agree that the standard derivation of the fact that the ELBO is a lower bound on the likelihood, which we present in Section 6.1, is a straightforward application of Jensen’s inequality.
% Nevertheless, as motivation goes, it leaves something to be desired.
% The fact that the ELBO is a tractable lower bound is only part of the story. For instance, the constant function 0 satisfies both criteria, but is an extremely poor loss function.
We agree that the standard derivation of the fact that the ELBO is a lower bound on the likelihood, which we present in Section 6.1, is a straightforward application of Jensen’s inequality.
Nevertheless, as motivation goes, it leaves something to be desired.

Why apply Jensen’s inequality and not some other bound?
Of the many lower bounds of the log likelihood, some of which may be both tractable and tight in some sense, why use this one?
What does the ELBO itself mean? Why is it negative?  If $p$ and $q$ are densitites, why does the quantity in the logarithm depend on the choice of units?
There are reasonably good answers to these questions, but they are not all simple.
The point remains: a proper motivation of the ELBO involves much more than the derivation of the bound itself.

% As further anecdotal evidence, we submit that it is easy to get confused and write the bound backwards, but

\begin{reviewer says}
    Also, the VAE does correspond to a graphical model (with specific choices made for inference).
\end{reviewer says}

% While this is technically correct,
While we do not dispute this,
the phrase ``specific choices made for inference'' does a lot of work, and so this framing sweeps much of the identity of a VAE under the rug.
Reviewer \#3 is right to point out that there is a standard sense in which a VAE corresponds to a graphical model consisting of the prior and the decoder; the likelihood of this graphical model is the one that appears in the variational bound.
That graphical model, while important to the motivation and bound, is only a simple Bayesian Network representing some components of the VAE, and does not correspond to the VAE itself.
The encoder does not even appear in it.

By contrast, the PDG consisting of the encoder, decoder, and prior, incorporates ALL of the relevant information, and also the loss function.
As a result, we argue that it captures the notion of a VAE in a much more complete and symmetrical way.


\begin{reviewer says}
    While the paper discusses the interpretations of losses as PDGs, it does little justification about why this whole machinery is useful in practical terms. It is also not clear how this aids learning or inference in any way. If this work is only intended to aid pedagogy, it would be useful to mention that. Otherwise, it would be helpful to be concrete about (maybe by use of an example) how this fits into a modeling workflow.
\end{reviewer says}


\begin{reviewer says}
    \textbf{[6: Relation to prior work]} The paper does not provide much in terms of previous work related to PDGs, perhaps because this is a new framework.
\end{reviewer says}

This is true: PDGs are indeed new framework, and we are not aware of any published work beyond the 2021 paper that we reference.  We will attempt to clarify this in


\section*{Reviewer \#4}

\begin{reviewer says}
    \textbf{[Component 3: Weaknesses]} This paper unifies many standard objective functions with inconsistency measure equipped with related PDGs. It's not bad to observe different losses through the same lens, but it's lack of discussion on the advantages of doing so.
\end{reviewer says}

\begin{reviewer says}
    If choosing loss function for a task is tend to empirical, would this measure of PDG inconsistency make the choice less opaque? and how?
\end{reviewer says}

\begin{reviewer says}
    Can we see an example of real cases, for example, by changing the PDG, a different loss naturally arises, and it works probably better than the old loss?
\end{reviewer says}
\section*{Reviewer \#5}

\begin{reviewer says}
 I was a bit confused about the concept of ‘subjective confidence’ (denoted as $\beta$). If I understood correctly, when choosing a distribution for the data we implicitly give an ‘infinite confidence’ to this distribution. What would other choices, i.e. other real values for beta, represent? E.g. a big number or zero? How would these choices affect the objects introduced (standard metrics, statistical distances, variational objectives)?
\end{reviewer says}

\section*{Reviewer \#6}
\section*{Reviewer \#7}

\begin{reviewer says}
    This is a relatively minor point but I believe the motivation behind the specific settings of the confidence parameters $\alpha$ and $\beta$ could be explained in more detail. For example, it is not obvious to me why the default choices for these parameters are $\alpha = 0$ and $\beta = 1$. It seems odd to me that by default we have zero confidence in our functional dependencies. Similarly, in Proposition 11 it is not obvious to me why we have $\beta = \infty$ for $q$ but only $\beta = 1$. I think it would be help if the parameters $\alpha$ and $\beta$ could be explained in more detail (it is fine if this is added to the Appendix as there is limited space in the main paper).
\end{reviewer says}

Reviewer \#7 raises an important shortcoming in the presentation: the values of the confidence parameters. In the meantime, here are some details to assuage some concerns.
First, note that doubling every $\beta$ (as well as $\gamma$) has the straightforward effect of doubling the ($\gamma$-)scoring function, and consequently the ($\gamma$-)inconsistency.

In some sense, every positive real number works equally well as a default value of $\beta$, and our choice of . The important features are relative


Second, the values of $\alpha$ do not affect the inconsistency. (Note that they do not appear in equation (1) or in the definition of inconsistency $\aar{~\cdot~}$; they only become relevant for $\gamma$-inconsistency $\aar{~\cdot~}_\gamma$ for $\gamma > 0$.)
We chose to set $\alpha=0$ by default simply because it suggests a lack of qualitative modeling; in this case, the qualitative scoring function becomes $\IDef{\dg M}(\mu) = -\H(\mu)$, which corre.
Of course, all of the propositions also hold for other choices of $\alpha$, including ones that better reflect one's belief in the functional dependencies.  This is a point which should be incorporated into the main paper.


Given that the results don’t depend on  $\alpha$, we were tempted to leave it out entirely, but we decided that it was still worth including, for a few reasons:
\begin{itemize}
    \item The choice of $\alpha$ is relevant for $\gamma$-inconsistency when $\gamma > 0$, which we employ in sections 7 and 8. In both sections, we give set $\alpha$ more precisely.
    \item Even in the quantitative limit ($\gamma \to 0$), where $\alpha$ does not affect the inconsistency, the \emph{optimal distribution} may still depend on $\alpha$. Setting $\alpha=1$ is important to capture the independencies of a Bayesian Network, for instance.
\end{itemize}


We address the question about Proposition 11 directly, since it is something of a special case.
According to the usual account, one is not really interested in


choice makes it possible to sample z efficiently.
