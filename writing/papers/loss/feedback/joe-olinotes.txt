
COMENTS ON OBJECTIEV PDG PAPER

	Ditch current introduction.
		There are all
		
		Framework for thinking about the right loss function? What's the right metric to use?
		
	Really focus on the benefits of thinking this way
	
	Want Readers to get the Message: if you didn't care about PDGs before, but you care about loss functions, you should now care about PDG semantics.
	
	Want some examples where common to use this reguarizer -> get the same effect by modifying the PDG to get performance
	
	In the PDG approach it makes sense to encode your knowledge in the PDG rather than using regularizers. 
	
	Technical result: can get regularizer in inconsistency by including the associated prior
	
	
	Philosophical question: Is there a role for useful fictions in a knowledge representation device?

Joe goes Yuk to notation
	(\beta : 0.7)
	exclcamation point
	definitely add intuition about limit as \beta -> infinity as high confidence
	
	Lemma 1 seems to be a nonsequitor. Expecting more intuition, not random fact. Move to first place used.
	
	Before Prop 2 pull out definition. Move sentence afterwards. Add references.
	
	Prop 4 part 4 seems specific. Make sure that the reader gets the story in your head.
	
	Say the connection between proposition 2 and 4.
	You can think of prop 2 as having a sample of size one. Imagine you have a big sample. ... we can now generalize...
	
	Absolutely need to discuss that you trust data.
	Briefly talk about how other confidences are possible before statistical distance section.