%BEGIN_FOLD
%oli8: this line allows me to use plain BibTex without reconfiguring my own settings.
% !TeX TXS-program:bibliography = txs:///bibtex
\documentclass{article}

%%%%%%%%%%%% FORMATTING INCOMPATIBLE WITH NEURIPS TEMPLATE %%%%%%%%%
%\usepackage[margin=1in]{geometry}
%\usepackage{parskip}
%\usepackage{lmodern}
%\setlength{\skip\footins}{1cm}
%\setlength{\footnotesep}{0.4cm}

%oli8: remove nonatbib option
\usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%joe4: I'm guessing that this part of the reason I'm having many
%problems.  Please don't use nonstandard packages!
%oli5: This is standard, it's just newer. Biblatex (+biber) is the
%much more powerful engine, is actively being developed (unlike
%bibtext/natbib) and the reason that they provide a nonatbib option to
%the NeurIPS template. As far as I can tell, the only reason not to
%use biber is compatibility with BibTex. See
%https://tex.stackexchange.com/questions/25701/bibtex-vs-biber-and-biblatex-vs-natbib.   
% It would be a lot of work to revert back to bibtex, and from my
% perspective would only only do fewer things. If you want to be able
% to compile the document, my preference would be for you to set up
% biblatex/biber, or alternatively set up an overleaf document with
% the correct setup.
%joe5: What extra functionality do you want/need?  Why would it take
%work to revert to bibtex?
%oli6: I switched the backend of the biblatex backend from biber to
%bibtex (no supported, but this should make it possible for you to
%compile the document); let me know if this works for you.
%joe6: unfortunately, it doesn't; I'll send you the log file.  I'm
%going to make it your problem to ensure that I can compile the
%paper.  I do *not* want to reconfigure anything, and I've never had
%any problems anywhere with citations.
%oli6: Things like the fully colored link including the cite style and
%parentheses are much harder to do in BibTex. In general bibLatex is
%strictly more powerful and bug-free (but biber, the backend that is
%highly recommended, requires configuration).
%joe6: The gain is *much* less than the cost if it's going to cause
%incompatibility problems, as far as I'm concerned.
%joe5: What are differences between the overleaf document and this document (which I can't latex)? 
%oli6: The overleaf document is an exact replica of this one, that you
%can modify online and recompile there. Many people I know use
%overleaf exclusively to write TeX precisely becuse it solves
%configuration issues.
%joe6: I don't not want to use overleaf to work on papers.   I
%have a lot of emacs macros, so prefer to work on paper locally.  I
%suppose that I could upload to overleaf to latex the paper.  But you
%really do need to figure out why I can't latex the paper.  My setup
%is quite vanilla, so you'll doubtless have this problem with other
%coauthors.  There is a cost to stubbornly being different, even if
%you think it's better.  I would argue strongly (the features that you
%seem to care about) are not worth the cost (annoying coauthors)
%joe5: I've written thousands of latex documents, and it's only with you and
%Matvey that I've had prolems latexing.
%oli8: removed biblatex.
%\usepackage[backend=bibtex,
%	style=alphabetic, %authoryear
%%	citestyle=authoryear,
%%	natbib=true,
%	maxcitenames=1,
%	url=true, 
%	doi=true]{biblatex}

%joe3: all this should be unnecessary with a decent bibstyle
%oli3: possibly, but I wanted to make the year a link.
%joe4: there are much better bibstyles that avoid these problems.
%Please don't use nonstandard packages; they're causing me all sorts
%of problems
%oli5: I don't know what is and is not standard, but setting up biber has definitely been worth the investment from my perspective. With regard to the cite command, this is just because I wanted the entire command to be a link, which cannot even be done with a BibTex style, as far as I can tell ...
%oli8: moving to BibTeX
%\DeclareCiteCommand{\cite}[\mkbibparens]
%	{\usebibmacro{prenote}}
%	{\usebibmacro{citeindex}%
%		\printtext[bibhyperref]{\usebibmacro{cite}}}
%	{\multicitedelim}
%	{\usebibmacro{postnote}}

%oli8: I'm putting this all inline, so you can see what packages I'm loading.
%\input{../model-commands.tex}
\usepackage{mathtools} %also loads amsmath
\usepackage{amssymb, bbm}

%\usepackage{blkarray} % for matrices with labels
\usepackage{relsize}
\usepackage{environ}% http://ctan.org/pkg/environ; for capturing body as a parameter for idxmats
\usepackage{tikz}
	\usetikzlibrary{positioning,fit,calc, decorations, arrows, shapes, shapes.geometric}
	\usetikzlibrary{cd}
	
	\pgfdeclaredecoration{arrows}{draw}{
		\state{draw}[width=\pgfdecoratedinputsegmentlength]{%
			\path [every arrow subpath/.try] \pgfextra{%
				\pgfpathmoveto{\pgfpointdecoratedinputsegmentfirst}%
				\pgfpathlineto{\pgfpointdecoratedinputsegmentlast}%
			};
	}}
	%%%%%%%%%%%%
	\tikzset{AmpRep/.style={ampersand replacement=\&}}
	\tikzset{center base/.style={baseline={([yshift=-.8ex]current bounding box.center)}}}
	
	\tikzset{dpadded/.style={rounded corners=2, inner sep=0.7em, draw, outer sep=0.3em, fill={black!50}, fill opacity=0.08, text opacity=1}}
	\tikzset{active/.style={fill=blue, fill opacity=0.1}}
	\tikzset{square/.style={regular polygon,regular polygon sides=4, rounded corners = 0}}
	\tikzset{octagon/.style={regular polygon,regular polygon sides=8, rounded corners = 0}}
	
	
	\tikzset{alternative/.style args={#1|#2|#3}{name=#1, circle, fill, inner sep=1pt,label={[name={lab-#1},gray!30!black]#3:\scriptsize #2}} }
	
	
	\tikzset{bpt/.style args={#1|#2}{alternative={#1|#2|above}} }
	\tikzset{tpt/.style args={#1|#2}{alternative={#1|#2|below}} }
	\tikzset{lpt/.style args={#1|#2}{alternative={#1|#2|left}} }
	\tikzset{rpt/.style args={#1|#2}{alternative={#1|#2|right}} }
	\tikzset{pt/.style args={#1}{alternative={#1|#1|above}} }
	

	\tikzset{mpt/.style args={#1|#2}{name=#1, circle, fill, inner sep=1pt,label={[name={lab-#1},gray]\scriptsize #2}} }
	\tikzset{pt/.style args={#1}{name=#1, circle, fill, inner sep=1pt,label={[name={lab-#1},gray]\scriptsize #1}} }
	
		
		 %\foreach \x in {#1}{(\x) (lab-\x) } 
		 
	\tikzset{Dom/.style args={#1 (#2) around #3}{dpadded, name=#2, label={[name={lab-#2},align=center] #1}, fit={ #3 } }}
	\tikzset{bDom/.style args={#1 (#2) around #3}{dpadded, name=#2, label={[name={lab-#2},align=center]below:#1}, fit={ #3 } }}
	\tikzset{arr/.style={draw, ->, thick, shorten <=3pt, shorten >=3pt}}
	\tikzset{archain/.style args={#1}{arr, every arrow subpath/.style={draw,arr, #1}, decoration=arrows, decorate}}
	%\tikzset{every label/.append style={text=red, font=\scriptsize}}
	\tikzset{dpad/.style args={#1}{every matrix/.append style={nodes={dpadded, #1}}}}
	\tikzset{light pad/.style={outer sep=0.2em, inner sep=0.5em, draw=gray!50}}
	\tikzset{fgnode/.style={dpadded,inner sep=0.6em, circle},
	factor/.style={light pad}}	
	
	
	\newcommand\cmergearr[4]{
		\draw[arr,-] (#1) -- (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\mergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1,#2=1,#3=1.2);
		\cmergearr{#1}{#2}{#3}{center-#1#2#3}
	}
	\newcommand\cunmergearr[4]{
		\draw[arr,-, , shorten >=0] (#1) -- (#4);
		\draw[arr, shorten <=0] (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\unmergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1.2,#2=1,#3=1);
		\cunmergearr{#1}{#2}{#3}{center-#1#2#3}
	}

	
	\usetikzlibrary{matrix}
	\tikzset{toprule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] 
	            (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east);%
	        }
	    },
	    bottomrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    leftrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west);%
	        }
	    },
	    rightrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    table with head/.style={
		    matrix of nodes,
		    row sep=-\pgflinewidth,
		    column sep=-\pgflinewidth,
		    nodes={rectangle,minimum width=2.5em, outer sep=0pt},
		    row 1/.style={toprule=thick, bottomrule},
  	    }
	}

	

\NewEnviron{ctikzpicture}{\begin{center}\expandafter\begin{tikzpicture}\BODY\end{tikzpicture}\end{center}}
%\newenvironment{ctikzpicture}
%	{\begin{center}\begin{tikzpicture}}
%	{\end{tikzpicture}\end{center}}

\usepackage{color}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage[colorlinks=true, citecolor=deepgreen]{hyperref}


\usepackage{trimclip}
%joe7: I tried your suggestion, but it didn't help.
%\usepackage{stmaryrd}
%\usepackage[only,rrbracket,llbracket]{stmaryrd}
%\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}

\makeatletter
\DeclareRobustCommand{\shortto}{%
	\mathrel{\mathpalette\short@to\relax}%
}

\newcommand{\short@to}[2]{%
	\mkern2mu
	\clipbox{{.5\width} 0 0 0}{$\m@th#1\vphantom{+}{\shortrightarrow}$}%
}
\makeatother


\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{
	nameref,%\nameref
	hyperref,%\autoref
%	cleveref,% \cref
	% n.b. cleveref after! hyperref
}

%\usepackage[noabbrev,nameinlink]{cleveref}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=magenta}

% \begingroup
% \makeatletter
% \@for\theoremstyle:=definition,remark,plain\do{%
% 	\expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
% 		\addtolength\thm@preskip\parskip
% 	}%
% }
% \endgroup
% \makeatother

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{coro}{Corollary}[theorem]
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}

\theoremstyle{definition}
%oli8: end markers for definitions
%\newtheorem{defn}{Definition}[section]
%\declaretheoremstyle[ 
%	headfont=\normalfont\bfseries\itshape,
%	numbered=unless unique,
%	bodyfont=\normalfont,
%	spaceabove=1em plus 0.75em minus 0.25em,
%	spacebelow=1em plus 0.75em minus 0.25em,
%	qed={\itshape That's All Folks!},
%]{defiend}

\declaretheorem[name=Definition,numberwithin=section,qed=$\diamond$]{defn}

\newtheorem{examplex}{Example}
\newenvironment{example}
	{\pushQED{\qed}\renewcommand{\qedsymbol}{$\triangle$}\examplex}
	{\popQED\endexamplex%\vspace{-1.6em}\rule{2cm}{0.7pt}\vspace{0.5em}}
}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{xstring}
\usepackage{enumitem}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
	#1\;\delimsize\|\;#2%
}
\newcommand{\thickD}{I\mkern-8muD}
\newcommand{\kldiv}{\thickD\infdivx}%D_\mathrm{KL}


\makeatletter% Fix missing symbols i stix is loaded
\@ifpackageloaded{stix}{%
}{%
	\DeclareFontEncoding{LS2}{}{\noaccents@}
	\DeclareFontSubstitution{LS2}{stix}{m}{n}
	\DeclareSymbolFont{stix@largesymbols}{LS2}{stixex}{m}{n}
	\SetSymbolFont{stix@largesymbols}{bold}{LS2}{stixex}{b}{n}
	\DeclareMathDelimiter{\lBrace}{\mathopen} {stix@largesymbols}{"E8}%
	{stix@largesymbols}{"0E}
	\DeclareMathDelimiter{\rBrace}{\mathclose}{stix@largesymbols}{"E9}%
	{stix@largesymbols}{"0F}
}
\makeatother

%joe7: following your suggestion
%\DeclarePairedDelimiter{\ccbr}{\lBrace}{\rBrace}
%\DeclarePairedDelimiter{\bbr}{\llbracket}{\rrbracket}
%\DeclarePairedDelimiter{\ppr}{\llparenthesis}{\rrparenthesis}
\DeclarePairedDelimiterX{\bbr}[1]{[}{]}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand\lab[1]{(#1)(lab-#1)} % for generating point labels in diagrams.


\let\Horig\H
\let\H\relax
\DeclareMathOperator{\H}{\mathrm{H}} % Entropy
\DeclareMathOperator*{\E}{\mathbb{E}} % Expectation
\DeclareMathOperator*{\argmin}{arg\;min}
\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}} % Conditional Independence

%\newcommand{\newbracketcmd}[2]{ \newcommand{\csname#1\endcsname}{{\color{##2}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{##1:} #1}\ \!\smash{\textbf{]}}}} }
\newcommand{\todo}[1]{{\color{red}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{todo:} #1}\ \!\smash{\textbf{]}}}}
\newcommand{\note}[1]{{\color{blue}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{note:} #1}\ \!\smash{\textbf{]}}}}
\newcommand{\moveme}[1]{{\color{purple}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{moveme:} #1}\ \!\smash{\textbf{]}}}}

\newcommand\geqc{\succcurlyeq}
\newcommand\leqc{\preccurlyeq}
\newcommand\mat[1]{\mathbf{#1}}
%\newcommand{\mat}[1]{\mathbf{#1}}
%\newcommand{\indi}[1]{\mathbbm{1}_{\left[\vphantom{\big[}#1 \vphantom{\big]}\right]}}
%\newcommand\m[1]{\mathbf m_{\mathsf #1}}
%\def\cpm#1(#2|#3){\mathbf #1 \left[ #2 \middle|#3\right]}


\newcommand\recall[1]{\expandarg\cref{#1}:\vspace{-1em} \begingroup\small\color{gray!80!black}\begin{quotation} \expandafter\csname #1\endcsname* \end{quotation}\endgroup }

\input{labelmatrix.tex}


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%oli8: end of model-commands.tex


%joe4: Although I didn't do it, I would prefer to get rid of all the
%nonstandard stuff below
%oli5: I understand the urge to remove packages that are not required,
%but I already do this religiously. I'm only using the ones I roungly
%understand and make my life easier. The model_commands.tex file has
%dozens more, and I really do not want to re-implement that
%functionality myself. Like I say, I'm happy to set up an overleaf
%document or help you figure out how to install packages on your
%computer (though I understand why you might not want that).
%joe5: I will say again that you and Matvey are the only people with
%whom I've every had problems latexing papers.  I'm happy to leave it
%to you to figure out where the problems lie.  But it will cause major
%problems for us if I can't latex papers that you send me.  I also
%don't want to spend more of my time figuring out the problem.
%oli6: I agree that not being able to latex papers is problematic, and it 
% does take me a fair amount of time to track down errors in your edits;
% the way we do things now is not ideal. Our options are: (1) use 
% overleaf, (2) install biblatex on your computer (which is an extremely 
% popular package, and becoming moreso, but I understand resistance), or
% (3) I can revert this document to bibtex.
%joe6: if it's your biber file that's causing problems, I have a
%*strong* preference for (3)

\usepackage[noabbrev,nameinlink]{cleveref}
%\crefname{lemma}{lemma}{lemmas}
\crefname{examplex}{example}{examples}
\crefname{defn}{definition}{definitions}
\crefname{prop}{proposition}{propositions}

\hypersetup{colorlinks=true, linkcolor=blue!50!black, urlcolor=magenta}

\usepackage{float}
\usepackage{subcaption}
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
    \renewcommand\thesubfigure{(\alph{subfigure})}
	
\usepackage{comment}

% Wrap definitions...
\definecolor{fulldefncolor}{rgb}{0.7,0.7,.7}
\specialcomment{fulldefn}{\begingroup\color{fulldefncolor}\begin{defn}}{\end{defn}\endgroup}
\definecolor{quickdefncolor}{rgb}{0.7,0.7,.7}
\specialcomment{quickdefn}{\begingroup\color{quickdefncolor}\begin{defn}}{\end{defn}\endgroup}


%END_FOLD

%%%% Version knobs %%%%%. 

\excludecomment{notfocus}

\definecolor{vfullcolor}{gray}{0.7}
\specialcomment{vfull}{\begingroup\color{vfullcolor}}{\endgroup}
\excludecomment{vfull} %\includecomment{vfull}

\excludecomment{vleftovers}
% \definecolor{vleftoverscolor}{gray}{0.85}
% \specialcomment{vleftovers}{\begingroup\color{vleftoverscolor}}{\endgroup}

\excludecomment{vcat} %\includecomment{vcat}
\excludecomment{vnotation} %\includecomment{vnotation}

\excludecomment{quickdefn}
\excludecomment{fulldefn}

\specialcomment{revising}{\begingroup\color{red}}{\endgroup}
% \pagestyle{plain}

%BEGIN_FOLD
\definecolor{notationcolor}{rgb}{0.9,0.9,.9} % purple
\newcommand{\notation}[2][]{#1}
\begin{vnotation}
	\renewcommand{\notation}[2][]{{\color{notationcolor} #2}}
\end{vnotation}

\newcommand{\commentout}[1]{\ignorespaces}
\newcommand{\vfullfootnote}[1]{}
\begin{vfull}
	\renewcommand{\vfullfootnote}[1]{\footnote{#1}}
\end{vfull}
\excludecomment{nonintro}


%%%%%%% Commands to highlihght changes in the document.%%%%%%%%%%%%%%%%%%%%%
%\definecolor{note-fg}{rgb}{0.0,.4,0.2} % green
\definecolor{note-fg}{rgb}{.2,0,.4} % purple

%\newcommand\changed[1]{\colorbox{light gray}{\parbox{\linewidth}{#1}}}
\newcommand\changed[1]{{\color{note-fg} #1}}
\newcommand\changeon{\color{note-fg} }
\newcommand\changeoff{\color{black} }

%oli8: disable tikz externalization for now, dependence on etoolbox
%\usetikzlibrary{external}
%% \tikzexternalize[prefix=tikz/]  % activate!
%
%\AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
%\AtEndEnvironment{tikzcd}{\tikzexternalenable}

%\crefname{section}{\S\!}{\S\!}

%oli3: these are required for sure, for biblatex.
% If you don't want to manage it yourself, I'll put it on overleaf.
%joe4: Just apass them along
%oli8: I have do do this inline at the bibliography for BibTeX
%\addbibresource{../refs.bib}
%\addbibresource{../uncertainty.bib}
%\addbibresource{../maths.bib}
%\addbibresource{graphical-models.bib}

%%%%%%%%%%%%%%%% SHORTCUTS FOR COMMONLY USED THINGS %%%%%%%%%%%%%%

\newcommand\Set{\textbf{Set}}
\newcommand\FinSet{\textbf{FinSet}}
\newcommand\MeasSet{\textbf{MeasSet}}

% Semantics
% use sd for... Set of Distributions / Score of Distributions / Single Distribution?
%\DeclarePairedDelimiter{\SD}{\lBrace}{\rBrace}
%joe7
%\DeclarePairedDelimiter{\SD}{\llbracket}{\rrbracket_{\text{sd}}}
%\DeclarePairedDelimiter{\WD}{\llbracket}{\rrbracket}
%\DeclarePairedDelimiter{\UD}{\llbracket}{\rrbracket^*}
%\newcommand\Weighted{_{\text{WD}}
\DeclarePairedDelimiterXPP{\SD}[1]{}{[}{]}{_{\text{sd}}}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
\DeclarePairedDelimiterX{\WD}[1]{[}{]}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
\DeclarePairedDelimiterXPP{\UD}[1]{}{[}{]}{_*}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
		
%\newcommand\MaxEnt{{\substack{\mathbf{Max}\\\mathbf{Ent}}}}
\newcommand\MaxEnt{_{\mathbf H}}


\newcommand{\none}{\varobslash}
\def\sheq{\!=\!}
\newcommand{\lowrightarrow}[1]{\mathrel{\raisebox{-2pt}{$\xrightarrow{#1}$}}}
\DeclareMathOperator\dcap{\mathop{\dot\cap}}
\newcommand{\tto}{%
	\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}


%\def\bmu[#1|#2]{\boldsymbol\mu\boldsymbol{[} #1 \boldsymbol{|} #2 \boldsymbol{]}}

\newcommand\bb[1]{\mathbbm{#1}}

\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bp}[1][L]{\mat{p}_{\!_{#1}\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}

%oli8: factored out styling for PDGs (\dg) and variables (\var).
\newcommand{\dg}[1]{\mathfrak{#1}}
\newcommand{\var}[1]{\mathsf{#1}}

\newcommand\Pa{\mathbf{Pa}}
%joe4: I really don't like \oslash; let's use something more mnemonic
%\newcommand\Inc{\oslash}
%oli8: refactored commands.
%joe7*: the more I think about it, the less I like the term ``Extra
%informtion''.  This arguably measures how far the PDG is from
%representing the indepdencies in \mu, but that has nthing to do with
%extra information.  See my furthe comments on this
%\newcommand\Gib{\mathit{Extra}}
%oli10: changed name of command, and what it results in.
\newcommand\Gib{\mathit{Gib}}
% \newcommand\Incsymb{\mathit{Inc}}
%joe7*: I don't know why you switched this: i also still prefer commas
%\newcommand\extrainfo[2][\dg M]{\Gib(#2;#1)}
\newcommand\extrainfo[2][\dg M]{\Gib(#1,#2)}
% \newcommand\Inc[2][\dg M]{\Incsymb(#2;#1)}
%oli10: changed name of command everywhere.
\newcommand\Inc{\mathit{Inc}}


%oli8: The injection of BNs as PDGs. I wanted something more mnemonic,
% but discovered it made formulas much longer and uglier.
%\newcommand\PDGof{\mathit{PDG}}
%joe7*: Since we use \M to denote a PDG, this should be M_B, not
%\Gamma(B).  Can you change it everywhere?
%joe8*: I changed it
%oli10: I've gone through and fixed the sites that didn't get converted properly.
%\newcommand\PDGof{\Gamma}
%oli10: The notation M_B to me suggests to me that somehow B is indexing the data in M.
%  I like notation that to keep separate a particular pdg M, M', maybe N, L also (fraktur font) and a the function that converts them. To also satisfy your desire that it be visually clear that the result is a PDG, I propose naming the function in fraktur font. 
% \newcommand{\PDGof}[1]{{\dg M}_{#1}}
\newcommand{\PDGof}[1]{\dg G(#1)}
%oli10: My second choice is keeping everything with M and using parentheses, which is what I did with factor graphs. I would have preferred to do it like I suggest above with factor graphs too, but there are only so many fonts and it's not a paper on factor graphs.... This option is commented below:
% \newcommand{\PDGof}[1]{\dg M ( #1)}

%oli8: consolidate edge definitions
\newcommand{\ed}[3]{#2 \xrightarrow{\!\!\smash{#1}} #3}
%\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}
\newcommand{\alle}[1][L]{_{ \ed {#1\vphantom{g}}XY}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% What do I even call them anyway?
\newcommand{\ModelName}{Probabilistic Dependency Graph}
\newcommand{\modelname}{probabilistic dependency graph}
\newcommand{\modelnamehyper}{probabilistic dependency hypergraph}
\newcommand{\modelnames}{\modelname s}
\newcommand{\MN}{PDG}
\newcommand{\MNH}{PDH}
\newcommand{\MNs}{\MN s}

\numberwithin{equation}{section}

\title{Probabilistic Dependency Graphs}

\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.

%% UAI  reviewing is double-blind.
%
%% The author names and affiliations should appear only in the accepted paper.
%%
%\author{
%	{\bf Oliver E. Richardson%\thanks{Footnote for author to give an alternate address.}
%		}\\
%%	Computer Science Dept. \\
%	Cornell University\\
%	Address\\
%\And
%	{\bf Joseph Y. Halpern}  \\
%	Cornell University      \\
%	Address \\
%}
%

%END_FOLD
\begin{document}
	\maketitle
	\begin{abstract}
% %joe4: This is a nit
% % We introduce a new graphical model, the Probabilistic Dependency
%           % Graph (PDG). PDGs can capture inconsistent
% %oli5: plural vs singular? _a_ new graphical model? That's why I had
% %it the other way 
% %joe5: sigh
% %We introduce a new graphical model, \emph{Probabilistic Dependency
% %oli6:
% % We introduce a new family of graphical models, \emph{Probabilistic Dependency
% 	We introduce a new class of graphical models,
%         \emph{Probabilistic Dependency\ 
%           Graphs} (PDGs). PDGs can capture inconsistent 
%                 beliefs in a natural way and are more modular than
%                 standard approaches such as Bayesian networks (BNs),
%                 in that they make it easier for a modeler to
%                 incorporate new information and restructure the
%                 representation, without committing to unnecessary
%                 assumptions.
% %joe4: another nit
% %		We give PDGs a semantics motivated by thermodynamic
% %                free energy, which we show naturally extends the
%             		We give PDGs a semantics, motivated by thermodynamic
%                 free energy, that we show naturally extends the
%                 standard semantics for BNs, and explain how a small
%                 modification of a very different flavor results in the
% %joe4: the hyphen is a nit, but less of a nit is (a) is there an
% %obvious associated factor graph; if so, you should say that, and (b)
% %I have no clue what the ``free-energy landscape'' of a factor graph
% %is.  If this is standard terminology, leave it.  If not, either
% %remove it or explain it.
% %oli5: Let's come back to this when we figure out how to present that part.
% %                free energy landscape of the associated factor graph,
% %joe5: my comment will still stand, although I don't mind deferring
% %the discuss as to whether to remove or explain until we've figure out
% %the rest.
%                 free-energy landscape of the associated factor graph,
%                 viewed as a Markov Network. 
% %		As we show, a BN, when viewed as a PDG, may represent many distributions, but we can recover the distribution given by the semantics of BNs as the one that minimizes the uncertainty in the distribution beyond that actualized in the tables, among all the distributions consistent with the conditional probability tables in the BN. 
% 		We show by example how PDGs are an especially natural modeling tool.
%oli8: updated our abstract.
We introduce Probabilistic Dependency Graphs (PDGs), a new class of
directed graphical models.   PDGs can capture inconsistent beliefs in a
natural way and are more modular than Bayesian Networks (BNs), in that
they make it easier to incorporate new information and restructure the  
representation.    We show by example how PDGs are an especially natural
modeling tool.
%
We provide three semantics for PDGs, each of which can be derived from a
scoring function (on joint distributions over the
variables in the network) that can be viewed as representing a
distribution's incompatibility with the PDG. For the PDG corresponding
to a BN, this function is  uniquely minimized by the distribution the BN
represents, showing that PDG semantics extend BN semantics.
A slight variant of the scoring function (which does not affect the
score of a distribution that represents a BN) yields the variational
free energy of general factor graphs; we use this to explain how PDGs
and factor graphs generalize BNs in orthogonal directions.%
%
%joe-: I cut this, because I' not sure that we'll have the space to do 
% it, or that we should do it even if we have the space.
%, and argue the relative strengths of PDGs.
\end{abstract}

%	\tableofcontents

	\section{Introduction}

        
	\commentout{We introduce the Probabilistic Dependency Graph (PDG), a directed graphical model for specifying local beliefs. They are strictly more expressive and modular than existing directed graphical models, and in particular this allows them represent inconsistent belief states.
	
	
	To be clear, inconsistencies are bad. Still, constantly examining every possible interaction between beliefs is taxing and restrictive, making them difficult to avoid: reasonable people are often simultaneously unaware of any inconsistencies in their beliefs, and yet still think it probable that they are not entirely consistent.%
%oli1: what do you think of the point in this footnote? Do you think it's worth bringing up somewhere? 
%joe2: I don't think it adds much, and I think there are many other
%reasons that arguments exist.
		\footnote{In some sense, this is the reason arguments exist: it is possible to get a person to agree to premises and reject a conclusion, revealing inconsistency---inconsistency which can then be used to change someone else's mental state. }		
	Most graphical models eliminate the possibility of inconsistency by fiat, 
	making them poorly suited to representing inconsistent belief states.
	While we do not endorse inconsistency, we nonetheless think it important to represent: in addition to modeling an overwhelmingly common feature of humans, the possibility of inconsistency 
	allows our model (called a \modelname\ or \MN)
	to portray intermediate stages of belief updating (\Cref{sec:belief-update}), 
	provides a rationale for providing multiple justifications 
	(corroborating evidence means more in the presence of possible conflict; see \Cref{ex:corrob}),
	and	recasts standard algorithms such as belief propagation and conditioning on evidence as resolutions of inconsistency (\Cref{sec:algorithms}). }

In this paper we introduce yet another graphical for modeling beliefs,
\emph{Probabilistic Dependency Graphs} (PDGs). There are already many
%joe4: you're saying there are many such models twice
%such models in the literature, notably including Bayesian networks
%(BNs), factor graphs, but also many others \cite[For an overview,
%  see][]{koller2009probabilistic}. Why does the world need one more?
such models in the literature, including Bayesian networks (BNs) and
%joe6: this comes out as Kol+09.  This is a very strange bibstyle.  Is
%this the one recommended/required for NuerIPS?   By the way, your
%captalization is also nonstandard.  Books should always have their
%titles capitalized, while papers usually don't (although that does
%vary by style).  You seem to be inconsistent with regards to papers
%oli7: Right now the bibliography will probably not render right, because
% I'm still trying to get it to compile on your machine. I don't have
% strong opinions about the styling here, so we'll come back to this
% and do it however you recommed at a later point.
% TODO
%joe7: it's in my bib file
%factor graphs. (For an overview, see \cite{koller2009probabilistic}.)
factor graphs. (For an overview, see \cite{KF09}.)
Why does the world need one more?  

Our original motivation for introducing PDGs was to be able capture
%joe4: why bother saying that we don't endorse it.  Are we concerned
%that people will think that we do?
%oli5: It may not be the best use of space, but I thought of this
%sentence as a calculated move to keep readers who instinctively think
%"oh they're complaining about the standard approach because it can't
%represent broken humans so they've given us broken math instead". But
%the next sentence arguably does this also. 
%inconsistency. While we do not endorse inconsistency, we wanted to be
%able to model the process of resolving it; to do so, we had to model
inconsistency. We want to be
able to model the process of resolving inconsistency; to do so, we have to model
the inconsistency itself. But our approach to modeling inconsistency
has many other advantages. 
In particular, PDGs are significantly more modular than other directed
%joe4
%that cause problems for other representations.
%oli5: what does it mean for a way of combining things to be
%difficult? Difficult to do what?
%joe5: In what way does taking unions ``caus problems'' for BNs?  It's
%not that it causes problems; it can't be done.
% That's why I keep changing this back to ``cause problems''.
% are difficult for other representations.  
%that cause problems for other representations.
%oli6: I don't think that's entirely true---it just cannot be done
%nicely.
%joe6: so how can it be done ``not nicely''?
%oli7: for instance, by generating a joint cpd as a product distribution
% of two incoming cpds, so that it can be interpreted as a BN.
%The way in which fails to be nice differs by graphical
%model. For BNs, it often requires assuming something about
%interaction effects and computing a new table;
%joe6: Are you claiming a general way that inconsistency can be captured in
%BNs?  If so, we perhaps should say something about it.  You tried to
%show me something once which failed.  Unless you can convince me
%otherwise, then I stand by the claim that it can't be done.
%oli7: BNs cannot capture inconsistency in a reasonable way, although 
% they can represent distributions over impossible possible worlds, which 
% might be seen by some as enough.  What I'm trying to say here is that
% not that BNs capture inconsistency, but that they can be "modular",
% if we are willing to give information we don't have, and coerce tables 
% into being of the correct form where necessary. I just don't think it's
% true that "BN's can't be recombined and restricted". It  IS true that
% they can't be recombined and restricted **while preseving the nice local 
% semantics that they offer**.  
%oli6:
%	for factor graphs,
%	unions and restrictions are actually really easy to do, but doing
%	this has global effects
%joe6*: what does ``has global effects'' mean?  
%oli7: I mean that adding a single factor can have arbitrary global impact
% on the semantics of the resulting factor graph, as explained in the 
% shortcomings of factor graphs section.
%oli6:
%	and it is NP hard to recompute anything after
%	such an operation. They also don't really have local semantics, so
%	this recombination doesn't have to satisfy any nice properties.    
%oli6: It "causes problems" in the sense that it forces you to make
%	extra assumptions
%joe6: needing to make extra assumptions means that it can't be done.
%	and do computation, and in some cases (for
%	inconsistencies) there's no way of doing it so that everything works
%	out. But in many cases it's totally reasonable. Perhaps "causes
%	problems" is not the right wording for other reasons. New attempt: 
%	graphical models: they can be readily restricted and combined in ways
% 	that cannot be done in  other representations.
%joe6*: First, this is absolutely not worth the time we've spent on
%it.  Second, this is far too mysterious for an introduction.  
%oli7: I agree on both counts and like the new sentence.
%graphical models: restrictions and combinations of PDGs are 
%	straightforwad and better behaved than analogs in other
%        representations.
%graphical models: restrictions and combinations of PDGs are 
%	straightforwad and better behaved than analogs in other
%        representations.
graphical models: operations like restriction and union that are easily
done with PDGs are difficult or impossible to do with other
representations.

%joe7
%We start with some examples to motivate PDGs and display some
We start with some examples to motivate PDGs and illustrate some
        of these properties.  
%joe3*: I don't know ho	w we modify the graph without changing it's
%informational content.  That doesn't make sense to me.  I also don't
%now what it means to add information ``efficiently''
%oli3: Since our meeting, I've remembered what I meant: you can change
% representations around, e.g., with products. I recognize how confusing
% this is now, and have said something else instead.
%oli3: As for "efficiently", it costs something to add things to a BN
% because you have to recompute new tables. Just collecting the
% information is "free". Computationally. 
%We can efficiently both add new information to a graph without changing the existing structure, and also modify the graph without changing its informational content.
% Though we may incur inconsistency by doing so, relaxing this
% coupling makes it easier to talk about representational learning.
%joe2: Cut; I don't understand what it means to ``consolidate
%computational costs'', and we don't discuss this issue. 
%oli2: Rather than paying the update cost for every observation and
%thought, you can just incorporate observations and think about them
%later. This seems very congruous with the story, and in particular
%with the ``you may not want to resolve conflicts right away'' bit,
%partly because it's expensive. I don't know the bit I wrote
%specifically about this is still in the document or not. 
%joe2: I also don't know what it would mean for a representation to be
%more dramatic. 
%This is a case of ``show, don't tell''.  Let the examples do the talking.
%oli2: I agree and I should probably get some more examples. Still I
%wanted this paragraph to go somewhere: it seems to me the reason it's
%beneficial to the things you're keeping track of, and the processing
%you need to make sense of it, is because you can do weirder graph
%transformations and resolve all of the inconsistency at once, rather
%than ensuring things are up-to-date at all times. 
%oli2*: Analogy: constantly making sure the document has the right
%formatting. Ultimately the formatting may help but making sure it's
%corretly formatted after every character you type is `taxing and
%restrictive'. Fixing your habits allows you to make bigger
%modifications to the document, and consolidate the cost of fixing the
%formatting.
%joe3: maybe so, but if you're going to claim this, you have to give
%examples.   You can't just make the claims
%	As a result, we can make more dramatic transformations of
%        representation, and consolidate the computational costs of
%        resolving them. 
%
%
%joe2: Oliver, instead of spraying the reader with a lot of
%unexplained bullets, you need to go over these points carefully
%later, and explain them.  I assume that by ``theoretical backing''
%you mean semantics.
%oli2: I am really referring to how nicely the energy and composition
%bits work out, compared to some other graphical models which I view
%as hackier: by thinking of the tables as being in the edges, lots
%things seem to magically fall into place. Most of these things I know
%I won't be able to explain in this space, but I want to get to at
%least the energy and composition. I know I need to support this later
%if I want it in the intro. If I can't get to convincing examples by
%the end I agree we need to cut it.
%joe3: not only do you have to support it.  You shouldn't bring it up
%until you've illustrated it.
%joe2: The semantics is discussed later.  If you mean something else, you
%have to make it clear.  I don't know what it means to ``promise
%deeper parallels''.  Let the examples show how PDGS are a natural
%etension, rather than saying it. 
%oli2: While examples are definitely be better than hand waving,
%though this would not be the place for them. We are advertising the
%story at a high level here; I'm trying to mark this as something I
%want in the story, in the same way that we're trying to advertise
%modulairty without examples at this point.
%joe3: You don't ``advertise'' modularity just by saying you have it.
%Sometimes you can get away with saying ``As we shall show, ...'', but
%you want to minimize that
%joe2: We really don't want to talk about
%type thoery here, nor do we want to make mysterious claims about
%locality.  That's not what an introduction is for.  I cut all this.
%	Furthermore, the theoretical backing is clean, promising
%        deeper parallels to other fields.  
%	PDGs themselves are natural extensions of BNs, and the nature
%of this extension complements both the information theoretic
%foundations of graphical models (\Cref{sec:thermo}) and the type
%theory we gesture towards. 
%	The modifications we make also bring graphical models in line
%        with other notions of locality in mathematics,  
%joe1: If you're going to bring these up, you would have to explain
%the connection.  We do not want to do that here (maybe somewhere much
%later in the full paper), so we shouldn't mention it here.  If you
%mention it, you need to add references.
%oli1: why should it here? We haven't fully explained anything yet. I mention it again when I the union precisely.
%oli1: I've now added one possible reference but I'm not really leaning on it. There are a bunch of 
% related constructions and I call out manifolds because everyone has heard of them, but if you can read the reference
% you probably also know this happens all over the place. You might not have realized that this was similar to the union
% construction if I don't mention it though.
%joe2: NO! we don't want to bring this up in the second paragraph of
%the introduction, before you've given the reader some
%intuitions. Maybe you can hint at some of this stuff in the
%discussion section at the end.  But it definitely shouldn't go here
%oli2: Ok. I'm now on the same page about cutting this, but I'm still
%confused about how this is different from the sentence advertising
%and then explaining modularity.
%joe3: in that case, you explained it right away.  Here you don't
%	such as manifolds \cite[e.g.,][p.12]{atlases}, in which
%        multiple restricted pictures can be glued together to form
%        something with global structure. 
%
%	We start by giving some examples that motivate PDGs and display some of their advantages over Bayesian Networks in particular.
% We start with some examples to motivate PDGs and display some
%joe3: it's OK to recap some advantages at the ed, but many of the
%ones on your list are ones we won't get to in the paper.
%oli3: Don't you think it's useful to list the things we're able to do that couldn't go in the paper? In any case, this section will be modified accordingly; what's there will be what we want to give them.
%        of these advantages; those looking for a more comprehensive
        %        list can skip to \Cref{sec:list-of-benefits}.
% of these advantages.
%	We use the rest of the section to give some of examples focusing on the benefits of this improved modularity, motivate the design of agents that might , and highlight the failure of Bayesian Networks to capture what we have in mind (we compare to other graphical models in \Cref{sec:other-graphical-models,sec:many-relations-graphical-models}). PDGs have other desirable features as well; for a complete list, see \Cref{sec:list-of-benefits}.
	
%joe4: I'm not sure that this is the ``simplest inconsistency''.  In
% fact, we later have a sipmler one (which is modeled using a
% multigraph), where you literally have two inconsistent beliefs about
% Pr(A|B). 
%\begin{example}[the simplest inconsistency]
\begin{example}
		\label{ex:guns-and-floomps}
You arrive in a foreign country known for having very clear laws. From
prior reading, you ascribe probability 0.95 to owning guns being
against the law. Upon arrival, you end up talking to some teenagers
who use the local slang, after which you believe that with probility
.1, the law prohibits floomps.  
		
		The obvious way to represent this as a BN involves two
                binary random variables, $F$ (taking values $\{f,
                \overline f\}$), indicating the legality of floomps,
                and $G$ (taking values $g, \overline g$) indicating
                the legality of guns. The semantics of a Bayes Net
%joe4: I have a slight preference for ``offers'' rather than
%``offer''.  The web is conflicted about whether to view ``semantics''
%as singular or plural in thi context. 
%oli5: I prefer 'offer' especially in this case where we explicitly
%give multiple semantics.  
                offer us two choices: we either assume that $F$ and
                $G$ are independent and give (unconditional)
                probabilities of $F$ and $G$, or we choose a direction
                of dependency, 
%joe3: incorporate one of the two what?  what does ``incorporate''
%mean here?
%oli3: I'm 
   and give one of the two unconditional probabilities and a conditional probability distribution. As there is no reason to believe that either variable depends on the other, the natural choice is to assume independence, giving us the following BN:
		
		
		\begin{center}
			\scalebox{0.9}{
			\begin{tikzpicture}[scale=0.8,ampersand replacement=\&]
	
				\node[dpadded, circle] (floomp) at (-1.2,0) {$F$};
				\node[dpadded, circle] (gun) at (1.2,0) {$G$};
	
				\matrix [table with head, column 1/.style={leftrule},
					 column 2/.style={rightrule}, row 2/.style={bottomrule}] at (-3.5,0) {
					\vphantom{$\overline f$} $f$ \& $\overline f$\\
					.9 \& .1\\
				};
				\matrix [table with head, column 1/.style={leftrule},
					 column 2/.style={rightrule}, row 2/.style={bottomrule}] at (3.5,0) {
					 \vphantom{$\overline g$}$g$ \& $\overline g$\\
					 .05 \& .95\\
				};
			\end{tikzpicture}
			}
		\end{center}


		Now suppose that you later discover that ``floomp'' is
                likely to be another word for gun, and come to believe
                that if floomps are legal (resp., illegal), then
                there's a 92\% chance guns are as well, and vice
%joe7: r seems like a atrange letter to use, although it's not a big deal
                versa. Let $\mat r$ be the  
%oli8: changed cpt to cpd everywhere, because (1) the term is more common in ML, (2) it's more general, so that any continuous generalization is not be so linguistically distant, (3) 
% I also included the word ``table'' here becaues it's a useful shorthand and we use it a lot.
		\emph conditional \emph probability \emph distribution
%joe8: line shaving, but I don't think we need to say this in any case.
                %                (cpd), represented as a table,
                                (cpd)
%		conditional probability table (cpt)		
		that describes this belief. A first reaction might be to simply incorporate this conditional information by adding $F$ as a parent of $G$, and then associating the cpd $\mat r$ with $G$. But then what should we do with the original probability we had for $G$?  Should we just discard it?
			% which amounts to throwing out our old prior distribution on $G$; 
			% another instinct might be to perform a Bayesian update, but this new information is conditional and probabilistic, not an event.
		It is easy to check that there is no probability distribution that is consistent with the two original priors on $F$ and $G$ and also the cpd $\mat r$, so if we are to represent our information with a BN, which always represents a consistent distribution, we must resolve the inconsistency.  
%		In fact, $E$ conflicts with the other two tables, in the sense that there is no joint distribution on $\{F, G\}$ such that all three tables are accurate. 
%		Therefore, there is no Bayes Net that contains all three pieces of information exactly: one has to resolve the inconsistency first.
                %joe4: removed paragraph berak
		%
		%There are three different dimensions along which this could be resolved: rejecting either prior belief, or $E$; in general any mixture would do. 
                %joe3: removed paragraph break, and discussion of
                %``your interests'', which are largel irrelevant
%		
%However, it may not be in your best interest to sort this out right away.
		However, it may be better not to sort this out right away. How to resolve it may be clearer if you can get confirmation that guns are indeed floomps, or read the laws more carefully.

		By way of contrast, consider the corresponding \MN.
		In a \MN, the cpds are attached to edges, rather than nodes of the graph. The cpd associated with an edge $e$ from $X$ to $Y$ is a matrix $\mat e$, where the element $\mat e_{x,y}$ at row $x$ and column $y$ is the conditional probability $\Pr(Y \!\!=\!\!y \mid X \!\!=\!\! x)$. In order to represent unconditional probabilities, we introduce a \emph{unit variable} $\var 1$ which takes on only one possible value,	which we denote $\star$. Thus, we have the PDG depicted in \Cref{fig:gun-floomp-diagram}, where the edges from $\var 1$ to $F$ and $G$ are associated with the unconditional probabilities of $F$ and $G$, and the edge from $F$ to $G$ is associated with the cpd $\mat r$.


                \begin{figure}[htb]
			\centering
			\scalebox{0.8}{
			\begin{tikzpicture}
				\node[dpadded] (true)  at (0,1.7) {$\var 1$};
				\node[dpadded] (floomp) at (-1.5,0) {$F$};
				\node[dpadded] (gun) at (1.5,0) {$G$};			
				
				\draw[arr] (true) -- coordinate(A) (floomp);
				\draw[arr] (true) -- coordinate(B) (gun);
	
				\node[above left=0.5em of A] {
					\begin{idxmat}{$\star$}{$f$, $\overline f$}
						.90 & .10 \\
					\end{idxmat}
				};
				\node[above right=0.5em of B] {
					\begin{idxmat}{$\star$}{$g$, $\overline g$}
						.05 & .95 \\
					\end{idxmat}
				};

                                
				\definecolor{heldout}{rgb}{0.6, 0.6, .8}	
				\draw[heldout, dashed, arr] (floomp) -- node[fill=white] (C) {$\mat r$} (gun);
		%oli8: move to the right to save space.
		%		\node[below=1em of C] {
				\node[anchor=center] at (5.5, 1) {
					\color{heldout}
					$\mat r =\!\!\!$\begin{idxmat}[\color{heldout}\smalltext]{$f$,$\overline f$}{$g$, $\overline g$}
						0.92 & 0.08 \\ 0.08 & 0.92 \\
					\end{idxmat}
				};
			\end{tikzpicture}
			}
			\caption{An inconsistent \MN, requiring resolution}
			\label{fig:gun-floomp-diagram}
		\end{figure}

                
		The original state of knowledge consists of all three
                nodes and the two black edges from $\var 1$. This is
                like Bayes Net that we considered above, except we no
                longer assume $F$ and $G$ to be independent; we merely
                record the constraints imposed by the given
                probabilities. 
	
		The key point is that we can incorporate the new information into
		our original representation (the graph in \Cref{fig:gun-floomp-diagram}
		without the edge from $F$ to $G$) simply  by adding the edge from $F$
		to $G$ and the associated cpd $\mat r$.
                Doing so does not change the meaning
		of the original edges.  This presentation lets us simply include
		information, and resolve inconsistencies later. Unlike a Bayesian
		update, the operation is even reversible: all we need to do recover
		our original belief state is delete the new edge, effectively making
		it possible to mull over and then reject an observation.
	\end{example}

%joe4*: I'm not sure that the example suggests that there is something
%to be gained by avoiding the independence assumptions.  It just shows
%that the particular way we modeled the information graphically did
%not impose the independence assumptions of BNs.  More importantly, while it's
%true the independencies are notoriously difficult to demonstrate,
%people do in practice want to assume a lot of independence, and it
%seems approximately correct.  So I wouldn't make a fuss about the
%fact that it's hard to demonstrate.  That's not the point we want to make.
%oli5: I think the example does suggest there is something to be
%gained by avoiding independence assumptions: we made them because we
%didn't have a better option (we would have liked not to assume
%anything at all). Making the assumption turns out to be wrong.  
%joe5*: Oliver, this is absurd.  You're spending *way* too much time
%overthinking this.  We definely do not want to take the independence
%assumptions ``less seriously''.  We have to decide whether or not we
%want to make them.  I strongly disagree that it's ``wrong'' to make
%the assumption.  The question is whether it's useful.  Suggesting
%that it's wrong is a huge net negative.  I sholdn't need to spend
%time responding to this (although I wil), and you have many
%better things to do.
%oli5: for your second point: I don't think that the approximate
%correctness is a serous motivator, and why would we have any reason
%to believe that not knowing about something means that independent =
%approximately correct? I still want to make the fuss about it being
%hard to demonstrate because that's the thing that PDGs buy you. It's
%not out of nowhere: not making the independence assumptions is one of
%the primary things that we do, and we have good reasons for doing
%it.
%joe5: I will say again that (a) making these assumptions  seems to be
%helpful in practice and (b) discussing it here would be a major net
%negtative. 
%	\Cref{ex:guns-and-floomps} suggests that there is something to
%        be gained by avoiding the independence assumptions enforced by
%        BN (that every node is independent of its non-descendants
%        given its parents).
%oli6: It seems uncontrovertial that making assumptions (especially
%ones that are hard to conferm) is negative, but the things you get at
%the other end are positive. The right story to me seems to be: we no
%longer make independence assumptions, but fortunately we can get the
%useful stuff back by adding a term to our interpretation. This is not
%an assumption that an agent using PDGs ever has to make, because the
%semantics implicitly makes a weaker assumption in the background.  
%joe5: Yes; I agree with that, and I said it.
%After all, independences are notoriously
%        difficult to demonstrate emperically and unobserved
%        confounders cannot be eliminated.
%joe5: While I agree with what you said above, that's *not* the main
%reason that I think we should avoid the independence assumptions, and
%I think that raising it is a bad move.
% 
%oli5: On second thought I think your replacement sentence, below, is much worse:
% (1) The logic seems more backwards: from an outside perspective,
% modeling inconsistency is undesirable, and the independence
% assumptions are something that you adopt because otherwise you're
% forced to give the raw data for an entire distirbution, which you
% neither know about, nor have enough memory to store.
%joe5: I disagree; that's not why Pearl is using independence in most
%cases.  Let's not go there.
%oli6: Ok. I am curious about the prototypical use case of independence then.
%It would be bad
% to model inconsistency and good to get rid of these
% assumptions. This sentence, suggests the reverse: we would like to
% model inconsistent agents, but unfortunately we lose our
% independence assumptions. This seems just wrong!
%joe5: Why is it wrong?  It's not that we ``like'' modeling
%inconsistent agents, but, as we explicitly said at the beginning, we
%do want to modle them.  And some people will think that it is indeed
%unfortunate that we've lost independence.  You have to acknowledge that.
% (2) "the ability of" is used twice in a row.
%joe5: Fine, rewrite that, rather than spending hours on a response.
%oli6: I will start doing this more. I have been scared to change things
% that you've rewritten without significant justification.
% (3) It is, in fact, true that we've lost this benefit of BNs. We're
% not going to talk about how we can model independences at all,
% because we're trying not to. It just so happens that we can get the
% distribution back, but we think of this from an information
% theoretic lense, not an independence one.
%joe5: That's fine
%		The ability of PDGs to model inconsistency, as illustrated in 
%		Figure~\ref{ex:guns-and-floomps}, seems to have come with a
%		significant cost.  We have lost a key benefit of BNs: the ability to
%		model independencies.  
%oli5: I've  slightly modified my original sentence.
%joe5*: cut your sentence, and reinserted mine, for all the reasons
%above while dealing with tyour second point.
%\Cref{ex:guns-and-floomps} suggests that there is something to
%        be gained by taking independence assumptions enforced by BN  
%	%(that every node is independent of its non-descendants given
        %its parents) 
%	less seriously. After all, independences are notoriously
%        difficult to demonstrate emperically and unobserved
%        confounders cannot be eliminated. Still, conditional
%        independences are useful, and as Pearl
%        \cite{pearl1989conditional} has argued forcefully,
%        omnipresent.  
%		It seems that PDGs have lost this key benefit of BNs.
		The ability of PDGs to model inconsistency, as illustrated in 
		% Figure~\ref{ex:guns-and-floomps}, seems to have come with a
		Example~\ref{ex:guns-and-floomps}, 
%oli8: avoid conflict with below
		%seems
		appears
		 to have come at a
		significant cost.
%oli8: I still don't follow the train of thought, the independences
%are not at all 
% in my mind at all
		We 
%oli8: inserted "seem to", as we plan to model this fairly directly.
		seem to
		have lost a key benefit of BNs: the ease with which they can
capture (conditional) independencies,
which, as Pearl \cite{pearl1989conditional} has argued forcefully,
are omnipresent.  

	%	Most of the time, we do not make the independence
	%assumption in a BN because we know for certain that the
	%variables are independent; rather, we just suspect that the
	%identified edges are by much more important than the
	%others. Determining for sure that smoking  and second hand
	%smoke are independent, controlling for parents' smoking
	%habits, would extremely difficult, and would require
	%empiricism to validate. 
	

	\begin{example}[emulating a BN]\label{ex:smoking}
		We now consider the classic (quantitative) Bayesian
                network $\cal B$, which has four binary variables
                indicating whether a person ($C$) develops cancer,
                ($S$) smokes, ($\mathit{SH}$) is exposed to
%joe7
%          second hand smoke, and ($\mathit{PS}$) has parents who smoke,
           second-hand smoke, and ($\mathit{PS}$) has parents who smoke,
%joe4: another nit: the use of \Cref gives us 2a; I would prefer 2(a)
%(since we use ``(a)'' in the figure.  There are lots of potential fixes.
%oli5: I fixed this in the header.
                presented graphically in \Cref{subfig:smoking-bn}. We
                now walk through what is required to represent $\cal
%joe8: made this change globally
%           B$ as a \MN, which we call $\PDGof{\mathcal B}$, shown
            B$ as a \MN, which we call $\PDGof{{\mathcal B}}$, shown
                as the solid nodes and edges in
                \Cref{subfig:smoking-pdg}. 
% 
		\begin{figure*}[ht!]
			\centering
			
			\begin{subfigure}[b]{0.3\textwidth}
				\scalebox{0.9}{
				\begin{tikzcd}[center base, column sep=1.8em, row sep=1em, dpad={fill opacity=0, draw=gray}, 
					ampersand replacement=\&]
				\& S \ar[dr] \\
				PS \ar[ur]\ar[dr] \&\& C \\
				\& SH \ar[ur]
				\end{tikzcd}}
%joe7
                                %\caption{Bayesian Network, $\cal B$}
                                \caption{The Bayesian network $\cal B$}
				\label{subfig:smoking-bn}
			\end{subfigure}%			
			\hspace{2em}\vline\hspace{2em}
			\begin{subfigure}[b]{0.55\textwidth}
				\scalebox{0.9}{
				\begin{tikzpicture}[center base]
				\fill[opacity=0.2, fill=orange!80!black] (2.7,1.35) rectangle (7.1, -1.35);
				
				\node[dpadded] (1) at (0,0) {$\var 1$};
				\node[dpadded] (PS) at (1.65,0) {$\mathit{PS}$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (S) at (3.2, 0.8) {$S$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (SH) at (3.35, -0.8) {$\mathit{SH}$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (C) at (4.8,0) {$C$};
				
				\draw[arr] (1) -- (PS);
				\draw[arr] (PS) -- (S);
				\draw[arr] (PS) -- (SH);
				\mergearr{SH}{S}{C}
				
				\node[dpadded, fill=black!.16, fill opacity=0.35, dashed] (T) at (6.5,0) {$T$};
				\draw[arr,dashed] (T) -- (C);	

				\draw[very thick, |-|, color=orange!50!black] (2.7, 1.35) --node[above=0.5em]{Restricted PDG in Examples~\ref{ex:grok-ablate}~and~\ref{ex:grok-union}} (7.1,1.35);
				
				\end{tikzpicture}}
%joe7
%          \caption{Corresponding \MN, $\PDG$,(\mathcal B)$, and a
%            restriction} 
                              
          \caption{The \MN\ $\PDGof{{\mathcal B}}$ corresponding to
            ${\mathcal B}$, and a 
            restriction of it.} 
				\label{subfig:smoking-pdg}
			\end{subfigure}
		
			\caption{Graphical models representing conditional relationships in \Cref{ex:smoking,ex:grok-ablate,ex:grok-union}}
			\label{fig:smoking-bn+pdg}
		\end{figure*}
		
%		The BN is a compact representation of a joint distribution over all four variables, which achieves compactness by taking advantage of independence between variables. 
%joe3: a BN is just a graph; it's not ecnoding anything
%                distribution by encoding the an assumption that every
%oli3: 
% The BN achieves a compact representation of a join distribution by assuming that every node is independent of its non-descendants given its parents. 
%joe3: removed paragraph break. Rewrote sentences; we're not using
%different lenses, but giving differentsemantics
%oli3: that's the same thing.
%
%A PDG does not make these assumptions. However, by viewing the
%PDG through a different lens (we offer several in
%\Cref{sec:semantics}), we can further interpret the constraints. In
%particular, the maximum entropy distribution consistent with the
%oli3: this is false. Maximum entropy is exactly these assumptions.
% We do not make these assumptions when giving semantics to a PDG.

%joe4                
%	We start with just nodes corresponding to the variables in $\cal B$,
%	and also a special node $\sf 1$ from \Cref{ex:guns-and-floomps}. and
%oli5: the edit to the first part is good but makes the sentence run-on.
We start with the nodes corresponding to the variables in $\cal B$,
together with the 
special node $\sf 1$ from \Cref{ex:guns-and-floomps}%
%oli5:
% and
%joe5: something we actually agree on :-)
; we add
%joe7
%an edge ${\sf 1}$ to $\mathit{PS}$, to which we associate the
an edge from ${\sf 1}$ to $\mathit{PS}$, to which we associate the
unconditional probability 
%joe7: line shaving
%that one's parents smoke, given by
%the table attached to $\mathit{PS}$ in $\cal B$.
given by the cpd for $\mathit{PS}$ in $\cal B$. 
%joe4
% We can also directly re-use the cpds on $S$ and
% We can re-use the cpds on $S$ and
%oli5: a middle ground because we're re-using the first cpd above.
%joe7
%We can also re-use the cpds on $S$ and
We can also re-use the cpds for $S$ and
                $\mathit{SH}$, assigning them, respectively, to the
                edges $PS \to S$ and $PS \to SH$ in $\PDGof{{\mathcal
                B}}$. There are two remaining problems
				(1) modeling
                the remaining table in $\cal B$, which corresponds to
%joe7
%                the conditional probability of $C$ given $S,SH$, and
                the conditional probability of $C$ given $S$ and $SH$; and
                (2) recovering the additional independence assumptions
                in the BN. 

%joe4: we ``started with'' in the last paragraph.
%		We start with (1). We should not simply add the edges
%joe7
%           For (1). We cannot just add the edges
      For (1), we cannot just add the edges
                $S \to C$ and $SH \to C$ that are present in $\cal B$,
                because, as we saw in \Cref{ex:guns-and-floomps}, this
                would mean supplying two \emph{separate} tables, one
                indicating the probability of $C$ given $S$, and the
                other indicating the probability of $C$ given
                $\mathit{SH}$.
%joe4: although the point about marginalization is certainly true,
%it's not the point we want to make
%  Though we could get both tables by
                %                marginalizing, doing so would lose an important
%                                correlation that is present in $\cal B$.   
Doing this would lose significant information that is present in $\cal
B$  about how $C$ depends jointly on $S$ and $SH$.
		To distinguish the joint dependence on $S$ and
                $\mathit{SH}$, for now, we draw an edge with two
                tails, sometimes called a \emph{hyperedge}, which
                completes the diagram in \Cref{subfig:smoking-pdg}. 
%joe4: I view the next paragraph as part of the dsicussion (since the
%previous paragraph started with ``We start with ...'').
%	\end{example}
%oli5: I totally agree, but this is frustrating because this is the
%way I initially presented it --- then after getting advice to better
%weave discussion & examples, I broke the example into two examples
%with the same text, and you seem happier with it even though your
%advice is to put it back in the form that it was in originally.
%joe5: What made sense before your various rewrites makes less sense
%after the rewrites.                
%oli6: Or equivalently, after clarificaiton, my original presentation
                %turned out to be appropriate
%joe6: definitely not equivalent!
%joe4                
%        We now return to the issue of independence (2). There are many
        With regard to (2), there are many
                distributions consistent with the conditional marginal
        probabilities in the cpds, and the independences presumed by
        $\cal B$ need not hold for them. Rather than encoding the
        extra probabilistic information as cpds, we develop a
%joe7
%        maximum-entropy semantics for PDGs, which allows us to
        a scoring-function semantics for PDGs, and show that,  
        %joe4; why is it temporary?
%        (temporarily) view  $\PDGof{\mathcal B}$ as representing the
%oli5: because the point of a PDG is not to represent
%distriutions. Distributions are consistent, but there is more
%information in the PDG. We can temporarily put on our unique
%distribution glasses, but we haven't changed the information in the
%PDG. We haven't made any assumptions about independence. We just
%temporarily put on our information theory goggles. 
%oli5: I want to stress this point, but I don't have an opinion on placing the word "temprarily" here. 
%joe7
% We show that  us to
%        view  $\PDGof{\mathcal B}$ as representing the
%joe5
%distribution which provides the \emph{least additional
%information} beyond the information already contained in
%joe7
%        distribution that provides the least additional
%information beyond the information already contained in the
        %        cpds, among all those consistent with them. In
%        \Cref{thm:bns-are-pdgs}, we show that this distribution
%        is the same as the one specified by $\cal B$. 
        among all distributions consistent with $\PDGof{{\mathcal B}}$,
%joe8*: I think we need to throw out hints about how we're going to
%use scoring functions.  I view this as critical
for the appropriate scoring function,
        the
        unique distribution with a minimum score is the one specified by
        ${\mathcal B}$ (\Cref{thm:bns-are-pdgs}).
%joe8*: added next sentence.        
Roughly speaking, we can think of the scoring function as capturing
the extent to which we view the edges in a PDG as encoding some
information about independence.
        This allows us to recover the semantics of Bayesian
networks without requiring the independencies that they assume.
%oli1: I added the following sentence, but I'm not
                %sure how worried people will be and the current plan
%is not to talk about sampling in the abstract.
%joe3: definitely not for here
%		\begin{edge}
%			On might worry that we've merely constructed
%		some object we have no computational access to, but note that in this
%		case, sampling is no harder in the \MN, We discuss this problem in
%		more depth in \Cref{sec:sampling}. 
%		\end{edge}
%joe7: I don't know what this means
%	Doing this, however, does not limit us to the features of BNs. 

	
%joe4
%	\begin{example}[continues=ex:smoking]
        %		Suppose we read a thorough empirical study which
%        demonstrates that people who use tanning beds have a
%joe7
%But now suppose that we get information beyond that captured by the
Next suppose that we get information beyond that captured by the
original BN.  Specifically, we read a thorough empirical study 
        demonstrating that people who use tanning beds have a
                10\% incidence of cancer, compared with 1\% in the
%joe7: \mat p comes out as a strange symbol in my pdf file.  Why do
%you need to use nonstandard fonts like \mat?
                control (call the cpd for this $\mat p$); we would
                like to add this information to $\cal B$.  
		The first step is clearly to add a new node labeled
                $T$, for ``tanning bed use''.  But simply making $T$ a
                parent of $C$ (as clearly seems appropriate, given
                %joe4
                that 
%joe7
%            the incidence of cancer depends on tanning bed use),
%            requires a substantial expansion of the cpd, and, in
%                particular, requires us to guess at the interactions
                the incidence of cancer depends on tanning bed use)
            requires a substantial expansion of the cpd; in
                particular, it requires us to make assumptions about
                the interactions 
                between tanning beds and smoking.  
%joe4: removed paragraph break
%		
	The corresponding PDG, $\PDGof{{\mathcal B}}$, on the other hand, has no trouble:
		We can simply add the node $T$ with an edge to $C$ that is associated with $\mat p$. 
                %joe4
%		
%		It is now technically possible for our knowledge to be
%                inconsistent. For instance, if the distribution on $C$
%                given $S,H$ by its original cpd were always
                But note that doing this makes it possible for our
                knowledge to be 
                inconsistent. To take a simple example, if the
                distribution on $C$ 
%joe7
%                given $S,H$ encoded in the original cpd was always
                given $S$ and $H$ encoded in the original cpd was always
                deterministically ``has cancer'' for every possible
                value of $S$ and $H$, but the distribution according
%joe4: there's no couunterfactual
%                to the new cpd from $T$ were deterministically ``no
%                cancer''. 
                to the new cpd from $T$ was deterministically ``no
%oli5: result -> resulting
                % cancer'', the result PDG would be inconsistent.  
                cancer'', the resulting PDG would be inconsistent.  
		% In this particular case, such a selection of cpds seems like an implausible edge case; we conclude that the additonal modularity we gain from a PDG is useful even when there is no inconsistency.
%oli8: removed line breaks
%
%		\begin{center}
%			\scalebox{0.9}{
%			\begin{tikzpicture}
%			\node[dpadded] (1) at (0,0) {$\var 1$};
%			\node[dpadded] (PS) at (1.65,0) {$\mathit{PS}$};
%			\node[dpadded, fill opaci semi-colon representty=0.16] (S) at (3.3, 0.8) {$S$};
%			\node[dpadded, fill opacity=0.16] (SH) at (3.3, -0.8) {$\mathit{SH}$};
%			\node[dpadded, fill opacity=0.16] (C) at (4.8,0) {$C$};
%			\node[dpadded, fill opacity=0.05,dashed] (T) at (6.5,0) {$T$};
%			
%			\draw[arr] (1) -- (PS);
%			\draw[arr] (PS) -- (S);
%			\draw[arr] (PS) -- (SH);
%			\mergearr{SH}{S}{C}
%			\draw[arr,dashed] (T) -- (C);
%			\end{tikzpicture}}
%		\end{center}		
%	       
		% This is again an illustration of the modularity and the possibility for inconsistency; compare the right half of the diagram (shaded slightly darker) with the topological equivalent in example \ref{ex:planet}.
	\end{example}	


% Goal here: introduce a graph with more stuff:
%  - Graph union.
	We have seen that we can easily add information to \MNs;
%joe4
        %        forgetting information is equally painless.
removing information is equally painless.   

	\begin{example}[restriction]\label{ex:grok-ablate}
		After the communist uprising, children were raised communally, and so parents' smoking habits no longer had any impact on them. Grok is reading her favorite book on graphical models, and she realizes that while the node $\mathit{PS}$ in \Cref{subfig:smoking-bn} has lost its usefulness, and nodes $S$ and $\mathit{SH}$ no longer ought to have $\mathit{PS}$ as a parent, the other half of the diagram---that is, the node $C$ and its dependence on $S$ and $\mathit{SH}$---should apply as before.
		%oli4: this next sentence is less useful, and can be
                %removed; its purpose is to pre-emptively push against
                %a desire to margnialize and get a new BN.  
%joe4: let's remove it
% \begin{edge} 
% 	The rise of the communist party also came with changes in smoking habits, so a new unconditional distribution on $S$ could not be obtained by eliminating the variable $PS$. 
% \end{edge}
%		
	%oli5: see below for reasons for edit.
		% Grok knows that this restricted model is no longer a
		Grok has identified two obstacles to modeling deletion of information from a BN by simply deleting nodes and their associated cpds.
                %joe5: removed paragraph break
		%
		First, this restricted model is technically no longer a
                BN (which in this case would require unconditional distributions on $S$ and $\mathit{SH}$), but rather a \emph{conditional} BN
%joe7
                %                \cite{koller2009probabilistic}%
                                \cite{KF09}%
%oli5:
				%. While a BN would
				, which
%joe4
%                require an association prior beliefs with $S$ and
%oli5: not "us" because the subject is Grok.
%                require us to associate an unconditional probability
                % with $S$ and $\mathit{SH}$, a conditional BN (CBN) 
				allows for these
                nodes to be marked as observations; observation nodes
                do not have associated beliefs.  
%joe7
%		Second, even regarded as a conditional BNs, the result
		Second, even regarded as a conditional BN, the result
                of deleting a node may introduce \emph{new}
                independence information, incompatible with the
                original BN. 
		For instance, by deleting the node $B$ in a chain 
%joe7: I don't know why you're doing this; something much simpler
%looks better to me
%		\scalebox{0.6}{
%		\begin{tikzcd}[dpad={light pad}, column sep = 1.3em, AmpRep]
%			A \ar[r] \& B \ar[r] \& C
                %		\end{tikzcd}},
$A \rightarrow B \rightarrow C$,                
		one concludes that $A$ and $C$ are independent, a
               conclusion incompatible with the original BN
               containing all three nodes.   
		% note: it's incompatible with the structure, but not the rest.
%
%joe4		
%		PDGs are more modular in both respects.
%oli5: the replacement you suggest below is not quite what I meant. The two respects I have in mind are (1) no longer requiring cpds on S and SH, and (2) compatibility of the ablated PDG with the original. 
%	PDGs allow for the easy removal of both nodes and edges.
%oli5: I've reinstated what I had and changed the text in the paragraph above to clarify.
%joe5: I'm not happy with this because I didn't know what the two ``respects'' were
%oli6: Is this actionable? Why aren't you happy with this?
%joe6: You have to rewrite it in a way that makes clear what the two
%``respects'' are (as I did, but you kept on rewriting it).  I would
%suggest ``PDGs make it easy to remove nodes, edges, and cpds'', which
%I think is much simpler and more comprehensible than what you wrote above.
%leave this to you, since I'm tired of wasting so much time on this.
%oli7: I agree but it also has entirely different content from what I wrote.
%oli7: I want to say that, while it is possible to remove nodes and edges in a BN,
% the semantics of BNs (and even conditional BNs, which are designed to 
% improve this kind of modularity) do not view such deletions as providing less
% information. It's a much more subtle point, but it might not be well-articulated.
% I'll try again after I fix the story.
               % TODO
               %joe7*: shortened significantly.  I don't think it's
               %worth agonizing this over this.
PDGs do not suffer from either problem.  We can easily delete the
nodes labeled 1 and $PS$ in Figure~\ref{subfig:smoking-pdg} to get the
restricted PDG shown in the figure, which captures Grok's updated information.
%	\todo{reword again}
%               PDGs are more modular in both respects.
%			Consider the PDG fragment analogous to the CBN
%we first examined, 
%            in which $\mathit{PS}$ and the arrows out of it have
%%joe4: it doesn't look like a box to me, but I don't have a better
%%suggestion, beyond saying ``on the right'', as I did
%%            been deleted (see the box in
%% \Cref{subfig:smoking-pdg}). In this case, one can see that there are
%                been deleted. The shaded box on the right of
%\Cref{subfig:smoking-pdg} is a PDG with
%                no edges leading to $S$ or 
%%joe4: minor nits
%                $\mathit{SH}$, and hence no distributions specified on
  %                them; no special modeling distinction between
                %             observation nodes and other nodes are required.
%       		Because they make no independence assumptions, the
%                $\mathit{SH}$; hence no distribution are specified on
%                them.  No special modeling distinction between
%                observation nodes and other nodes is required.  
%		Because PDGs do not make independence assumptions, the
%                information in this fragment is truly a subset of the
%                information in the whole \MN. 		 
	\end{example}

%oli5: reworded.
% Being able to restrict your knowledge and look at a local
% picture may be a nice property, but a much more compelling
 	Being able to form a well-behaved local picture and restrict
        knowledge is useful, but an even more compelling 
%joe4
%        reason to PDGs is their ability to aggregate information. 
        reason to use PDGs is their ability to aggregate information. 
	
	\begin{example}\label{ex:grok-union}
		Grok dreams of becoming Supreme Leader ($\it SL$), and
                has come up with a plan. She has noticed that people
                who use tanning beds have significantly more power and
                than those who don't. Unfortunately, her mom has
%joe4: more minor changes, typo corrections
%                always told her that tanning beds causes cancer: in
%                particular, that 15\% of people who use tanning beds
%                get it, compared to the baseline 2\%. Let $\mat q$ be
%                                the cpd associated to this beleif.  
                always told her that tanning beds cause cancer: in
                particular, 15\% of people who use tanning beds
                get it, compared to the baseline of 2\%. Let $\mat q$ be
                the cpd associated with this belief.  
		Grok believes people will make fun of her if she uses
                a tanning bed and gets cancer, making becoming 
%joe4
% $\it  SL$ impossible. This mental state is depicted as  a PDG 
                %                on the left of \Cref{fig:grok-combine}.
Supreme Leader impossible. This mental state is depicted as  a PDG 
        on the left of \Cref{fig:grok-combine} (where we have left out
        the cpds, to avoid clutter).
		
		\begin{figure}
			\colorlet{colororiginal}{blue!50!black}
			\colorlet{colorsmoking}{orange!80!black}
			\tikzset{hybrid/.style={postaction={draw,colorsmoking,dash pattern= on 5pt off 8pt,dash phase=6.5pt,thick},
				draw=colororiginal,dash pattern= on 5pt off 8pt,thick}}
			\centering
			\scalebox{0.8}{
			\begin{tikzpicture}[thick, draw=colororiginal, text=black]
				\node[dpadded] (C) at (0,0) {$C$};
				\node[dpadded] (T) at (2,0){$T$};
				\node[dpadded] (SL) at (1,-1.5){$\it SL$};
				
				\draw[arr] (T) to[bend right] node[above]{$\mat q$} (C);
				\mergearr{C}{T}{SL}
			\end{tikzpicture}
			\hspace{2em}\vline\hspace{2em}
			\begin{tikzpicture}

				
				\begin{scope}[postaction={draw,colorsmoking,dash pattern= on 3pt off 5pt,dash phase=4pt,thick}]
					
					\node[dpadded,hybrid] (C) at (0,0) {$C$};
					\node[dpadded,hybrid] (T) at (2,0){$T$};
				\end{scope}
				
				\begin{scope}[thick, draw=colororiginal, text=black]
					\node[dpadded] (SL) at (1,-1.5){$\it SL$};
					\draw[arr] (T) to[bend right] node[above]{$\mat q$} (C);
					\mergearr{C}{T}{SL}
				\end{scope}


				\begin{scope}[thick, draw=colorsmoking, text=black]
					\node[dpadded] (S) at (-1.4, 0.8) {$S$};
					\node[dpadded] (SH) at (-1.45, -0.8) {$\mathit{SH}$};
					\draw[arr] (T) to node[fill=white, fill opacity=0.5,text opacity=1]{$\mat p$} (C);
					\mergearr{S}{SH}{C}
				\end{scope}
				
				
				
			\end{tikzpicture}
			}
			
			\caption{\small Grok's prior (left) and combined (right) knowledge}
			\label{fig:grok-combine}
		\end{figure}
		
		Grok is reading about graphical models because she
                vaguely remembers that the variables in
%joe4: no comma
%                \Cref{ex:smoking}, match the ones she already knows
                \Cref{ex:smoking} match the ones she already knows
                about. When she finishes reading the statistics on
                smoking and the original study on tanning beds
                (associated to a cpd $\mat p$ in \Cref{ex:smoking}),
                but before she has time to reflect, we can represent
                her (conflicted) knowledge state as the union of the
                two graphs, depicted graphically on the right of
                \Cref{fig:grok-combine}.  
	 
		The union of the two PDGs, even with overlapping nodes
%joe4
%                and is still a \MN. This would not have been possible
%                in general with a BN. In order to represent the two
%                and study) regarding the distribution on $T$ given
%joe4:I wouldn't call them ``parallel'' edges
%               $C$,  we needed to give $\dg M$ two parallel
%                edges. Had we not done this, we would have needed to
%                chose between the two or decide on a middle ground
%               immediately.  As we are already allowing inconsistency,
                and is still a PDG. This is not the case 
                in general with a BN. Note that the PDG that Grok used
                to          represent her two 
                different sources of information (the mother's wisdom
                and the study) regarding the distribution of
               $C$ is a \emph{multigraph}: there are two 
                edges from $T$ to $C$, with inconsistent information.
                Had we not not allowed multigraphs, we would need to
                choose between the two edges, or represent the
                information some other (arguably less natural) way.
                As we are already allowing inconsistency,
                merely recording both is much more in keeping with the
                way we have handled other types of uncertainty. 
		%
		%TODO: I should not say this yet. This is a related story that I haven't told yet. 
		%Moreover, if Grok were to later discover that her mother had been faithfully transmitting the results of an unrelated study, she would be justified in increasing her certainty that a cpd roughly like $\mat p$ and $\mat q$ were correct.
		% This suggests a result that is perhaps obvious in retrospect: the mere \emph{possibility} of inconisistency increases the value of consistency. For an agent that is guaranteed to be consistent by design, corroborating evidence has no value. 
	\end{example}

%joe4: I'm not sure that ``coherence'' is the issue        
%	Modeling inconsistency does not mean giving up on coherence:
%oli5: agreed. I've simplified it further.
% While it is important to us to be able to model inconsistency using PDGs,
% we do not want to view all inconsistenciese as equivalent.
Not all inconsistencies are equally egregious.
For example,
even though the cpds $\mat p$ and $\mat q$ are different, they
%joe4
%are numerically close---not all inconsistencies are equally
%        bad. Quantifying this will be the focus of
are numerically close, so, intuitively, the PDG on the right in
\Cref{fig:grok-combine} is not very inconsistent.  
Making this precise will be the focus of	
\Cref{sec:scoring-semantics}.


%joe4*: While I don't have an intrinsic problem with this paragraph,
%I'm not sure it belongs in the introduction.  Do we discuss this in
%more detail elsewhere in the paper?   If so, we have to say more
%about it.  As it stands, it seems like a letdown, after quite a
%compelling introduction.  I cut it for now.
%
%oli5: I agree with your assessment that it either needs to be followed
% up by something, or removed---although I'm not sure I agree there needs
% to be more text here. I strongly prefer to follow it up with something;
% I think path composition is one of the most important selling point of PDGs, 
% on par with the ability represent inconsistency, and showcases
% modularity in a useful, compositional way. To reflect this preference,
% I'm uncommenting this, but you're welcome to re-comment it in the next iteration.
%joe5*: commenting out, until you come up with a story for it that
%fits in the paper.  I strongly suspect that it won't make it into a
%NIPS submission, so by commenting it out, we'll be able to better
%judge space.
\commentout{
While a PDG is in some sense merely a set of constraints (the cpds), these constraints themselves have a useful computational meaning. Regarding cpds as stochastic matrices, we can get cpds corresponding to paths by multiplying them; equivalently, thought of as probabilistic functions, we can compose them.
	For instance, in \Cref{ex:grok-union}, if we were to give Grok
        unconditional probabilities in the form of vectors
        $\smash{(\vec s, \vec h, \vec t)}$ over the possible values of
        $\mathit{S, SH}$ and $\mathit T$ respectively, she could
        compute three distinct estimates for $\mathit{SL}$. This is
        perhaps clearest visually, but for clarity, if $\mat S$ is
        the cpd for the orange hyperedge that computes $C$ from
        $\mathit{S, SH}$, and $\mat L$ is the cpd for the
%joe4: the colors may not come across for some people, so you may want
%to use some other way of distinguishing them
%oli5*: Can I rely on colors to distinguish things in general? I've been using it throughout the document. I've seen papers that do this, but I can see why it might be poor taste (e.g., black and white printers). I can add letters to the hyper-edges here.
%        blue hyper edge, which computes $\mathit{SL}$ from $\mathit{C, T}$, and
%        $[\vec a; \vec b]$ is a vertical stacking of the vectors $\vec
    blue hyperedge that computes $\mathit{SL}$ from $\mathit{C, T}$, and we 
%oli5:
% use the notation 
		write
        $[\vec a; \vec b]$ for the matrix with rows $\vec
        a$ and $\vec b$, then 
	\[ \mat L \Big[\mat p \vec t; \vec t\ \Big],
		\qquad \mat L \Big[\mat q \vec t; \vec t\ \Big], \quad\text{and}
		\quad \mat L \Big[\mat S \big[\vec s; \vec h\big], \vec t\ \Big]  \]
	        are all probabilistic estimates of $\mathit{SL}$, which
                can be used in different circumstances: the first two are
        applicable even if given only $\vec t$, and the last requires
        all three values. 
	This property gives PDGs more useful structure than most
        collections of constraints.  
}
%joe5: \end{commentout}
	
	%
	% This will important intuition for understanding why PDGs are different from other sets of constraints, though we leave its formal treatment and implications to a forthcoming paper.

        
	These examples give a taste of the power of \MNs.  In the coming sections, we formalize PDGs and relate them to other approaches.		
% \begin{notfocus}
%	\begin{enumerate}[nosep]
%		\item This representation more naturally matches what humans are aware of, encoding small locally consistent models rather than one giant probability distribution
%		\item It is a strictly more general representation--- we can easily convert BNs to these diagrams (section \ref{sec:convert2bn})
%		\item This allows composition of arrows to be defined, and gives meanings to paths (section \ref{sec:composition}).
%		\item Allowing variables to be added and removed makes
%		\item Changing and partially determining arrows is more reasonable.
%		\item We can now represent inconsistency, which will allow us to capture mental states which, and . While we agree with the classical picture in that inconsistency is bad, now we can talk about it
%	\end{enumerate}


	% Redundency is important: types in programming languages, more data in ML systems.
	% Puts gurads
	% Makes it possible to combine knowledge without destroying old knowledge.
	% preference updating
	
	
%joe7
        %	\section{Formalism And Syntax}\label{sec:formal+syntax}
        	\section{PDGs: Syntax}\label{sec:formal+syntax}

	        
%joe7: line shaving
%	Having seen some examples of \MNs, we now provide the formal
        %        definitions.
We now provide formal definitions for PDGs.        
%joe4*: no!  Don't distract the reader this 
%        \footnotemark 
%joe4
%	Although it is possible to formalize the hyperedges directly,
%        we opt for a different approach here, in which they are
%        in which they are
%        actually a shorthand for a small widget with an extra node.  
        %	The following definition therefore only contains ordinary
	Although it is possible to formalize PDGS with hyperedges directly,
        we opt for a different approach here, in which PDGs 
	%oli5:
%joe5: besides the typo (missing ``edges''), I don't know what it
%means to ``admit'' an edge; changed back
        %		admit only
%oli6: This is minor, but because I tend to use the word 'admit' a lot
%and you don't like it: the connotation of 'admit' (a statement of
%what it the formalism will reject) feels more appropiate than 'have',
%which feels like it refers to what PDGs somehow already exist. For
%some reason "contain" also feels better than "have" though I don't
        %know why.
%joe6: while I could live with ``contain'', I think ``have'' is much
%is more standard.  Graphs have nodes and edges.  Graphs do not
%contain nodes and edges, nor do they admit nodes and edges.        
%oli7: I cede here. Added a clarificatory adjective.
		have only regular edges,
        % have only edges,
		% contain only edges,
	 	and hyperedges are captured using a simple construction
        that involves adding an extra node.
%joe7
%        The following definition therefore contains only ordinary
%        edges with a single head and tail.% 
%joe4*: I'm not sure why you separated the footnote and the
%footnotetext, in any case, we should not distract the reader with
%this here!
%oli5*: I'm willing to negotiate about where and in what form we say
%this, but I'm still completely convinced that non-strict PDGs are the
%more natural object. I do plan on referring to these as strict PDGs
%in the future, and want to make sure readers are aware of this. 
%joe5*: If ``the future'' is a different paper, then it's a bad idea
%to waste space in this paper to make the reader aware of it.
%	\footnotetext{Although we call the objects in
%                  \cref{def:model} \MNs, they are a specific case of a
%                  very similar, but more general representation for
%                  which we want to reseve the name; to reduce
%                  confusion for future readers, these are technically
%                  \emph{strict} \MNs.} 
%oli6: I see why it is negative to include things you don't talk about
%in a paper. But conflation of terminology drives me insane and here I
%have an opportunity to fix it, for the small price of a footnote
%somewhere. I would like to call the paper "strict PDGs" but then I'd
        %have to waste more space explaning why properly.
%joe6: Oliver, you're going to have to learn to live with this and
%many other things ...          
%oli7: ... but is there really no place where we can mention this at all? 
% The conclusion? 
%joe4*: I would definitely *not* say this, not even in the full paper.
%Why muddy the picture?   I also don't like the terminology.  The term
%``marginal'' applies to a single distribution; as I understand it,
%``pseudomarginal'' applies only to a collection.  If you look at a
%single distribution, you can't tell if it's a pseudomarginal.
%Finally I believe that there's a more standard usage of the term
%``pseudomarginal'' in the literature.
% oli5: can you point me to the more standard usage of ``pseudomarginal''?
%joe5: just type ``pseudomarginal'' into Google
%oli6: I've done this. You may be referring to paper such as "The
%pseudo-marginal approach for efficient Monte Carlo computations",
%which in fact use pseudomarginal because they're doing exactly what
%we are: they're keeping track of several conditional marginals which
%may not relate. The paper got a lot of traction, making me doubt that
%"pseudo-marginal" will turn people off, and also more certain that
%"pseudo-marginal" is the correct technical description of the flavour
%of our approach in multiple communities who should care about this.   
%%joe6: so the question is whether the gain is worth the cost.  I
%claim it's not.
 % oli5: It's true that this applies only to a collection, but that's what a PDG is. 
% oli5: As far as being poor terminology, I can empathize with the complaint of the two not being distinguishable, but this seems to be much more an issue with the term "marginal" than with "pseudomarginal"--- you cannot tell if a cpd is part of a collection of marginals (of a single distribution) by looking at a single one, but every cpd is a pseudomarginal (it does in fact apply to just one, it just coincides with the other term), and every collection of them is a collection of pseudo-marginals. In some sense the unnatural thing is the implication that the collection of marginals all come from a given distribution.
% oli5: I have also discovered that the term "pseudo-marginal", with
% precisely this meaning, does seem to be much more standard in the
% graphical model litterature than I initially thought. It appears in
% the Koller & Friedman book over 35 times, across 3 different
        % chapters.
%joe5*: I stand by my original statement.  We definitely should not
%say this here.  It's a distraction
%oli6: Can we connect to pseudomarginals somewhere? It seems important
%to signal that we are 
        % awarae of several communities doing work of this variety.
        %joe6: If (and only if) we can find a place to say it where it
        %fit naturally, which is definitely not here
	\vfullfootnote{In the factor graph literature,
                  especially with regard to loopy belief propagation
                  \cite{wainwright2007graphical}, it is common to
                  call a collection of marginals that are not
                  necessarily all compatible with a distribution
                  \emph{pseudomarginals}, making a PDG in some sense a
                  collection of `conditional' pseudomarginals. This
                  gives an alternate, more technically precise
                  expansion of PDG as ``Pseudomarginal Dependency
                  Graph''.} 

%	\moveme{Rather than representing a probability distribution, PDGs can be thought of as \emph{constraints} on distributions.}
% joe2: I cut this.  What is the point of the example?  Why is it here? What does it tell me that other examples haven't already told me.
% oli2: It is not particularly helpful. It is here because I was told
% many times not to go straight into the definitions; to slow down and
% motivate things, and then once I put this in you stopped mentioning
        % it.
%joe3: you certainly shouldn't go straight into the definitions, and
%you should certainly slow down and motivate things.  I stand by that!
%But you need to have in mind the big picture story.  Every example
%has to carry it's weight.            
	\commentout{Compared to a Bayesian Network, a PDG still consists of a directed graph, and the edges still inform conditional probabilities, but now each edge is interpreted individually. Consider the graph
	\[ A \!\rightarrow\! C \!\leftarrow\! B,\]
	which would be interpreted as three tables $\Pr(C\mid A, B), \Pr(A), \Pr(B)$ in a BN. Interpreting it as a \MNs, there are no distributions on $A$ or $B$, and the arrows into $C$ would be split into two separate tables $\Pr(C \mid A)$ and $\Pr(B \mid A)$, rather than a joint one. }

%joe8: why should \beta get a superscript?  That seems strange.  It
%will also come in handy later
        %	\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1, \beta)}
        \def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1, \beta#1)}
%        	\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1, \alpha#1,\beta#1)}
	\begin{defn}[\MN]\label{def:model}
		A \emph{\modelname} is a tuple $\mnvars[]$ where
		\begin{description}[nosep]
			\item[$\N$] $\notation{:\Set}$~is a finite
%oli6: I agree "set" is better here, esp. if we are avoiding the word
%collection. 
%				collection of nodes
				set of nodes%
		      	, corresponding to variables;
			\item[$\Ed$] $\notation{\subseteq \N \times \N
%joe4*: If you really want labels, you'll going to need to introduce a
%set of labels as part of the definition of PDG.  I think  much better
%approach is to use multisets rather than sets.  You're being too
%fuzzy when you say ``collection'' in any case
 %         \times \mathit{Label}}$~~is a collection of
%         directed edges, 	each with a source,
%target, and a (possibly empty) label\notation{$\ell \in \mathit{Label}$}. 
%oli5*: Multisets are technically wrong: they don't distinguish
%         between ``identical'' elements. For the example with the two
%         different sources of information, it matters which is from
                          %         where.  I just want a set of labeled edges.
%joe5*: I disagree, as a purely technical matter.  You're looking at a
%multiset of edges, but associating with different elements of the
%multiset different cpds.  The cpd tells you what the source of
%information is.  Notationally, working with multiset is much
%easier than working with labeled edges.  
%oli6*: I still think this is technically wrong. The formal definition
%of a multi-set is a pair (S, m), where $S$ is the underlying set, and
%$m : S -> Nat$ is a multiplicity. I am unable to even find a
%definition for a function whose domain is a multi-set, but it is
%problematic because already in a multi-set, say, {{2,2,3,4,4}},
%there's no way of distinguishing the two different elements "4". The
%only thing in a multi-set is the multiplicity. How could you
%consistently assign one cpd p, and the other cpd q, if you can't even
%tell which is which in the represntation? To make them
%distinguishable we need to add data which lets us know which is
%which: some kind of label.
%joe6: So why not take the label to be the cpd itself?  That is, we
%can take E to be a set of labeled edges such that the label is a cpd.
%Or say that we'll abuse notation slightly and allow different instances of the
%same element in a multiset to be associated with different cpds?  We
%already have enough notation in the paper.  Why burden the reader
%with more?  Carrying around labels l in some undefined set of labels
%is ugly. 
%oli5: I think the intuition of collection is appropriate. "Set"
%implies that equality is determined in a particular way. If I
%implemented this, it would be a list, but I don't care about the
%order. It can litterally be a set if I include the label, but can
                          %ellide the details if I use the word "collection".
%joe5: But ``collection'' is an undefined notion, and this is meant to
%be a formal definition.  What is a collection of nodes?
%oli6: I think of a collection as an abstraction that allows
%itteration
% (This is certainly the definition in TypeScript and Scala). I have
%it seen it mathematically formalized this way as well before but it's
%not standard. It makes the definition more general: we may not
%actually need all the properties of a SET (here we don't care about
%uniqueness, or maybe even being able to test if something is an
%element); we only need to be able to add things, remove things that
%we know to be in it, and be able to iterate over all of them.
%joe6: Again, the maount you're spending on this is absurd, Oliver.
%But, in general, I don't think it's a good idea to use idiosyncratic
%terminology. 
%oli6: I agree that the minimum amount of time that we can spend to communicate the idea to the largest number of people involves replacing "collection" with "set". And so we will do that.
% But I do believe that it muddles intuitions and for a formal
%definition that people would use in the future, and it feels morally
%wrong to define the main object of inerest this way because it
%requires more than required, and in particular . 
                          %oli5: Other equivalent options: a function
                          %(or equivalently indexed set): Label -> (\N
                          %\times \N), plus function \N \times \N ->
                          %(set of labels).   
                          \times \mathit{Label}}$~~is a set of
                          directed edges, each with a source and target in $\N$, as well as an arbitrary label;
% \footnote{Recall that a \emph{multiset} is just like a set,
%   except that repetitions are allowed.  We use the standard notation
%   $\{\{ \ldots \}\}$ to denote a mulitset.  Thus, for example,
%   $\{\{1,1,1,2,2\}\}$, $\{\{1,2,2,\}\}$, and $\{\{1,2\}\}$ are
%   different multisets.}
%oli5: I still think this is a helpful footnote.
%  \footnote{Though likely to be
%  inconsistent, a PDG might admit parallel edges, in which we need to
%  tag edges with a label to identify them, but in most cases, we will
%  use empty labels.} 
%joe2: CUT! graphs don't have ``parallel edges''  Multigraphs (which we are not considering) do except that they're not called parallel edges.
%oli2: I think the footnote was already cut. I know they're called
%multi-graphs. It is commmon to say "graph" when they mean
%"multi-graph" (from a topological perspective, multi-graph is the
%standard object, and called a graph) and to not deal with the edge
%definitions carefully; after being told my formal symbolic
%definitions were too much I was trying here to follow the more
%cavalier style (I would have totally said multi-graph, but
%hyper-graph was so taboo so I just said graph). Unimportant, but I am
%certain that `parallel edges' is a fairly standard term for two edges
%with the same terminals in a multi-graph; Bobby used it in lecture
%yesterday, and a google result reveals tons of these use cases; I
%don't care what we call them though. 
% In any case, I still feel strongly that a multigraph is much more
% natural here in particular. I know this is not the best use of my
                          % time to respond here now,
%joe5: Indeed it's not!
%but since we keep clashing on this, and a
% bunch of my results later on hinge on this definition, I'll try to
% explain more carefully. Some reasons: 
% (1) When combining two graphs that share a link, you want to be able
% to combine them even if you don't know for sure that the eges are
% disjoint. Doing otherwise would be structurally enforcing
% consistency in the way that we are trying to argue against. 
% (2) There's a nice special case of resolving parallel edges (of the same temperature): interpreting two parallel edges from 1 in the max entropy semantics is exactly Dempster's rule of combination. I think this is very cool. We decided to leave it out of the abstract b/c it's no longer something people care about as much, but it's true, and not possible to even frame this without parallel edges. There are are other similar cases.
% (3) Composition of directed edges, which I want to talk about all over the place, almost invariably will result in parallel edges. In particular:
% (3.1) When we use inconsistency to modify beliefs later on. In figure 8, $p'$ and $p$ are parallel. 
% (3.2) THe whole story of being able to use "cached beliefs" which we have kind of dropped for this paper but will be important for the discussion of preferences, and is still applicable here, relies on 
% (3.3) We can define ``strong consistency'' (Dexter's suggested term, not mine; this is the alternate notion of inconsistency arising from having different paths?) as the special case where the path multi-graph is just a normal graph
% (4) Many other ideas I haven't presented to you yet require multi-edges to do properly. Doing this leaves open the possibility of keeping belief histories, for example. 
% Who knows if I can motivate the multi-graph to your satisfaction, but I don't want to write a bunch of hacky case work to define things later on, when this _slightly_ more general version gives me so much more. 
%joe3: I don't think that any of the above is going to make it into
%the paper.  If and when we find a need for multigraphs, I'm happy to
%have them, with the appropriate motivation.
%oli8: reduce to line width
%			\item[$\V$] $\notation{\N \to \mathbf{Set}}$
%                          associates each node $N \in \N$ with a set
%                          $\V(N)$, 		
%                          set of values that node $N$ can take; 
			\item[$\V$] $\notation{\N \to \mathbf{Set}}$
                          associates each variable $N \in \N$ with a set
                          $\V(N)$ of values that the variable $N$ can take;
          	\item[$\mat p$] $\notation{\colon \big(\!({A,B,\ell})\colon \! \Ed \big) \to \V(A) \to \Delta\V(B)}$
			% HYPERGRAPH \mat p TYPE: $\colon\!\big(\!({\bf A,B})\colon \! \Ed \big) \to \prod\limits_{A\in \bf A} \!\! \V(A) \to \underline\Delta\left[\prod\limits_{B \in \bf B}\!\!\V(B)\right]$
%joe4
%			  associates, for each edge $L = (X,Y, \ell)
%			is a function that associates with each edge $L = (X,Y, \ell) \in \Ed$ 
% 			and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$. 
%oli8: definition update with "cpd"
			associates to each edge $(X,Y,\ell) \in \Ed$ a cpd $\bp(Y \mid X)$, which is a distribution $\bp(x)$ on $Y$ for each $x \in \V(X)$;
%joe8*: I thought of adding \alpha, then decided it was a bad idea; see below
%\item \alpha  $\notation{}$ associates a non-negative real number $\alpha_L$
% to each link $L$, indicating an agent's confidence in the
% independence structure 
 			\item[$\beta$] $\notation{:\Ed \to \mathbb R^+
%joe8*: can't the number be 0?  Also, is higher intended to be better;
%if so, we should say that.  But that means that we can't indicate
%complete certainty.  I would prefer to use a number in [0,1].  Is
%there a reason we shouldn't take \beta \in [0,1]
%oli10: 
% I think \beta=0 should be allowed, but if there is a \beta=0, we will use uniqueness guarantees. Also, this would effectively just contribute to the qualitative picture of the PDG, which is currently not 
% (\beta also has a connotation as inverse temperature in thermodynamics, which has commonly be used in related belief literature as a certainty, and used in exactly the same manner.)
%                        }$ associates a positive real number $\beta_L$
                        }$ associates a non-negative real number $\beta_L$
%joe8
%              to each link, indicating an agent's
              to each link $L$, indicating an agent's
                 subjective confidence in the reliability of cpd $\bp$. 
\end{description}                          
\end{defn}

	If $\dg M$ is a \MN, we reserve the names $\mnvars[^\dg M]$
        for its components, so that we may reference one (e.g.,
        $\Ed^\dg M$) without naming them all explicitly. 
	%; when the choice of $\dg M$ is clear we may omit the subscript.
%joe4: this seems more natural to me, but I won't insist.  You may
%also want to use this notation more generally, for \V(X), for
%example, where X is a subset of variables
%	We write $\V(\dg M)$ for set of all possible joint settings of
%oli5: Can you clarify: what is the 'this', that is more natural?
%oli5: I agree with the suggestion; I've edited this.
% 	We write $\V(\mnvars[^\dg M])$ for set of all possible joint settings of
% 	the variables in $\dg M$. 
	We write $\V(S)$ for the set of possible joint settings of a set $S$
        of variables; in particular, 
	we write $\V(\dg M)
		= \prod_{N \in \N^\dg M} \V^\dg M(N)$
	 for all settings of the variables $(\N^\dg M, \V^\dg M)$.
	

	While the definition above is sufficient to represent the class of all legal \MNs,
	we often use two additional bits of syntax to represent common constraints:  
	\begin{itemize}
	\item A special variable $\sf 1$ 
%joe4
%that always takes its unique value $\star$. It is used to represent
whose range consists of only element, which we denote $\star$.
It is used to represent
          unconditional distributions, as in
                  \Cref{ex:guns-and-floomps,ex:smoking}.  
% For strict \MNs, this is no different from any other variable that can take only a single value. 
%joe3*: I'll take any additidddon compelling examples. I didn't find them
%compelling before
%joe4: This breaks the flow and is not necessary
	\begin{vleftovers}
		\begin{examplex}\label{ex:worldsonly}
			A probability distribution $p$ over a measurable set $W$ of possible worlds is represented as 
			\begin{center}
				\scalebox{0.8}{
				\begin{tikzpicture}
					\node[dpadded] (1) at (0,0) {$\sf 1$};
					\node[dpadded] (W) at (3,0) {$W$};
					
					\draw[arr] (1) to node[fill=white]{$p$} (W);
				\end{tikzpicture}}
			\end{center}
		\end{examplex}
	\end{vleftovers}
%joe4: \end{commentout}
		\item Double-headed arrows, $A \tto
                  B$, which visually indicate the degenerate special
                  case of a cpd that assigns probability 1 to $f(a)$
                  for each $a \in A$ (corresponding to a deterministic
                  function $f : A \to B$). 
	\end{itemize}


        We can now explain how we capture   the multi-tailed edges that 
        were used in 
\Crefrange{ex:smoking}{ex:grok-union}. 
That notation can be viewed as shorthand for the graph that results by adding a new node at the junction representing the joint value of the nodes at the tails, with projections going back.  For instance,
% the diagram of the PDG in the shaded box of \Cref{subfig:smoking-pdg}
the diagram displaying Grok's prior knowledge in \Cref{ex:grok-union}, on the left of \Cref{fig:grok-combine}
%joe7: moved up from below, to save a line
%is really shorthand for the following \MN:
is really shorthand for the following \MN, where
where we insert a node labeled $C \times T$ at the junction:
	\begin{center}
		\scalebox{0.8}{
		\begin{tikzpicture}
			\node[dpadded] (SL) at (-1.0,0) {$\mathit{SL}$};
			
			\node[dpadded,light pad] (CT) at (-2.9, 0){$\scriptstyle C \times T$};
			\node[dpadded] (C) at (-4.8, -0.6) {$C$};
			\node[dpadded] (T) at (-4.8, 0.6) {$T$};
			
	%				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
	%				\draw[arr, color=violet] (X) -- (S);
	%				\draw[arr, color=violet] (X) -- (C);
	%				\draw[arr, dashed, color=violet] (X) -- (SC);
			
			\draw[arr, ->>] (CT) -- (C);
			\draw[arr, ->>] (CT) -- (T);
			\draw[arr] (CT) -- (SL);
			\draw[arr] (T) to [bend right=90, looseness=2] (C);
	\end{tikzpicture}}
	%%%%%%%%%%%%%%%%%  smoking fragment: %%%%%%%%%%%%%%%%%%%%%%
% 		\scalebox{0.8}{
% 			\begin{tikzpicture}
% 				\node[dpadded] (C) at (-1.0,0) {$C$};
% 				\node[dpadded] (T) at (0.5,0) {$T$};
% 
% 				\node[dpadded,light pad] (SSH) at (-2.9, 0){$\scriptsize \mathit{SH} \times S$};
% 				\node[dpadded] (S) at (-4.8, 0.6) {$S$};
% 				\node[dpadded] (SH) at (-5.0, -0.6) {$\mathit{SH}$};
% 
% %				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
% %				\draw[arr, color=violet] (X) -- (S);
% %				\draw[arr, color=violet] (X) -- (C);
% %				\draw[arr, dashed, color=violet] (X) -- (SC);
% 
% 				\draw[arr, ->>] (SSH) -- (S);
% 				\draw[arr, ->>] (SSH) -- (SH);
% 				\draw[arr] (SSH) -- (C);
% 				\draw[arr] (T) -- (C);
% 		\end{tikzpicture}}
	\end{center}
% That is, we inserted a node labeled $SH \times S$ at the junction.  As
% the notation suggests, $\V( \mathit{SH} \times S) = \V(\mathit{SH}) \times \V(S)$.
% The cpd for $(h,s) \in \V(\mathit{SH} \times S)$  associated with 
% the edge from $\mathit{SH} \times S$ to $\mathit{SH}$ gives probability 1 to $h$;
% similarly, the cpd for $(s,c)$  associated with 
% the edge from $ C \times C$ to $S$ gives probability 1 to $s$.
%joe7
%        That is, we inserted a node labeled $C \times T$ at the junction.
As the notation suggests, $\V( C \times T) = \V(C) \times \V(T)$.
%joe2: this is not the time to start talking about matri\mathit{SL}es
%Thus, $\V(S \times \mathit{SL}) = \V(S) \times \V(\mathit{SL})$; the matrix asso\mathit{SL}iated with
For any joint setting $(c,t) \in \V(C \times T)$ of both variables, the cpd for
the edge from $C \times T$ to $C$ gives probability 1 to $c$;
similarly, the cpd for the edge from $ C \times T$ to $T$ gives probability 1 to $t$.

%joe4*: cut this paragraph.  It's a distraction.  Moreover, BN's don't
%need this ``trick'', because of the way they interpret the edges.  
\commentout{
This trick will not work for a BN. We need cpds to be associated with
edges, so that the projections do not get in the way of the other
information we would like to encode. In this case, we would like to
give some probabilistic information on the edge from $T$ to $C$, but
if we think of this picture as representing a BN, then $C$ would
require a table for joint settings of $(C \times T, T)$. The only
reasonable way to provide this cpd is to ignore the second component,
and use the value of $c$ determined by $C \times T$---which clearly
has no more information than the projection itself.     
}

%joe4
%\subsection{Weighted PDGs}
%oli5: you've removed a lot of the section headers. Is this because
%you don't think they're useful structure, or because they take up
%space? 
%oli5: I have no problems removing them for any version of the paper,
%but they help me visually track and write more effectively so I'm
%temporarily putting them back in.
%joe5: I have no intrinsic objection to section headers, but in
%moderation.  It's definitely a bad idea to have a single subsection
%of a section, as this one is.  It should definitely be cut.
%\subsection{Weighted PDGs}


%joe4
%	Not all beliefs are the same, and as we give PDGS semantics, it will
%	prove useful to also keep additional metadata about the cpds.  
%joe4:
%	We can also generalize PDGs by associating with each edge a
%	\emph{weight}, which is just a number in $[0,1]$, that can be viewed
%	as indiciating our degree of confidence in the cpd encoded by that edge.
%oli5: Two issues with this rewrite: (1) We're actually assigning two
%weights, and (2) \beta \in [0,\infty] while \alpha is [0,1].   
%oli5: I'm taking another shot:
%joe6*: cut; this does not belong in the Nuerips submission, nor is
%this the right place for it in the full paper.  Introduce it if and
%when we use it.
\commentout{
	We can also generalize PDGs by associating with each edge $L$
        two additional parameters, a confidence $\beta_L \in [0,
          \infty)$, which can be viewed as indicating the strength of
          the belief that the cpd attached to $L$ is correct, and a
%joe5: I don't understand your intuition behind ``degree of
%causality'', and really don't like that name.  Is it standard?  
%oli6: It's not standard. It is a knob one might want to turn,
%effectively controls how important it is to preserve the cpd
%counter-factually, and my limited experience playing with it suggests
%that it lines up with other causal intuitions. Other suggestions for
%terminology are appreciated.
%joe6: I can't suggest good terminology for a notion I don't
%understand. But you definitely should avoid notation that suggests
%inappropriate connections.  I would have called ``degree of
%causality'' as the extent to which a is a cause of b.  That would
%make it somewhat related to what Hana Chockler and I called the
%degree of responsibility.  But I don't think that that's what you had
%in mind at all.           
          degree of causality $\alpha_L \in [0,1]$, which indicates
          the degree to which this cpd should be trusted if the
          distribution at its source were to change: 
	  %oli5: is this extra explanation overkill?
%joe5: It is potentially useful, but I don't undersatnd it.  In any
%case, I feel very strongly that this is the wrong place for this
%discussion.  You should discuss it when you use it.  This should be
%cut from here.
          %joe6: emoved paragraph break
%          
	$\alpha_L = 0$ corresponds to a conditional observation that
          one does not necessarily believe will generalize, and
          $\alpha_L = 1$ corresponds to a belief that there is a real
          mechanism with the same underlying direction of dependence.  
	As we will see in \Cref{ex:alpha-motivation}, this difference
        should change our calculus for approximating a PDG with a
        distribution, and will be a crucial point of distinction
        between a BN and a factor graph. 
%In particular, for each edge $L \in \Ed$, we associate two real parameters $\beta_L, \alpha_L$ indicating the certainty in a belief, its degree of causality, respectively. 


	\begin{example}\label{ex:alpha-motivation}
		Suppose $\V(X) = \{x_0, x_1\}$ and $\V(Y) = \{y_0,
%joe6: corected typo, on the off chance that this stays somewhere.
%                \ldots, y_100\}$, and consider the cpd $\mat p$
                \ldots, y_{100}\}$, and consider the cpd $\mat p$
                associated with an edge from $X$ to $Y$: 
		
		\begin{equation}
			\bp(x) = \begin{cases}
				\mathit{Uniform}_Y & x = x_0 \\
				\delta_{y, y_7} & x = x_1
			\end{cases}
		\end{equation}

		For instance, Grok starts to notice that there is a letter in her mailbox ($X = x_1$), then her neighbor always has the same letter in his mailbox ($Y = y_7$). If Grok has no letters ($X = x_2$), her neighbor has a one of many letters (Grok has been keeping track of the distribution, which is nearly uniform)
		
		You have no idea how often or Grok gets letters or why. Which of the following intuitively seems to be a more likely marginals on Grok and her neighbor's letters?
		
		\begin{enumerate}
			\item She get a letter with 50/50 probability, and her neighbor gets a particular one with 50\% probability, and others with small, equal probabilities which add to 50\%. \label{item:unifprior}
			\item Grok is much more likely not to get a letter, and her neighbor gets one of 100 letters with roughly uniform probability. \label{item:infoprior}
		\end{enumerate}
		 	
				
		Though we could tell other stories with the same cpd,
                where (\ref{item:unifprior}) feels more appropriate,
                in this case it seems natural to conclude that option
                (\ref{item:infoprior}) is more reasonable. The reason
                is that the difference in information between the two
                settings is very large. If have no a priori reason to
                think one world is more likely than another, then with
                this constraint, there are 100 possible worlds where
                 $X = x_1$ and only one where $X = x_0$. Thinking that
                (\ref{item:infoprior}) is more approriate corresponds
                to calcluating the distribution of maximum entropy
                satisfying the cpd, thought of as a constraint. The
                larger the number of possible values of $Y$, the
                smaller the intuitive value of $\alpha$. 

		
		On the other hand, thinking that (\ref{item:unifprior}) is more appropriate corresponds to the causal view: in this case we start by acknowledging we know noting about $X$ and therefore assigning it the maximum entropy distribution, and then asserting that . This is rougly equivalent to asserting that $\mat p$ holds counterfactually, independent of the distribution on $X$. 
	\end{example}
	
	We therefore adopt the following definition, with both $\alpha$ and $\beta$ parameters:

	\begin{defn}
		A weighted PDG $(\dg M, \vec \beta, \vec \alpha)$ is a PDG
	%joe4*: I would replace ``certainty'' by ``confidence''.  If you want
	%to include what you're calling ``degree of causality'' as well, you
	%*must* give some intuition here.  It can't come out of the blue.  You
	%don't have to go into detail, but you have to say something.  I have no
	%clue what it reprsents.
	%oli5: I've added more context above, and an example below.
	        $\dg M$ together with a \emph{confidence} $\beta_L \in \mathbb
	        R^{\geq 0}$ and \emph{degree of causality} $\alpha_L \in
	        [0,1]$ for each edge $L \in \Ed^\dg M$. 
	\end{defn}

	%joe4: please use ``edge'' consistently, rather than ``link''.
	%oli5: right. Oops.
	We identify an unweighted PDG with a weighted PDG where $\alpha_L
	= \beta_L = 1$ for each edge $L$.
}
%joe6: \end{commentout}
%
%joe7: line shaving
%        In the next section, we give semantics to PDGs that make use of
%these parameters
%oli5: I've now already done most of the justification I was planning
%to do later. 
%, and justify their names.
 %joe5: Your justification didn't work for me, I'm afraid ...



%\subsection{Operations on PDGs}
%As illustrated in \Cref{ex:grok-ablate,ex:grok-union}, it is possible to syntactically combine and restrict PDGs in constant time without additional assumptions. We leave the formal details to \Cref{sec:pdg-operations}.


%joe7
        %	\section{Semantics}\label{sec:semantics}
        \section{PDGs: Semantics}\label{sec:semantics}

%joe1: Much too wordy, and hides the key points
	% We view of \modelnames\ as being a representation of beliefs in and of themselves, rather than a compression of something more fundamental such as a probability distribution. That said, it is still incredibly important to interpret them in various ways, as this will give us ways of comparing them to other graphical models, prove things about \MNs, and give a lot more intuition about how they work.
%oli2: Your changes are better written, but it does not mention the
%point that I was trying to make: that the constraints themselves,
%rather than the distsributions, should be manipulated. The semantics
        %are just views of the distribuiton.
	There is more than one way of giving semantics to a \MN.  We discuss three related approaches.
	The first is the simplest: we associate with a PDG the set of distributions that are consistent with it. This set will be empty if the PDG is inconsistent.
	%
	The second approach associates a PDG with a scoring function,
        indicating the fit of an arbitrary distribution $\mu$, and can
        be thought of as a \emph{weighted} set of distributions
%oli8: added punctuation and reference back in
%joe7:
        %	\cite[cf.][]{halpern2015weighted}.
\cite{HL12}.
%joe4: I'm not sure what \cite is.  Is this the suggested
%approach for NIPS?  It seems weird. ``(cf.\ \cite{...})''.
%        \cite[cf.][]{halpern2015weighted}.  For an inconsistent
        %        \MN, some distributions may still be better than others.
This approach allows us to distinguish inconsistent PDGs, while the first
appraoch does not.        
  %joe3*: Added some sentences here, which incorporates some of your
  %material below.  We need to make that we're using the same notation
  %for a distribution throughout.  We'll probably want to move the
  %discussion of free energies in factor graphs to the factor graph
  %section (since that's where the paper will probably end).  
%joe4: in retrospect, this is premature; I cut it from here.
%	Roughly speaking, the larger the changes required to the cpds
%        in a PDG before a distribution $\mu$ becomes consistent with
%        the PDG, the lower the weight associated with $\mu$.  
      %	Even if it is now intuitive how PDGs can be used to represent beliefs, by providing them with semantics---that is, by viewing them in terms of standard, well-established notions of uncertainty---we will be able to better articulate the relationships between PDGs and other graphical models.
%joe3*: I strongly object to this.  It's like saying that we give
%semantics to Python so that we can compare it to C++..  That's not
%the primary motivation!  We give semantics so that the objects we use
%(PDGs) have a clear and unambiguous meaning (or more than one such).
	%oli2: I originally had adopted your phrase "There is more
	%than one way to do this", but decided that this is
	%misleading: for a given target (distributions, sets of
	%distributions, etc.), we only provide one, and indeed think
	%that our constructions are natural.  
%joe3: I strongly disagee with the next sentence.  Does that mean that
%if we had decided that we wanted PDGs to represent a single
%probability that our semantics that associates with a PDG a set of
%PDGs would be in appropriate?   There's nothing intrinsic about a BN
%or a PDG that says its semantics has to be a unique probablity (or  a
%set ofprobabilities)
%	Ultimately, we find (\Cref{sec:scoring-semantics}) that a
%        \emph{weighted distribution}
%        \cite[cf.][]{halpern2015weighted} much better captures
%        these features; furthermore, as we will see, the weighted
%        distribution semantics is closely related to a configuration
%        energy, which in \Cref{sec:thermo} we compare with the already
%        well-developed theory of free energies in factor graphs.  
%joe4*: this wasn't in the .tex file, although it is in the pdf.  I
%included it.  Question: do we always got a unique distribution?  I
%have no problem with ties.
%Our third approach chooses a particular distribution from the weighted
%set of distributions defined by the second approach, thus associating
%joe
%The third approach chooses the best distributions from the weighted
%set of distributions defined by the second approach, typically associating
The third approach chooses the distributions with the best score, 
typically associating with a PDG a unique distribution.
%joe7: line shaving 
%As we shall show, it is this third approach that allows us to recover
%BNs from PDGs.  

\subsection{PDGs As Sets Of Distributions}\label{sec:set-of-distribution-semantics} 
%joe3: I really don't like this.  That's not how I think of a PDG. I
%don't thinking that bringing in BNs is helpful.  A PDG hs no
%independence assumptions.  I think that thi muddies the waters.
%oli3: I don't see why what you wrote above would be problematic for the  bit that I have... indeed, PDGs have no assumptions.  
	% If we think of a PDG as BN except without independence assumptions, then just as a BN represents a single distribution on joint settings of its variables, a PDG might be thought of as representing the  \emph{set of all distributions} that are compatible with the given tables, with or without the independence assumptions.
	% If we had a (quantitative) BN is of a DAG $G$ labeled by variables, and a assingment $f$ of cpds to variables, we could obtain a qualitative one by simply looking at the graph structure, which corresponds to a collection of independence assumptions. There is then a set of all distributions that have those indepies. Alternatively, we could just take the assignments $f$ at the nodes. 
	
	We have been thinking of a PDG as a collection of constraints
        on distributions, specified by matching cpds. From this
%joe4
%        perspective, the first natural thing to consider is the set of
        %        all distributions that match.
                perspective, a natural thing to consider is the set of
        all distributions that are consistent with the constraints.
	
		% As the data of PDG consists of a collection of cpds, the natural thing to do . 

	\begin{defn} \label{def:set-semantics} %oli2: made words better. 
		If $\dg M\sheq\mnvars[]$ is a \MN, let $\SD{\dg M}$ be
%joe3
%                the set of distributions over the variables in $\dg M$
                the \emph{s}et of \emph{d}istributions over the
                variables in $\dg M$ 
%joe3
%                that whose conditional marginals on every edge are
                for which the conditional probabilities are exactly 
%joe7
                %                those given by $\boldsymbol\mu$.
                                those given by $\mat p$.
%joe3: added a bit of formality
%oli4: there are labels on edges again 
%joe4*: no there aren't :-).  More importantly, please be consistent
%about your notation for distribution.  Use p or use \mu, but don't
%use both.  That makes life hard(er) for the reader
%          That is, $p \in \SD\dg M$ iff, for all edges $L = (X,Y, \ell) \in 
%joe7*: Please be consistent with p vs \mu.  I've switched this to \mu
%        That is, $p \in \SD{\dg M}$ iff, for all edges $L = (X,Y) \in
  That is, $\mu \in \SD{\dg M}$ iff, for all edges $L = (X,Y) \in 
                \Ed$,  $x \in \V(X)$,  and $y \in \V(Y)$, we have that
%joe7
%    $p(Y\sheq y \mid X\sheq x)$ is the $x,y$ entry in the cpd given by
    $\mu(Y\sheq y \mid X\sheq x)$ is the $x,y$ entry in the cpd given by
                $\bp$.
%oli4: I don't understand how this easier to read than my definition below, and and it's way less visually distinct and intentionally typeset. But I'll keep it.    
		\notation{Formally,		
		\[ \SD[\Big]\dg M = \!\left\{\mu \!\in\! \Delta \V_\none (\dg M) \middle|\!
		\begin{array}{l}
		\mu(B\!\! =\!\!b \mid A\!\!=\!\!a) \geq \boldsymbol\mu_L(b \mid a) \\[0.1em]
		~\text{$\forall (A, B,\ell) \!\in\! \Ed$, $a \!\in\!\mathcal V_A$, $b \!\in\! \mathcal V_B$} \end{array}\!\!\! \right\}\]
		}
%joe7: I'm not sure it was mentioned earlier
%	\end{defn}
%
                %	As mentioned earlier, we say that the PDG
                $\dg M$ is
\emph{consistent} if 
$\SD{\dg M} \ne \emptyset$, and
%joe7: !
%consistent otherwise.
inconsistent otherwise.
\end{defn}

	
%joe4*: this is the wrong place for these results, even in the full
%paper.  I can imagine that the convexity result is useful, but I
%would defer it to the stronger version for the second semantics.  The
%discussion of over-constrainedness is a distraction.  Do we really
        %even need this definition?
%oli5: This is the place we first place we've finished the definitions
%required to state these results, and I'm not using them yet. I don't
%know what does or does not go in the full paper, but right now I'm
%using "vfull" as the place where I store all of the relevant facts
%and useful intuitions that I will otherwise forget, in an order that
%is readable, though not a coherent story. If that's not what the full
%paper should be, I will add an intermediate level of commenting that
        %uses those as well
%joe5: That's definitely not what the full paper should be!  It's not
%just a collection of useful facts!  There has to be a story, and this
%is the wrong place in the story for this result.        
%oli5: with reference to the over-constrainedness, I think I can get
%away with not using it (which I plan to because of space), but I find
%it useful. It is the property that separates the BN-like PDGs from
%the ones that have at least one merge in the PDG sense. This is a
%useful technical characterization of over-constrained PDGs as the
%non-simple ones that exhibit weird behavior that we don't see in any
%other graphical model.  
%oli5: I've removed the \commentout because I want to leave the proof
%in the appendix for now, but I will defer on where to put it. I
        %mostly want to remember that it exists.
%joe5: A better fix is to change the semantics of vfull so that it's
%``commentout''.  Then when you want to write the full paepr, you can
%change the sematics so that it appears in gray.
%oli6: Done. To do this yourself (e.g., if I turn it back on and forget to
% turn it off again), uncomment the line "\excludecomment{vfull}".
% at the top of the document. 
% To avoid further confusion, there are now two environments
% that comment things out: vfull, for things which are only relevant in 
% THIS paper, and a "vleftovers" for stuff which is would go here if we
% writing down everything, but could be useful in a different paper.
	\begin{vfull}
		It turns out that this semantics only results in convex sets. This may provide useful intuition, and we will prove a stronger version of this statement that corresponds to our second semantics.
		\begin{lemma}[restate=thmsetconvex] 
			\label{prop:convex}
			$\SD{\dg M}$ is convex, for any PDG $\dg M$.
		\end{lemma}
	
		Note that being inconsistent is not the same things as \emph{over-constrained}: 	
		\begin{defn}

			$\dg M = \mnvars[]$ is over-constrained if there exists
			  \emph{some $\mat p'$} assigning cpds to the same edges as
			  $\mat p$, such that $(\N, \Ed, \V, \mat p')$ is inconsistent
			  \notation{(i.e., $\SD{\N^\dg M, \Ed^\dg M, \V^\dg M, \mat p}
				= \emptyset$)}, and under-constrained if there are
			  multiple distributions in $(\N, \Ed, \V, \mat p')$ for
			  \emph{every such $\mat p'$}, making this a property of the
			  qualitative PDG $(\N, \Ed, \V)$.  
		\end{defn}

		We know that an under-constrained PDG is consistent without even looking at the tables. However if a we know that an \emph{over-constrained} PDG is actually consistent (when it could have easily contradicted itself), the information provides corroborating evidence, and one can take this as support in favor of the beliefs. 
	\end{vfull}
        
%joe3        
        %	\subsection{\MN S AS WEIGHTED
%joe7
        %	\subsection{PDGs As Distribution Scorings}
        	\subsection{PDGs As Scores of Distributions}
        \label{sec:scoring-semantics}   
%oli3: It's still not a set of distributions with a weight. 
%	If a probability distribution is a soft constraint on the set
%of possible worlds, then a PDG is a soft constraint on the set of
        %probability distributions.
%joe4*: While I accept that the metaphor of energy may be useful for
%some readers, it will not be useful for others, and you can't
%introduce it out of the blue.  Moreover, I see no benefit in using it
%here, when the ``scoring'' metaphor, which you've already used, works fine.
        %	We now generalize the previous semantics by viewing a PDG as
%        an energy landscape on distributions, assigning high energies
%        to distributions which do not fit the PDG well.  
%oli5: I have no problem using "score" instead of "energy" (both are
%        standard in different litteratures). In my eyes they seem
%        roughly equivalent, except on two fronts: (1) the intuition
%        of good for scores is reversed: we want HIGH scores and LOW
%        energies---in our case small is good---and (2) The energy
%        analogy is necessary for undirected graphical models, and
%        makes the connection to statistical physics more explicit,
%        which I view as positive. I think we can explain either
        %        metaphor without leaving people hanging.
%joe5: My preference would be to use only ``score'' ere, and introduce
%the physics language if and when you discuss things like exponential
%families.  
%oli5: The bit you've written below strikes me as overly verbose. I
%don't think it's a good use of space or says much. 
	% We now generalize the previous semantics by viewing a PDG as
	% as a \emph{scoring function} that associates a score with each distribution.
	% We associate scores not just to distributions consistent with the PDG;
	% distributions inconsistent with the PDG also get a score, where,
	% intuitively, the more consistent a distribution is with the consraints
	% in the PDG, the better its score.  Not only may distributions
	% inconsistent with the PDG get different scores; distributions
	% consistent with the PDG may also get different scores.  We choose here
	% a particular approach to assigning scores to distributions, which
	% turns out to be quite useful.  
% This sentence is the kind of thing I've been avoiding: due to your feedback it hints at something that is not in the paper, and feels like premature reaction against "why did you choose this one"? I also think I have better responses to that question, and actually believe that the semantics is natural---though I know I have yet to convince you of this.
	% Of course, other approaches may  prove  useful as well.
%oli5: I've tried to rewrite it again, incorporating your material.
	We now generalize the previous semantics by viewing a PDG $\dg M$ as
%joe8
%	as a \emph{scoring function} which, given an arbitrary
	a \emph{scoring function} that, given an arbitrary
        distribution $\mu$ on $\V(\dg M)$, 
	returns a real-valued score indicating how well $\mu$ fits $\dg M$%
%oli5: optional:
	% ; the score is thought of as a penalty, making smaller scores better
	. Distributions with the lowest (best) scores are those that
%oli8:
	% both (1)  are closest to being consistent with $\dg M$, 
	% and (2) for which $\dg M$ explicitly details
	% the correlations in $\mu$.
	most closely match the cpds in $\dg M$, and contain the fewest unspecified correlations.
%oli8: maybe the sentence below is unnecessary...
	We now make this precise.

%joe4
%oli5:
%Again, I'm happy to delete these later but they help me track for now.
%joe5: deleted.  Here I think it breaks the flow in a way that I find
%not useful
%	\subsubsection{Continuous Inconsistency}
%    The semantics of \Cref{def:set-semantics} treats all inconsistent
%    PDGs as equivalent, which is unsatisfying. For instance, in
%oli5: This is very misleading: we're assigning the component of the
%score which measures consistency, to ALL distributions.  
%	 We start by assigning scores to distributions inconsistent with PDGs.
%joe5: Sorry; what's misleading about this?  If you want, you can add
%a parenthetical comment saying that we give a score of 0 to all
%consistent distributions.
%joe7: I don't think that it heps to say it's a continuous relaxtion
%oli9: It is a generalization of the previous definition though. Do you think this is obvious without mentioning it? If not I'd like to say something to this effect.
%	We start with the first component of the score--a continuous 
% relaxation of \Cref{def:set-semantics}---which assigns higher numbers
	We start with the first component of the score,
which assigns higher scores       
to distributions that require a larger perturbation in
        order to be consistent with $\dg M$.  
%
%joe4: this seems quite repetitive
%oli5: There's one problematic sentence, but I think the rest is useful. I think it's mostly repetetive in the context additions you made above.
%oli5: What do you think of referring to the example?
%joe5: I have no problem referring to the example.  
        %To understand the intuition, recall that in
%\Cref{ex:grok-union}, the two cpds $\bf p$ and $\bf q$ were close
%but did not quite agree. To address this, we generalize the
%    semantics presented in
%    \Cref{sec:set-of-distribution-semantics}. For a candidate
%    distribution $\mu$, rather than assigning it an all-or-nothing
%    score of whether or not it is consistent, we give it a continuous
%    one, encoding how far we have to nudge our tables to make our PDG
%    consistent. Intuitively, distributions that would be consistent
%    with only small perturbations of the cpds $\mat p$ are better
%    those that would require larger ones.   
%oli5: I'm also commenting this out, because I can add it above.
% As we said, we view distributions that are consistent
%     with only small perturbations of the cpds in the PDG as being better that
%     those that would require larger perturbations.
%
%joe3*: It would be good to have an example of degrees of
%inconsistency here, before we give them a score.  One of the
%xamles from the intro would work, with variants of the cpd making it
%more or less consistent.
	We measure the magnitude of this perturbation with relative
%joe7: there's no point saying this unless we eplain it, which we
%won't have the space to do.
%oli9: we don't need to say this if it takes too much space, but I do 
% think the factor graph section will deliver here.
        %        entropy, which results in interesting connections to
                entropy.
%joe4: Again, please use edge consistently  (or use link
%consistently); also please remove all labels
%oli5: I've reinstated the labels because the multi-sets are
%tecnically insufficient, but I'd like to understand why you're so
%against them. Here's an alternative notation if you're worried about
%complexity: `` for each edge X \xrightarrow{L} Y$. Then the label L
%is conflated with the name of the edge, and there is no \ell.   
        %oli5: This is more precisely what I mean:
%joe5: I don't mind talking about labeled edges, but I would take the
%label to be the cpd, not an element of some arbitrary set of labels.
%oli6: I am ok with the conflation, but I don't want the underlying
%formalism use the 
% be cpd as the label, because we there's no reason to preclude two
        %copies of the same cpd.
% intuitively, the label is some sort of context. Intuitively, the
%label identifies the source 
% of knowledge in our intro example. Vaguely relatedly, I suspect that
% having control over the label set toplogy enables some interesting things. 
%joe6: OK; I guess I can see that different edges correspond to
% different sources of information.  If that's your intuition, we
% should say that in the paper.  But I still don't think it's worth
% the overhead in notation to carry around the label.
%joe7
%    the existing theory of graphical models and variational inference.
		% other measures of uncertinty.
                          In particular, for each edge $L =
		       (X,Y, \ell)$, and each $x \in \V(X)$, we measure the
		% measures of uncertinty. In particular, for each edge $L =
		% (X,Y)$ and $x \in \V(X)$, we measure the
%joe6: this notion seems wrong.  You should use $(\mu\mid X)_Y$; I
%could also live with \mu(Y|X=x), but not  \mu_Y|(X=x), both of which you use
%later.  (So you actually have three differen notations for this
%notion in the space of four paragraphs.
%oli7: oops...
                %		divergence from $\bp$ to $\mu_{Y \mid x}$ ($\mu$
                %joe7
                expected 
%joe8
%      		divergence from $\bp$ to $\mu(Y\mid X=x)$,
%oli10: it's not symmetric, so linguistically the direction matters. (and unfortunately it's written backwards, so "from the second argument to the first argument" is correct.)
%      		relative entropy between  $\bp$ and $\mu(Y\mid X=x)$,
			relative entropy from $\bp$ to $\mu(Y\mid X=x)$,
                %joe7: I'm not sure this helps
%                ($\mu$  conditioned 
%                		on $X=x$, and then marginalized to $Y$). 
where the expectation is taken with respect to $\mu_X$ (that is, the
marginal of $\mu$ on $X$). 
We then sum over all the edges $L$ in the PDG,
weighted by their reliability.
	        % which is intuitively the expected number of extra bits required to transmit a code optimized for $\mu$ if using the cpd $\bp$ instead. 
%oli10: this is a duplicate of the above
%	We weight these costs by the probabilities $\mu(X=x)$ that
%joe7
%        $\mu$ prescribes, and sum across all edges to get following
%        $\mu$ prescribes, and sum over all edges to get following
%        definition: 
        
%joe3 	
	% 	We are missing two important details we need to make this work. 
	% 	First, we want to score distributions based on exactly the
	%        marginals for the edges $\Ed$ that an agent claims to know
	%        something about.  
	% 	Second, we measure distance by relative entropy.
%oli4: this is not correct, we don't compare this way.
	% There are many ways to capture this intuition.  We consider a
	% particular approach that has interesting connections to other
	% representations  of uncertainty.  The idea is to measure the distance
	% between two distributions using relative entropy
	% Specifically, we
	% compare two PDGs $\dg M_1 = (\N, \Ed, \V, \mat p)$ and $\dg M_1 = (\N,
	% \Ed, \V, \mat p')$ 
	% that have the same set of nodes, ranges of the nodes,
	% and edges, but may differ in the function associating cpds with
	% edges.  For each edge $L \in \Ed$ and each value $x \in \V(X)$, we
	% compute the entropy of $\bp(x)$ relative to
	% $\bp'(x)$; then we sum over all $x \in \V(X)$ and all edges
	% $L$. 

	\begin{defn}\label{def:zeta-score}
%joe3: can we use a friendlier letter than \zeta
%oli3: what do you mean by friendlier?
          %	  The inconsistency, $\zeta$, of a PDG $\dg M =
%oli8: slightly alter connotation: compatibility > consistency here? Acronym can be the same.
% Is this a positive or negative conflation?
%       	The \emph{inconsistency} of a PDG $\dg M = \mnvars[]$ with
%oli10*: 
% So what's the verdict? Incompatibility or inconsistency? We talk about inconsistency a lot, so if we don't include the definition of inconsistency of a PDG (without a distribution) below, we probably should call this inconsistency -- even though I agree that this feels slightly wrong.
	       	The \emph{incompatibility} of a PDG $\dg M = \mnvars[]$ with
%jeo4
%          respect to a distribution $\mu \in \Delta[\V(\dg M)]$ is  
%oli8: "respect to"
%          respect to a distribution $\mu \in \Delta[\V(\dg M)]$,
		a joint distribution $\mu$,	
%joe7: Why do you use a semicolon rather than a comma (which seems
%more natural to me). I changed it globally
                %                denoted $\Inc(\dg M;\mu)$, is
                                denoted $\Inc(\dg M,\mu)$, is  
	                        \[
			\Inc(\dg M , \mu) := %\inf_{p \in \Delta(W^{\mathcal V})}~ 
			\!\!\!\sum
			%oli8: using new command
				\alle
%			_{X \xrightarrow{\!\!L} Y}
			%oli7: added missing \beta (and change spacing)
%joe8: Note that, before, the \beta came out of the blue.
				\beta_L
                        \mathop{\mathbb E}_{x \sim \mu_{_X}}
                        \left[\kldiv[\Big]{ \mu(Y\!= \cdot\ | X \sheq
                                                        x) }{\bp(x) } \right] ,
		\]
%oli2: I suppose I also need to define relative entropy?
%joe3: yes
		where $\kldiv{\mu}{\nu} = \sum_{w \in \text{Supp($\mu$)}} \mu(w) \log\frac{\mu(w)}{\nu(w)}$ is the 
%oli8: insert "relative"
		relative
		entropy from $\nu$ to $\mu$.
%oli2: I cut this description, as I did a lot of explaining above.
%		taken between the two distributions over $B$: one given by the edge from $\bmu_{A,B}(a)$ and the other given by the marginal distribution of $p$ conditioned on $A = a$, over the variable $B$. 
                %joe3: moved back from below; this is where it belongs:
%joe3: you're inconsistent between using M and \dg M for a PDG.
%Fortunately, it's hard to distnguish them.  But you should still be
                %consistent.
%joe7*: It seems strange to talk about the inconsistency being the
%minimum incompatibility; it would be more natural to use the same
%word.  More importantly, do we ever use this?  I cut it for now
%oli9: Do you prefer one over the other? This change was partly in response to your comment that "inconsistency" wasn't the right word to capture a failure for a particular distribution to match the PDG. But I think it is very important to define inconsistency (the fact that PDGs let you talk about it is supposed to be one of their strengths).
%oli9: we mention "inconsistent PDGs" a lot in the intro, and this
%definition may clarify what we mean. The inconsistency is also
%related to the normalization constant of a factor graph, which we
                %would ideally explain if we had room.
%        The \emph{inconsistency of PDG $\dg M = \mnvars[]$}, 
%                denoted $\Inc(\dg M)$, is the
%        minimum possible incompatibility of $\dg M$ with any
%        distribution $\mu$,  
%joe3: you're switching between \mu and p again
%		\[ \Inc(M) = \inf_{ \mu \in \Delta [W_{\cal V}]}
%\Inc (M; \mu) . 
%joe8*: we never use it technically, and we don't have room, so I cut
%it.  We can reinsert it if we use it.                
%oli10: We now use it a lot in our proofs. We can either uncomment it here (benefit: we deliver on describing inconsistent PDGS, not just with respect to distributions), or in the appendix (which will give us space for higher value things. For now I've uncommented it but either resolution is ok with me.)
%		\[ \Inc(\dg M) = \inf_{\mu \in \Delta [\V(\dg M)]}
%                \Inc (\dg M; \mu) . \]
%joe4: It would be useful to have a marker for the end of the
%definition.  I'm not a big fan of the triangle marker that you used
%before (and it's quite nonstandard).  
        \end{defn}

%oli8: I now think the three paragraphs here discussion are way out of scope. If people know things about KL divergence, they may see where this is going; otherwise, it's very much not worth the time or space. It's tricky to convey and not particularly important.
\commentout{
%joe4*: you can't start using the terminology of encoding bits (which
%I realize is standard in the max ent literature) out of the blue.
%You need a sentence that explains what's going on and puts it in context.
%As written I find neither this paragraph nor the next helpful.
%oli5*: I don't understand the problem very well. I spent a lot of
%time on these paragraphs trying to expand everything. I can keep
%going but I worry that it's not a good use of my time or the
%available space. Maybe we can talk through this and you can tell me
        %how you would convey it next time.
        %joe5: OK
        % 	We want to choose a distribution $\mu$ that minimizes the
        The idea behind this definition of inconsistency is that
 	we want to choose a distribution $\mu$ that minimizes the
        total number of bits required to encode all of the relevant
        conditional marginals. 
 	More precisely, fix a distribution $\mu$. For each edge $L = (X, Y,
%joe4* Please address the %joe3 comment below (and remove the label)
%joe3: Where is the optimal code that we are supposedly given?
%oli5: I don't understand this complaint either. The code might be a
%complicated object 
% and its details don't matter---what matters is that if it's
% optimized to transmit samples from the model, then using it on
% samples generated from a distribution will be less efficient (longer
% code words for things that were thought to be infrequent, end up
        % being used more frequently). What do you mean by where?
%joe5*: My problem is that I have no clue of what you're talking about.
%(Well, that's not completely true, since I've seen some of the
%connection of entropy to codes before. But for those who haven't seen
%it, or haven't really absorbed it -- which includes me -- this is
%comletely mysterious.  You start talking about optimal codes that we
%are ``given'', and I have no idea what this is
%joe6*: what is a code for Y?  
%oli7*: It is an encoding of the values of $Y$ as sequences of bits.
% A code that is  "optimal for a distribution \mu" is one for which 
% the expected number of bits of a sample of Y, drawn from \mu, is minimal.  
        %        \ell) \in \Ed$ and $x \in \V(X)$ we are given a code
                \ell) \in \Ed$ and $x \in \V(X)$, we are given a code
        for $Y$ optimized for the distribution $\bp(x)$, and asked to transmit
%joe6: I would expect the cost to involve a difference between two
%quantities, but the formal expression doesn't seem to involve a difference.
%There is a difference in the defintion of ``extra information''
%below.  So perhaps this explnation is premature.  
%oli7: the definition of relative entropy has a log of a ratio; expanding it 
% results in the diference you're looking for.
        data from $\mu(Y\mid x)$; we incur a cost for each bit
        required beyond what we would have used had we used a code
%joe6
        %        optimized for the actual distribution $\mu(Y\mid x)$.
                optimized for the actual distribution $\mu(Y\mid X=x)$.  
%joe3: I don't understand the following sentence at all
%oli4: modified.
       To obtain the cost for $L$, we take a weighted average of these costs, where the weight for the value $x$ is the probability $\mu_X(x)$. We do this for every edge $L \in \Ed$, summing the cost.

%oli7: named the agents for discussion further on
	% For even more intuition, imagine two agents with identical
	For even more intuition, imagine two agents ($A$ and $B$) with identical
%joe6: 
%        beliefs $\dg M$, about a set of variables that are in fact
        beliefs described by a PDG $\dg M$ about a set of variables
        that are in fact 
        distributed according to $\mu$. For each edge $L = (X,Y, \ell)
        \in \Ed^\dg M$, values $x,y \in \V(X)$ are chosen according to
%joe6*: Is this what you meant?  Or are you choosing (x,y)\in \V(X,Y)?
%oli7: I intended to the second. It is als possible to do the first, and then
% also choose $y$ according to $(\mu|X=x)_Y$ at some point (which would 
% be longer but maybe clearer?). 
% The motivation is that I need to compare to what happens if I chose $y$ according 
% to the cpd (as the model imagines), as compared to by the true distribution.
               $\mu_{_{XY}}$ and $x$ is given to both
                % $\mu_{X}$, and $x$ is given to both
        agents. 
%oli7*: added a lot, to clarify procedure. I realized I left out a large and
% critical part as I lost focus writing this, and never came back to it.
% I am very sorry about this.
		At this point, the agents, having the same conditional beliefs, and
		the same information about $Y$, agree on the optimal encoding 
		of the possible values of $Y$ as sequences of bits, so that if $y$
		were drawn from $\bp(x)$, the fewest number of bits would be needed
		to communicate it in expectation. The value of $y$---which is distributed
		not according to $\bp(x)$, but $\mu(Y \mid X=x)$---is now given to agent A.
		The agents pay a cost equal the number of bits needed to encode $y$ according
		to the agreed-upon optimal code, but reimbused the (smaller) cost that would have been paid,
		had the agents beliefs lined up with the true distribution $\mu$.
		
		Repeating for each edge and summing the expectations of these costs, we can view
		$\Inc(\dg M;\mu)$ as the total number of \emph{additional} expected
%oli7: (end of additions). At this point I suspect the discussion here is too long for the short paper, overlaps with the previous paragraph in content, but I do want to execute it correctly so you can have the intuition and help me decide what is worth keeping.
        bits required to communicate $y$ with a code optimized for
        $\bp$ instead of the true conditional distribution
%joe6
        %        $\mu_Y|(X=x)$.
                $\mu(Y \mid X=x)$. 
%oli7: oops, thanks.
%joe6*: is this obvious?  Is it obvious that there's no cost if M is
%the correct description of the PDG?  I would have expected no cost if
%there had been a difference between two expressions, but as I said
%above, the formal notion doesn't seem to involve a difference.  I'm lost.
%As you can see, I'm not finding this ``intuitive'' definition at all
%intuitive. 
%oli7: Hopefully it should be more obvious with the changes I've made, but
% just to be sure we're on the same page: we incur a cost of zero on an edge
% if and only if the edge matches the marginal $\mu$.
% Therefore, there is no way of having a single $\mu$ that exactly matches
% every cpd, at least one edge will incur a positive cost, and Inc(-),
% which is a sum of non-negative quantities, at least one of which is positive,
% must itself be positive. In terms of the analogy: the only way for the agents
% to avoid paying any cost at all, is to exactly use the bit credit they are given,
% which is the best as could possibly have been done, knowing $\mu$ exactly. If 
% there is an inconsistency, then there is no way for them to make this bound
% no matter what $\mu$ happened to be.
	If $\dg M$ is inconsistent, then there will be a cost no matter
%jeo6
%        which distribution $\mu$ is used to generate the data. 
        what distribution $\mu$ is the true distribution.
%joe6*: This needs to be argued better.  It's not the least bit obvious
%to me.
%oli7: let me know if the arguent in the comment above is sufficient,
% and to what to degree it is overkill. 
% If you now understand completely, I can also get out of your way and let you draw from that explanation to make one more suitable to the current context and space limitations.
	Conversely, if $\dg M$ is consistent, then any distribution
        $\mu \in \SD{\dg M}$ will have $\Inc(\dg M; \mu) = 0$.  
}	
        %joe4*: I don't think that this example is all that helpful
		%oli5: Ok. What about for the full version? It's a
		%nice sanity check, it also clearly gives the right
		%free energy, and several of my full version examples
	%build on it.
%joe5: I suspect that we can find better examples.        
        \commentout{
	\begin{example}[continues=ex:worldsonly]
		Recall our simplest example, which directly encodes an entire distribution $p$ over the set $W$. 
		In this case, there is only one edge, the expectation
				is over a single element, and the marginal on $W$ is
%joe3: now you're using both \mu and p to represent distribution.
%This is really bad notation.  Pick one and stick with it (and its variants)
				the entire distribution. Therefore,
                                $\Inc(\dg M; \mu) = 
%joe3: sorry; I don't understand the intuition of the information
%required to turn on distribution into another.  I don't understand
%how information can affect distributions.
%oli3: I'm again trying to provide intuition about the coding theory! I'll try again.
%                \kldiv{\mu}{p}$, and so the inconsistency is just the
%               information required to turn $\mu$ into $p$, minimized at
%                $p^* = \mu$. Furthermore, for $\alpha > 0$,
				\kldiv{\mu}{\mu}$, so the inconsistency is just the information $\mu$ and $p$, so is minimized uniquely when $\mu$ is $p$,
				\begin{vfull}
					and corresponds exactly to the distribution of minimum Gibbs free energy for the the log liklihood defined by $\dg M$ (see \Cref{sec:thermo}).
				\end{vfull}
%joe3: I couldn't parse this; to the extent that I could parse it, I
%couldn't understand it.  I cut it.  If you want to reinstate it, you
%have to explain it better.
% Furthermore, if $\alpha > 0$,
%                $\mathcal U_\alpha (\dg M; p)$ may be minimized
%                somewhere more entropic than $\mu$, intuitively
%                corresponding to the idea that if your environment is
%                chaotic, a more uncertain distribution may fit reality
%                better than the beliefs you actually hold. 
	\end{example}
}
	
%joe4
        %oli5: heading temporarily reinstated.
%joe5: I really don't like this heading (although, to be fair, I think
%that most people are bigger fans of headings than I am).  But even so,
%this is not a good heading.
%	\subsubsection{Extra Information}\label{sec:extra}
%	While $\SD{\dg M}$ and $\Inc(\dg M;-)$ distinguish only
$\SD{\dg M}$ and $\Inc(\dg M;-)$ distinguish only
        between distributions based on their compatibility with
%joe4*: For what it's worth, I have a lot of problems with the use of
%the word ``natural'' here. 
%oli5: Taken litterally, I think what I wrote is almost inarguable. Do
%you really think that the distribution that puts mass 0.67334 on x3,
%0.1241308 on x7, and 0.2025292 on x8 is just as natural as the
%uniform distribution, for a PDG with only a single variable X taking
%10 elements, and no edges? 
% In any case, we're about to say that distributions like the first
% one are worse, so I think it's important to say so, rather than
% reitterate that "not all distributions are equivalent". 
% 
%oli5*: If this is in reference to the semantics, would make it more
%natural? This gives back both BNs and MRFs, explains somd differences
%between them, has a coding theory interpretation, matches up with the
%techniques used in variational inference, and even has knobs for
        %causality and certainty that seem to do the right thing.
%joe5: These are all (good) arguments that the choice of uniform
%distribution is useful, but you used ``natural''.  What makes a
%distribution ``natural''?        
%oli5: Is it the symmetry? Because there are extremely good reasons
%not to use a symmetric distance measure in this case (there's a big
%difference between eliminating a possibility, e.g., through any
%observation at all), and observing something you were convinced was
%impossible.  
%oli5: I do believe that these semantics are natural, and want to
%understand why you feel so strongly that it is ad-hoc. I think in
%some sense the biggest real choice I've made is the selection of
%basis: which parameters apply to which information theoretic
        %terms---but this is something I did just for interpretability.
%joe5: I don't think that ``ad hoc'' is the opposite of natural.
%``Natural'' to me has the flavor of ``obviously right'' (as in ``the
%natural choice'').  I don't see what makes this obviously right.  The
%fact that it's so dependent on the basis is one example of what makes
%it not so obviously right (to me).  The fact that it's not obviously
%right doesn't make it ad hoc though.
%        $\dg M$, but even among distributions consistent with a set of
        %        constraints, some seem more natural than others. In
%oli5: this is repetetive. I dislike the phrase "as we've said", the necessarily seems like unecessary hedging. 
        % $\dg M$, as we said, we do not necessariliy want to treat all
        % the distributions  consistent with a set of 
        % constraints equally.
%oli10:reword
	% $\dg M$, but even among distributions consistent with a set of constraints
		$\dg M$, but even among distributions that match the marginals, some more closely match the qualitative structure of the graph than others. 

		Suppose an agent has a PDG $\dg M$ in mind, and imagines that all sample variation in a joint distribution $\mu$ over $\V(\dg M)$ arises as a result of sampling the value of a target variable $Y$ of some link $\ed LXY$, given the value already at its source. If this were the case, one would expect the total amount of information required to communicate a sample of $\mu$ to be the same as the total amount of the information required to separately encode, for each edge $\ed LXY$, the the randomness $Y$ given $X$. 
		% If all variables are constant according to $\mu$, or $\dg M$ corresponds to the BN that  $\mu$, the two quantities will be identical.

		For an arbitrary PDG and $\mu$, these two quantities will differ; if the amount of uncertainty in the distribution is lower than one would expect from the links, the distribution has an information defecit, indicating that there are additional correlations between variables, that are not suggested by the graph. The higher the information defecit, the worse the qualitative fit of the distribution. 
%
		On the other hand, if a distribution has surplus uncertainty, using it confers a benefit. For instance, a PDG containing only the variable $X$ and no edges encodes awareness about $X$ but no beliefs about it. Given that $X$ could be distributed in any way (even chosen adversarially), it is safest to act as though you believed $X$ to be uniform. 
		%
		We measure the surplus / defecit with a quantity we call the \emph{Graph Information Balance} ($\Gib$)
		\begin{equation}
	%joe7
	%          \Gib'(\dg M; \mu) := \sum\alle \H_\mu(Y\mid X) - \H_\mu.
		\Gib(\dg M; \mu) := \sum_{\ed{L}XY \in \Ed^{\dg M}} \H_\mu(Y\mid X) - \H(\mu).
		  \label{eqn:alt-extra}
		\end{equation}
		%oli10: there might not be space to see...
		% As we will see, 
		This approach neatly combines the benefits of choosing the maximum-entropy distribution consistent with constraints \cite{Jaynes57}, with the ability to articulate qualitative independences. 
		% It can be thought of as a more 
		
		


%oli10: all new, up to %oli10.end-add
	
%oli10.end-add.
%joe7*: I have no idea what ``natural'' means, and I don't see
%anything that we're doing that prefers ``naturalness''.  I think
%we're trying to match the independence structure, and using entropy
%to measure it.
%oli9*: I don't like my wording before, but I strongly dislike this
%wording. I was trying to hide the fact that we're implicitly
%specifying an independece structure: to talk about this properly we
%need \alpha, and even then it seems like we're selling ourselves
%short by starting in the middle of the argument. We make a big deal
%out of not having to keep independencs around earlier, and I think
%that story is a better one.  
%oli9*: I also think this is even less precise. There is literature
%about the ``most natural'' distribution in the maimum entropy
%literature, but there is no definition of an independence structure
%specified by a PDG. We also cannot define it properly without
%\alpha. 
%joe8*: I have now added a discussion of indpendence
%structure.  This was critical.
        %        some seem more natural than others.
%oli10:  commenting out this discussion for now, but we can use this as a fallback.
\commentout{
we prefer those whose independence structure comes closer to that of
the PDG.       
We measure how closely a distribution $\mu$ comes to the independence
structure of the PDG using entropy. 
%oli9**: I strongly disagree with this motivation. I don't think it works unless the PDG is litterally a BN. What are even the independencies of the PDG? In general it is not the kind of object where we can just port d-separation. 
%oli9*: Moreover, it covers more than independence. It also covers the maximum entropy case. I feel like until you look at the information diagrams a little more carefully, motivating this will be very tricky, and I know that you don't trust me to motivate it. 
%oli9: Perhaps we can try to write it together on a shared doc. 
To motivate our approach, we need recall some definitions and results.  
Recall that the entropy function $\H(\mu) = -\sum_{w \in
  \text{Supp($\mu$)}} \mu(w) \log \mu(w)$ (where $Supp(\mu)$ denotes
the \emph{support} of $\mu$, that is, the elements to which $\mu$
gives positive measure) can be understood as the number of bits required
to describe the exact state  \cite{Jaynes57}.  If $\mu$ is a
distribution over variables $X_1, \ldots, X_n$, then $H_\mu(X_i \mid
X_j) = \sum_{\{x_i,x_j: \mu(x_i,x_j) \ne 0\}} \mu(x_i,x_j)
\log(\mu(x_i)/\mu(x_i,x_j))$.  It is well known that entropy satisfies
the \emph{chain rule}: 
$H(\mu) = \sum_{i=1}^n H_\mu(X_i \mid X_1, \ldots, X_{i-1})$.
Morover, if $X_i$ is independent of $X_k$ given $X_j$, then $H(X_i
\mid X_j, X_k) = H(X_i\mid X_j)$.  It follows that if the
independencies in $\mu$ are
correctly represented by the PDG ${\dg M}$
then $\Gib(\dg M;
\mu) = 0$, where
%joe7**: Is there any reason to believe that this is always positive
%(or always negative, for that matter).  I'm prety sure that if we
%allow multigraphs (as we do) this could go either way.  For example,
%if we have a PDG with no edges, its clearly just -H(\mu).  On the
%other hand, if we havea multigraph with n edges from X to Y, we have
%nH_u(\X) - H(\mu), which is clearly going to be positive for n
%sufficiently large.  So this needs *much* more discussion if you want
%to view it as a distance measure.

	\begin{equation}
%joe7
%          \Gib'(\dg M; \mu) := \sum\alle \H_\mu(Y\mid X) - \H_\mu.
    \Gib(\dg M; \mu) := \sum_{\ed{L}XY \in \Ed^{\dg M}} \H_\mu(Y\mid X) - \H(\mu).
	  \label{eqn:alt-extra}
	\end{equation}
}
%joe8*: Unless I'm missing somethings, this result is true, no matter
%what \M and \mu are, and is not hard to prove.   The key point is
        %that H(X|Y) \le H(X).  But it would *not* be true if we added
        %alphas (unless we also multiplied H(\mu) by the smallest
        %\alpha).  To see this, suppose that \alpha_L = 0 for all L;
        %then we get -H(\mu), which is clearly negative.  As a
        %consequence of this, I would strongly prefer not to discuss
        %the \alphas here.  It's not at all clear to me how to deal
        %with them.
%oli10: Deleting this proposition; it's false unless we modify the formula.
% \begin{prop}
%        For all $\dg M$ and $\mu$, $ \Gib(\dg M; \mu) \ge
%   0$.%
% \footnote{All proofs can be found in the appendix.}
% \end{prop}
%joe8
%        In general, we can view $\Gib'(\dg M; \mu)$ as a measure of how
%oli10: Deleting
        % In general, we can view $\Gib(\dg M; \mu)$ as a measure of how
        % far the PDG is from representing the independencies in $\mu$.

%joe7**: Again, why should this always be positive?  More importantly,
%I ave no clue what this is telling me.  Your English below doesn't
%help.  As we observed when you were trying to give me the intuition
%for \alpha, this is a weird mix of qualitative information and some
%qualitative information that depends on details of the distribution.
%If we don't use it for BNs I would prefer to cut it altogether
\commentout{ 
\[ \extrainfo\mu :=
\sum_{\ed{L}XY \in \E^{\dg M}} \Big[ \E_{x
                \sim \mu_X}  \H (\bp (x))  \Big] - \H(\mu). \]  
%	\begin{defn}\label{def:extra}
%%oli8: deletd symbol for extra info
%	% The \emph{extra information}, $\H^{\dg M}$ given by a distribution $\mu$ 
		The \emph{extra information}, given by a distribution $\mu$ 
%%oli8
%		% that satisfies the constraints of a PDG $\dg M = \mnvars[]$ 
		with respect to a PDG $\dg M = \mnvars[]$ 
%oli8: removed \alpha
		% with causal weights $\vec \alpha$
		is given by
%joe6*: don't clutter things up with the \alpha_L here
%		\[ \H^{\dg M}(\mu) := \sum\alle \alpha_L \Big[ \E_{x
		\[ \extrainfo\mu := \sum\alle \Big[ \E_{x
                \sim \mu_X}  \H (\bp (x))  \Big] - \H(\mu) \]  
		where $\mu_X$ is the marginal of $\mu$ on the variable
                $X$, and $\H(\bp(x))$ is the entropy of the
                distribution on $Y$ specified by the cpd $\bp$ when $X
                = x$.
%joe6*
%The extra information of an unweighted PDG is given by assuming
%$\alpha_L = 1$ for each $L \in \Ed^\dg M$. 
	\end{defn}
	% The cpds may not (in the entropy sense) 
	The extra information is the sum of the entropies that
        \emph{actually} result from each table, in the context of
        distribution $\mu$, minus the total entropy of the
        distribution. 
        %joe4L*: I don't understand the next sentence, but it seems like a
%useful intuition.  Could hou explain it better.
%oli8: deleted "Alternatively"
%	Alternatively, we can think of
	We can think of
%oli8: shortened, deleted unnecessary symbol 
		% the negation of the extra  information, $-\H^\dg M(\mu)$ 
		its negation
%joe7*: I have no idea what this means, nor why it represents a
%quantity of interest
		as the uncertainty in $\mu$,
        which has not already been specified by the cpds in $\dg M$.
%joe4*: I now think that this is premature        
%	We will show in \Cref{sec:bn-convert} the distribution with
%        minimal extra information with respect to the cpds of a BN
%        $\cal B$ is the unique one specified by $\cal B$. 
%joe4*: premature!
%	\begin{vfull}
%		One might also recognize the extra information as
%having the form of a free energy; we explore this connection in
%\Cref{sec:thermo}.  
%	\end{vfull}

%joe4*: I don't undrestand this, since I don't understand the causal
%interpretaiton.  I would strongly prefer to cut it from here, and
%then have a separate section on the causa interpretaino.
%	Note that when every $\alpha_L = 0$, minimizing the extra
%        information corresponds to maximizing entropy subject to
%        constraints, which is arguably the right thing to do if we
%        take the constraints at face value, rather than causally (see
%        \Cref{ex:counterexample}), justifying their naming. 

%%joe3*: Can you give some intuition for why this is reasonable?
%%oli3: yeah. Here are several arguments:
%% (1) if the constraints are all you know, and you choose another distribution, you're somehow claiming to know more than you do. This is the maximum entropy distribution associated with some other constraints. 
%% (2) Specifying the wrong distribution is costly --- but much cheaper if you specified a uniform distribution. For this reason, the relative entropy from uniform to any other distribution is cheaper than the relative entropy to go back: the maximum entropy distribution is the most adaptable, paying the smallest price to specialize, where price is the expected surprise, = log (1 / p), related to energy of the associated Boltzmann distribution.
%%oli3*: More generally, I do not think it's worth the space to do any motivation like this at all. I could do it, and it would make me a better person, but it takes a lot of time, will require a lot of energy from readers, and it's a very standard thing. My guess is at least half are already on board with maximizing entropy, and providing a remedial introduction to such a deep topic in a 9-page abstract is not a good use of space. I also imagine that it will come off as patronizing to those who know what they're doing in the context of an original research paper. 
%We think of distributions with higher entropy as being ``better''.
%%joe3: reorganizing what you wrote.  
%Since we want to minimize inconsistency and maximize entropy, we
%subtract one from the other (with relative weighting $\alpha$), to get
%a score $\mathcal U_\alpha(\dg M; p)$:
%	\begin{equation}
%		\mathcal U_\alpha(\dg M; p) := \Inc(\dg M;p) - \alpha
%                H(p). \label{eqn:full-score} 
%	\end{equation} 
%joe3: This may be true, but why say it?        
%	Thought of this way, in specifying a \MN, a modeler has not
%        only specified a a distribution, but also a higher-order
%        object, that scores all distributions.  
%oli3: because mixing up the levels is problematic. Maybe it doesn't need to be said.
%joe3: why does this mean someting is wrong?
%oli3: rewording and uncommenting.
% $\Inc$ provides us with a meaningful continuous score, but as
% a semantics for PDGs, there's still something missing: $\Inc$ can
% only distinguish between those distributions $p$ that are
%         \emph{inconsistent} with $\dg M$.
% 
%oli8:added remark
\begin{remark}
	For distributions $\mu$ that exactly match the cpds in $\dg M$, then the term $\E_{x \sim \mu_X} \H (\bp (x))$ is can be recognized as the conditional entropy $\H_\mu(Y \mid X)$.
%TODO: reference fact 3 in the proof of Theorem 4.1
	If we take a variant of the exta information given by
	\begin{equation}
		\Gib'(\dg M; \mu) := \sum\alle \H_\mu(Y\mid X) - \H_\mu
			\label{eqn:alt-extra}
	\end{equation}
 	which uses this simpler term directly, we can get a clearer
        picture of the ``qualitative'' structure of a PDG. Motivating
%joe7*: If this is really true (I'm hoping it's not, and that the
%intuition I gave is sufficient), then I would not agree to include it
%in the paper.  We can't use what we can't motivate and explain!
        this properly is trickier and beyond the scope of the paper,
        but this definition will be useful point of comparison when we
        discuss factor graphs.  
\end{remark}



%joe4
%        \subsubsection{Putting The Scores Together}
%joe7* I'm sorry, Oliver, I don't see what ``fitting'' has to do with
%anything, nor what Inc has to with including specified features of a
%distribution, nore what Etra has to do with including unspecified
%ones.  Certainly nothing you said above matched this intuition.  I
%rewrote the intuition.  Do NOT change what I wrote, nor write me a
%long reply.  This is better discussed by skype.
%Both the extra information $\Gib$ and incompatibility $\Inc$ 
%oli8: fix grammar, shorten
%		% can be thought of measures of cost of fit for a
%	should be thought of as costs of fitting a distribution $\mu$,
%with higher numbers being worse. 
%%oli8:
%	 $\Inc$ is the cost of not including the specified features
%of a distribution, and $\Gib$ is the cost of including
%unspecified ones. 
%joe4: we can think of this but I'd rather not introduce this metaphor
%now.  I don't think it adds.
%        We can
%        think of either as defining an energy landscape on possible
%        distributions, in which we intuitively expect distributions
%        with low energy to be more likely.   
} %joe7: \end{commentout}

%joe7*: a little more discussion
%joe8: shortened a bit, since I said it above.
%We now have two measures for how good a PDG ${\dg M}$ is at capturing
%a distribution $\mu$. The first, $\Inc({\dg M}, \mu)$ measures how
$\Inc({\dg M}, \mu)$ and $\Gib{\dg M},\mu)$ give us two measures
%oli10: "how close" is the wrong intuition, but measure of consistency is still fine.
% for how close $\mu$ is to being consistent with ${\dg M}$.
of compatibility between ${\dg M}$ and a distribution $\mu$.
%joe8
%, and is 0 if $\mu$
%is consistent with $\dg M$.  The second, $\extrainfo({\dg M},\mu)$,
%measures how close ${\dg M}$ is to describing the independencies of $\mu$.
%We now consider the sum of the two scores, with the trade-off
We take the score of interest to be their sum, with the tradeoff
        controlled by a parameter $\gamma \ge 0$:
%oli8
		%giving us a score 
%joe7: line shaving
%	as follows:  
%joe7
        %	\begin{align*}
        \begin{equation}
%oli8: Only need the one symbol [[M]]
%joe7: change ; to , clobally
          %	  % \mathcal U_{\gamma}(\dg M; \mu)
          	  % \mathcal U_{\gamma}(\dg M, \mu)
	  \bbr{\dg M}_\gamma(\mu)
%joe8*: your formullation does not let us ignore Inc.  I don't see why
%we shouldn't allow ignoring Inc
%			 := \Inc(\dg M,\mu) + \gamma \extrainfo\mu
%oli10: This is a good point. I am actually sympathetic to this framing of it, 
% Howeer, it's not mathematically necessary becasue we already have scaling parameters for each $\beta$. 
% I actually now buy that convex combinations are the "right" thing to do, but it involves adding two extra scalar parameters that change nothing to get the units right.
% In the full paper I would prefer to do this properly as a convex combination.
%oli10: Because of our discussion and my rewrite above, I am reverting this. 
% I'm not trying to be stubborn though; my preference has weakened, and if you care you should change it back. Just be aware that the factor graph section will require a few factors of 1/(1-\gamma). 
	 % := (1-\gamma) \Inc(\dg M,\mu) + \gamma \extrainfo\mu
	 := \Inc(\dg M,\mu) + \gamma \extrainfo\mu
          %oli8: I'm not sure this equation is actually very helpful
          %joe7: it's not; let's cut it
   %		&= \!\! \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \!\!
%                \E_{x \sim \mu_{\!_X}}  \left[\gamma 
%oli8: no more \alpha
% \alpha_L\cdot
% 	\;\H (\bp (x)) + \beta_L\;\kldiv[\Big]{ \mu(Y | X \sheq x) }{\bp(x) }  \right]  - \gamma \H(\mu)
                %			\numberthis
                \label{eqn:full-score}
                %	\end{align*}
    	\end{equation}

%oli8: deleted. Higher value to talk about trade-off parameter
%	For $\gamma = 0$, the only thing that matters is consistency, as $\mathcal U_0 = \Inc$.
%joe7: I have no idea of what ``regularization strength'' means.
%The trade-off parameter $\gamma$ can be seen as a regularization strength,
%                trading off quantitative fit for
                %        qualitative accuracy.
%joe7**: why do we focus on this case?  We need to motivate this.
%joe8: I cut this from here: it's the wrong place.
%        In this paper, we focus on the case
%        where $\gamma$ is small, making $\Inc$ the more important term. 
%joe4*: cut; this is false, unless you're allowing infinitesimals.
%Why complicate things?
%	For $\gamma>0$ but arbitrarily small, the preference for
%        consistency dominates, and extra information is used only to
%        break ties. If an agent is very certain that their cpds are
%        well-informed, which we will call the `low ambient
        %        uncertainty' case, this may be appropriate.
        %joe4*: I don't understand this intuition, Oliver.  It needs to
        %be explained better.  In what way do you get more ``safety''
        %if \gamma is larger?
%oli8: this was a little incoherent, and referenced a commented-out example. Deleted.
	% As $\gamma$ grows, the semantics trade some consistency for
    %     safety. For instance, an agent that does not even have
    %     consistent knowledge might be better off hedging with an
    %     uncertain estimate, for the same reason that uniform
    %     randomness is optimal in rock-paper-scisors against an unknown
    %     adversary. Even in situations where there is a consistent
    %     distribution, one might be justified in picking something
    %     else, as the next example shows.  
	
        %joe4: cutting for now, since we'll have a space crunch anyway
\commentout{
	\begin{example}
		Consider the PDG with variables $\sf 1$ and $X$, and two distributions $p, q$ on $X$, that only overlap with $\epsilon$ mass.
		\todo{This works out nicely but I actually don't think this can be done without elliding a row of a cpd (which I've been calling sub-stochasticity). This is the one discussed in meeting, where two experts disagree on their except for $\epsilon$ overlap.}
	\end{example}

%joe3: I don't understand what ``ambient background uncertainty''
%means, or why more background uncertainty should mean that we weight
%the entropy factor more.  This is presumably bound up with
%someintuitions that I haven't got  (and you haven't shared with the reader).
%oli3: Imagine: probability \alpha that you got really unlucky with what you saw and are just totally wrong about how the world works. Your subjective assesment of the extent of your unknown unknowns. Alternatively, analogous to the probability \delta of a total disaster in a Chernoff or Hoeffding bound.
%This modification can be thought of as modeling ambient background
%uncertainty at level $\alpha$.  
%joe3*: why on earth should this be true?  Why should a more uniform
%distribution be better than a less uniform one just because I think
%the world may have changed?
%oli3: because if you know nothing, maximum entropy is cheapest. I'm certain this has been motivated a lot in information theory, but I don't know the best place to find a convincing reference. I was told to look up Nima Anari's white paper on maximum entropy programs and why we bother, but I haven't done this yet. Again, I don't think it's our job to motivate this, and none of the other UAI papers do. It has been motivated before. 
%If you cannot trust your own beliefs or think the world may have
%changed a lot since you formed them, higher-entropy distributions are
%better fits.	 
%[...corrupted text deleted ...]
%oli3: I want this case to be the default. I meant "reducing inconsistency / \Inc"
%A very small $\alpha \in O(\epsilon)$ corresponds to a lexicographical
%ordering, in which 
%reducing consistency is most important, and ties are broken by
%maximizing entropy.  
%joe3: I see no reason to identify more uncrtain with greater
%entropy.  Why is an agent who is certain that a coin is far more
%uncertain than one who thinks that the probability of heads is 1/3.
%You have a certain intuition (which I realize is shared by some
%others, but definitely not me) for entropy.  You need to share it
%with the reader if you're going to say things like this.
%and distributions with lots of uncertainty become increasingly
%appealing; as $\alpha \to \infty$, the best distribution approaches
%the uniform one.
%joe3*: I would strongly prefer not to think of negative values of
%alpha.  You don't use them anywhere, as near as I can tell.  Why
%distract the reader with them?  I also don't understand this
%intuition at all.
%oli3: I haven't used them yet, so I'll leave them uncommented.
% I'll try to share my intuition for now, so you have a head start when I explain it wrong later: they correspond to cases where there's the exact opposite incentive: when the loss is about the expectation of the micro-state, rather than getting the distribution right and minimizing surprise / coding errors.
% For instance, the optimal Bayes classifier, which guesses a binary label that has the highest probability, is an instance of a setting where $\alpha$ is negative. I suspect this will come up a lot in the paper I want to write next on decision theory.
% However, this setting is very non-convex, and so we won't be able to prove much about it with optimization.
%Negative values of alpha, intuitively corresponding
%to a belief that the world is extremely ordered, are also
%psychologically plausible.  
	
%	if $\Inc(\dg M; p)$ is the additional information of communicating $p$ with knowledge $\dg M$, and $H(p)$ is the amount of information in $p$, then $\mathcal U_\alpha(\dg M; p)$ is the information required to communicate $p$ given a code optimized for the marginals of $\dg M$, 
	



$\mathcal U_\gamma(\dg M, p)$ can be thought of as the energy state of
        a distribution (larger numbers indicate a worse fit) and can
        be anywhere between $-\infty$ and $\infty$.
        To get a semantics in the style of \cite{halpern2015weighted},
        we transform it so that it is non-negative and larger scores
        are better (making it distribution-like, by analogy to a
        Boltzmann distribution) and normalize so that the lowest
        energy distributions get weight 1. We introduce a scaling
        parameter $k > 0$ (the corresponding inverse temperature)
       which does not affect the rankings of distributions. 
%joe4*: I would like to add a sentence relating this to Halpern and
%Leung, but I don't undrestand the scaling parameter at all.   Why
%complicate matters. We can just take it to be 1.
       \[ W^k_\gamma(\dg M, \mu) := \exp\Big(  -k \Big\{\mathcal U_{\gamma}(\dg M,\mu)- \inf_{\mu'} \mathcal U_\gamma(\dg M,\mu')\Big\} \Big).\]
Now 
$W^k_\gamma(\mu) \in  [0,1]$, for all $k$, and hence acts like a weight in the sense of  Halpern and Leung \citeyear{halpern2015weighted}.
For now, we set $k=1$, but note that $k$ does not affect the ranking of distributions of scores according to $W^k_\gamma(\dg M, -)$, and so $\arg\max_{\mu'} W^k_\gamma(\dg M, \mu')  = \arg\min_{\mu'} \mathcal U_\gamma(\dg M,\mu')$ is independent of $k$. 
%
$W_\gamma$ and $\mathcal U_\gamma$ carry roughly equivalent information aside from the normalization, but the latter is slightly more useful and will play a larger role in our analysis, and so we notatationally use this as our second semantics: $\bbr{\dg M}_\gamma(\mu) := \mathcal U_\gamma(\dg M, \mu)$.
}
%joe4: \end{commentout}

\commentout
{        
	We now offer generalizations of some results found in in
        \Cref{sec:set-of-distribution-semantics}:
        \Cref{def:cont-inconsist} is the continuous version of
        inconsistency we were searching
        for. \Cref{prop:union-weight-semantics} shows that the
        weighted semantics $\bbr{-}$ also has the modularity
        properties we wanted, and \Cref{thm:zetaconvex} will let us
        define concrete distributions just like its more specific
        counterpart. 

	%	\Cref{prop:union-weight-semantics} shows that the weighted distributions also have the modularity properties we're interested in.
	\begin{prop}[name=\Cref{prop:union-set-semantics} analog]\label{prop:union-weight-semantics}
		$\bbr{\dg M \cup \dg M'} = \bbr{\dg M} + \bbr{\dg M'}$
	\end{prop}
	
	\begin{coro}\label{cor:u-convex}
		$\mathcal{U}_\alpha(\dg M, p)$ is convex in $p$ for $\alpha \geq 0$, and strictly so for $\alpha> 0$. Furthermore, $\bbr{-}$ is quasiconvex---that is, all of its level sets are convex sets.
	\end{coro}

	As a result of \Cref{cor:u-convex}, $\bbr{M}$ is a quasiconvex function 
	%, and hence every local minimum is a global minimum. 
	In particular, \Cref{prop:convex} is the special case for the level set $\mathcal U \leq 0$ for $\alpha = 0$. 
}

	% unhelpful?
\commentout{
	The semantics given in \Cref{sec:set-of-distribution-semantics} does not allow us to express independencies very effectively.
	For instance, in \Cref{ex:smoking}, to fully get the joint representation given by the BN, we also need to somehow assume that $SC \CI S \mid PS$. This is possible to do explicitly with an extra arrow meaningless arrow, but this solution doe not scale well, looks like it dependence (the opposite of our intention), and clutters the diagram. 
	We now show that by applying the principal of maximum entropy, we can recover all of a BN's independence assumptions at once. This should make intuitive sense: maximizing entropy tend to `make things as independent as possible'. 
	Those familiar with the view of graphical models as separable exponential families may have even seen a similar result in an undirected setting \cite[e.g..][pp. 37-39]{wainwright2008graphical}. 
	We proceed with the theorem, and discuss some consequences below.
}

    \subsection{PDGs As Unique Distributions}\label{sec:uniq-dist-semantics}

    Finally, we provide an interpretation of a PDG as a probability distribution.  
%joe7: I'm not sure that we've said it before, so it's not a reiteration
%    Before we provide this semantics, we would like to reiterate that
    Before we provide this semantics, we stress that
    this distribution does \emph{not} capture all of the important
%joe7
%    information in the PDG---in particular, as we have stressed, a PDG
    information in the PDG---for example, a PDG
    can represent inconsistent knowledge states. 
%oli8
%	That said, the
	Still,
%joe6
%    semantics we give here is still important, as it will allow us to
%oli8
%    semantics we give here is still important, as 
%	it allows us to compare with the semantics of existing graphical models
	by giving a distribution, we enable comparisons with other
        graphical models. 
%joe6
%    ultimately will prove PDGs to also be a surprisingly flexible tool
%    for specifying a distribution. 
%oli8
%	makes PDGs  a surprisingly flexible tool
%	for specifying distributions. 
%joe7
%	and in the process reveal PDGs to be a surprisingly flexible
%        tool for specifying distributions as well. 
	It also shows that PDGS are a surprisingly flexible
        tool for specifying distributions. 
        %joe7: removed paragraph break
	%
%oli8:
	% The idea is to 
%joe8
%	To do this, we
        %	simply select the distribution with the
        The idea is to select the distributions with the best score.
        %joe8*:
We thus define 
\begin{equation}
\bbr{\dg M}_\gamma^* = \argmin_{\mu \in
				   \Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu).
\end{equation}   

In general, $\bbr{\dg M}_\gamma^*$ does not give a unique
distribution.  But if $\gamma$ is sufficiently small, then it does:
%joe8*: cleaner statement, which gets at what you want.  Are there any
%other useful sufficient conditions that we can give (like the ones
%that arise in factor graphs)?
%oli10: In its current state, this encompasses factor graphs.
% The only other sufficient condition I can see:
%   - It's true for any \gamma > 0 if the PDG has edges that are a subset
%		of those in a BN.
%  - There are some more involving \alphas (e.g.,  we can take convex combinations of BNs this way). 
\begin{prop}\label{prop:sem3}
  If $\dg M$ is a PDG and
%oli10: added lower bound of zero
$0 < \gamma
  %oli10: removed 1-\gamma factor. Also this holds more generally with equality.
	% /(1-\gamma)
 	\leq \min_L \beta_L^{\dg M}$, then
  $\bbr{\dg M}_\gamma^*$ is a singleton.
\end{prop}

%joe8: added some motivation and discussion
In this paper, we are interested in the case where $\gamma$ is small;
this amounts to emphasizing the accuracy of the probability
distribution as a description of probailistic information, rather than
the independence structure of the PDG.  This is what was going on in
all the examples in the introduction.  This motivates us to consider
what happens as $\gamma$ goes to 0.  If $S_\gamma$ is a set of
probability distributions for all $\gamma \in [0,1]$, we define
$\lim_{\gamma \rightarrow 0} 
S_\gamma$ to consist of all distributions $\mu$ such that 
there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with
$\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in
S_{\gamma_i}$ for all $i$. 
%oli10*: We still need to prove that this limit is unique. This is non-trivial, so I think we should reference the appendix.. I think figured out a way to do this that will be rather hellish, but haven't slogged through the algebra. I'm still holding out hope.
We then define%
%oli10: added footnote, equantion* environment
%oli10*: I'm now convinced that this proposition is (a) true, and (b) not trivial to prove.
	\footnote{The limit is well-defined by \Cref{prop:lim-exist}}
\begin{equation*}
	\bbr{M}^* = \lim_{\gamma \rightarrow 0} \bbr{M}_\gamma^*.		
\end{equation*}
	

%joe8: this should go in the appendix, where we  can discuss
%convexity.
\commentout{
        There is a unique such distribution because, as we now
    show, the score is strongly convex%
%oli8:
	%. It then follows from standard
    % results \todo{GIVE A REFERENCE HERE} that there is a unique minimum
    % score. 
	. It follows that there is a unique minimum \cite{Rockafellar1970ConvexA}
%oli8: the reference above is the standard one for convex analysis, but only explicitly mentions
% strict conexity, which strong convexity trivially implies. I asked Bobby what to reference and he
% wasn't sure. The actual proof of this is very stragithforward, and I think we should either include
% it as a silly appendix lemma or not go into these details (which would be standard) 
 	which can be found efficiently \cite{strongconvexopt}.
}
%oli9: this prop is false for now
\commentout{
    \begin{prop}\label{prop:u-convex}
%joe6
%      $\mathcal U_\gamma(\dg M,\mu)$ is strongly convex in $\mu$, for 
      %      any $\gamma > 0$.
%oli8: oops forgot paramter on strong convexity. Also changed \U to [[M]]
%joe7*: What does \gamme-strongly convex mean?  This needs to be explained.
%oli9: It means that its more convex than a 2nd order parabolic term with coefficient gamma, everywhere. This is not a terribly important thing to explain in my opinion. Not only are most NeurIPS paper just chalk full of this stuff, but I also don't think it's at all important to get it takes you much anergy. The important feature is the local minimum (but the strong convexity enables a lot of algorithms for finding minima, so ML people will want to know).
%oli9: This is also no longer quite true with Extra'; I have to rephrase.
      $\bbr{\dg M}_\gamma(\mu)$ is $\gamma$-strongly convex.% in $\mu$.
      %joe7
		%oli9: moving this note to the other proposition.
      % \footnote{All proofs can be found in the appendix.}

    \end{prop}
	%oli7: I can do a better job here; I was feeling rushed when I wrote it.
%joe7*: Is this result still true, now that we're going with the other
%definition of extra information?
%oli9: Only if \alpha\gamma < \beta for all links.
    \commentout{
    \begin{proof}
	%joe6*: Is it obvious/well known that \Inc is
	%convex?  Perhaps because it's a convex combination of
	%D's, each of which is convex.  If this is a standard result
	%(which I suspect it is), you should give a citation.  Also, is
	%it not strongly convex?
	%oli7: The proof that $\Inc$ is convex, was already in the appendix, but I forgot
	% to reference it. You're correct that I'm using the fact that
	% a convex combination of convex functions is convex to deal with expectations,
	% and that a positively weighted linear combination (a conic
	%combination) of  
	% convex functions is convex, to sum across the edges.
	%oli7:@strong convexity. In general, none of these functions will be strongly convex on the set of all
	% possible distributions, because shifting mass between two possible values of $z$ will
	% not strictly decrease p(y | x), for instance. 
	%oli7: minor updates for clarity
	%oli8: inserted reference, added parameters, effectively rewrote the proof.
      $\Inc(\dg M, \mu)$ is convex in $\mu$
      (\Cref{thm:inc-convex}), and $\gamma\sum\alle \E_{x\sim \mu_X}
%joe7*: what does it mean that it's linear in \mu?
%oli9:
% 1 1 + 2 2  = 1 Extra(1) + 2 Extra(2)
      \H(\bp(x))$ is linear in $\mu$.  
		Negative entropy is $1$-strongly convex
		(\Cref{prop:neg-ent-convex}), so $- \gamma \H(\mu)$ is $\gamma$-strongly convex.
		The sum of a $\gamma$-strongly convex, linear, and
%joe7
%                convex function must be $\gamma$-strongly convex. 
                convex functions must be $\gamma$-strongly convex. 
                %		, and strongly so when the coefficient on $-\H$ ($\gamma$) is positive. 
		%(see \cite{Rockafellar1970ConvexA})
	\end{proof}
    }
}%oli9: end commentout
    %joe7: \end{commentout}   
 %oli8: change text
	% We can now define the unique distribution semantics:
%joe7
    %    We use this to get our desired semantics
%oli9 added alternate proposition for Extra.
%joe8*: I simplified the wording.  In any case, this needs a proof and
%can go to the appendix.  We'll need the space.
\commentout{
\begin{prop}\label{prop:convex-if-gamma-small}
  If $\dg M$ is a PDG and
%joe8*
  %  $\beta_0$ is a constant less than any
  %        $\beta_L \in \beta^{\dg M}$, then for any $\gamma < \beta_0$,
  $\gamma < \min_L \beta_L^{\dg M}$, then
  $\bbr{\dg M}_\gamma$ is a strictly convex function of $\mu$.%
%  		\footnote{All proofs can be found in \Cref{sec:proofs}.}
\end{prop}

                          
                          
%oli9: expanded this, added footnote.
% Proposition~\ref{prop:u-convex} allows us to define our desired
\Cref{prop:convex-if-gamma-small} allows us to define our desired
semantics by ensuring the limit%
	\footnote{$\mu$ is in this limit iff there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in \bbr{\dg M}_{\gamma_i}$ for all $i$.}
 in \eqref{eq:uniqdist} is well-defined.
	
%oli8: reformat with equation, added the limit.
	\begin{equation}
		 \bbr{\dg M}_* := \lim_{\gamma\to 0^+}\argmin_{\mu \in
%joe7
%                   \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu) 
%oli9: I missed this instance of \U when I eliminated it.
                   % \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu). 
				   \Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu). 
		   \label{eq:uniqdist}
	\end{equation}
}
%joe8: \end{commentout}

%joe8: they do *not* agree.        
%With this definition, all three semantics agree.
The semantics has an important property: 
\begin{prop}\label{prop:consist}
%	If $\dg M$ is a consistent PDG, then $\UD{\dg M} \in \SD{\dg
  %M} = \{ \mu : \bbr{\dg M}_0(\mu) = 0 \}$
If $\dg M$ is consistent,
then $\bbr{\dg M}^*$ is a singleton, and its 
%oli10: line shave 
%unique
element is
consistent with ${\dg M}$.  
\end{prop}

    % \begin{defn}
    % 	For $\gamma > 0$,
    % 	$\bbr{\dg M}^*_\gamma := \arg\min_{\mu \in \Delta\V(\dg M)} \mathcal U_\gamma(\dg M;\mu)$
    % \end{defn}

        %oli8:
        %joe7
\commentout{        
	\begin{remark}
		If $\dg M$ is a consistent PDG, then 
		$\bbr{\dg M}'_* = \bbr{\dg M}_*$
	 	where $\bbr{\dg M}'_*$ is the variant of \eqref{eq:uniqdist} which uses the alternate formulation $\Gib'$ of the extra information in place of $\Gib$.
	\end{remark}
}
%joe6
%    $\UD\dg M$ will be used extensively to relate PDGs with other
%    graphical models in \Cref{sec:other-graphical-models}.
%oli8: I feel like the below is extraneous sign posting and doesn't really help flow.
        % I'm not sure how to fix yet.
%joe7: line shaving
%        $\UD{\dg M}_\gamma$ allows us to relate PDGs to other
%    graphical models, as we show in \Cref{sec:other-graphical-models}.
%joe4: you can explain it later, when you discuss it
%    and can be thought of as the result of a free energy minimization
%    process---a connection we explore in \Cref{sec:thermo}. 

%joe4*: this comes out of the blue, since you haven't discussed union
%for the other two semantics %(which is as it should be; it's a distraction)   

\begin{vleftovers}

      In contrast with the other two semantics, $\UD{\dg M_1 \cup
          \dg M_2}$ cannot be calculated from $\UD{\dg M_1}$ and
        $\UD{\dg M_2}$. However, it is effectively the only semantics
        offered by alternative graphical models, which contributes to
        their relative lack of modularity. We return to this after a
        more careful treatment of unions in
        \Cref{sec:pdg-operations}.
\end{vleftovers}
        

%joe4*: This is a distraction, althogh it is cute.  If I were writing
%a text on this material, it woudl be an exericse.
\commentout{
	\subsection{Recovering Other Semantics As Special Cases of Distribution Scores}

	We have just seen that $\gamma >0$, $\mathcal U_\gamma$ has a unique minimum,
        $\UD{\dg M}$. By increasing $k$ in $W^k_\gamma$, the weights of any distribution that is not optimal go to zero. Therefore, taking a limit as $k \to \infty$, we see that $W^k_\gamma$ converges to the weighted
        distribution that puts weight 1 on $\UD{\dg M}$ and weight 0 on all
                        other distributions, as shown below:
%joe3*: I have no idea why this should put weight 1 on a single
%distribution.  I'm lost!  I think we should just give a third
%semantics that amounts to picking the distribution of higest weight, 
%after stating a theorem that says that there always is one.  (Your
%results on convexity will be used in the proof of the theorem, but
%don't need to be stated separately.
%oli3*: I'm just using the standard trick to achieve the maximum without taking 
% a supremum. A supremum is not continuous, but we can get it for these strongly
% convex spaces by taking the associated temperature of the Boltzmann distribution to
% zero.  We're just doing this one level up, for the weighted distributions...
%oli4: I've made a bunch of changes so it's more readable. It still probably won't make it into the short paper, but I think it is useful intuition and verification for the naturality of the semantics.
	\[ \lim_{k \to \infty} W_\gamma^k(\dg M, \mu)
        = \lim_{k \to \infty} \exp\Big\{-k \Big(\mathcal
        U_\gamma(\dg M;\mu) - \mathcal
        U_\gamma(\dg M;\UD\dg M)\Big) \Big\} = \begin{cases}
        	1 & \UD\dg M = \mu \\
        	0 & \text{otherwise}
        \end{cases} \] 
\begin{vleftovers}
   		A weighted distribution is closely related to a second order
    	distribution: a distribution over distributions, which can be
    	naturally collapsed to a single distribution by taking an expectation. 
   		\begin{align*}
    		\Big(\bbr{\dg M}_{\alpha_0}^*\Big)(w) &:= \lim_{k \to \infty} 
    		\E_{\mu \sim W_{\gamma}^k(\dg M,-)} [\mu(w)] \numberthis\label{eqn:higher-expectation} \\
    		&= \lim_{k \to \infty}  \frac{1}{Z} \int_{\Delta\V(\dg M)} W_\gamma^k(\dg M;\mu) \mu(w) d\mu
    	\end{align*}
    	defined where the sum is finite; $Z$ is a normalization constant across all worlds $w \in \V(\dg M)$. 
\end{vleftovers}

	For $\gamma = 0$, and $\dg M$ is consistent ($i.e., \Inc(\dg M) = 0$) we recover the set-of-distribution semantics with the same trick:
%oli4:
%	\note{thereby providing an alternate notational justification for $^*$ as a limit as $k\to \infty$}:
	\begin{align*}
		 \lim_{k \to \infty} W_0^k(\dg M, \mu)
		&= \lim_{k \to \infty} \exp\Big\{-k \Big(\mathcal U_0(\dg M;\mu) - \inf_{\mu'} \mathcal U_0(\dg M; \mu')\Big) \Big\} \\
		&= \lim_{k \to \infty} \exp\Big\{-k \Big(\Inc(\dg M;\mu) - \Inc(\dg M)\Big) \Big\} 
		= \begin{cases}
			1 & \mu \in \SD\dg M \\
			0 & \text{otherwise}
		\end{cases} 
	\end{align*}

	In this sense, a weighted distribution provides a much more expressive semantics for a PDG than a single probability distribution, or set of them.

%joe1*: I cut this.  You haven't explained in what sense a conditional
%distribuiton is a program, you havne't motivated it, it takes us too
%far afield ...	
	\begin{vleftovers}
	\subsection{As Probabilisitic Programs}\label{sec:prog-semantics}
	
	One final way of viewing PDGs is as a set of probabilistic programs, corresponding to the edges. 
	Conditional distributions can be thought of as probabilistic programs. As a result, we can compose and run them: paths from $A$ to $B$ correspond to noisy estimates of $B$ from $A$.
	
	Specifically, if $f(b \mid a) : \mathcal V_A \to \Delta \mathcal V_B$ and $g(c \mid b) : \mathcal V_B \to \Delta \mathcal V_C$ are conditional distributions, then the probabilistic composition $g\circ f : \mathcal V_A \to \Delta\mathcal V_C$ is
	\begin{align*}
		(g\circ  f) (c \mid a) :=  \sum_{b \in \mathcal V B}\!\! f (b \mid a)\ g(c \mid b)
	\end{align*}
	
	This can be recognized as a matrix multiplication $f$ and $g$ regarded as sub-stochastic matrices.
	Thinking about graphical models this way makes thinking about chains of reasoning simpler, gives us a way out of storing probability tables, and suggests additional applications.
	
	More formally, we can define
	\[ \bbr{M}_\lambda = \left\{
			\begin{aligned}
				 \text{paths } p = N_0 \xrightarrow{p^1} N_1 \xrightarrow{p^2}\cdots\xrightarrow{p^n}N_n \\
				 \text{such that } (N_{i-1}, N_i, p^i) \in \Ed^\dg M
			\end{aligned}
		\right\} \]
	
	\begin{example}
		Composition of arrows in a tables in chain is simply an easy case of varaible elimination. 
		
		\[
			\scalebox{0.8}{
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& C
			\end{tikzcd}\hspace{3em}
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& B \ar[r] \& C
			\end{tikzcd}}
		\]	

		Conversely, factorization of a table $A \to C$ into tables $A \to B$ and $B \to C$ (i.e., a stochastic matrix factorization) corresponds to splitting a program into two steps, and the data necessary to describe it will be smaller if $|B|$ is small.
	\end{example}	
	
	% It also has not escaped us that PDGs have a particularly nice description in categorical terms, which we do not pursue further here.
	
	Furthermore, thinking about the mental state of an agent as a collection of programs you could run from any concept gives our first natural interpretation of a sub-distribution (more in \Cref{sec:full-model}): probability mass assigned to $\none$ by a edge $p$ has not terminated yet (if at all). 
	Even given infinite time, some paths in $\bbr{\dg M}_\lambda$ may be infinite.
	
	\begin{defn}
		A PDG $\dg M$ is \emph{strongly consistent} if every collection of paths $P \subseteq \bbr{\dg M}_\lambda$ is compatible, in that 
		$$\bigcap_{p \in P}\ \SD*{\vphantom{\Big|}p^0\circ \cdots\circ p^k} \neq \varnothing$$
	\end{defn}

	\begin{example}
		Bayesian Networks and conditional Bayesian Networks are strongly consistent.
	\end{example}

	\begin{prop}
%		$ \text{strongly consistent}  \subsetneq \quad 
%		\text{strictly consistent}  \subsetneq  \text{consistent} $
		Any PDG $M$ that is strongly consistent is also consistent, but some strongly consistent PDGs are not strictly consistent.
	\end{prop}

%	\begin{example}
%		The trace graph of a 
%	\end{example}


% This means we can sample them
% Also, compose them
% and a

	\end{vleftovers}
}
%joe4: \end{cmmentout}


%\begin{notfocus}

	\section{Relations to Other Graphical  Models}\label{sec:other-graphical-models}
	We now relate PDGs to two of the most popular graphical
        models: BNs and factor graphs. PDGs 
%oli10:
		% are strictly more general
        % than BNs, in a way that is related, but 
		% in some sense orthognal to
		% the factor graph approach. 
	are strictly more general than BNs, and can emulate factor graphs for a particular value of $\gamma$.
%oli8: Unecessary, I'll get to it. Would require updating anyway. Deleted.
	% More concretely, we will see
    %     that we can get the standard free energy of factor graphs, and
    %     more generally, of the full exponential family that it
    %     corresponds to, by setting each $\alpha$ to zero, and removing
    %     an implicit  `local regularization' term in $\mathcal U$. 
%	; for others, consult \Cref{fig:model-transformations} and its explanation in \Cref{sec:many-relations-graphical-models}.
		
	\subsection{Bayesian Networks} \label{sec:bn-convert}
		
        %joe8: simplified story.
\commentout{	
	A (quantitative) Bayesian Network $(G, f)$ consists of two parts: its qualitative graphical structure $G$, indicating a set of
%oli8: inserted 
	variables and
	conditional independencies, and its quantitative data $f$, an assignment of 
%oli8: expanded for clarity, removed paragraph break
	% a cpd to each node.
	a cpd $p_i(X_i \mid \Pa(X_i))$ to each variable $X_i$.
%
%joe4: this may be true, but why bother saying it?
%oli5: I guess I really have a terrible model of what you view as
%worth saying. This might not be the most efficient use of space, but
%I think provides useful historical background, explains why the
%problem hasn't been solved yet, provides a great deal more intuititon
%about how this solution works than the proof. It also tells a story.  
%joe5: My model is ``have a clear conception of the story and
%ruthlessly restructure things so as to bring it out''
        %oli5: I've changed it to vfull, as I understand we're short on space,
%but I remain confused about why you don't view it as worth
%saying---especially in contrast to the verbose expansions of
%sentences you employ when you rewrite my texts, and reitterations of
%previous points with "as we've said". 
%joe5: HOw does it fit the story?  We are not telling a story about
%BNs, but about PDGs.  Even in the full paper, it doesn't belong.
        %oli5: I also think the narrative and reasons for dropping the independences are important for discussing BNs, which have historically had that focus.
% I've therefore reinstated this paragraph, and promoted the rest of the comment to the full version.
	The first is usually seen as more fundamental
%oli8: Updated to reflect new understanding of \alpha, though could use further editing
%	; one can think of the corresponding PDG as keeping only the second. 
%joe7
%	, but equation \eqref{eq:uniqdist}, specifically the limit as
%        $\gamma \to 0$, can be thought of as elevating the
%        quantitative data above the independence assumptions.  
        but the third semantics ($\bbr{\dg M}^*$) can be
        can be understood as viewing the quantitative information as
%joe7*: But this begs the question.  Why are we doing this?
%oli9: Because with BN's it's impossible to break the independece assumptions.  Worse, there's no way to sepcify constaints ---even constraints consistent with the independence assumptions--- unless they lie on one of the edges of the graph. 
% In a BN, independence is primary. But I think it's really easy to argue that those independencies aught to take a back seat to the data. This way you can do both at once.
        more important that the qualitative independence assumptions.  
%oli5: I can do without this sentence though:
%	Fortunately, there is an intuitive way to recover the
%independencies by optimizing for a natural information-theoretic
%quantity: the extra information (\Cref{sec:extra}). 
%joe5*: Kept the sentence above. Cut the rest.  I don't even know what
%contravariant means in this context.
%oli6: I find this a useful explanation of why keeping track of 
% independences messes up modularity. I explain what I mean by contravarient
% immediately below. It can be cut for space but I'm marking it for
% the full paper 
%joe6: No!  This is a distraction.  We are not writing a paper on BNs,
%or what is the right way to interpret things to get modularity. Focus
%on the story!
\begin{vfull}
	This is the more desirable option if one cares about
	modularity, because the two components are contravariant: a subset of
	the graph, and hence of the cpds, results in a \emph{super-set} of
	the independencies, and vice versa. It is in part for this reason
	that a BN does not say monotonically less when edges are deleted, or
	more when edges are added. 
\end{vfull}

%joe7: I don't understand the net paragraph, so I'm just cutting it.
\commentout{
To do without the independence assumptions, one might hope
        that maximizing entropy would recover the conditional
        independencies, as maximizing entropy tends to make things as
        independent as possible given the constraints --- but
        maximizing entropy alone is not enough
        (\Cref{ex:counterexample}).
}
%joe7: \end{commentout}
\begin{vfull}	
	In response, some \cite{williamson2001foundations}\cite{holmes2001independence} have added alternate constraints of a causal flavor, which are perhaps smaller and more palatable than the full set of conditional independencies.  Williamson, for instance, introduces what he calls the \emph{principle of causal irrelevance}--- that extending a BN with variables $\{C_i\}$ with children $\{D_j\}$ where no $C_i$ depends on a $D_j$, restricts to the same distribution as the original.  However, these constraints are also overkill: by merely maximizing entropy one can already get the BN distribution for rooted trees, disconnected graphs, and even graphs that have nodes with multiple incoming edges, so long as every row in each such target node's cpd has the same entropy---none of which are reflected as a weakening of assumptions in a Williamson's principle of causal irrelevance.
	
%	\begin{enumerate}
%		\item (1) In contrast with a Bayesian Network, in which each node has a set of parents, each node of a PDG has possibly many sets of parents, where each set of parents corresponds to a different constraint, associated to a different table, and (2) We no longer require conditional independence of non-descendants given children
%		\item A BN is just a PDG where every cycle commutes		
%		% \item A tree. 
%	\end{enumerate}

\end{vfull}
%oli5: I've rewritten this more dramatically and pulled it out of the comment, as a transition
%joe5
%The key insight%
%joe8: cut this too.  it's no longer consistent with what we do (and I
%never undrstaood it anyway).
The key insight
	is that we can recover the BN distribution if we control for
%joe5: sorry; I don't understand this.  What does it mean to control
%for the counterfactual nature of the cpd?  For that matter, what's
%counterfactult about it?
%oli6: This is the motivation for the extra information. We
%acknowledge that a cpd 
% results in a distribution at its target, whose entropy depends on
% the distribution at its 
% source. Therefore, the cpd results in a different constraint,
% depending on what the distirbution 
% at the source is (the cpd counterfactually contains information
% about the distribution at $Y$, even if the distribution at X were to
% be completely different). In minimizing the information we know
% about the distribution, we have to control for the fact that cpds
% have this property, making them very unlike the standard constraints
% that are used (e.g., for exponential families). The resulting
        % correction gives us the extra information.
%joe6*: If you want to keep this, you need to slow down.  Look at a BN
%of the form X -> Y and point out that the cpd for X gives us the
%actual distribution on X, but the cpd lets us detemine the marginal
%probabilty of Y for all distributions on X.  In that sense, its
%giving us counterfactual information.  As I said in an earlier joe6*
%comment, you probably should say this earlier.
        %Then exlain (slowly) how your definition does account for it.
%This will be a mysterious definition to many readers, so you have to
%motivate it much better.         
        the counterfactual nature of the cpd as a constraint, as we
        do in \Cref{sec:scoring-semantics}, allowing us to recover the
%joe6
        %        independences without assuming them.
        independencies without assuming them.
%}        
Nevertheless, as we shall show, our third semantics still allows us to
recover the independencies.
}
%joe8: \end{commentout}
%joe7: added


	%oli5: The second of these (new) felt like a better
	% 	transition. Neither sentence has information content
	% 	though, so I've removed them both. 
	% 	We can convert a BN to PDG as follows:
	%	To state the theorem, we first formalize the conversion of a BN to a PDG.

%oli8: This needs to be rewritten; I don't think either of us are happy with it.
% Would it be reasonable to describe it without the formality, and include a more formal version in the appendix? It's actually a rather simple, intuitive transformation --- but writing out the details in a way that I think is technically correct is annoyingly cumbersome and difficult to read. 
%joe7: added
%joe8
%oli10: deleted first line, as this is basically the last few words above.
% 	In this section, we show that PDGs are more general than BNs.
We start by generalizing the construction of
%oli9: Cref makes link easier to click
% Example~\ref{ex:smoking} to show how we can, in general, transform a
\Cref{ex:smoking} to show how we can, in general, transform a
BN to a PDG.
\begin{defn}[Transformation of a BN to a PDG]\label{def:bn2PDG}
%joe8: moved from above.  This is where it belongs
Recall that a (quantitative) Bayesian Network $(G, f)$ consists of two
parts: its qualitative graphical structure $G$, 
%oli8: inserted
%joe8
%indicating a set of variables and
%	conditional independencies,
described by a dag,
and its quantitative data $f$, an assignment of 
%oli8: expanded for clarity, removed paragraph break
	% a cpd to each node.
	a cpd $p_i(X_i \mid \Pa(X_i))$ to each variable $X_i$.
  If $\cal B$ is a Bayesian network on random variables
    $X_1, \ldots, X_n$, we construct the corresponding PDG
%oli5: I am not attached to the $\Gamma$ notation, but $\dg M$, $\sf
%N$, ...  
% are symbols I've reseved in my head for specific PDGs. In a context where
%\dg M is already defined, I want $\Gamma(\sfN)$ to have nothing to do with $\dg M$.
% Therefore I have reverted the symbols, though I'm also happy to keep looking for suitable notation.
	%$\dg M_{\cal B}$ 
	$\PDGof{{\mathcal B}}$
%oli5: fixes a bug present in both formulations, described below:
				as follows: we take $\N := \{X_1, \ldots, X_n \} \cup
                % as follows: we take $\N := \{\{X_1\}, \ldots, \{X_n\} \} \cup
%joe4*: 
%                \bigcup_{i=1}^n\{ \Pa(X_i) \}$ to be the set of all of
%                the BN's variables, plus a new variable for each
     %         collection of parents, if not already in the collection.
%oli5: This presentation is nicer than mine, but unfortuantely doesn't
%work for technical reasons: 
% First, we need the union to collapse identical values of parent sets, and second, we want to collapse
% singleton parents to their values (which my original formulation did
%not do either, but can be fixed by using \{\{X_1\}, \ldots, \{X_n\}
%\} instead of \{X_1, \ldots, X_n \}).  
%joe5: I think that mh presentation should work fine, with minor
%modifications, that I suspect will lead to something simpler than
%yours.  Let's discuss.
%oli5: If these symbols are fresh, then they are distinct, forcing
%$|\V(\PDGof{\mathcal B})| = 2 * |\V(\mathcal B)|$. which is
                                %unfortuantely not what we want.
%joe5*: \Pa(X_i), by definition, is a set of variables, not a
%variable.  It's *not*, as you say below, a variable ``coresponding'' to the
%parents of X_i.  That's why you need the Pa_i notation.  I can
%understand that you want to identify two variables that correspond to
%the same set.  So perhaps the right thing to do is to have variables
%Y_{\Pa(X_i)}:  i \in {1, ..., n}, |Pa(X_i)| > 1}.  You can point out
%that if Pa(X_i) = 
%Pa(X_j), then Y_{Pa(X_i)} and Y_{Pa(X_j)} are the same variable and
%that if  Pa(X_i) = X_j (so |Pa(X_i)| = 1) then we identify
%Y_{Pa(X_i)} with X_j.
                % \{ \Pa_1, \ldots, \Pa_n\}$.  
				\{ \Pa(X_1), \ldots, \Pa(X_n)\}$.  
%joe6*: This still needs to be corrected
%oli5: \N is required only to be a set. It now has the correct number
%of distinct elements. 
%joe4
That is, the variables of 
  %oli5: see above.
		  %	$M_{\cal B}$
		  $\PDGof{{\mathcal B}}$
consist of all the variables in
%oli5:
%	 ${\cal B}$ together with a new variable corresponding to the parents
${\cal B}$ together with a variable corresponding to the parents
of $X_i$%
%oli5:  this case has already been taken care of.
% if $X_i$ has more than one parent
.  (This will be used to deal with the hyperedges.) 
%oli5: No longer necesary to mention explicitly.
%joe5: I disagree; see above.
% 	For simplicity, we can identify $\Pa(X_i)$ with the unique parent of $X_i$ if $X_i$ has only one parent;
%oli5: This already happens automatically.
	% if $X_i$ has no parents, then we can take $\Pa(X_i) = \emptyset$ to be $\var 1$.   
                The values $\V(X_i)$ for a random variable
                $X_i$ are unchanged, 
%oli5: added
	(i.e., $\V^{\PDGof{{\mathcal B}}}(\{X_i\}) := \V(X_i)$)
%joe4: This is where the extended \V notation that I mentioned when
%you first defined \V would come in useful
%                and $\V(\Pa(X_i))$ is defind on
                %               sets as above.
%oli5: The shorthand confuses the two definitions of \V(set of
%vars). They conincide for good reason, but I don't want to even get
                %into this by using the shorthand here.
%joe5: what two definitions?                  
%	and $\V(\Pa_i) = \V(\Pa(X_i))$ 
	and $\V^{\PDGof{{\mathcal B}}}(\Pa(X_i)) := \prod_{Y \in \Pa(X_i)} \V(Y)$
%oli5: This case does not require special attention, because there is
%a unique random variable $\sf 1$ which takes one value, and the
%cartesian product of zero sets. This makes the definition feel
        %cleaner to me.
%joe5*: Oliver, I find this frustrating.  You've rewritten something
%that is easy to understand to something which is longer, uses
%undefined verbiage %(``nullary product'') that will be harder for the
%reader.  If you don't believe me, ask your friends!  
 (if $\Pa(X_i) = \emptyset$, so that $X_i$ has no parents, then we 
         take $\V(\Pa(X_i)) = \{\star\}$).
%(as is standard, we take the nullary product $\prod_\emptyset$ to be a
%        (as is standard, if $\Pa_i(Y)
%        we take the nullary product $\prod_\emptyset$ to be a
%        singleton set, which results in a the unique random variable
%        $\sf 1$ which takes only a single value; therefore the above
%        holds even when $X_i$ has no parents).   
%joe4
%joe7: I don't see why you need singleton sets; it's inconsistent with
%the definition of \N
%oli9: It's only inconsistent with the definition of \N because you changed my definition of \N and I didn't wnat to touch this further.
% I want to note that my presentation had the benefit of not duplicating every node 
% in a chain X1 -> X2 ->  ... -> Xn. 
% As currently written, we get X1 <- { X1 } -> X2 <- { X2 } -> ... , which is equivalent
% but way uglier. My suggestion is to just change \N so everything is a singleton set.
% Otherwise, we're lying about the conversion earlier (it's an insignificant lie, but still). I think I wrote it properly the first time but you reacted very strongly that it was too hard to read. 
%
%We take the set of edges $\Ed^{\PDGof{\mathcal B}} := \{ (\Pa(X_i), \{X_i\}) : 
We take the set of edges $\Ed^{\PDGof{{\mathcal B}}} := \{ (\Pa(X_i), X_i) : 
         i = 1, \ldots, n \} \cup \{ (\Pa_i, \{Y\}) : Y \in
                \Pa(X_i)\}$ to be the set of edges to a variable $X_i$
%joe7: ``projection edge'' is undefined
   %                from its parents, plus also projection edges from
          from its parents, together with an edge from
%joe4
%                the                 sets $\Pa(X_i)$ to their elements.
%oli5:.
% $\Pa_i$ to the variables in $\Pa(X_i)$.  
%joe7
%          from each $\Pa(X_i)$ to every singleton set containing
%          one of its elements. 
          from $\Pa(X_i)$ to each of the elements of $\Pa(X_i)$, for
          $i = 1, \ldots, n$.  
          %joe7: removed paragraph break
          %
		Finally, we set $\mat p^{\PDGof{{\mathcal
%joe8: this doesn't typecheck
%                  B)}_{(\Pa(X_i), \{X_i\})}$ to be the cpd associated
%                with $X_i$ in $\cal B$;  for each from $\Pa(X_i)$ to
%                $X_j$ for $X_j \in \Pa(X_i)$, we set
%        		\[ \mat p^{\PDGof{\mathcal B}}_{(\Pa(X_i),
%that is, given a setting $(\ldots, y', \ldots)$ of a set including the
%variable $Y$, we give a distribution on $Y$ by  1 if $y = y'$ and 0
%otherwise. 
                    B}}}_{(\Pa(X_i), X_i)}$ to be the cpd associated
                with $X_i$ in $\cal B$, and for each node $X_j \in \Pa(X_i)$,
                we define
		\[ \mat p^{\PDGof{\mathcal B}}_{(\Pa(X_i),
                  X_j)}(\ldots, x_j, \ldots) = \delta_{x_j};\]
that is, given a setting $(\ldots, x_j, \ldots)$ of $\Pa(X_i)$, 
we get the distribution $q$ on $X_j$ such that $q(y) = 1$ if $y = x_j$ and 0
otherwise. 
	\end{defn}
	
%joe4
%	We now present the theorem:
	
	\begin{theorem}[restate=thmbnsRpdgs]\label{thm:bns-are-pdgs}
          If $\cal B$ is a Bayesian network, then
          %joe8*: isn't this more general statement true?
%oli10: Yes, thank you for fixing this. I had this version in earlier but then it got cut.
%oli10: change the bounds
% 		for all $\gamma \in (0,1]$,
for all $\gamma > 0$,
we have $\bbr{\PDGof{{\mathcal B}}}_\gamma^* =
\bbr{\PDGof{{\mathcal B}}}^*$.  Moreover, the unique probability distribution in
$\bbr{\PDGof{{\mathcal B}}}^*$ is the distribution specified by
            ${\mathcal B}$.
%joe8
%%oli9
%          % unique probability distribution $\mu^* \in
%the          unique probability distribution $\mu_* \in
%%joe4: I would prefer to use M_{\cal B} rather than \PDGof{\cal B}$,
%%oli9: a distribution doesn't represent a BN, because the BN has extra
            %%stuff like directions of arrows.
%joe8: that might be rue, but you can't introduce the notation Pr_B
%out of the blue           
%\bbr{\PDGof{\mathcal B}}_*$ is the one given by $\Pr_{\mathcal B}$.
%oli9: this should have been commented out
%                the conditional independencies of $\cal B$.	 
	\end{theorem}

%joe4: added
        %joe7: OK; we've said this three times now.  I cut it
        \commentout{        
 Theorem~\ref{thm:bns-are-pdgs} tells us that we can recover BNs, and
%joe6
%all the independencies they encode, within the farmework of PDGs,
all the independencies that they encode, within the farmework of PDGs,
while not giving up on all the advanatages of PDGs, such as their
modularity and the ability to encode inconsistent information.
}


%joe6*: Note for the Nuerips submission, nad we don't want to talk
%about \alpha_L yet
\commentout{
    %joe4*: I don't understand the corollary (or even the notation)
	%oli5: I've introduced all of the notation, but I can explain
	%it better. This is the more powerful version of the theorem,
	%and in my opinion, is the real payout of this appraoch: not
	%only is it true for some settings of the parameters, but you
	%get th BN distribution out for ALMOST EVERY setting of the
	%parameters. 
%\commentout{
    \begin{coro}
		Let $\cal B$ be a BN. For any $\gamma > 0$, and vector of positive numbers $\vec \beta > \vec 0$, 
		\[ \UD[\Big]{\PDGof{\mathcal B}, \vec{1}, \vec \beta}_\gamma = \Pr\nolimits_{\cal B} \]
%oli5: added. I did this because you say you don't understand the
%statement above, but I have all of this explanation below as well. We
                %should probably remove one of the two.
%joe5: I didn't read this; I don't want to think about \alpha yet (and
%I still don't understand it).                  
		That is, so long as we take each $\alpha_L = 1$, the
                distribution defined by the cpds of $\PDGof{\mathcal
                B}$ with \emph{any weights} is precisely the one given
                by $\cal B$. 
	\end{coro}
	The corollary extends \Cref{thm:bns-are-pdgs} through the other two semantics, showing that the result is not sensitive to the additional parameters ($\beta, \gamma$), and works for the default value of $\alpha = 1$.
	This is true for PDGs which are structurally just subsets of BNs, where every node has at most one incoming edge. In such a structure, every cpd can be simultaneously attained perfectly regardless of how little you are attached to them ($\beta$) and the strength of the bias towards uncertainty $(\gamma)$.
	However, not all PDGs have the particularly nice structure,
        and these parameters will be important once there starts to be
        possible conflict between beliefs.  
%}

}
%joe6: \end{commentout}
	% d-separation? I don't have a lot to say but it the specialness of the ``colider'' or head-to-head nodes in determining connectedness  is related to the difference in interpretations I think.

	
%oli2: I was thinking of putting a list of comparisons here but I can't think of good ones right now; we've covered most everything earlier.
%	\begin{fact}
%		\begin{enumerate}
%			\item Subgraphs of BNs are not always BNs, but subgraphs of PDGs are.
%			\item 
%		\end{enumerate}
%	\end{fact}

	
	\subsection{Factor Graphs} \label{sec:factor-graphs}	
%oli8: moved all of the original material to the appendix, this section is new.
%joe7
%	Factor graphs \cite{koller2009probabilistic}, make some
%        similar promises to PDGs. They generalize BNs, the barrier to
%        adding observations is extremely low, and their failure to
%        normalize in general may be viewed as a kind of inconsistency
%        in a very similar fashion \cite{wainwright2008graphical}.
	Factor graphs \cite{KF09}, like PDGs, generalize BNs and have a
        low barrier to adding observations.  Moreover, their failure to
        normalize in general may be viewed as a way of representing
%oli9: line shave
%joe8: reinstated
        some
        inconsistency.
%joe8
        In this section, we consider the relationship between factor graphs
and PDGs.        

%oli9: inserting a lot of material. Some taken from appendix and added, so 
% End of insert marked below with %%oli9-end insert
\begin{defn}
%joe8
  %  A \emph{factor graph} is a collection of random variables
%        $\mathcal X = \{X_i\}$ and a collection of \emph{factors}
  A \emph{factor graph} $\Phi$ is a collection of random variables
        $\mathcal X = \{X_i\}$ and \emph{factors}
%joe8: I can't parse this: you have a bunch of notation that comes out of the blue
%oli10: I think it's useful. It linguistically matches the above, makes it clear that the object is an indexed set, and immediately gives the function type of a factor, without having to read the whole paragraph. Also we've defined all of the nonstandard symbols already, and we define everything else below
       $\{\phi_J\colon \V(X_J) \to \mathbb R_{\geq0}\}_{J \in
         \mathcal J }$, %where each $J \in \mathcal J$ is associated
%        to a subset of $X_J \subseteq \mathcal X$.  
%	Together with a vector of weights $\{ \theta_J \in \mathbb R
%        \}_J$ (or otherwise assuming each $\theta_J = 1$),  
        that define relationships between subsets of variables in
        $\mathcal X$;
more precisely, each factor $\phi_J$ is associated with a subset
$X_j\subseteq \mathcal{X}$,
%oli10
% and maps $\V(X_j)$ to $\mathbb{R}^+$ (the non-negative reals).
and values of $\V(X_j)$ to non-negative real numbers.
%oli10:
% I'm not sure how to do this, but it's not the factor graph itself that has the weights. It's just the natural exponential family, which is why I was careful to say these weights were part of the factor graph; they're not.  But for the conversion to be invertible I need to consider the appoprate associated member of the exopnential family.
% Each factor $\phi_J$ also has a weight $\theta_J \in \mathbb{R}$.   The
% factor graph specifies the distribution
The factor graph $\Phi$,
together with a vector of non-negative weights $\{ \theta_J \}$,%
	\footnote{This parameterization is generally external to the factor graph, and is the associated exponential family; to describe what is genreally called a ``factor graph'', one would take each $\theta_L=1$.}
specifies a scoring function $\mathcal G_\Phi$ of its own, called its ``free energy'', and a distribution $\Pr_{\Phi}$ which minimizes it.
%joe8
%oli10: I'm trying again. I think this is important.
	\[ \Pr_\Phi(\vec x)
        	% $ \Pr_{\Phi}(\vec x) 
		= \frac{1}{Z_\Phi} \prod_{J \in \cal J} \phi_J(\vec
                x_J)^{\theta_J}, %$  
%joe8*: I'm sure you spent a long time typestting this.  I found it
%extremely difficult to parse, and unncessary.
%oli10: It takes up almost no space, and will give us a lot. I think at this point theres
% baseically nothing complicated left to explain, and we'll get a much stronger result by introducing this.
		% \quad\parbox{0.9in}{\centering minimizing its
        %          \\ ~~\emph{free energy},  }\quad 
	\qquad\text{and}\qquad
	\mathcal G_\Phi(\mu) := \!\E_{\vec x\sim\mu}\left[  \sum_{J \in
                   \cal J} \theta_J \log\frac1{\phi_J(\vec
                   x_J)}\right] - \H(\mu)  , \]
	where $\vec{x}$ is a joint setting on all of the variables,
        $\vec{x}_J$ is the restriction of $\vec{x}$ to only the
        variables $X_J$, and $Z_\Phi$ is the constant required to
        normalize the distribution.  
\end{defn}
%joe8: we've never alked about ``global semantics'' before.  This is
%the wrong story
%The global semantics of a BN make it the product of its cpts. We
%extend this to arbitrary PDGs.
We can associate with each PDG a factor graph, and vice versa.
\begin{defn}[PDG to factor graph]\label{def:PDG2fg}
%joe8: we have notation; let's use it.
  %  If $\dg M = \mnvars[]$ is a PDG, define
  If $\dg M$ is a PDG, define   
%joe6*: Sorry, I don't undestand this notation.  Is ((\Ed,\in), \mat
%p)$ supposed to be a factor graph? So you're somehow identify \in
%with \iota?  This shows that the use of \iota is making life worse
%...  I think that you have to spell this out better.
	% $ \Phi(\dg M) := ((\Ed,\in), \mat p)$
%oli9: using the less clever, compressed notation.
%joe8: 
  %  $\mathcal J := \Ed$ and the associated factor graph	on the
the associated factor graph $\Phi_{\dg M}$ on the	
%oli9.question: is (\N, \V) appropriate? you need both \N and \V to determine the
% variables, but it looks strange.
variables $(\N,\V)$ by
%joe8: I find it *extremely* difficult to make sense of what you wrote
%Please chck if I've got it right.  Note also p_l(y|x) does not typecheck.
%oli10: It's correct. 
%oli10: As for the note, it does typecheck, because now it's defined as a conditoinal probability distrribution. It's also obvious shorthand and sometimes clearer.
%	$\Phi(\dg M) := \smash{ \{ x,y \mapsto \bp(y \mid x)\}_{\ed
%                    LXY \in \Ed}}$ and weights $\theta_L := \beta_L$. 
%oli10 changing "link" to "edge" for consistency, and using notation
% taking the factors to be given by the links in ${\dg M}$, and for a
taking the factors to be given by the edges in $\Ed^{\dg M}$, and for an
edge $L$ from $X$ to $Y$, taking $\phi_L(x,y)$ to be $(\bp^{\dg
  M}(x))(y)$, and taking the weight $\theta_L = \beta_L$.
\end{defn}
%joe8: moved the next sentence down
%The translation in \Cref{def:PDG2fg} clearly destroys the direction of
%the arrows, which makes the structure impossible to recover.
%joe8: I can't make sense of the following sentence.  I cut it.
%Instead, we can add a new edge, with a completely different asserting
%a joint probability.  

\begin{defn}[factor graph to PDG] \label{def:fg2PDG}
If $\Phi=(\{\phi_J\}_{J \in \cal J})$ is a factor graph, then 
$\PDGof{\Phi}$ is the PDG generated by inserting
%joe8: we don't have the notion of a  ``joint variable node'' nor are
%there ``factor nodes''
%joint variable node
%$X_J := \prod_{j \in J} X_j$ for every factor node $J \in \mathcal
%J$, together with projection edges $X_J \tto X_j$ for each $X_j \in X_J$
node  $X_J := \prod_{j \in J} X_j$ for every factor $J$,
together with edges $X_J \tto X_j$ for each $X_j \in X_J$
(as done in \Cref{def:bn2PDG}), and an edge $\sf 1 \to X_J$ whose
associated cpd $\bp[J]$ is the joint distribution on $X_J$ obtained by
normalizing $\phi_J$; let $\beta_L := \theta_L$.% 
\end{defn}

%joe1: rewrote
%Surprisingly, despite garbling the structure (see
%\Cref{fig:fg2PDG,fig:fg-intro-examples}), when we fix $\gamma=1$, the
%two operations preserve most of their semantics.
PDGs are directed graphs, while factors graphs are undirected.  The
map from PDGs to factor graphs thus loses some important structure.
As shown in \Cref{fig:fg2PDG,fig:fg-intro-examples}) in the appendix,
the mappings can change the graphical structure signfiicantly.
%joe8*: note the next line
%oli10: Because "equally" is not really clear, I'm removing the last part of the sentence (and changing \gamma back to 1)
Nevertheless, if we take $\gamma=1$,
%so that we weight
%the qualitative and quantitative information in the PDG equally, 
the two operations have the same semantics.  

\commentout{
%oli9: this holds more generally, illustrating that PDGs can carry factor graph data regardless of \theta, \beta.
	\begin{prop}[restate=propfgpdglossless]%\label{prop:fg-pdg-lossless}
		$\Phi \circ \PDGof = \mathrm{Id}_{\text{FG}}$. That is, if $F$ is a factor graph, then $\Pr_{\Phi(\PDGof{F})} = \Pr_F$.
	\end{prop}
}

\begin{theorem}[restate=thmpdgisfg]\label{thm:pdg-is-fg}
%joe8*: can we say anything when the weights are not 1?
%oli9*: Yes, If every \beta_L = \gamma. I've just updated with the more general result.
%
% We get a whole lot more if we introduce \alphas. Specifically, if \alpha_L = \beta_L , then it's equivalent to a factor graph with parameter \theta_L. I guess maybe without the \alphas, the \thetas are not helpful. 
%
  %  If $\dg M$ is a PDG with every $\beta_L = 1$, then
If $\dg M$ is a PDG with all the weights $\beta_L = \gamma$, then
    % If every $\beta_L^{\dg M} = 1$, then
	% for any joint distribution $\mu$ on $\V(\mathcal X)$, we have
%joe8*: what is \mathcal G?  You can *not* introduce notation out of
%the blue!  I took a guess at what you meant. 
%oli10: putting both result together, now that I explained free energy slightly better. Removing the 1/2.
    $\gamma \mathcal G_{\Phi(\dg M)} = \bbr{\dg M}_{\gamma}$.
and in particular,
$\bbr{\dg M}_{\gamma}^* = \{\Pr_{\Phi({\dg M}}) \}$. 
 	% $\kldiv{\mu}{\Pr_{\Phi(\dg M)}} = \bbr{\dg M}_{1}(\mu)$
	% In particular, $\Pr_{\Phi(\dg M)} = \bbr{\dg M}_*^{\gamma := 1}$
\end{theorem}
\begin{theorem}[restate=thmfgispdg]\label{thm:fg-is-pdg}
%oli10: "On X" is not helpful, save the line!
	% If $\Phi$ is a factor graph on $\mathcal X$, then
	If $\Phi$ is a factor graph, then
 	% for any joint distribution $\mu$ on $\V(\mathcal X)$, we
        % have $\kldiv{\mu}{\Pr_\Phi} = \bbr{\PDGof{(\Phi)}}_{1}(\mu)$ 
%joe8: again
%oli10: adding back in
	$\gamma \mathcal G_\Phi = \bbr{\PDGof{(\Phi)}}_{\gamma} + k$ where $k$ is a constant, and in particular,
        $\bbr{\PDGof{\Phi}}_{\gamma}^* = \{\Pr_{\Phi} \}$. 
\end{theorem}
%joe8: unnessary
%It immediately follows that in both cases, that the distribution
%$\mu_*$ that minimizes the functions is the same; for the factor graph
%$\Phi$, $\mu_*$ is the distribution $\Pr_\Phi$, while for $\dg M$, we
%get $\mu_* = \bbr{\dg M}_*^{\gamma=1}$, where we have fixed $\gamma$
%to 1 rather than taking the limit in \eqref{eq:uniqdist}.  

%joe8*: I cut this.  I don't know what a ``canonical scoring function
%is''.  I suspecgthat all the material on free energies will be cut
%Therefore, when $\gamma = 1$, a PDG with each $\beta_L = 1$ is
%equivalent to the associated factor graph with $\theta = 1$. Not only
%do they determine the same distribution, but they are are also
%equivalent in the stronger sense of sharing a cannonical scoring
%function. We explore the parallel with free energies further in
%\Cref{sec:thermo}. 

%oli9: moved here and changed a fair amount
%joe7*: Whoa!  Slow down!  The good news is that we are using hte
%alternate formulation anyway.  But if we can't, then this needs to be
%introduced *far* ore slowly.  For example, you'd have to explain
%*why* it's appropriate to suddenly switch to the alternate
%formulation.  This would have been unacceptable had there been no
%intuition for the alternate formalism.  But I simply don't
%understand where this rewrite came from.  You must explain how you
%got it alittle more slowly.  What hapened to the
%	By using the alternate formulation $\Gib'$ of the extra
%        information \eqref{eqn:alt-extra},
%joe8*: I cut this.  I still can't follow the calculation.  You can
%spell out the details in the proof of the theorems, which will need
%to be written up carefully.
%oli10: By this, you don't know how it got to be this way? (I now have a proof in the appendix which I would reference)
% Or do you mean you can't read the equation?
% Either way, I think this equation is the single highest value use of space in the paper as far as explaining the scoring function --- not just to myself, but to the large group of ML people who are used to skimming through the paper to find the equation detailing the loss function and ignoring everything else.
% I won't push it right now because I already reinstaed the free-energy, but I'll be back.
\commentout{
\Cref{thm:pdg-is-fg,thm:fg-is-pdg} can be almost seen directly by
rewriting the scoring semantics in \eqref{eqn:full-score} as follows,
and comparing the defintion of $\mathcal G_\Phi$. 
	\begin{equation}\label{eq:semantics-breakdown}
		\bbr{\dg M}_\gamma(\mu) = \E_\mu \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \bigg[~\color{gray}\underbrace{\color{black}
			\beta_L \log \frac{1}{\bp(y\mid x)}
 				}_{\color{gray}\smash{\text{log liklihood}}} + \underbrace{\color{black}
 			(\gamma - \beta_L ) \log \frac{1}{\mu(y \mid x)} 
				}_{\color{gray}\smash{\mathclap{\text{local regularization}}}}\bigg] - \underbrace{\color{black}
			\gamma \log \frac{1}{\mu(w)} 
				}_{\color{gray}\smash{\mathclap{\text{global regularization}}}}\color{black}\Bigg\} 
	\end{equation}

Comparing \eqref{eq:semantics-breakdown} shows us that the qualitative
correspondence only occurs if the middle term is zero, which can only
happen when every $\beta_L=\gamma$. This view also clarifies
additional properties. 
}


%joe8
%In particular, although both are measures of confidence, the way that
The proofs of \Cref{thm:pdg-is-fg,thm:fg-is-pdg} show that, although
both $\theta$ and $\beta$ are measures of confidence, the way that 
a factor graph varies with $\theta$  
	% \footnote{\cite{wainwright2008graphical}}
is quite different from the way a PDG varies with $\beta$. Increasing
%joe8*: Why is this true?   I took a guess at a correct formulation
%$\theta$ makes the resulting distribution more deterministic, while
%oli10: there's nothing special going on here. It's just proportional to \phi(x)^2 instead of to \phi(x), so big things get bigger and small things get smaller.
%oli10: This is correct, but "all other tuples" is ambiguous, and particularly misleading when you set up the second bit (the other tuples for the same cpt).
$\theta_J$ for some factor $\phi_J$ makes the resulting distribution
more deterministic
%oli10: After fixing, I decided this was not a good use of space; it just expands the words above and interrupts the story
\commentout{
	(more precisely, the probabilities of the most probable tuples $\vec{x}_J$ 
	%such that $\phi(\vec{x}_J)$ is maximal 
		increase, while the
		% probabilities of all other tuples decreases). 
	probabilities of the least probable ones decrease).} 
 On the other hand,
%joe8
%increasing $\beta$ makes an agent more certain that
%the cpt is the way it is, as in \Cref{ex:overdet}
increasing $\beta_L$ says that the agent more certain about the
correctness of the cpt on edge $L$.
%oli10: why ay this? This is almost true for factor graphs as well. I don't 
%		 (and has no effect on other probabilities).


%joe8*: I think that the story above was wrong
%oli10: I don't understand what 
As the discussion in the appendix shows, taking $\gamma = 1/2$ is
essentially necessary to get \Cref{thm:pdg-is-fg,thm:fg-is-pdg}.
However, the analogue of \Cref{prop:consist} does not hold
in general for this choice.  This leads to some arguably unacceptable
behavior in factor graphs trying to capture the same phenomena as PDGs.

\begin{example}\label{ex:overdet}
Consider the PDG $\dg M$ containing just $X$ and $1$, and two edges
$p, q: 1 \to X$.
%joe8:
(Recall that such a PDG can if we get different information about the
probability of $X$ from two different sources; this is a situation we
certainly want to be able to capture!)
%joe8: this doesn't typecheck
%suppose $p$ and $q$ are both attached to the same
%$p(x) = q(x)$ which places 0.7 probability on $x_1$ and 0.3 on
Consider the simplest situation, where $p$ and $q$ are both associated
with the same distribution on $X$.  For definiteness, suppose that
$\V(X) = \{x_1,x_2\}$, 
%oli10*: I don't understand this example. Why does Gib alone matter? 
% Also I'm not convinced by the argument in any setting: just because the ones that give probaility 1 and 0 are the ones that minimize \Gib, doesn't mean that all intermediate distributions score the same. Actually the scores definitely will not cancel.
%oli10: I'm happy to reinstate this or a variant if you still think it's useful and true.
%
% and the distribution ascribes is $\mu_{1/2}$,
% which ascribes probability $1/2$ to
% $x_1$ and $x_2$.  Clearly $\mu_{1/2}$ is the only distribution
% consistent with this PDG, so $\bbr{\dg M} = \{\mu_{1/2}\}$, by
% Proposition~\ref{prop:sem3}.  
% But it is not hard to see that there are two distributions that
% minimize $\Gib(\dg M; \mu)$, the ones that give $x_1$ either
% probability 1 or probability 0.  As a consequence,
% $\bbr{\dg   M}_{1/2}^*$ consists of \emph{all} distributions.
%
% If instead we consider ${\dg M'}$, which is just like ${\dg M}$ except
%
%oli10 added
and
%
that the distribution associated with both links is $\mu_{.7}$, which ascribes
probability $.7$ to $x_1$, then  $\bbr{\dg M} = \{\mu_{.7}\}$, 
%  Now $\UD{\dg M} = p(x) = q(x)$,
%$\UD{\dg M}^{\gamma=1} =
while it can be shown that 
$\Pr_{\Phi{{\dg M')}}} = \mu_{.85}$, so  $\bbr{\dg M'} = \{\mu_{.85}\}$.
%$X\!=\! x_1$ and $0.15$ on $X\!=\! x_2$.
\end{example}

%joe8: cut
%This is undesirable behavior for PDGs as we have described them; we
%point out further issues in \Cref{sec:fg-issues}. Taking a limit as
%$\gamma$ goes to zero fixes some of these issues, intuitively by
%ensuring that there is local regularization, so that the resulting
%distribution is consistent with the semantics, by
%\Cref{prop:consist}. 
%oli9: I know "regulariation" is not defined, but it's a very pervasive concept and
% defining it here would be cumbersome.


%%oli9-end insert
%
%joe7*: I have no idea what the next sentence means.  I just cut it.
%But we do need a succinct sentence that captures the distinction
%between Factor graphs and PDGs.  
%        Unlike PDGs, however, the constraints in a factor graph are
%        can only be interpreted globally.  



%joe7*: Oliver, the next paragraph is unacceptable.  SLOW DOWN.  Do
%not use terms like ``least surprising''.   
%oli9: removed. Explaining this carefully is too much work, but in exchange I want to 
% label the equation a little better.
	% The outer terms are the simplest. The first is a likelihood
    %     term: it is maximized when the distribution $\mu$ places all
    %     of its mass on a single world which is least surprising
    %     according to the CPTs, and final term is a global
    %     regularization, acting as a uniform prior.  
	% The second term is a local regularization: this ensures that the local uncertainty in each edge indeed matches the cpt... \todo{}

\commentout{
	The relative entropy from $\mu$ to the product of the factors, called the \emph{free energy} [\Cref{sec:thermo}] of a factor graph%
		\footnote{Without fixing the values of $\beta_L$, this is rather the associated exponential family}
	, consists of precisely these two terms:
%joe7*: Sorry, I don't understand where the next line came from.  We
%were never taking the relative entropy of something to the power
%\beta_L.  I'm lost.  
%oli9: Updated notation to use the definition.        
	        \[
		\kldiv[\Big]\mu{\Pr\nolimits_{\Phi(\dg M)}}
		= \E_\mu \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  }
			\beta_L \log \frac{1}{\bp(y\mid x)}  - \log \frac{1}{\mu(w)} \Bigg\},
	\]
	Let $\phi$ denote this product distribution. It should be clear that 
	if $k \in \mathbb R$ is a constant, then $\bbr{\dg M}_k(\mu) = k\,\kldiv{\mu}{\phi}$ whenever every $\beta_L = k$. As a result, for a fixed setting of $\gamma$, the minimum score distribution is the one given by the product of cpts.
%joe7*: I have no idea what's going on, and have no idea how this fits
%in with anything else.  I gave up trying to read this
        
	The limit will not generally be of this form. The locality
        this affords PDGs is best illustrated with a simple
        example. Suppose $\dg M$ consists of a single variable $X$ (in
        addition to the unit variable), and two edges $1 \to X$,
        associated with distributions $p(X)$ and $q(X)$. Even if $p =
        q$ (which is the only possible way $\dg M$ could be
        consistent), the resulting product distribution, given by the
        factor graph, will not be the same. Effectively, it is
        impossible to express certainty that a non-degenerate
        distribution is correct. PDGs, by contrast 
%oli8: note:
	, can express these higher level statements without 
 	%but the limit as $\gamma \to 0$ may not necessarily be of this form.
}

%joe8: this is not a paper about factor graphs!
%	See \Cref{sec:factor-graphs-long} for a more careful
%        introduction to factor graphs, and a description of how PDGs
%        with fixed $\gamma$ can emulate arbitrary factor graphs.% 
% Maybe the last phrase is too much.
 	% , and \Cref{sec:thermo} for more of the thermodynamic analogy.

	\subsection{Other Probabilistic Graphical Models and Related Work}
	Directed Factor graphs \cite{frey2012extending} impose constraints on factor graphs to resolve some of their issues and bring them more in line with BNs, but they have no semantics if the constraints are not satisfied, making them a way of visualizing factor graphs more than a novel modeling tool.

	PDGs are in some ways more closely related to Dependency Networks \cite{heckerman2000dependency} in that they explicitly discuss the possibility of inconsistency and make the case for modularity, but dependency networks only have the expressive capability of a Markov Network, and any non-symmetric dependency network is inconsistent.
	
	% \todo{more related work if this is the right place to put it.}

%oli8: Removed the two large sections on themodynamics and moved them to the appendix. 
%TODO: I still have to merge the two thermo sections there.

%joe8: This will not make it to the submission.  It's totally
%unrelated to the rest of the paper.  We should definitely write a
%paper on incosnsiency (the one we were originally going to write) but
        %this isn't it
\commentout{        
	\section{Using Inconsistency}
%oli8: remove subsection
	% \subsection{BELIEF UPDATING} 
%oli8: I don't know if it's reasonable to keep this for the final version, but I think it's 
% one of the most important classes of example (where we add scaffolding and reason about a digram without altering its contents). It does not require any additional theory, and we keep cutting the things of this flavor.
	\label{sec:belief-update}
	Belief revision, both through Bayes' and Jeffrey's rules, can be thought of as the addition of a new marginal to a distribution, and then a resolution of inconsistency. In Dietrich, List, Bradly \cite{dietrich2016belief}, a belief revision is an update $p \mapsto p_I$ of a belief state $p$ to a new one consistent with the input $I$. 
	
	For us, belief revision consists simply of the addition of a new edge to the picture, followed by a resolution of the resulting inconsistency. 
	With reference to \Cref{fig:belief-update}, an extension of \Cref{ex:randomvars}, consider the following update. 
	Upon noisily observing the variable $B$ to with probabilities indexed by $\pi = \{\pi_b\}_{b \in \V(B)}$, Jeffrey's rule prescribes a posterior probability $p'$ of any event $E$ by:
	\[ p'(E) = \sum_{b \in B} p(E \mid B\sheq b) \pi_b \]
	Bayes Rule corresponds the particular case of Jeffrey's rule, in which the variable is binary and the outcome is certain.

	
	\begin{figure}[htb]
		\centering
%		\scalebox{0.8}{
%			\begin{tikzpicture}[center base]
%				\node[dpadded] (1) at (0,3) {$\sf 1$};
%				\node[dpadded] (W) at (0,0) {$\cal W$};
%				\node[dpadded] (B) at (-2,1) {$B$};
%				
%				\draw[arr] (1) to node[fill=white]{$p$} (W);
%				\draw[arr] (1) to node[fill=white]{$\pi$} (B);
%				
%				\draw[arr, gray] (W) to[bend left=10] (B);
%				\draw[arr, dashed] (B) to[bend right=30] (W);	
%		\end{tikzpicture}}
		\scalebox{0.8}{
		\begin{tikzpicture}[center base]
			\useasboundingbox (-3,-1) rectangle (3.5,4);
			\node[dpadded] (1) at (0,3) {$\sf 1$};
			\node[dpadded] (W) at (0,0) {$W$};
			\node[dpadded] (B) at (-2,1) {$B$};
			\node[dpadded] (E) at (2.5, 0){$E$};
			\coordinate (Q) at (6,0); % to even out controls
			
			\draw[arr] (1) to node[fill=white]{$p$} (W);
			\draw[arr] (1) to node[fill=white]{$\pi$} (B);
			
			\draw[arr, gray, ->>] (W) to[bend left=10] (B);
			\draw[arr, dashed] (B) to[bend right=30] (W);	
			
			\draw[arr, ->>] (W) to (E);
			
			\draw[arr,blue!50] (1) .. controls (-5.5,1.5) and (-2,-2) .. node[fill=white]{$p'(E)$} (E);
			\draw[arr,orange!70] (1) .. controls (0.5,1) and (1,0.5) .. node[fill=white]{$p(E)$} (E);
		\end{tikzpicture}}
		\caption{PDG Belief Updating via Inconsistency}
		\label{fig:belief-update}
	\end{figure}
	
	To understand the update visually in \Cref{fig:belief-update}, imagine the original distribution $p$ from $\sf 1$ to $W$ being replaced by the path $p' := p(W \mid B) \circ \pi$  on the left. The gray arrow on the bottom left is the definition of the random variable, as in \Cref{ex:randomvars}, and the dashed one is its inversion, which can be computed by Bayes' rule.  %that factors through $B$ via the new observation $\pi$.
	To query the resulting distribution on, an arbitrary event $E$, with an indicator variable of the same name. Initially, we got a marginal on $E$ by going through $p$; we now use $p'$. Effectively, the orange path to $E$ has been replaced by the blue one.

	
	To observe $\pi$, we simply view it as a cpd conditioned on $\star$ and add it to our collection. 
	Although it is likely to be inconsistent, resolving this inconsistency in a way that retains $\pi$ is a belief update. 
	Even failure retain $\pi$ entirely may not be a concern: so long as an they continue to observe or remember, an agent endures discomfort until $\pi$ is incorporated. This setting is arguably more natural than a standard one: without spending energy, it is easy to forget or partially reject implications of the observation.	
	Once again, with a \MN, the resolution need not happen immediately. This makes the approach more convincing for cognitively bounded agents, who might have more pressing matters than sorting through beliefs, and who might do them out of order.

	\begin{vfull}
	\section{Algorithms} 
		\label{sec:algorithms}
	\subsection{Belief Propagation}
	
	
	\subsection{Sampling}
	
	One of the nice about directed graphical models is that the model itself is roughly a sampling algorithm. For instance, taking a Bayes Net $\cal B$ and generating samples according to the tables is an efficient way to sample $\Pr_{\mathcal B}$.

	This works because there is only one path, but more generally, for any conditional marginal $Y|X$, we can think of all of the different paths in the PDG different ways an agent with knowledge $\dg M$ can get probabilistic estimates of the conditional distribution $\bbr{\dg M}\MaxEnt(Y | X)$. The next result states that, in a precise sense, these various estimates bound the location of the marginal for this maximum entropy distribution, which suggests an efficient sampling algorithm for $\bbr{\dg M}\MaxEnt(Y | X)$, after learning some weights.
	
	\begin{conj}\label{thm:maxent-hull}
		For any PDG $\dg M = \mnvars[]$ containing variables $X, Y$, the maximum entropy conditional marginal $\bbr{\dg M}\MaxEnt(y \mid x)$ is a convex mixture of the conditional marginals generated by the paths from $X$ to $Y$.  That is, there exist weights $\{\alpha_i \geq 0\}$ on the paths in $\dg M$ and a bias weight $\alpha_0$ with $\sum_i {\alpha_i} = 1$ and
		\[ \bbr{\dg M}\MaxEnt(Y \mid X) = \alpha_0 \  p^{\text{unif}}_Y \sum_{p \in \bbr{\dg M}_\lambda(X, Y)} \alpha_i (p_1 \circ \ldots \circ p_k) \]
		where $p^{\text{unif}}_Y$ is the uniform distribution on $Y$, and $\bbr{\dg M}_\lambda(X,Y)$ is the set of paths from $X$ to $Y$ generated by composition and Bayes Rule in $\dg M$. 
	\end{conj}

	One natural choice of these $\alpha$'s is the certainty scores for each edge, given by a weighted \MN, but we do not have any further formal results in this direction.
	Note that it is common for humans to make decisions in this way: to estimate whether something is realistic by following multiple chains of reasoning weighting them by strength of argument.
	
%	\begin{conj}
%		The conditional marginal of the maximum entropy distribution $\bbr{M}\MaxEnt(b \mid a)$ is in the convex hull of the compositions of paths $A \to B$. 
%	\end{conj}
	\end{vfull}
}


	\section{Discussion}
%oli8: entirely rewritten.
%joe7* It will need to be rewritten again, I'm afraid.  We should hint
%at what PDGs are good for: the fact that we can keep track of
%different, possibly conflict sources of information (we should make a
%bigger deal of that in the intro), the fact that we can capture lots
%of distributions (we hinted at that; can we say more?), the fact that
%they are modular (which lets us put together different sources of
%information) and the fact
%that we can model inconsistency and how people recover from
%inconsisency.  This will require us to get into the dynamics of PDGs
%(how people recover from inconsistency) and will require another
%paper, but we should mention it here.  A good discussion will easily
%fill up the 8th page.
%oli9: I agree, this plan sounds excellent. While you do your pass, I will write a draft of this in a separate document. 
%oli10        
\commentout{
	Given more computation, would your beliefs be more consistent? Or would you explore further, forming more extensive networks of them?
	The canonical picture of an idealized agent has always given the first answer, but this may not necessarily be the case.
}
 
 	% Do dogs have more or less consistent beliefs than we do? 
%oli8
	% They may just not have as many concepts.
	% It may be that they are no less consistent, but just have fewer concetpts.

%oli10: deleted the previous text, 
	% We have given the semantics of PDGs, a modeling framework that enables formal reasoning about this kind of mental state, and is strictly more expressive than the class of Bayesian Networks. The scoring semantics for fixed $\beta_L = \gamma = 1$ recovers the factor graph. The distribution given by a PDG, however is not generally this one, but rather one that is as consistent as possible with the supplied cpds.
	% 
	% The material covered in the present paper is only part of the picture. There are two very important generalizations that we plan to cover next. First, by minorly relaxing the definition of a cpd so that it doesn't need to provide a distribution for a special \texttt{null} value, we gain a huge amount of expressive power, allowing for simpler representations of events and partial knowledge.
 	% Second, while we have discussed what happens in the limit as $\gamma \to 0$, emphasizing the quantitative part of the PDG, there is also a rich story to be told about the qualitative half. Together with the modularity provided by the PDG, and the relaxation to strict PDGs, PDGs are able to function as causal models.
%oli10: new text.
	PDGs are a powerful tool for representing local probabilistic information.
	Though represented by a graph, the edges are interpreted differently. Each edge alone determines a cpd, making PDGs formally analogous to a commutative diagram, instead of a flow-chart. A more familiar network can be obtained with the use of multi-tailed arrows.

	PDGs have a parameterized semantics $[[ - ]]_\gamma$, which always generalizes Bayesian Networks, and precisely becomes a factor graph when $\gamma=1$.  This exposes an implicit trade-off between quantitative and qualitative data; the two behave very differently, but are unfortunately fused in a factor graph. Both qualitative and quantitative information can be inconsistent, although the former is less straightforward, this preliminary paper we focus on the quantitative limit.

	Incorporating new variables, data, or restricting to subgraphs of a PDG is simple, making it possible to construct one by simply throwing together some pre-trained statistical models. Moreover, in the quantitative limit, PDGs continue to have local meaning, in stark contrast with energy-based models such as factor graphs. As a result, PDGs are not only a flexible representation, but modular as well.

	The most dramatic feature of PDGs is their ability to deal with inconsistency. A PDG can track conflicting information from different sources, and its semantics identify when this occurs, rather than quietly sweeping problems under the rug.
	As a result, a user may resolve inconsistency in multiple ways. Inconsistency can be dealt with by updating one or multiple tables, by introducing introducing or splitting variables, or even left unresolved as as one searches for clarification. The pain of inconsistency can be mitigated by expressing a decreased confidence, without altering any data. 



	% \subsection{Probabilities Still Encode Well}	
	% In some sense, while we have yet to find a mental state that
	% is not encoded in some probability distribution, the choice
	% of underlying space is extremely important, and we argue
	% that it changes rapidly. Moreover, sometimes one has to make
	% up new internal mental variables, which also changes the
	% underlying space. PDGs offer a way to describe
	% distributions, together with a number of internal parameters
	% one might not be actively aware of. The relevant parameters,
	% can always be internalized \todo{define internalization}
	% until we reach a distribution.  
	% 
	% However, storing knowledge in the form of another graphical model is extremely cumbersome if the set of worlds changes quickly.	
	% 
	% \begin{example}
	% 	\todo{recall coin example, internalize biases, sets of dists, etc.}
	% \end{example}
	% \begin{example}
	% 	\todo{Point to appendix where we discuss factor graph conversions: these internalize the energy}
	% \end{example}
	
	



	\begin{vfull}
		\subsection{Inconsistency} \label{sec:consistency-ethos}
	
		Believing a logically inconsistent formula can lead you to arbitrarily bad conclusions, having an infeasible set of constraints makes all answers you could give wrong, and having inconsistent preferences can lose you infinite money. We don't want to build inconsistent systems or agents with incoherent views of the world, and so, where possible, we design them so they cannot possibly be broken in this way. Suppose, for example, that we are trying to represent some quantity that must be a point on the unit circle. We could do it with an $x$ and $y$ coordinate, but this could be problematic because $x^2+ y^2$ might not be 1 --- it would be safer and harder to go awry if we parameterize it by an angle $\theta \in [0, 2\pi)$ instead. In the absence of performance benefits (like needing to regularly use the $y$-coordinate and not wanting to compute a sine), why would we take the first approach, introducing a potentially complex data-invariant, when we could avoid it?
		
		This line of thought, though common and defensible, is flawed if we are not perfectly confident in the design of both our system and the ways it can interact with the outside world. Using similar logic, we might ask ourselves: Why ask programmers for type annotations when all instructions are operationally well-defined at run-time?  Why use extra training data if there's already enough there to specify a function? Why estimate a quantity in two ways when they will yield different answers? Why repeat and rephrase your ideas when this could make you contradict yourself? Why write test cases when they could fail and make the project inconsistent? Why conduct an experiment if it could just end up contradicting your current knowledge?
		
		These questions may seem silly, but there is a satisfying information theoretic answer to all of them: redundancy, though costly, is the primary tool that we use to combat the possibility of being wrong. Maintaining data invariants can be expensive but provides diagnostic information; in the example above, settings of $x$ and $y$ that don't lie on the unit circle provide diagnostic information that something has gone wrong.
		In many cases, it is also possible to paper over problems by forcibly re-instating local data invariants: for instance, we could re-normalize any values of $x$ and $y$ (so long as $xy \neq 0$; we can chose an arbitrary point otherwise) at every step. While this would reduce inconsistency, it also hides red flags.
		
		Using a Bayesian Network to represent a probability distribution is like representing a circle with $\theta \in [0, 2\pi)$.
		By construction, the result must be a distribution, and nothing can possibly go wrong so long as we can always decide on exactly one distribution which is sufficient for our purposes.
		%	By construction, the result must be a point on the circle, and nothing can possibly go wrong so long as we're sure that we will always have exactly enough information to determine such a point (for instance, we could never be totally clueless about the point, or just know its $x$ coordinate).
		
		
		The process of mechanistically forcing invariants is homologous to the standard practice for factor graphs: practitioners will often just assume that the density it defines is normalizable, and either forcibly re-normalize or cleverly avoid computing the normalization constant while still assuming that one exists; behavior is usually left unspecified in the unlikely event that it is not defined or zero.
	\end{vfull}
	
%oli8: removing
	% \section{Conclusions}
	
	% \subsection{A LIST OF PDG BENEFITS}\label{sec:list-of-benefits}
	% \todo{Remove and refactor into appendix}
	% \begin{enumerate}[nosep]
	% 	\item PDGs can represent both over-constrained and under-constrained mental states. 
	% 	\item In particular, they may be inconsistent, which gives agents using PDGs the qualitatively new kind of `epistemic modesty': the possibility of realizing that something is wrong with their beliefs.
	% 	\item Many standard algorithms, including as belief propogation, conditioning, and belief revision, can be regarded as resolution of inconsistency.
	% 	\item PDGs can emulate the functionality of other graphical graphical models.
	% 	\item PDGs are more modular, making it much less invasive to combine, reduce, or partially interpret parts of the model, compared to alternatives.
	% 	\item The modularity enables type-forming rules which can be used to implement deductive inference.
	% 	\item The many standard ways of adding and eliminating variables provides an answer to the question, ``why these possible worlds?''
	% 	\item Compared with a standard constraint satisfaction problem, individual components have of have limited impact on the semantics.
	% 	\item The class of free energies defined by PDGs is strictly more expressive than those given by alternative graphical models.
	% \end{enumerate} % trade-off: harder to analyze.
	
	
	
	\section*{Broader Impact}
	% Authors are required to include a statement of the broader impact of their work, including its ethical
	% aspects and future societal consequences. Authors should discuss both positive and negative outcomes,
	% if any. For instance, authors should discuss a) who may benefit from this research, b) who may be
	% put at disadvantage from this research, c) what are the consequences of failure of the system, and d)
	% whether the task/method leverages biases in the data. If authors believe this is not applicable to them,
	% authors can simply state this

	\begin{ack}
		% \section*{Acknowledgments and Disclosure of Funding}
	\end{ack}

%	\section*{References}
	%	\printbibliography[heading=none]
	{
		\small
		\bibliographystyle{alpha}
		% \bibliography{../refs,../uncertainty,../maths,graphical-models}

%joe7: I already saved your bib files as oliver1, 2, 3,
%                \bibliography{allrefs}
                \bibliography{oliver1,oliver2,oliver3,z,joe}        
	}
	%\addbibresource{../uncertainty.bib}
	%\addbibresource{../maths.bib}
	%\addbibresource{graphical-models.bib}
	\onecolumn
	\appendix
	
	\section{Proofs} \label{sec:proofs}
	%oli10: added this subsection and reorganized propositions / definitions accordingly.i
	\subsection{Standard Definitions and General Facts}
	For brevity, we use the standard notation and write $p(x, y)$ instead of $p(X \!=\! x, Y \!=\! y)$, $p(x \mid y)$ instead of $p(X \!=\! x\mid Y \!=\! y)$, and so forth. So long as $x$ is bound solely as an element of $\V(X)$, the meaning is unambiguous. 
	
	
	\begin{defn}[Conditional Entropy]
		If $p$ is a distribution over a set $\Omega$ of out comes, and $X$ and $Y$ are random variables on $\Omega$, then the \emph{conditional entropy}, $\H_p(X \mid Y)$, is defined as 
		\[ - \sum_{x \in \V(X), y \in \V(Y)} p(x,y) \log \frac{p(x,y)}{p(x)} \]
	\end{defn}
	
	
	\begin{defn}[Sets as Variables] \label{def:set-rv}
		Sets of random variables as random variables. If $S$ is a set of random variables $X_i : \Omega \to \V(X_i)$ on the same set of outcomes $\Omega$, we consider $S$ itself to be the random variable taking values $\V(X) = \{(x_1, \ldots, x_i \ldots) \}$ for $x_i \in \V(X_i)$. Formally, we define its value on a world $\omega$ to be $S(\omega) := (X_1(\omega), \ldots, X_i(\omega), \ldots)$. 
	\end{defn}

	%oli10: added
	\begin{defn}[Strong Convexity] \label{def:strong-convexity}
		A real-valued function is $m$-\emph{strongly convex}, if there is a quadratic lower bound, with coefficient $m$, away from its first order approximation. More precisely, it is $m$ strongly convex if for every $x, y$ in its domain, 
		\[ f(y) \geq f(x) + \Big\langle\nabla f(x), y-x \Big\rangle + m\norm{x-y}^2_2 \]
	\end{defn}


	\begin{prop}\label{prop:neg-ent-convex}
	%joe8*: you can't pull 1-strong convexity out of a hat, and define it
	%in the proof.  You need to define it, and explain why you care.  Your
	%proof also looks at hte function xlog x, so whynot state the
	%proposition in terms of that?
	%oli10: definition added above
	  Negative entropy, restricted to a finite probability
				simplex, is 1-strongly convex. 
	\end{prop}
	\begin{proof}
		%https://math.stackexchange.com/questions/3077287/how-to-show-negative-entropy-function-fx-x-logx-is-strongly-convex
		Let $X$ be a finite set; the function $f: \Delta(X) \to \mathbb R$ given by $\vec x \mapsto \sum x_i \log x_i$ is strongly convex, as 
		\begin{equation*}
			\partial_j f(\vec x) =  \partial_j\left[\sum_i x_i \log x_i \right] = 
				x_j \partial_j \big[\log x_j \big] + \log x_j = 1 + \log x_j
		\end{equation*}
		So
		\begin{align*}
			\Big\langle \nabla f(x) - \nabla f(y),~ x-y\Big\rangle 
				&= \sum_i \Big((\partial_i f)(\vec x) - (\partial_i f)(\vec y)\Big)(x_i - y_i) \\
				&= \sum_i \Big(\log x_i  - \log y_i \Big)(x_i - y_i) \\
				% &= \sum_i x_i \log x_i + y_i \log y_i + 2 
			\intertext{As $\log$ is concave, we have $\log(y_i) \leq \log(x_i) + (y_i-x_i) \frac{\mathrm d}{\mathrm d x_i} [\log(x_i)]$, and so $\log x_i - \log y_i \geq (1/x) (x - y)  \geq (x-y)$, we have}
			\Big\langle \nabla f(x) - \nabla f(y),~ x-y\Big\rangle
				&= \sum_i \Big(\log x_i  - \log y_i \Big)(x_i - y_i) \\ % from above
				&\geq \sum_i (x_i-y_i)^2 \cdot \frac1{x_i}\\
				&\geq \sum_i (x_i-y_i)^2 \\
				&= \norm{x-y}^2_2 \numberthis\label{proofeqn:strong1}
		\end{align*}
		At the same time, the condition for convexity can be phrased in terms of gradients as the condition that for all $x,y$,
		\[  \Big\langle \nabla f(x) - \nabla f(y),~ x-y\Big\rangle \geq 0\]
		So together with \eqref{proofeqn:strong1}, we conclude that the function $f - \norm{x-y}^2_2$ is convex. Therefore, $f$ is 1-strongly convex.
	\end{proof}


	\subsection{Properties of Scoring Semantics}
	\begin{vfull}
		\thmsetconvex*
		\begin{proof}
			Choose any two distributions $p, q \in \SD{M}$ consistent with $M$, any mixture coefficient $\alpha \in [0,1]$, and any edge $(A,B) \in \Ed$.
			
			By the definition of $\SD{M}$, we have $p(B = b \mid A = a) = q(B = b \mid A = a) = \bmu_{A,B}(a,b)$.  
			For brevity, we will use little letters ($a$) in place of events ($A = a$).
			Therefore, $p(a\land b) = \bmu_{A,B}(a,b) p(a)$ and $q(ab) = \bmu_{A,B}(a,b) q(a)$. Some algebra reveals:
			\begin{align*}
				\Big( \alpha p + (1-\alpha) q \Big) (B = b \mid A = a) &= 
				\frac{\Big( \alpha p + (1-\alpha) q \Big) (b \land a)}{\Big( \alpha p + (1-\alpha) q \Big) (a)} \\
				&= \frac{ \alpha p(b \land a) + (1-\alpha) q(b \land a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
				&= \frac{ \alpha \bmu_{A,B}(a,b) p(a) + (1-\alpha) \bmu_{A,B}(a,b) q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
				&=\bmu_{A,B}(a,b) \left(\frac{ \alpha  p(a) + (1-\alpha) q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)}\right)\\
				&= \bmu_{A,B}(a,b)
			\end{align*}
			and so the mixture $\Big(\alpha p + (1-\alpha) q \Big)$ is also contained in $\SD{M}$.
		\end{proof}
\end{vfull}
		
	\begin{lemma}
		% [name=\Cref{prop:convex} analog, 	restate=thmincconvex]
		\label{thm:inc-convex}
		$\Inc(\dg M, \mu)$ is a convex function of $\mu$.
	\end{lemma}
	\begin{proof}
		It is well-known that $\thickD$ is convex, in the sense that 
		\[ \kldiv{\lambda q_1 + (1-\lambda) q_2 }{ \lambda p_1 + (1-\lambda) p_2} \leq \lambda \kldiv {q_1}{ p_1} + (1-\lambda) \kldiv{q_2}{p_2} \]
		Choose any edge $L \in \Ed$ from $A$ to $B$, and also any $a \in \mathcal V(A)$. 
		Setting $q_1 = q_2 = \bp(a)$, we get
		\[ \thickD(\bp(a) \ ||\ \lambda p_1 + (1-\lambda) p_2) \leq \lambda \thickD (\bp(a) \ ||\ p_1) + (1-\lambda) \thickD(\bp(a)\ ||\ p_2) \]
		Since this is true for every $a$ and edge, we can take a weighted sum of these inequalities for each $a$ weighted by $p(A=a)$, and therefore
		\begin{align*}
			\E_{a\sim p(A=a)} \kldiv{\bp(a)}{\lambda p_1 + (1-\lambda) p_2} &\leq \E_{a\sim p(A=a)}\lambda \kldiv {\bp(a)}{p_1} + (1-\lambda) \thickD(\bp(a)\ ||\ p_2) \\
			\sum_{(A, B) \in \Ed}\mskip-10mu\E_{a\sim p_A} \kldiv{\bp(a) }{\lambda p_1 + (1-\lambda) p_2} 
				&\leq \sum_{(A, B) \in \Ed}\mskip-10mu\E_{a\sim p_A}\lambda \kldiv{\bp(a)}{p_1} + (1-\lambda) \kldiv{\bp(a)}{p_2} \\
		\intertext{and so}
			\Inc(\dg M, \lambda p_1) + (1-\lambda)p_2) &\leq \lambda \Inc(\dg M,p_1) + (1-\lambda) \Inc(\dg M,p_2)
		\end{align*}
		Therefore $\Inc(\dg M, \mu)$ is a convex function of $\mu$
	\end{proof}


	\begin{prop} \label{prop:nice-score}
		\[ \bbr{M}_\gamma(\mu)= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} + (\gamma - \beta_L ) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\}  \]
	\end{prop}
	\begin{proof}
		\begin{align*}
			\bbr{M}_\gamma(\mu) &:= \Inc(\dg M, \mu) + \gamma \Gib(\dg M, \mu) \\
				% Next, replace expressions for Inc and Extra
				&= \left[\sum\alle \beta_L \E_{x\sim \mu_X}\kldiv[\Big]{ \mu(Y | X \sheq x) }{\bp(x) } \right]  + \gamma \left[\sum\alle \H_\mu(Y\mid X) ~-\H(\mu)\right]\\
				% Combine the summations and expectations
				&= \sum\alle 
					\E_{x \sim \mu_{\!_X}}  \left[ \beta_L\; \kldiv[\Big]{ \mu(Y \mid x) }{\bp(Y \mid x) } + \gamma \; \H(Y \mid X\sheq x) \right]  - \gamma \H(\mu) \\ 
				% Now, Expand relative and conditional entropy
				&= \sum\alle 
					\E_{x \sim \mu_{\!_X}}  \left[ \beta_L\; \left(\sum_{y \in \V(Y)} \mu(y \mid x) \log\frac{\mu(y\mid x)}{\bp(y\mid x)}\right) + \gamma \; \left(\sum_{y \in \V(Y)} \mu(y\mid x) \log \frac{1}{\mu(y\mid x)} \right) \right]  - \gamma  \H(\mu) \\ 
				%combine common \sum \mu(y | x) 
				&= \sum\alle 
					\E_{x \sim \mu_{\!_X}}  \left[ \sum_{y \in \V(Y)} \mu(y \mid x) \left(  \beta_L\; \log\frac{\mu(y\mid x)}{\bp(y\mid x)} + \gamma \; \log \frac{1}{\mu(y\mid x)} \right) \right]  - \gamma  \H(\mu) \\
				% Expand entropy and reduce sum to expectation
				&= \sum\alle 
					\E_{x \sim \mu_{\!_X}}  \left[ \E_{y \sim \mu(Y \mid X=x)} \left(  \beta_L\; \log\frac{\mu(y\mid x)}{\bp(y\mid x)} + \gamma \; \log \frac{1}{\mu(y\mid x)} \right) \right]  - \gamma \sum_{\mat w \in \V(\dg M)} \mu(\mat w) \log \frac{1}{\mu(\mat w)} \\  
				% combine expectation.
				&= \sum\alle 
					\E_{x,y \sim \mu_{\!_{XY}}}  \left[ \beta_L\; \log\frac{\mu(y\mid x)}{\bp(y\mid x)} + \gamma \; \log \frac{1}{\mu(y\mid x)}  \right]  - \gamma  \E_{\mat w \sim \mu} \left[ \log \frac{1}{\mu(\mat w)}\right] \\
				% swap sum and expectation, and use log rule to split kl divergence
				&= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
					\beta_L \log \frac{1}{\bp(y\mid x)}   - \beta_L  \log \frac{1}{\mu(y \mid x)}+ \gamma \log \frac{1}{\mu(y \mid x)} \right]\Bigg\}  -  \gamma  \E_{\mat w \sim \mu} \left[\log \frac{1}{\mu(\mat w)}\right] \\
				% combine
				&=  \E_{\mat w \sim \mu} \Bigg\{ \sum_{ X \xrightarrow{\!\!L} Y  } \left[
					\beta_L \log \frac{1}{\bp(y\mid x)} + (\gamma - \beta_L ) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} 
		\end{align*}
	\end{proof}

	\begin{prop} \label{prop:convex-if-gamma-small}
		For a PDG $\dg M$, and any $\gamma$ such that $0 < \gamma \leq \min_L \beta_L^{\dg M}$, then $\bbr{\dg M}_\gamma$ is a strictly convex function of $\mu$ .%
	\end{prop}
	\begin{proof}
		We can rewrite the semantics as
		\begin{align*}
			\bbr{M}_\gamma(\mu) 
				&= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
					\beta_L \log \frac{1}{\bp(y\mid x)} + (\gamma - \beta_L ) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
				&= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[ \gamma \log \frac{1}{\bp(y\mid x)} + 
					(\beta_L - \gamma) \log \frac{1}{\bp(y\mid x)} - (\beta_L -\gamma) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\}  \\
				&= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[ \gamma \log \frac{1}{\bp(y\mid x)} + 
					(\beta_L - \gamma) \log \frac{\mu(y\mid x)}{\bp(y\mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
				&=  \sum_{ X \xrightarrow{\!\!L} Y  } \left[ \gamma \E_{x,y \sim \mu_{\!_{XY}}} \left[\log \frac{1}{\bp(y\mid x)} \right] + 
					(\beta_L - \gamma) \E_{x\sim\mu_X} \kldiv[\Big]{\mu(Y\mid x)}{\bp( x)} \right] - \gamma \H(\mu)
		\end{align*}
		The first term, 
		\( \E_{x,y \sim \mu_{\!_{XY}}} \left[-\log {\bp(y\mid x)}\right] \) 
		is linear in $\mu$, as $\bp(y\mid x)$ does not depend on $\mu$. As for the second term, it is well-known that KL divergence is convex, in the sense that 
		\[ \kldiv{\lambda q_1 + (1-\lambda) q_2 }{ \lambda p_1 + (1-\lambda) p_2} \leq \lambda \kldiv {q_1}{ p_1} + (1-\lambda) \kldiv{q_2}{p_2} \]
		Therefore, for a distribution on $Y$, setting $p_1 = p_2 = \bp(x)$, we discover that for any two conditional marginals $\mu_1(Y \mid X=x)$ and $\mu_2(Y\mid X=x)$,that
		\[ \kldiv{\lambda \mu_1(Y\mid x) + (1-\lambda) \mu_2(Y\mid x) }{ \bp(x) } \leq \lambda \kldiv {\mu_1(Y\mid x)}{\bp(x)} + (1-\lambda) \kldiv{\mu_2(Y\mid x)}{\bp(x)} \]
		So $\kldiv*{\mu(Y\mid x)}{\bp( x)}$ is convex. As convex combinations of convex functions are convex, the second term, $\E_{x\sim\mu_X}\kldiv*{\mu(Y\mid x)}{\bp( x)}$, is convex.        Finally, negative entorpy is 1-strongly convex, by (\Cref{prop:neg-ent-convex}).

		By addition and scaling of the convexity inequalities, any non-negative linear combinations of the three terms is convex, and if this combination applies a positive coefficient $\gamma$ to the negative entropy, it must be $\gamma$-strongly convex. Therefore, so long as $(\beta_L \geq \gamma)$ for every $L \in \Ed^{\dg M}$, $\bbr{\dg M}_\gamma$ is $\gamma$-strongly convex, and in particular, strictly convex.
	\end{proof}

	\begin{prop}\label{prop:lim-exist}
		The limit set
		\(\displaystyle \smash{\lim_{\gamma\to0}\argmin_{\mu \in \Delta\V(\N^{\dg M})}}  \bbr{\dg M}_\gamma\)
		is a singleton if every $\beta_L > 0$.
	\end{prop}
	\begin{proof}
		Let $\gamma_0 := \min_{L \in \Ed^{\dg M}} \beta_L$.
		By \Cref{prop:convex-if-gamma-small}, for any $\gamma < \gamma_0$, $\bbr{\dg M}_\gamma$ is a strictly convex function of $\mu$ and hence has a unique global minimum. 

		It remains to show that the function $f : (0, \gamma_0] \to \Delta(\V(\dg M))$ given by $\gamma \mapsto \argmin_{\mu} \bbr{\dg M}_\gamma(\mu)$, converges as $\gamma \to 0^+$.
		\todo{}

		
	\end{proof}

	\begin{prop}\label{prop:sd-is-zeroset}
		For any PDG $\dg M$, $\SD{\dg M} = \{ \mu : \bbr{\dg M}_0(\mu) = 0\}$.
	\end{prop}
	\begin{proof}
		 By taking $\gamma = 0$, the score is just $\Inc$. By  definition, any $\mu \in \SD{\dg M}$ satisfies all constraints, hence satisfies $\mu(Y \mid X=x) = \bp(x)$ for any $L \in \Ed^{\dg M}$ and $x$ with $\mu(X=x)>0$. By Gibbs inequality, $\kldiv{\mu(Y|x)}{\bp(x)} = 0$. Since this is true for all edges, we must have $\Inc(\dg M, \mu) = 0$. Conversely, if $\mu \notin \SD{\dg M}$, then it fails to marginalize to the cpt $\bp$ on some edge $L$, and so again by Gibbs inequality $\kldiv{\mu(Y|x)}{\bp(x)} > 0$. As relative entropy is non-negative, the sum of these terms over all edges must be positive as well, and so $\Inc(\dg M, \mu) \neq 0$. %This is true whether or not $\dg M$ is consistent.
	\end{proof}

	\begin{prop}\label{prop:consist}
		If $\dg M$ is a consistent PDG, then $\UD{\dg M} \in \SD{\dg M} = \{ \mu : \bbr{\dg M}_0(\mu) = 0 \}$
	\end{prop}
	\begin{proof}
		%define upper and lower bounds.
		\def\lb{m}
		\def\ub{M}    
		% \textbf{}Proof that when $\dg M$ is consistent, we have $\UD{\dg M} \in \SD{\dg M}$. 
		First, note that $\Gib$ is a finite sum of entropies and conditional entropies, over the variables $\N^{\dg M}$, which have finite support --- and therefore is bounded. As a result, there exist real constants $\lb$ and $\ub$ depending only on $\N^{\dg M}$ and $\V^{\dg M}$ such that $\lb \leq \Gib(\dg M, \mu) \leq \ub$ for all $\mu$.

		% Therefore, for any distribution $\mu \in \Delta(\V(\dg M))$, we have
		\begin{alignat*}{4}\relax
			&\forall\gamma,\mu.~&\gamma\lb &~\leq~& \gamma\Gib&(\dg M, \mu)  &~\leq~&  \gamma\ub \\
		% \intertext{\centering Adding $\Inc(\dg M, \mu)$ to each quantity}
		% \implies
			&\forall\gamma,\mu.~&
			\Inc(\dg M, \mu) + \gamma\lb &~\leq~& \Inc(\dg M, \mu) +& \gamma\Gib(\dg M, \mu)  &~\leq~&  \Inc(\dg M, \mu) + \gamma\ub \\
			&\forall\gamma,\mu.~&
			\Inc(\dg M, \mu) + \gamma\lb &~\leq~& \bbr{\dg M }_\gamma&(\mu)  &~\leq~&  \Inc(\dg M, \mu) + \gamma\ub \\
		\intertext{Since this holds for every $\mu$, it in particular must hold for the minimum across all $\mu$, which must be achiveved as $\Inc$ and $\Gib$ are bounded below and continuous, and $\Delta\V(\dg M)$ is compact.}
		% \implies
		&\forall\gamma.~&
			\min_{\mu \in \Delta\V(\dg M)} \Big[ \Inc(\dg M, \mu) + \gamma\lb \Big]&~\leq~& 
				\min_{\mu \in \Delta\V(\dg M)}& \bbr{\dg M }_\gamma(\mu)  &~\leq~&  
				\min_{\mu \in \Delta\V(\dg M)} \Big[ \Inc(\dg M, \mu) + \gamma\ub \Big]\\
		% \implies
		&\forall\gamma.~&
			\min_{\mu \in \Delta\V(\dg M)} \Big[ \Inc(\dg M, \mu)\Big] + \gamma\lb &~\leq~& 
				\min_{\mu \in \Delta\V(\dg M)}& \bbr{\dg M }_\gamma(\mu)  &~\leq~&  
				\min_{\mu \in \Delta\V(\dg M)} \Big[ \Inc(\dg M, \mu) \Big] + \gamma\ub\\
		% \implies
		&\forall\gamma.~&
			\Inc(\dg M) + \gamma\lb &~\leq~& 
				\min_{\mu \in \Delta\V(\dg M)}& \bbr{\dg M }_\gamma(\mu)  &~\leq~&  
				\Inc(\dg M) + \gamma\ub\\
		\intertext{Since this holds for all $\gamma$, it must hold in the limit as $\gamma \to 0$ from above.}
		% \implies
		&&
			\Inc(\dg M) + \lim_{\gamma\to 0} [\gamma\lb ]&~\leq~& 
				\lim_{\gamma\to 0}\min_{\mu } &\bbr{\dg M }_\gamma(\mu)  &~\leq~&  
				\Inc(\dg M) + \lim_{\gamma\to 0} [\gamma\ub] \\
		% \implies
		&&
			\Inc(\dg M) &~\leq~& 
				\lim_{\gamma\to 0}\min_\mu & \bbr{\dg M }_\gamma(\mu)  &~\leq~&  
				 \Inc(\dg M)\\
		\end{alignat*}
		Therefore, we must have
		\[\lim_{\gamma\to 0}\min_\mu \bbr{\dg M }_\gamma(\mu) = \Inc(\dg M) \]
		and in particular, $\lim_{\gamma\to 0}\min_\mu \bbr{\dg M }_\gamma(\mu) = 0$ when $\dg M$ is consistent, by \Cref{prop:sd-is-zeroset}. Therefore any $\mu_* \in \lim_{\gamma \to 0}\argmin_\mu \bbr{\dg M}_\gamma(\mu)$ must satisfy $\bbr{\dg M}_0(\mu_*) = 0$, and thus $\mu_* \in \SD{\dg M}$.
	\end{proof}


%oli8
% \subsection*{BNs are PDGs.}
\subsection{PDGs as BNs and \Cref{thm:bns-are-pdgs}}


% \footnote{Contrary to common assertion, this is \emph{not} an abuse of notation so long as $\mathcal V(X) \cap \mathcal V(Y) = \emptyset$, which is always possible by simply tagging values with type information, by $x \mapsto (x, X)$, for instance.}   
When we say a distribution $p$ ``satisfies the constraints given by a PDG $\dg M$'', we mean that for every edge from $X$ to $Y$ in $\dg M$, associated to the cpd $\mathbf e$, the table of conditional marginals $p(y \mid x)$ is equal to $\mathbf e$.

To prove our theorem, we now present a helper lemma, which will do most of the work. For context, skip to its usage in the proof of Theorem~\ref{thm:bns-are-pdgs}.

\begin{lemma} \label{lem:bnmaxent-component}
	If $\mu$ is a probability distribution over a set of outcomes, and $X$, $Y$, $Z$ are random variables (or sets of random variables, by Definition~\ref{def:set-rv}), then 
	\[ \tilde H_\mu(X \mid Y; Z) := \E_{y \sim \mu_{_{Y}}} \Big[ \H_\mu(X \mid Y \!=\!y) \Big]  - \H_\mu( X \mid Y, Z)\]
	is (a) non-negative, and (b) equal to zero if and only if $X$ and $Z$ are independent given $Y$.
\end{lemma}
\begin{proof}
	% We start by giving this quantity a name. Let's call it $\tilde H$.
	\begin{align*}
		\tilde H_\mu(X \mid Y; Z) &= \E_{y \sim \mu_{_{Y}}}  \Big[ \H_\mu(X \mid Y \!=\!y)\Big] - \H_\mu( X \mid Y, Z)  \\
		&=  \left[\sum_{y} \mu(y) \sum_x  \mu(x\mid y) \log \frac{1}{\mu(x \mid y)} \right]+ \left[\sum_{x,y, z} \mu(x, y, z) \log \frac{\mu(x,y,z)}{\mu(y, z)}\right] \\[0.5em]
		&= \left[\sum_{x,y} \mu(x,y) \log \frac{\mu(y)}{\mu(x,y)}
		% \cdot \left( {\color{red} \vphantom{\sum_{z}}\smash{\overbracket{\color{black} \sum_{z}~\mu(z \mid x, y)}^{=1}}}\right)
		\right] + {\left[\sum_{x,y, z} \mu(x, y, z) \log \frac{\mu(x,y,z)}{\mu(y, z)} \right]} \\
		%(below is optional)
		% &= \left[\sum_{x,y, z} \mu(x,y) \mu(z \mid x, y) \log \frac{\mu(y)}{\mu(x,y)} \right] + {\left[\sum_{x,y, z} \mu(x, y, z) \log \frac{\mu(x,y,z)}{\mu(y, z)} \right]} \\
		&= \left[\sum_{x,y, z} \mu(x,y ,z) \log \frac{\mu(y)}{\mu(x,y)}
		\right] + {\left[\sum_{x,y, z} \mu(x, y, z) \log \frac{\mu(x,y,z)}{\mu(y, z)} \right]} \\
		&= \sum_{x,y, z} \mu(x,y ,z) \left[ \log \frac{\mu(y)}{\mu(x,y)} + \log \frac{\mu(x,y,z)}{\mu(y, z)} \right] \\
		&= \sum_{x,y, z}  \mu(x,y ,z) \log \left[\frac{\mu(y)\ \mu(x,y,z)}{\mu(x,y)\ \mu(y,z)} \right]  \\
	\end{align*}
	% \intertext{
	Define $q(x,z,y) := {\mu(x,y)\ \mu(y,z) }/{\mu(y)}$, wherever $\mu(y)\neq 0$, and $\mu(x,y,z) = 0$ otherwise. $q$ is in fact a distribution over the values of $X$, $Y$, and $Z$, since it 
	is clearly non-negative, and sums to 1, as we now show:
	\[
	\sum_{x,y,z} q(x,y, z) = \sum_{x,y,z} \frac{\mu(x,y)\ \mu(y,z)}{\mu(y)}
	= \sum_{x,y,z} \mu(x \mid y) \mu(y,z)
	= \sum_{y,z} \left(\sum_x \mu(x \mid y)\right) \mu(y,z)
	= \sum_{y,z}  \mu(y,z)
	= 1
	\]	
	With this definition, we return to our computation of $\tilde H_\mu(X \mid Y; Z)$:
	% }
	\begin{align*}
		\tilde H_\mu(X \mid Y; Z) &= \sum_{x,y, z}  \mu(x,y ,z) \log \left[\frac{\mu(y)\ \mu(x,y,z)}{\mu(x,y)\ \mu(y,z)} \right]  \\ % this is a duplicate line, for readabilitz
		&= \sum_{x,y, z}  \mu(x,y ,z) \log \frac{\mu(x,y,z)}{q(x,y,z)}  \\
		&= \kldiv{\mu_{_{XYZ}}}{q}
	\end{align*}
	where $\mu_{_{XYZ}}$ is the marginal of $\mu$ on the settings of $XYZ$, and $\kldiv{\mu_{_{XYZ}}}{q}$ is the relative entropy to $\mu_{_{XYZ}}$ from $q$. By Gibbs' inequality (non-negativity of relative entropy), $\tilde H$ is  (1) non-negative, and (2) equal to zero if and only if $\mu_{_{XYZ}} = q$, meaning that 
	\[  \mu(x,y,z) =\begin{cases} \frac{\mu(x,y)\ \mu(y,z)}{\mu(y)} & \text{if }\mu(y) > 0\\ 0 & \text{otherwise} \end{cases} \qquad \implies \qquad \mu(x,y,z) \mu(y) = \mu(x,y) \mu(y, z) \] 
	and so $\tilde H_\mu(X \mid Y; Z)$ is (1) non-negative, and (2) equal to zero if and only if $X$ and $Z$ are independent given $Y$ according to $p$.
\end{proof}

		
	\thmbnsRpdgs*	
	\begin{proof}%[Proof of Theorem~\ref{thm:bns-are-pdgs}]
		% \label{proof:bns-are-pdgs}
		Choose an arbitrary distribution $\mu$ over the variables, subject to the sole constraint of being compatible with $\PDGof{\cal B}$ (which again means that each cpd in $\cal B$ must agree with the conditional marginals of $\mu$), and let $X_1, \ldots, X_n$ be any ordering of the variables in $\mathcal B$, such each node $X_i$ has parents $\Pa(X_i)$ with strictly smaller indices (we call such an ordering $\cal B$-topological). At least one $\cal B$-topological ordering is possible because the underlying graph of $\cal B$ is acyclic. 
		
		% We prepare to decompose $\H^{\PDGof{\cal B}}$ by recalling two facts. 
		We now put the following facts in equational form for ease of use:
		\begin{description}
			\item[Fact 1] (Entropy Chain Rule). using the chain rule for conditional entropy, we can write 
			\[ \H(\mu) = \sum_{i = 1}^n \H_\mu(X_i \mid X_1, \ldots X_{i-1}). \]
			%
			\item[Fact 2] (Only $\cal B$'s cpds have entropy).
			In $\cal B$, there is exactly one cpd for each node. Recall from Definition~\ref{def:bn2PDG}, that $\PDGof{\cal B}$ contains all of these cpds, and possibly also degenerate cpds $\pi_{i,j}$, which, for a setting $(y_1, y_2, \ldots, y_m)$ of the new joint variable $\Pa(X_i)$, determines a (degenerate) distribution over a particular parent $Y_j \in \Pa(X_i)$, by putting all mass on $y_j$. This cpd is equal to $\delta_{y, y_j}$, and $\H(\delta_{y, y_j}) = 0$. Although the value of $y_j$ may change between settings of $\Pa(X_i)$, in each case it is a point mass, which always has no entropy. 
			Therefore, the projeections satisfy $H(\pi_{i,j}(y)) = 0$ for any value of $y \in \V(\Pa(X_i))$, and so the only cpds which could have non-zero expected entropy are the original ones from $\cal B$. As a result, we can write the sum of expected entropies in $\PDGof{\cal B}$ for all edges can be expressed as
			\[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim p_Y}  \H (\bp ( y)) = \sum_{i=1}^n\E_{\vec y \sim p_{\Pa(X_i)}}  \H (\bp[(\Pa(X_i),X_i)] (\vec y))\]
			
			% since $\cal B$ is a BN, $\PDGof{\mathcal B}$ has $n$ cpds\footnote{exactly $n$ if no cpd is deterministic, otherwise at most $n$} whose target distributions (that is, the distribution that they give for $X_i$) could could have positive entropy, corresponding to the $n$ cpds describing the conditional probability of each variable given settings of its parents.% 
			%  	\footnote{Projections, of course, have zero entropy, and so this is true for both the hyper-graph and standard presentations of PDGs.}
			% Moreover, since $p$ is compatible with every cpd, $\bp[\Pa(X_i),X_i]$
			\item[Fact 3.] (Compatibility). Since $\mu$ is compatible with every cpd, $\bp[\Pa(X_i),X_i] = \mu(X_i \mid \Pa(X_i))$. Therefore, $\H_\mu(X_i \mid \Pa(X_i) = \vec y) $, which depends on only on the probability of $X_i$ given $\Pa(X_i)$ according to $\mu$, is equal to $\H(\bp[\Pa(X_i),X_i](\vec y))$. 
		\end{description}
		We can now calculate $\H^{\PDGof{\cal B}}$ directly.
		
		% Putting these two together, we expand the \extrainfo\ of $p$ with respect to $\PDGof{\cal B}$:
		
		\begin{align*}
			\H^{\PDGof{\mathcal B}}(\mu) &= \Bigg[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim \mu_Y}  \H (\bp (y)) \Bigg] - \H(\mu) \\
			&= {\Bigg[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim p_Y}  \H (\bp (y)) \Bigg]} - \sum_{i = 1}^n \H_\mu(X_i \mid X_1, \ldots X_{i-1}) & \text{Fact 1} \\
			&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim \mu_{\Pa(X_i)}} \H (\bp[\Pa(X_i), X_i] (\vec y)) \Bigg] { - \sum_{i = 1}^n \H_\mu(X_i \mid X_1, \ldots X_{i-1})} & \text{Fact 2} \\
			&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim \mu_{\Pa(X_i)}}  \H_\mu (X_i \mid \Pa(X_i) \!=\! \vec y) \Bigg] 
			{ - \sum_{i = 1}^n \H_\mu(X_i \mid X_1, \ldots X_{i-1})} & \text{Fact 3} \\
			&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim \mu_{\Pa(X_i)}} \H_\mu (X_i \mid \Pa(X_i) \!=\! \vec y)  - \H_\mu(X_i \mid X_1, \ldots X_{i-1}) \Bigg]  \\
			\intertext{Applying the definition in Lemma~\ref{lem:bnmaxent-component},
				with $Y := \Pa(X_i)$,~$Z := \{X_1, \ldots, X_{i-1}\} \setminus \Pa(X_i)$, and $X := X_i$}
			&= \sum_{i = 1}^n  \Bigg[ \tilde H\Big(X_i
                          ~\Big|~\Pa(X_i);~~\{X_1, \ldots, X_{i-1}\}
                          \setminus \Pa(X_i)\Big) \Bigg]
                        \numberthis\label{eqn:maxentsum} 
		\end{align*}%
		% \footnotetext{To do this, we need to think of sets of variables as variables themselves. Doing so is straightforward (the joint variable takes valeues which are tuples, with probabilities given by the joint distribution on the set of variables), but those that are worried can verify that nothing in the proof of the lemma changes by recognizing this explicitly and writing $x,y,z$ as vectors.}%
		Lemma~\ref{lem:bnmaxent-component} tells us that each individual term of the sum in \eqref{eqn:maxentsum} is non-negative, and equal to zero if and only if $X_i$ is independent of every previous (that is, $j < i$) non-parent variable $X_j$ for $j < i$, given its parents. 	
		Therefore $\H^{\PDGof{\mathcal B}}(\mu)$, is non-negative, and equal to zero if and only if \emph{every} variable is independent of all previous variables given its parents, according to $\mu$. 
		% As conditional independence is symmetric, we conclude that $\H^{\PDGof{\mathcal B}}(\mu) = 0$ iff $\mu$ causes every variable $X$ to be independent of any other $Z$ given $\Pa(X), \Pa(Y)$, which happens iff each varaible is independent of its non-descendants given its parents.
		% Here are two alternate ways of using this to conclude that if $\H^{\PDGof{\mathcal B}}(p) = 0$, then $p = \Pr_{\cal B}$.
		
		\textbf{Extending these independences to all variables.}
		% We claim that the following are equivalent:
		% \begin{enumerate}[label=(\alph*)]
		% 	\item $\H^{\PDGof{\cal B}} = 0$ \label{item:noextrainfo}
		% 	\item $X_i \CI X_j \mid \Pa(X_i)$  if $j  < i$ for some $\cal B$-topological ordering of the variables.\label{item:someorder}
		% 	\item $X_i \CI X_j \mid \Pa(X_i)$  if $j  < i$ for every $\cal B$-topological ordering of the variables.\label{item:allorders}
		% \end{enumerate}
		% We have just shown the equivalence of (\ref{item:noextrainfo}) and (\ref{item:someorder}). Now suppose 
		
		% The equivalence of \ref{item:noextrainfo} and \ref{item:someorder}
		%   easily follows, since if there were some topological sort for which the independence didn't hold, then your proof shows that $\H^{\PDGof{\cal B}}(p) \ne 0$.
		% 
		We have shown that, for any topological ordering on the variables of $\cal B$, $\H^{\PDGof{\cal B}}(\mu) = 0$ if and only if, according to $\mu$,  each $X_i \CI X_j \mid \Pa(X_i)$ for $j  < i$; we will refer to this as $(\star)$.
		
		Now, suppose $X_j$ were a non-descendent of $X_i$, with $j > i$. Because $X_j$ is not a descendent of $X_i$, we can construct a second toplogoical sort of the variables in $\cal B$, in which $\#(X_j) < \#(X_i)$, where $\#(X)$ is the index of $X$ in the new ordering. 
		We can obtain $\#$, for instance, by topologically sorting $X_j$ and its ancestors, and then adding the rest of the variables (which we call $\bf R$) in their original order. The concatination of these two is a valid topological sort because the ancestors of $X_j$ are topologicaly ordered, and the parents of each $X \in \bf R$ occur no later than before.
		
		
		With this new order, suppose that $\H^{\PDGof{\cal B}}(\mu) = 0$. By $(\star)$, since $\#(X_j) < \#(X_i)$, we know that $X_i \CI X_j \mid \Pa(X_i)$ according to $\mu$. Since this is true for an aribitrary $i$ and $j$ without changing the distribution $\mu$, we conclude that if $\H^{\PDGof{\cal B}}(\mu) = 0$, then $\mu$ makes \emph{every} variable $X_i$ independent of its non-descendents $X_j$, given its parents.
		Conversely, if every variable is independent of its non-descendents given its parents, then $\mu$ is the unique distribution determined by $\cal B$, and since each variable of $\cal B$ is independent of previous variables given the values of its parents,  we know by $(\star)$ that $\H^{\PDGof{\cal B}}(\mu) = 0$. Therefore, if $X_j$ is a non-descendent of $X_i$, 
		\[ \H^{\PDGof{\cal B}}(\mu) = 0 \qquad\iff\qquad X_i \CI X_j \mid \Pa(X_i) \] 
		% Conversely, if $\H^{\PDGof{\cal B}}(\mu) \neq 0$, then by $\star$ it cannot be the case that in some order, every variable is independent of all previous variables given its parents, and so in every order, some variable is not independent of all previous variables given its parents.  
		
		Because $\Pr_{\cal B}$ is the unique distribution that satisfies these independences, we conclude that $\H^{\PDGof{\cal B}}(\mu) = 0$ if and only if $\mu = \Pr_{\cal B}$. 	
		As $\H^{\PDGof{\cal B}}(\mu)$ is non-negative, $\Pr_{\cal B}$ is its unique minimizer. 
		
		
		% \textbf{v2. Uniqueness by strong convexity.}
		% Part (a) of Lemma~\ref{lem:bnmaxent-component} tells us that $\H^{\PDGof{\mathcal B}}$ is a sum of strongly convex functions, and hence strongly convex itself. Because the set of distributions that are compatible with $\PDGof{\cal B}$ is convex (Lemma~\ref{lem:convex}), $\H^{\PDGof{\mathcal B}}$ has a unique minimum $\mu^*$ on this set. At the same time, the distribution $\Pr_{\cal B}$ described by $\cal B$ satisfies the independences from Lemma~\ref{lem:bnmaxent-component}, so we must have $\H^{\PDGof{\mathcal B}}(\Pr_{\cal B}) = 0$, and since $\H^{\PDGof{\cal B}} \geq 0$ and  has a unique minimizer, $\Pr_{\cal B} = \mu^*$.
	\end{proof}

	\subsection{Factor Graph Proofs}
	\thmpdgisfg*
	\begin{proof}
		By \Cref{prop:nice-score},
		\[ \bbr{\dg M}_\gamma(\mu)= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} + (\gamma - \beta_L ) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\}.  \]
		Let $\{\phi_L\}_{L \in \Ed} := \Phi(\dg M)$ denote the factors of the factor graph associated with $\dg M$.
		Because we have $\gamma  = \beta_L$, the middle term cancels, leaving us with
 		\begin{align*}
		\bbr{\dg M}_\gamma(\mu) &= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} \\
			&= \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
				\gamma \log \frac{1}{\phi(x,y)}  \right] - \gamma \log \frac{1}{\mu(\mat w)} \Bigg\} 
					&\text{as $\beta_L = \gamma$}\\
			&= \gamma \E_{\mat w \sim \mu} \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
				\log \frac{1}{\phi(x,y)}  \right] -\log \frac{1}{\mu(\mat w)} \Bigg\} \\
			&= \gamma \mathcal G_{\Phi(\dg M)}
 		\end{align*}
		It immediately follows that the associated factor graph has $\bbr{\dg M}^*_1 = \{\Pr_{\Phi(\dg M)}\}$, because the free energy is clearly a constant plus the KL divergence from its associated probability distribution.
	\end{proof}
	\thmfgispdg*
	\begin{proof}
		In $\PDGof{\Phi}$, there is edge from $1 \to X_J$ for every $J \in \mathcal J$, and also edges $X_j \to X_J$ for each $X_i \in X_J$. Because the latter edges are deterministic, any distribution $\mu$ that does not satisfy them has $\bbr{\dg M}_\gamma(\mu) = \infty$. Even though $\mu$ may be technically defined on a larger space, any distribution that has a finite score must match the constraints. Moreover, every such edge has an associated conditional entropy $\H(X_j \mid X_J) = -\E_\mu\log(\mu(x_j \mid x_J)) = 0$. Therefore both per-link terms can be safely ignored for these edges.

		Let $\mat p_J$ be the joint distribution $\frac{1}{Z_J}\phi_J$ over $X_J$.

		\begin{align*}
		\bbr{\PDGof{\Phi}}_\gamma(\mu) &= \E_{ x \sim \mu} \Bigg\{   \sum_{ J \in \mathcal J } \left[
			\beta_J \log \frac{1}{ p_J(x_J) } + (\gamma - \beta_L ) \log \frac{1}{\mu(x_J)} \right] - \gamma \log \frac{1}{\mu(\mat x)} \Bigg\} \\
			&= \E_{ x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J }\left[
				\gamma \log \frac{1}{p_J(x_J)}  \right] - \gamma \log \frac{1}{\mu(\mat x)} \Bigg\} 
					&\text{as $\beta_L = \gamma$}\\
			&= \gamma \E_{x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J } \left[
				\log \frac{Z_J}{\phi_J(x_J)}  \right] -\log \frac{1}{\mu(\mat x)} \Bigg\} \\
			&= \gamma \E_{x \sim \mu} \Bigg\{  \sum_{ J \in \mathcal J } \left[
				\log \frac{1}{\phi_J(x_J)} + \log Z_J \right]  - \log \frac{1}{\mu(\mat x)} \Bigg\} \\
			&= \gamma \mathcal G_{\Phi} + \gamma \log \prod_{J} Z_J
		\end{align*}
		and so again the two functions differ only by a constant $\gamma \log \prod_{J} Z_J$.
		% We know that $ $, and so 
		% \[ Z_\Phi \Pr_\Phi = \prod_J \phi_J  = \prod_J p_J Z_J \]
	\end{proof}
\commentout{
	% \propfgpdglossless*
	\begin{proof}
	%joe4: what's a local normalization?      
	%oli5: we are required to normalize each cpd 1->X because they are
	%distributions. It's local because it's done for each cpd, and these
	%normalizations are unlikely to ultimately be compatible with the
	%joint distributions on these variables.    
		Because each local normalization results in a local joint
				distribution $\bp[J] = \frac{1}{Z_J}
	%joe4*: I'm confused.  What differs from what?  is this what you meant
	%                \phi_J$, which only differs by a multiplicative
	%               constant, their product will only differ by a
	%oli5: You're right, this was super unclear. I rewrote to clarify.
				\phi_J$ on the variables associated with $J$, and these distributions differ from the original factors $\phi_J$ by only a multiplicative 
			   constant, the product of these locally normalized factors differs from the product of the factors by only a constant, and so 
		\[ \Pr\nolimits_F(\vec x) \propto \prod_{J \in \cal J} \phi_J(\vec x) \propto \prod_{J \in \cal J} \left(\frac{\phi_J(\vec x)}{Z_J}\right) \propto \Pr_{\Phi(\PDGof{F})}(\vec x) \]
		and since the two distributions are normalized, they must be equal.
	\end{proof}
}

	\section{PDGs And The Standard Statistical Physics Analogy}
	We now explain PDG's scoring semantics in more detail, relating it to factor graph's corresponding property, its free energy.
	% and show how by coupling two information theoretic quantities to the same parameter, w 
	
	\subsection{Specifying potentials: Exponential
          Families}\label{sec:fg-expfam}

%joe6: Pointing to an example that appears 14 pages later is not
%helpful.  And since I never got the intuition of factors as relative
%likelihoods, this is not helping me at al.
	As \Cref{ex:fg-exam} illustrates, notions of relative likelihood, while in some sense correct in isolation and for the specific factor graphs which are BNs, are not a very precise way to think of factors in general. 
	For this reason, it is more standard to present them in terms of energy potentials, eliminating the illusion of local control. 
	
%joe6*: Sorry, Oliver, this is not 
	Consider only factors that are strictly positive,%
		\footnote{or equivalently, by the Hammersley-Clifford theorem, to Markov Random Fields}
	and define $ \varphi_\alpha := -\log \phi_\alpha$, which is can be thought of the additive component of the energy state of a joint setting $\vec x$ due to factor $\alpha$. 
	Low values of $\varphi_\alpha(x_\alpha)$ indicate settings judged to be low probability, or equivalently, of high energy. 
	To obtain the total energy of a point $\vec x$ we take a sum of the individual factors' energies at $\vec x$. By weighting weight each factor's energy by a positive scalar $\theta_\alpha$, which intuitively corresponds to the importance of the factor $\phi_\alpha$ in the total total energy $\sum_\alpha \theta_\alpha \varphi_\alpha$,
	%This is the total energy of a point $\vec x$; we now ask: what's the total energy of a distribution $\mu$? It will include the average energy 
	and corresponding Boltzmann distribution at inverse temperature $\gamma$:
	\[ \Pr_{\Phi, \vec\theta} (\vec x)  := \exp \left\{ -\gamma \sum_\alpha \theta_\alpha \varphi_\alpha(\vec x_\alpha)  + \ln Z_\Phi(\vec \theta, \gamma) \right\} \] 
	which is the form of an exponential family with parameters $\theta$ (in which it is standard to absorb $\gamma$ into the parameters $\theta$ for compactness).
	
	Why have we chosen this distribution as the most favorable for our reaction, instead of the one with all probability mass on the point $\vec x$ that has minimum energy? Because we imagine that there is a cost to keeping things orderly, so long as there is ambient temperature. Choosing the exponential family distribution, which is equivalent to minimizing the \emph{free energy} of the system---that is, minimizing the average energy, but also imposing an energy cost for putting too much mass in one place. It turns out that a factor graph specifies a free energy landscape
	
	\begin{align*}
		\mathcal G_\Phi(\mu) &=  \E_{\vec x \sim \mu} \left[\sum_\alpha \theta_\alpha \varphi_\alpha(\vec x) \right] - \gamma H(\mu) \\
			&= \E_{\vec x \sim \mu} \left[\sum_\alpha \theta_\alpha \log \frac{1}{\phi_\alpha(\vec x)} \right] - \gamma H(\mu)
	\end{align*}
	For comparison, here is a slightly manipulated version of the energy landscape we defined in \Cref{sec:scoring-semantics}, with an extra scalar $\lambda$ inserted in the second term, intended to lie in $[0,1]$.
	\begin{equation*}
		\bbr{\dg M}(\mu) \!=\mskip-18mu \sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \!\!\!\!\E_\mu  \Bigg[\!
			\underbrace{\beta_L \log \frac{1}{\bp(y\mid x)} \vphantom{\Bigg|}}_{\text{Average Energy (1)}}  - 
			\underbrace{\beta_L \lambda \log \frac{1}{\mu(y \mid x)}  \vphantom{\Bigg|}}_{\text{Local Uncertainty (2)}}  + 
			\underbrace{\alpha_L \gamma \frac{\bp(y \mid x)}{\mu(y \mid x)}\log \frac{1}{\bp(y\mid x)}  \vphantom{\Bigg|}}_{\text{Causal Barrier (3)}}\! \Bigg] - 
			\mskip-31mu\underbrace{\gamma \H(\mu) \vphantom{\Bigg|} }_{\text{Global Uncertainty (4)}}
	\end{equation*}

	The first term is the expected surprise information content of seing $\bp$, or the cross entropy. Optimizing this results in a maximum likelihood estimate. The second term is a regularization, which pushes each local distribution towards uncertainty. For $\lambda = 1$, the regularization is perfectly calibrated to ensure that $\bp(y \mid x)$ is the optimal value of $\mu$, yielding the expected divergence $\kldiv{\mu(y\mid x)}{\bp(y\mid x)}$, or after summing each of the edges, the inconsistency $\Inc(\dg M, \mu)$. We can also recognize (3) + (4) as the extra information.
	
	For this section, the important thing to note is that if we could set $\lambda = 0$ and $\alpha = 0$, we would have exactly the free energy landscape of the factor graph $\Phi(\dg M)$, thought of as an exponential family with parameters $\vec \theta i:= \vec \beta$. Recalling that for a PDG $\dg M$, $\Phi(\dg M)$ only has edges that originate at $\sf 1$, term (3) is constant, giving us the following theorem.
	
	\begin{theorem}
		Consider an alternate semantics $\UD{-, \alpha, \beta, \lambda}$ that allows for the setting of the parameter $\lambda$ as described above. Then for any value of $\vec\alpha$, $\Pr_{\Phi(\dg M),\theta} = \UD{\dg M, \vec{\alpha}, \vec{\theta}, \vec\lambda \!=\! \vec 0 }^{\gamma=1}$.
	\end{theorem}

	Though we find it instructive to see the version of the semantics with a locality paramter $\lambda$ included, we opt not to include it in the presentation of a weighted PDG, for reasons described in the next section.
	
	\begin{vfull}
	\section{Operations on PDGs}\label{sec:pdg-operations}
	\subsection{Graph Operations}
	To model the process of adding information to a \MN, we use a graph union. While clearest in \Cref{ex:grok-union}, we can also view adding the individual cpd $\mat r$, as a union of the original PDG with the single-cpd PDG $[F \smash{\xrightarrow{\mat r}} G]$ as we did in \Cref{ex:guns-and-floomps}; \Cref{ex:smoking} similar, but with an extra endpoint.
	
	Though it seems to be a natural construction, there is a subtlety that makes this definition non-standard: we take the \emph{ordinary} union of the nodes, but the \emph{disjoint} union of the edges. We need an ordinary union of the vertices so that we can glue the two models together in the right places, but we need the disjoint union of the edges, because if two PDGs share an edge, the tables may not match and the only clear thing to do is to keep both, as we do in \Cref{ex:grok-union}. 
	We now define the graph union formally. 

	\begin{defn}[union] \label{def:model-union}
		If $\dg M, \dg M'$ are \MN s such that $\V^\dg M(N) = \V^{\dg M'}(N)$ for every $N \in  \N^{\dg M} \cap \N^{\dg M'}$, then $\dg M \cup \dg M'$ is a PDG with the ordinary union of their nodes (necessary to align and glue PDGs together), and \emph{disjoint union} of their edges. \notation{Explicitly,
		\begin{align*}
			\N^{\dg M \cup \dg M'} &= \N^\dg M \cup \N^{\dg M'},  \\
			\Ed^{\dg M \cup \dg M'} \!=& \Ed^\dg M \sqcup \Ed^{\dg M'}\!
				=  \{ (A, B, \text{inl}(\ell)P) : (A,B,\ell)\in \Ed^\dg M \}  \\
					&\qquad\qquad \cup \{ (A, B, \text{inr}(\ell)) : (A,B,\ell)\in \Ed^{\dg M'} \} \\ 
			\V^{\dg M \cup \dg M'} (N) &= \begin{cases}
					\V^{\dg M}(N) & \text{if }N \in \N^\dg M \\
					\V^{\dg M'}(N) &\text{if }N \in \N^{\dg M'} 
				\end{cases}\\
			\bmu^{\dg M \cup \dg M'}_L &= \begin{cases}
				\bmu^{\dg M}_{A, B, \ell} &\text{if } L = (A, B, \text{inl} (\ell)) \\
				\bmu^{\dg M'}_{A, B, \ell} &\text{if } L = (A, B, \text{inr} (\ell)) 
			\end{cases}
		\end{align*}}
	\end{defn}
	The condition that $\V^\dg M$ and $\V^{\dg M'}$ agree on the shared variables is necessary for $\V^{\dg M\cup \dg M'}$ or $\bmu^{\dg M \cup \dg M'}$ to be well-defined.%
		%oli4: this is commented out, don't worry.
		\vfullfootnote{For those familiar with manifolds, it is analogous to a gluing condition for an atlas of charts}
	The restriction from \Cref{ex:grok-ablate} is more straightforward.%
	%oli4: how much story, vs terseness?
	%, but we provide it for completeness.
	
	\begin{defn}[restriction]\label{def:restriction}
		The \emph{restriction} of $\dg M = \mnvars[]$ to a subgraph $(\N' \subseteq \N, \Ed' \subseteq \Ed)$ of $(\N, \Ed)$, is the PDG, $\dg M|_{\N', \Ed'} = (\N', \Ed', \V |_{\N'}, \bmu|_\Ed')$, where 
		$\V|_{\N'}$ and $\bmu|_\Ed'$ are the same functions on the their respectively smaller domains $\N$ and $\Ed$. 
	\end{defn}
	


	%oli2: this first sentence I believe to be overkill, but I'm including it because I'm now trying really hard to claim that I've motivated the graph union.
%joe3: ``enjoying modularity'' seems like strange wording to me.  What
%we've said, in any case, is that PDGs are more modular than other
%approaches. 
%	We have said repeatedly that PDGs enjoy modularity, and seen
%oli3:
	We have seen
        in \cref{ex:guns-and-floomps,ex:grok-union,ex:smoking} cases in
        which capturing the relevant information involves taking a
%joe3*: as I said, you've never talked about these examples in terms of
%union.  I think that there may be a useful discussion to be had about
%how modularity corresponds to union, and I understand that once you
%have union, youll want multigraphs.  This isn't going to make it to
%the abstract, and there's no question that this is the wrong place
%for it.  I could imagine a section where you talk about modularity
%and union, say that PDGs make sense even if they are multigraphs adn
%prove the theorem.
%oli3: that's the plan now. BUt it's not that it even makes sense, so
% much as that it _only_ makes sense with multi-graphs.
        union of two graphs, some of which may include new
        concepts. We wish to verify that our semantics are
        well-behaved with respect to this composition.	  
	We therefore ask: what happens if we combine two PDGs $\dg M$
        and $\dg M'$ together? Intuitively, the set of distributions
        $\SD{\dg M \cup \dg M'}$ consistent with the combined
        constraints $\dg M\cup \dg M'$ should be the intersection of the
        distributions $\SD{\dg M} \cap \SD{\dg M'}$ consistent
        with each PDG separately. This is almost correct, but $\dg M$
        and $\dg M'$ may be over different set of variables, in which
        case the sets of distributions are automatically disjoint, as
        they are over different sets of possible worlds. To address
        this, we define a more sophisticated intersection of
        distributions that must agree on all overlapping
        marginals. %(\cref{def:marginal-dist-intersection}) 
	
	\begin{defn}[$\dcap$]\label{def:marginal-dist-intersection}
		If $R$ and $S$ are sets of distributions, $R \subseteq \Delta X$ over the set $X$ and $S\subseteq \Delta Y$ over the set $Y$, then
%oli: remove the coment below to hide the notation.
%		\notation[$R \dcap S$~]
			{$$R \dcap S := \Big\{ \mu \in  \Delta [X \!\times\! Y] ~\Big|~ (\mu_{X}, \mu_{Y}) \in R \times S \Big\}  $$}%
		is the set of distributions over joint settings of $X$ and $Y$, whose marginals $\mu_X$ and $\mu_Y$ are each compatible with some distribution in $R$ and $S$ respectively. 
		
		This it the natural extension of an intersection to distributions on different, possibly overlapping sets --- in particular, if $X = Y$, then $R \dcap S$ = $R \cap S$ and if \notation[$X$ and $Y$ are disjoint]{$X \cap Y = \varnothing$}, then $R \dcap S = R \times S$. 
	\end{defn}
	
	
%	It is now natural to ask: how does this semantics interact with the PDG union (\Cref{def:model-union})? 	
	Now that we have the correct definition, we immediately get our desired property:
	
	\begin{prop}\label{prop:union-set-semantics}
		$\SD{M \cup M'} = \SD{M} \dcap \SD{M'}$.
	\end{prop}

	\Cref{prop:union-set-semantics} can be interpreted as a statement of modularity: we can straightforwardly get the semantics for a combined diagram based only on its counterparts. 
	From the two special cases of $\dcap$ discussed above, one can see that adding new edges, (which we will see correspond to observations in \Cref{sec:belief-update}), cuts down the set of possible distributions, just like conditioning, and adding new variables to a consistent model freely increases the number of valid distributions like one would expect. We would like to emphasize that all of this is done through a by combining \MNs.
	
	\begin{example}\label{ex:sd-compose-unconditional}
		Suppose we now have two PDGs with only one edge apiece, $\dg A = {\var 1} \xrightarrow{p} X$ and $\dg B = X \xrightarrow{q} Y$. We would hope that the semantics treat this like composition: that the unconditional distribution on $X$ provided by $p$ would be `plugged in' to the conditional distribution $q(y \mid x)$; indeed, this is what happens:
		%
		\begin{align*}
			&\SD[\Big]{{\dg A \cup \dg B}} = \SD[\Big]{{\var 1} \xrightarrow{p} X \xrightarrow{q} Y} \\
				&= \Big\{  \mu \in \Delta(\V(X) \times \V(Y)) : \mu_X = p,~\mu_{Y|X} = q \Big\} 
		\end{align*}
		where $\mu_X$ is the marginal of $\mu$ on $X$, and $\mu_{Y|X}$ is the cpd of conditional marginals on $Y$ for each setting of $X$.
		For any choice of $p$ and $q$ there is exactly one such distribution, given by $\mu(x,y) = p(x) q(y \mid x)$.
	\end{example}


	% we can motivate composition here!
	\begin{example}[composition]
		Consider a slight alteration of \Cref{ex:sd-compose-unconditional} in which $\sf A$, which had an unconditional distribution on $X$, is replaced with $\dg A' := Z \xrightarrow{p'} X$, representing a distribution conditioned on $Z$. 
		As before,
		\[ \SD[\Big]{{\dg A' \cup \dg B}} = \SD[\Big]{{Z} \xrightarrow{p} X \xrightarrow{q} Y} \]
		Suppose we are interested in the conditional marginal of $Y$ given $Z$. In this case, $\SD{{Z} \xrightarrow{p} X \xrightarrow{q} Y} $ contains distributions $\mu$ with varying of $\mu(y \mid z)$, and so we can no longer conclude anything uniformly about this conditional marginal for all distributions in $\SD{{\dg A' \cup \dg B}}$. 
		
		Still, we can get an estimate of this quantity using the maximum entropy semantics, which conveniently turns out to be the composition of $p$ and $q$ as probabilistic functions.
		$$ \bbr{{\sf A \cup B}}\MaxEnt(y \mid z) = \sum_{x \in \V(X)}\!\! p (x \mid z)\ q(y \mid x) = q \circ p $$
		We claim more is true: if $\dg M$ is \emph{any} PDG that is not over-constrained, and with $\dg M \supseteq \sf A \cup B$, i.e., containing $\sf A \cup B$ as a sub-graph, then
		$ \bbr{\dg M}\MaxEnt(y \mid z) = q \circ p$,
		suggesting that this composition is in some sense the best guess we have for the conditional marginal. 
%		This can be verified directly, but we will instead prove the more general result in our next result (\Cref{thm:maxent-hull}.).
		%
	\end{example}

	%This will be interesting to explore in the full paper, but it's definitely not high priority here.
	\begin{vleftovers}
	If the intersection of two sets is convex, then 
	\begin{conj}\label{prop:intersect-set-semantics}
		\[ \SD{M \cap M'} = \text{ConvHull}(\SD{M} \mathop{\dot\cup} \SD{M'}).\]
	\end{conj}
	\end{vleftovers}
\end{vfull}
	
	\section{Thermodynamics of PDGs}\label{sec:thermo}
	
	\begin{figure}[htb]
		\centering
		\scalebox{0.9}{
		\begin{tikzpicture}
			%TODO left hand side of diagram, with worlds and mean parameters
			\node[ellipse,draw, outer sep=4pt] (DW) at (0,0) {$\Delta W$};
			\node[ellipse,draw, outer sep=4pt] (EW) at (0,2.4) {$\text{Energy}^W$};
			\node[ellipse,draw, outer sep=4pt] (DDW) at (4,0) {$\Delta (\Delta W)$};
			\node[ellipse,draw, outer sep=4pt] (EDW) at (4,2.4) {$\text{Energy}^{\Delta W}$};
			
			\node[right=0.5em of EDW, blue] {$\mathcal U_\alpha(\dg M, \cdot)$};
			\node[right=0.8em of DDW, blue] {$\bbr{\dg M}_{\alpha,\beta}$};
			\node[left=0.8em of DW, blue] {$\mu$};
			\node[left=0.5em of EW, blue] {$\log\frac{1}{\mu}$};
			
			\draw[->, transform canvas={xshift=3pt}] (DW) -- node[right]{$E_\beta$} (EW);
			\draw[->, transform canvas={xshift=-3pt}] (EW) -- node[left]{$P_\beta$} (DW);
			
			\draw[->, transform canvas={xshift=-3pt}] (DDW) -- node[left]{$E_\beta$} (EDW);
			\draw[->, dashed, transform canvas={xshift=3pt}] (EDW) -- node[right]{$P_\beta$} (DDW);
			
			\draw[->] (DW) to[bend left=10] node[sloped,fill=white]{$\thickD({-\Vert~})$} (EDW);
			
			\draw[->] (EW) to[bend left=15] node[above] {$\E^*$} (EDW);
			\draw[->] (EDW) to node[fill=white] {$\E$} (EW);

			\draw[->] (DDW) to node[below] {$\E$} (DW);
		\end{tikzpicture}}
		\caption{Energy / Distribution Transformations. 
			%The nodes are thermodynamic objects, the arrows are ways of constructing one from another
		}
		\label{fig:energies-and-dists}
	\end{figure}
	We now look at the weighted distribution semantics of PDGs from a thermodynamic perspective: this will provide better rationale for the parameter choices in \Cref{sec:scoring-semantics}, and draw some more explicit contrasts between PDGs and factor graphs.	Let $W$ be finite set of worlds, on which the distribution is supported, corresponding to a particle's possible ``micro-states''
	
	Our technical starting point will be the Boltzmann distribution \eqref{eq:boltzmann}, which asserts that the probability $P$ of being in a state exponentially decreases as its energy $U$ increases; the rate of exponential decay is related to the ``inverse temperature'', $\beta$; here $Z_U(\beta)$ is a normalization constant. Fixing $\beta$, we can of course, invert the Boltzmann distribution \eqref{eq:invbolz}, obtaining an energy from a probability. A probability distribution over $W$ is called a configuration, or macro-state.
 	\begin{align}
	 P_{\beta}(U) &:= w \mapsto  \frac{1}{Z_U(\beta)}\exp\Big(-\beta U(w)\Big) \label{eq:boltzmann} \\
 		E_{\beta}(\mu) &:= w \mapsto \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right) \label{eq:invbolz}
 	\end{align}
 	Conversions between the two correspond to going up and down on the left of \Cref{fig:energies-and-dists}. 
 	Now $\mathcal U$, as defined in \eqref{eqn:full-score} is an un-normalized badness score, making it like an energy; $W^k_\gamma(\dg M, \mu)$, is a strangely-normalized Boltzmann distribution for this energy. The parameter $\beta$, which we described earlier as a certainty, plays the physical role of an inverse temperature: lower is more chaotic. 
 	
 	$\mathcal U$ is not just an arbitrary construction either: it is analogous to a free energy. Why is the most favorable configuration not just a point mass as the minimum energy? Because in a world where an ambient temperature makes things more diffuse, doing things would require a lot more energy. Rather than just minimizing the average energy of a configuration $\nu$, you're better off minimizing the Gibbs free energy \eqref{eqn:gibbs-free-energy}. 
 	\begin{equation}
 		G_E(\nu) = {\E}_\nu( E )  - T \H(\nu) \label{eqn:gibbs-free-energy}
 	\end{equation}
 	Analogously, why not put all of your weight on the one distribution you think is most likely? Because in a slightly chaotic world, doing so could actually incur a lot more inconsistency. Instead, we're better off minimizing $\cal U$. $\alpha$ is more transparently a temperature here, with higher values indicating higher preparedness for background chaos. 
 	% The higher order expectation we take in \eqref{eqn:higher-expectation} corresponds to the bottom edge of \Cref{fig:energies-and-dists}, and the diagonal, which is the natural way to construct free energies from a distribution, is a KL divergence. This can be seen directly, as well, in \Cref{ex:energy-from-distrib}.
 	%
	See \ref{sec:thermo-background}, and \cite{bethe,friston2009free} for more comprehensive background on free energy
%oli8
	in graphical models.
	%and \cite{} for weighted probability distributions.
	
	A very weak version of this can already be seen in un-normalized factor graphs: by multiplying a factor $\phi$ by a constant $\alpha$, one obtains a free energy $G' = - \ln \alpha + G$, i.e., with a mere additive shift. However, this shift doesn't really distinguish belief states, which is part of why we're so eager to normalize the distribution.
	There is also an opportunity to modify $\beta$, but in standard graphical model literature, people set $\beta = 1$ and forget about it.%
		\footnote{A similar complaint, is lodged in \cite{fixing-broken-elbo}, in which many information theoretic trade-offs are hidden by assuming $\beta = 1$}


	\begin{examplex}%[continues=ex:worldsonly]
		\label{ex:energy-from-distrib}
		For the PDG $\dg M$ that encodes just a probability distribution $\mu$ over $W$,  $\Inc(\dg M, \nu) = \kldiv{\nu}{\mu}$. This quantity is also equal to $\mathcal G_{E(\mu)}$, the Gibbs free energy for the potential landscape associated to $\mu$ at temperature $\beta = 1$.
	\end{examplex}


	%oli8: modifications for correctness and to preserve references
	%	A priori, \Cref{thm:free-energy-strictly-more-expressive} might be thought of as merely a novel function we came up with, but in fact this is not the case--- when the PDG is a Bayesian network, this is just the normal Gibbs free energy.
	When the PDG corresponds to a Bayesian network, this is just variational Gibbs free energy of a distribution in the energy well constructed by the distribution specified by the BN.

	\begin{prop}\label{prop:bn-free-energy}
		For any Bayesian Network $B$, 
		\[ \bbr{\PDGof{B}} = D(- || \Pr\nolimits_B) = \mathcal G_{E(\Pr_B)} \]
	\end{prop}
	
	By playing with thermodynamic parameters, the weighted distribution semantics coincide with the notions of free energy on standard graphical models; we therefore can view PDGs as implicitly providing a more expressive class of free energies, corresponding to weighted distributions, which in turn can be naturally adapted to be distributions themselves.
	
%	\begin{prop}
%		The Bethe free energy is equivalent to the Gibbs free energy of $M$ iff $M$ is strongly consistent.
%	\end{prop}


%	\begin{conj}\label{thm:free-energy-strictly-more-expressive}
%		The weighted distributions generated by PDGs are strictly more expressive than those generated by BNs, Factor Graphs, or directed factor graphs.
%	\end{conj}
%	\begin{proof}
%		The first two parts come from \Cref{thm:fg-free-energy,prop:bn-free-energy}. Since 
%	\end{proof}
%	\begin{coro}
%		Local minima of the Bethe free energy are fixed points of loopy belief propagation in \MNs		
%	\end{coro}


%joe7*: all the material after this will be cut, so you can focus
%(after we clean up the main part of the paper) on making the earlier
        %pat of the appendix comprehensible
%oli9: uncommenting so I can read this material, and reorganize slightly --- we'll need to shuffle around and cut a lot of appendix things in a bit but some of the good stuff is at the end.
% \commentout
{        
	\section{Alternate Presentations}
	%I think we finally covered this.
	\commentout{\subsection{Random Variables}
	If $\mathcal W = (W, \mathcal F, \mu)$ is a measure space, and $\mathcal X = \{ X_i: W \to \mathcal V(X_i) \}_{i \in I} $ is a collection of measurable random variables on $W$,\footnote{that is: $\mathcal V(X_i)$ is a measurable space, taking the form $(D, \mathcal D)$, and $X_i : W \to D$ is a set function such that for every $B \in \mathcal D$, the set $X_i^{-1}(B) \in \mathcal F$} and 
	{\color{gray}$\Ed \subseteq I \times I$ is a collection of pairs of variables such that the agent is prepared to give  } 
	\todo{what is a way of phrasing this that doesn't sound like it's shoehorned in? $\Ed$ really can represent anything an agent knows. Any subjective conditional probability distribution $\mu'$ such that the only measurable subsets are ``axis aligned'', in that they involve queries on only one variable, can be represented by $\Ed$, and for other queries we can simply change variables.}, we call $(\cal W, X)$ an \emph{ensemble}.
	%and $(W, \mathcal F', p)$ is a subjective probability representing an agent's belief 
	
	
	\begin{prop}
		There is a natural correspondence between strict PDGs as defined in \Cref{def:model}, and ensembles such that \todo{spell this out explicitly to avoid vague categorical intuition} \ldots $\mu$'s are defined on same set and produce same values.
	\end{prop}
	\begin{proof}
		\textit{/outline:}
		On the one hand, $(\prod_{N \in \cal N} \mathcal V(N).\text{set}, \bigotimes_{N \in \cal N} \mathcal V(N).\text{algebra}, \bmu)$ is a measure space, with $\{X_N = \pi_N : \left(\prod\mathcal V(N')\right) \to  \mathcal V(N) \}_{N \in \cal N}$ a set of random variables
		
		and  on the other, $(I, \Ed, \mathcal X', \mu|_{\cal L})$ is a strict \MN.
	\end{proof}
	
	This is the technical underpinning of our flippant, noncommittal treatment of possible worlds: any time we are thinking in terms of random variables or probability distributions on a fixed set $W$, we can instead reduce
	
	
	The complexity of the representation is $O(XV + L V^2)$, compared to $O(XW)$}
	
	\subsection{Hyper Graph Conversion}\label{sec:hyper-convert}
	We have mentioned that the direct definition in terms of hyper-edges is possible; we give it below.
	
	\begin{defn}[\MNH]\label{def:hypermodel}
		A \emph{\modelnamehyper} is a tuple $\mnvars[]$ where
		\begin{itemize}[nosep]
			\item $\N$~~is a finite collection of nodes
			\item $\Ed \subseteq 2^{\N} \times 2^{\N} \times \mathrm{Label}$~~is a set of directed edges, each of which has a source and target subset of $\N$.
			\item $\V$ associates each node $N \in \mathcal N$ with a set $\V(N)$ or $\V_N$, representing the values that node $N$ can take.
			\item $\bmu$
			 % $\colon\!\big(\!({\bf A,B})\colon \! \Ed \big) \to \prod\limits_{A\in \bf A} \!\! \V(A) \to \underline\Delta\left[\prod\limits_{B \in \bf B}\!\!\V(B)\right]$
			%%% Above is the type of $\bmu$. I think it's important to have it there.
			associates conditional probability (sub)-distributions on the joint settings of $\bf B$ indexed by the joint settings of variables in $\bf A$ for every edge $({\bf A,B}) \in \Ed$. %
			% \note{The type of $\bmu$ is $\big(\!({\bf A,B})\colon \! \Ed \big) \to \V(A) \to \underline\Delta\V(B)$. It doesn't take up much space and answers lots of questions about the words above.}
		\end{itemize}
	\end{defn}
	
	Next, we can formaly show how one might convert between the two representations.
		
	\begin{example}%[continues=ex:planet]
		%oli8: updated to fix references.
		We consider an example, isomorphic to \ref{ex:grok-ablate}, in which we have the conditional probability of life ($L$) existing on a planet given its size ($S$) and composition ($C$), in separately 
		While we would like to maintain the intuition of a directed hyper edge, the way to do this is using only graphs with edges, as in \Cref{def:model}, is to use intermediate variables:
		
		\begin{center}
			\begin{tikzpicture}
				\node[dpadded] (S) at (-0.5, 2) {$S$};
				\node[dpadded] (C) at (3.1, 2) {$C$};
				\node[dpadded] (L) at (1.3,0) {$L$};
				\node[dpadded] (W) at (-2,0) {$W$};
				
				\node[dpadded,light pad] (SC) at (1.3, 1.4){$S \times C$};
				
				\draw[arr, ->>] (SC) -- (S);
				\draw[arr, ->>] (SC) -- (C);
				\draw[arr] (SC) -- (L);
				\draw[arr] (W) -- (L);
			\end{tikzpicture}
		\end{center}
		We will sometimes use double headed arrows like this to emphasize degenerate conditional distributions, which are deterministic.
		We can now present this PDG formally with the elements specified in definition \ref{def:model}.
		
		\hfill\begin{minipage}{0.4\textwidth}\small
			\begin{align*}
				&\mathcal N = \{S,~ C, ~L, ~W, ~S\times C \} \\
				&\Ed = \{ (S \times C, L), (W, L), (S\times C, S), (S\times C, C)\} \\
				\mathcal V &\left\{\begin{aligned}
					\mathcal V(S) &= \{\mathit{b}, \mathit{s} \}\\
					\mathcal V(C) &= \{ \mathit{r}, \mathit{g} \} \\
					\mathcal V(L) &=  \{ l, \lnot l \} \\
					\mathcal V(W) &= \{ \textit{none}, \textit{some}, \textit{mostly}\}\\
					\mathcal V(S \times C) &= \mathcal V(S) \times \mathcal V(C) 
					% = \small\text{$\{(big, rocky), (small,rocky), (big, gasseous), (small,gasseous)\}$}
				\end{aligned}\right.\\
			\end{align*}
		\end{minipage}%
		\begin{minipage}{0.5\textwidth}\small
			\begin{align*}
				\bmu & \left\{~\begin{aligned}
					\boldsymbol\mu[S\times&C, L] = &\boldsymbol\mu[S\times&C, S] = \\[-0.6em]
					&\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{$l$,$\lnot l$}
						.1 & .9 \\
						.2 & .8 \\
						.05 & 0.95 \\
						0.001 & 0.999
					\end{idxmat} 
					&&
					\quad\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{s,b}
						0 & 1 \\
						1 & 0 \\
						0 & 1 \\
						1 & 0
					\end{idxmat}
					\\[0.5em]
					\boldsymbol\mu[W, &L] =  &\boldsymbol\mu[S\times&C, C] =\\[-0.6em]
					&\begin{idxmat}{{none}, {some}, {mostly}}{$l$,$\bar l$}
						0 & 1 \\
						.005 & .995 \\
						.05 & 0.95 \\
					\end{idxmat}
					&&
					\quad\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{r,g}
						1 & 0 \\
						1 & 0\\
						0 & 1 \\
						0 & 1 
					\end{idxmat}
				\end{aligned}\right.\\[-1em]
			\end{align*}
		\end{minipage}
		\vspace{0.5em}
		
		This works, but the structural overhead of the additional de-sugaring: the $\boldsymbol\mu[S\times C\to S]$ and $\boldsymbol\mu[S\times C\to C]$ tables, as well as the set $\mathcal V(S \times C)$ seem like they didn't need to be specified, and one might even feel that it would be a mistake to allow any other table. Some reasons for this design decision include:
		\begin{itemize}[nosep]
			\item It is easier to prove things about graphs than directed hyper-graphs. Similarly, defining composition and paths becomes a lot simpler.
			\item %We can eliminate the clunkiness by fusing the model with an algebra, as in \Cref{sec:algebra}
				This clunkiness can be eliminated by fusing the model with an algebra so that this kind of recombination can be done automatically, as done in the codebase.
			 --- which will give us a lot more than modeling the hyperedges directly.
			\item We will eventually also want to allow for the possibility of keeping only a relaxed, approximate representation of $\mathcal V$ and $\bmu$, and in particular, of the ones constructed logically in this way.
%			By specifying them explicitly for now, we will have to do less work to regain manual control in \Cref{sec:abstraction}
		\end{itemize}
	\end{example}
	
	
	The choice to formalize PDGs this way is a design consideration that makes some things cleaner, but we can just as well formalize multi-tailed edges directly, as follows:
	
	\begin{defn}[\MNH]\label{def:modelhyper}
		A \textit{\modelnamehyper} (\MNH) is tuple $(\N, \boldsymbol\Ed, \V, \bmu)$ where $\N$ and $\V$ are as before, $\boldsymbol\Ed \subseteq 2^\N \times 2^\N \times \mathrm{Label}$ is a set of `hyperedges', i.e., edges whose source and target are sets of nodes, and for each edge $L = ({\bf A, B}, \ell) \in \boldsymbol\Ed$, we have a table of distributions $\bp$ on \emph{joint settings} of the variables in the set $\bf B$ for each joint setting of the variables in $\bf A$.
	\end{defn}
	
	\Cref{thm:hyperequiv} shows PDGs and \MNH s to be equivalent, though in different cases one may seem more natural than the other, as illustrated in the following theorem.
	
	\begin{theorem}[restate=thmhyperequiv]\label{thm:hyperequiv}
		Every \MNH\ $H$ is equivalent to a PDG $\dg M$ with additional variables. That is, for each semantics $\bbr{-}$ we define, $\bbr{H} = \bbr{\dg M}$.
	\end{theorem}
	\begin{proof}
		\todo{}
	\end{proof}
	
	This theorem justifies taking the PDG as primary, an ordinary collection of nodes and edges, which makes it cleaner to define and compose paths. 

	
	\section{Formalism for other Graphical Models}
	\begin{defn}
		A Baysian network (BN) is a tuple
		\[
		\mathcal B = \left(\mathcal N : \mathbf{FinSet}, ~~\mathrm{Par}: \mathcal N \to 2^{\mathcal N},~~ \mathcal S: \mathcal N \to \mathbf{FinSet},~~\Pr: \prod_{N : \mathcal N}  \left[ \mathcal S_N \times \left(\prod_{P : \mathrm{Par}(N)} \mathcal S_P\right)  \to [0,1] \right] \right)
		\]
		such that
		\begin{itemize}[nosep]
			\item the graph $\bigcup_{N, P \in \mathrm{Par}(N)}(N, P)$ is acyclic, i.e., there exists no cycle of nodes $N_0, N_1, \cdots, N_k = N_0$ in $\mathcal N^k$ such that $N_{i+1} \in \mathrm{Par}(N_i)$ for each $i \in \{0, 1, \cdots, k\}$.
			\item For all $N \in \mathcal N$, $\Pr(N)$ is a probability distribution on $\mathcal S_N$, i.e., 
			\[ \forall N\in \mathcal N.~\forall \vec{p} \in {\prod_{P : \mathrm{Par}(N)} \mathcal S_P}.~~ \sum_{n \in \mathcal S_{N}} \Pr_N(\vec{p}, n) = 1\]
		\end{itemize}
	\end{defn}
	
	
	\begin{defn} \label{def:bnconvert-formal}
		If $B = (\mathcal N, \mathrm{Par}, \mathcal S, \Pr)$ is a Bayesian Network, then let $\PDGof (B)$ denote the corresponding PDG given by the procedure in \Cref{sec:bn-convert}. Explicitly, 
		\[ \PDGof{{\mathcal B}} :=  (\mathcal N', \Ed, \mathcal V,
                \bmu) \] 
		where % $\mathcal N'$ is the original nodes, plus
		\begin{align*}
		\mathcal N' &=  \left\{ \Big.\{N\} \mid N \in \mathcal N\right\} \cup \left\{ \mathrm{Par}(N) ~\middle|~ N \in \cal N \right\} \\%
		\Ed &= \left\{ \vphantom{\Big|}(\mathrm{Par}(N), \{N\}) \mid N \in \mathcal N \right\} \cup 
		\left\{\vphantom{\Big|} (P, \{X\}) \mid X \in P, P = \mathrm{Par}(N) \text{ for some }N \in \mathcal N \right\} \\
		\mathcal V_N &= \prod_{X \in N} \mathcal S_X \\
		%					{\color{gray}\Sigma_N = \bigotimes_{X \in N} 2^{\mathcal S_X}, \text{the product algebra of discrete $\sigma$-algebras}} \\
		\mathbf p &= \begin{cases}
		(\mathrm{Par}(N), \{N\}) &\mapsto \lambda(p, B).~ \displaystyle\sum_{b \in  B} \Pr(b \mid p) \\
		(P, X) &\mapsto, \lambda (p, B).~ \displaystyle \mathbbm 1_{\displaystyle\pi_X(p) \in B}
		\end{cases}
		\end{align*}
		%\cpm p(\frac{a}{z}|b)
	\end{defn}
	All we've done is explicitly add parent nodes and projection edges to our graph, and also subtly (by adding curly braces in the right places and taking unions rather than disjoint unions) eliminated the duplicate nodes arising from edges in the original BN which only have a single parent.
	
	\section{Thermodynamics}\label{sec:thermo-background}
	Let $W$ be a finite set of states.
	
	\textbf{From Potentials to Distributions.}
	Suppose $U: W \to \mathbb R$ is a potential function, assigning an energy to each state. Imagine there's a particle that could be in any number of states, that the only consideration in transitioning from one state $w$ to another $w'$ is the energy of each state,%
		\footnote{The thermodynamics, of course, ignore the kinetics of the system. Thought of an Ising model, the edges form a complete graph, and the edge weights are uniform. Thought of as a stochastic matrix, it is rank one, whose latent variable is just the energy of a state.}
	and that low-energy states are more exponentially more likely,\footnote{this can also be replaced by weaker assumptions; see the thermodynamics literature for more motivation}
	the unique stationary state is the Boltzmann distribution:
	\begin{equation}
		 \mu(w) \propto \exp( - U(w) / kT ) \label{eq:boltzmann-appendix}
	\end{equation}

	where $k$ is the Boltzmann constant and $T$ is the thermodynamic temperature. Note that at unboundedly high temperatures, the differences between potentials don't matter (all states are equally likely), whereas at as the temperature approaches zero, the Boltzmann distribution puts zero mass on anything that's not a global minimum, and otherwise splits the mass equally. Therefore, if $U$ achieves a unique global minimum $w^*$, the corresponding $\mu(w) = \delta_{w,w^*}$ is a point mass on the minimum energy world $w^*$.
	
	It is standard and notationally useful to re-parameterize with the inverse temperature $\beta := 1/kT$ -- and we will refer to the Boltzmann distribution associated to a given potential $U$ (and inverse temperature $\beta$) as 
	\[ P_{\beta}(U) := w \mapsto  \frac{1}{Z_U(\beta)}\exp\Big(-\beta U(w)\Big) \]
	Where $Z_U(\beta) = \sum_{w \in W} \exp(-\beta U(w))$ is a normalizing factor, sometimes called the ``partition function''.	
	
	\textbf{From Distributions to Potentials}.	
	On the other hand, under similar assumptions, if given a probability distribution $\mu$ over $W$, there is a natural potential energy that resulted in it, 
	\[ E_{\beta}(\mu) := w \mapsto \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right)  \]
	which might be recognizable as negative log liklihood or the ``surprise'' of an event happening. By construction, $P_\beta \circ E_\beta$ is the identity on probability distributions:
	\begin{align*}
		 \Big(P_\beta \circ E_\beta(\mu)\Big) (w) &= \frac{1}{Z_{ E_\beta (\mu) }} \exp \left( - \ln \left(\frac{1}{\mu(w)}\right) \right) \\
		 &= \left(\frac{1}{\sum\limits_{w' \in W} \mu(w')}\right)\mu(w) \\
		 &= \mu(w)
	\end{align*}
	and $E_\beta \circ P_\beta$ is the identity on potential functions (up to a constant factor):
	\begin{align*}
		\Big(E_{\beta}\circ P_\beta(U)\Big)(w) &= \frac{1}{\beta} \ln \left(\frac{1}{\frac{1}{Z_U(\beta)}\exp(-\beta U(w))}\right) \\
		&=  \frac{1}{\beta} \Big[\ln Z_U(\beta) - (-\beta U(w)) \Big]\\
		&= U(w) + \frac{1}{\beta} \ln Z_U (\beta)
	\end{align*}
	The constant factor $-\frac{1}{\beta} \ln Z_U(\beta)$ coincides with the Heimholtz free energy of the system. Note that at constant temperature, this quantity is a durable feature of either a distribution or its associated energy landscape. 
%	
%	\begin{align*}
%		 0 = -\frac{1}{\beta} \ln Z_U(\beta) &= - \frac{1}{\beta} \ln \sum_{w \in W} \exp(-\beta U(w)) \\
%		 \iff 1 = \sum_{w \in W} \exp(-\beta U(w))
%%		 	&= -\frac{1}{\beta} \mathop{\mathrm{LSE}}_{w \in W}(-\beta U(\beta))
%	\end{align*}

	\textbf{Free Energy and Favorability.} Given a potential $U$, corresponding to a distribution $\mu$ as above, we now turn the question of how thermodynamically favorable a new distribution $\nu$ would be.%
		\footnote{From a statistical mechanics perspective, $W$ are the micro-states of the system, and a distribution over them is a configuration, or a macro-state.}
	For which we use the Gibbs free energy, $G_U(\nu) := {\E}_\nu( U ) - \frac{1}{\beta} H(\nu)$, which we think of a system as minimizing. The intuition here is that our new distribution $\nu$ is favorable if it has low average energy. However, at higher temperatures it also costs energy to compress the distribution: while a point mass at the minimum value of $U$ may be the lowest energy distribution, tightly controlling it to that degree also costs energy, when there's some ambient temperature causing randomness. From an epistemic perspective, even if a belief distribution $p$  is the one that best fits constraints, one might want to temper this by other possible configurations, and more so when there's higher ambient macroscopic uncertainty (temperature). Note also that the Gibbs Free Energy is a weighted probability distribution: it assigns a `favorability' score to distributions.
	
	If $U$ was generated by a probability distribution $\mu$, we then have
	
	\begin{align*}
		G_\mu(\nu) &= {\E}_\nu( E_\beta(\mu) )  - T S(\nu) \\
		&= \sum_{w \in W}\nu(w) \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right) - T \left[k \sum_{w \in W} \nu(w) \ln \left(\frac{1}{\nu(w)}\right)\right]\\
		&=  \frac{1}{\beta}\left[\sum_{w \in W}\nu(w) \ln \left(\frac{1}{\mu(w)}\right) - \sum_{w \in W} \nu(w) \ln \left(\frac{1}{\nu(w)}\right)\right]\\
		&=  \frac{1}{\beta}\left[\sum_{w \in W}\nu(w) \left(\ln \frac{1}{\mu(w)} - \ln \frac{1}{\nu(w)}\right)\right]\\
		&= \frac{1}{\beta} D \left(\nu || \mu \right)
	\end{align*}

	Where $D(\nu || \mu)$ is the relative entropy from $\nu$ to $\mu$. 
	
	Note that by Gibbs inequality, the $D(\nu || \mu) \geq 0$, and equal to zero precisely when $\nu = \mu$, and so the free energy of a configuration $\nu$ in a potential that was designed for $\mu$ is minimized by $\mu$ itself.	


	
	\textbf{Free Energy as a Design Tool.}
	
	This connection between thermodynamics and probability theory is already well utilized:
	\begin{enumerate}
		\item A Markov Random Field is specified with potentials $U_e$ for each edge; a factor graph is specified with potentials for a subset of cliques.
		\item The belief propagation algorithm computes local minima of the Bethe free energy, an approximation to the true Gibbs free energy.
	\end{enumerate}


	The dominant representation tool for mental states is the probability distribution, rather sets or weighted sets of them. % This is partly because they are easier to compute with, and because when faced with decisions at gun point, they are the most
	One issue with this is that there are distinct mental states that collapse to the same probability distribution (e.g., the coin flip: being uncertain about a process vs its outcome). The second one is that one might not have the right space for the distribution
	
	The insight here is that these are related: one can simply internalize the structure of the uncertainty. This some precedent for this: Pearl's rule, for instance, prescribes a new random variable to describe the uncertainty.	
	%%%
	
%	Consider a factor graph on a set of variables $\{ X_i \}$, with only a single factor $\phi$ which connects to every variable. The free energy is $G_\phi(U)$
%	
%	\[ \frac{1}{\sum_{\vec x} \phi(\vec x)} \phi(u) \]
%	
%%	The normalization constant $Z = \sum_{\vec x} \phi(\vec x)$
%	
%	Any factor graph defines a free energy by \todo{finish}
%	
%	The Bethe approximation to the free energy is an estimate based only on the marginals on single pairs of nodes.
%		
%	With a PDG, the free energy becomes
%	\[ \sum \]
%	\todo{Write out $\Inc$, proofs of theorems}


	\section{Overview And Conversions Between Graphical Models}
	\label{sec:many-relations-graphical-models}
	
	\usetikzlibrary{decorations.markings}	
	\begin{figure*}[t]
		\centering
		\tikzset{attn/.style={draw, fill=magenta, fill opacity=0.3, font=\Large\bfseries, inner sep=4pt}}
		\scalebox{0.75}{
			\begin{tikzpicture}
			\begin{scope}[every node/.style={ellipse, fill, fill opacity=0.05,text opacity=1,
					outer sep=3pt,font=\bfseries}, xscale=2.5,yscale=1.2]
				\node (KB) at (-3, 0.5) {KB};
				\node (CG) at (-3, 1.5) {CG};
				
				\node (CRF) at (-1.2, 0.4) {CRF}; % CFG
				%				\node (CRF) at (-2, 0) {CRF};
				
				\node (MRF) at (-2, 1) {MRF};
				\node[draw, attn] (FG) at (-1,1.35) {FG};
				\node (SDFG) at (0,1.5) {FG$^\rightharpoonup$};
				\node[attn] (DFG) at (1,1.35) {FG$^\rightarrow$};
				\node[attn] (BN) at (2,1) {BN};
				
				\node (CBN) at (2,0) {CBN};
				\node (DN) at (1.3, 0.6) {DN}; 
				
				\node (sPDGH) at (0,0.5) {sPDG$_{\text{hyper}}\!\!$};
				\node (PDGH) at (-0.8,-.5) {PDG$_{\text{hyper}}\!\!\!$};
				\node (PDG) at (0,-.85) {PDG};
				\node[attn, fill=black, fill opacity=0.9, text=white] (sPDG) at (0.8,-.5) {sPDG};
				
				\node (prog) at (3, -0.2) {PProgSet};
				
				\node (CPS) at (1, -1.4) {CPS};	
				\node (PlateBN) at (-2.5, -1.2) {PlateBN};
				\node (LPS) at (-1,-1.4) {$\underline {\mathcal P}$};
			\end{scope}
			
			% lossless		
			\begin{scope}[every edge/.append style={->}]%right hook->
				\draw (BN) edge (DFG) (DFG) edge (SDFG);
				\draw (MRF) edge (FG) (FG) edge (SDFG);
				%				\draw[->] (DFG) -- (FG);
				\draw (CBN) edge[bend left = 5, shorten >=7pt] (sPDGH);
				\draw (CG) edge[bend left=10] (FG);
				\draw (KB) edge (CG);
				
				\draw (sPDGH) edge (PDGH) (sPDG) edge (PDG);
				
				\draw (BN) edge (DN) (DN) edge (sPDGH);
				\draw (DFG) edge (sPDGH);
				
				\draw (MRF) edge (CRF);% (CRF) edge (CFG);
				\draw (BN) edge (CBN);
				\draw (FG) edge (CRF); %crf
				
				\draw (CG) edge[out=-55, in=195, looseness=1.5, shorten >=7pt] (sPDGH);
				\draw (prog) edge[bend left=5] (sPDG);
				\draw (CPS) edge[out=180,in=-30] (PDG);
				\draw (PlateBN) edge[bend right=5] (PDGH);
				\draw (LPS) edge[out=0, in=-150] (PDG);
			\end{scope}
			
			% PDG Equivalences
			%			\draw[->, transform canvas={yshift=2pt}] (PDGH) -- (PDG);
			%			\draw[->, transform canvas={yshift=-2pt}] (PDG) -- (PDGH);
			%			
			%			\draw[->, transform canvas={yshift=2pt}] (sPDGH) -- (sPDG);
			%			\draw[->, transform canvas={yshift=-2pt}] (sPDG) -- (sPDGH);
			
			\draw[double equal sign distance, shorten <=0pt, shorten >=0pt] (PDGH) -- (PDG);
			\draw[double equal sign distance] (sPDGH) -- (sPDG);
			
			
			% Projections. Lose information but preserve something.
			\begin{scope}[every edge/.append style={densely dashed, orange, ->}]
				\draw (sPDGH) edge[bend left=10, out=10] (FG) (sPDGH) edge[bend right=10, out=-5] (FG);
				\draw (SDFG) edge[bend right=20] (FG);
			\end{scope}
			% Inefficient conversions.
			\begin{scope}[every edge/.append style={ultra thick, dotted, line cap=round, shorten >=2pt,
					decoration={markings,mark=at position 1 with {\arrow[xshift=0pt,scale=.8]{>}}},
					postaction={decorate}}]
				\draw (CRF) edge (sPDGH);
				\draw (SDFG) edge (sPDGH);
			\end{scope}
			
			%			\draw[->, transform canvas={xshift=-3pt}] (DDW) -- node[left]{$E_\beta$} (EDW);
			%			\draw[->, dashed, transform canvas={xshift=3pt}] (EDW) -- node[right]{$P_\beta$} (DDW);
			%			
			%			\draw[->] (DW) to[bend left=10] node[sloped,fill=white]{$D({-\Vert})$} (EDW);
			%			
		\end{tikzpicture}
		}
		\caption{Transformations Between Graphical and Epistemic Models. Solid arrows indicate a model being a special case of another. Orange dashed transformations lose information, and the thick arrows are inefficient translations. For a full description, check \Cref{sec:many-relations-graphical-models}. }
		\label{fig:model-transformations}
	\end{figure*}
	\todo{There is a ton to do here.}

	\subsection{The Details: Factor Graphs and PDGs} \label{sec:factor-graphs-long}
	% What I want to see is a serious discussion of the advantages and disadvantages of factor graphs vs. PDGs, illustrated by examples. This is critical.


	We now compare PDGs with factor graphs, a general class of \emph{undirected} graphical models, often described as a generalization of BNs and Markov Networks.
	%todo: hint at MN relation in beginning. 
	%PDGs can simulate them (\cref{def:fg-convert}), but not without large cpds and sneaky use of inconsistency. 


	%% informal, unclear.
	\begin{quickdefn}
	A \emph{factor graph} is a collection of random variables $\mathcal X = \{X_i\}$ and a collection of \emph{factors} $\{\phi_\alpha\colon X_\alpha \to \mathbb R_{\geq0}\}_{\alpha \in \mathcal I }$ over subsets $\alpha$ of $\mathcal X$.
	\end{quickdefn}
	\begin{defn}
	%oli6: I have removed my "intuitive" cavalier version of the definition 
	% that you disliked (which has often been given in technical
	% overviews I've read, but bugs me by leaving things undefined so I
	% expanded it, but left a short version for people who aren't interested
	% in following the technical details closely).
		% A \emph{factor graph} on variables $\{X_j\}$ is a set of \emph{factors} $\{\phi_\alpha\colon X_\alpha \to \mathbb R_{\geq0}\}_{\alpha \in \mathcal I }$ over subsets $\alpha$ of the variables.
		% 
		% More precisely, a
		A factor graph $ (\{\phi_\alpha\}_{\alpha \in \cal I})$ on an indexed set of random variables $\mathbf X = \{ X_j \}_{j \in J}$, 
	%oli6:
	% I know the \iota bothers you, but I did think about this for a long
	% time, and think this is the cleanest presentation that can be
	% perfectly formalized without relying on any notions of equality
	% between natural objects, or lack of formality when constructing
				% them.
	%joe6: Most people don't use it, right?  There's a good reason...
	%Why would it hurt to do things the way Koller and Friedman do?                
	%Since I think that ultimately factor graphs will play only a small
	%role in the paper, we should use the simplest possible presentation
	%of them.  I don't mind being slightly informal.
		is a pair $((\mathcal I,\iota), \boldsymbol\phi)$, where $\cal I$ is a set,
		each element $\alpha\notation{\in \mathcal I}$ of which determines a selection $\iota(\alpha) \subseteq J$ of the variable indices, and
		$\boldsymbol\phi$ is an indexed collection of \emph{factors} $\{\phi_\alpha\}_{\alpha \in \mathcal I }$, 
		where each factor $\phi_\alpha \colon \mathcal X_\alpha \to \mathbb R_{\geq 0}$ assigns a non-negative score to joint settings $\vec x_\alpha \notation{\in \mathcal X_\alpha}$ of every variable in $\iota(\alpha)$, all values of which we denote by $\mathcal X_\alpha\notation{ := \prod_{j \in \iota(\alpha)} \mathcal V(X_j)}$. 
	\end{defn}
	\begin{fulldefn}
	A \emph{factor graph} $(\mathcal I, \phi)$ on an indexed set of random variables $X : \Sigma_{}$, where each $X_i$ can take values $\V(X_i) =: \mathcal X_i$, consists of  
	% technically, a dependent sum \mathbb X : \Sigma_{j : J} X_j
	%		a set $\cal I$, where each $\alpha \in \cal I$ is a
	% technically a multi-subset of 2^J...
	a set of \emph{factors} $\{\phi_\alpha\}_{\alpha \in \mathcal I }$, where each $\alpha$ determines a selection $\iota(\alpha) \subseteq 2^J$ of the variable indices, and the associated factor $\phi_\alpha \colon \mathcal X_\alpha \to \mathbb R^{\geq 0}$ assigns a non-negative score to a setting
	$\vec x_\alpha \in \mathcal X_\alpha := \prod_{j \in \iota(\alpha)} \mathcal X_j$ of the variables corresponding to $\iota(\alpha)$.

	\end{fulldefn}

	% $(J, \mathcal I)$
	While the qualitative structure $(\mathbf X, \mathbf{Pa})$ of a BN on variables $\mathbf X$ is a directed acyclic graph, the qualitative structure $(\mathbf X, \mathcal I)$ of a factor graph on $\mathbf X$ is
	%		technically an undirected \emph{multi-hyper-graph}
	%	 		\footnote{That is, a set of ``nodes'' $\N$ and
	%a collection (possibly containing multiple copies) of ``hyper edges''
		%$\Ed$, each of which corresponds to a subset of $\N$}
	%joe4*: what is a multi-hypergraph?   Is this by analogy to a
	%multigraph, there can be multiple hyperedges joining the same set of
	%nodes? Either explain the ``multi'' or remove it.
	%oli5: This is correct. I think this might actually be standard for a hyper-graph though, so I don't feel bad about removing it; just wanted to be precise.
	a hypergraph $(\mathbf X, (\cal I, \iota))$ where
		$\alpha \in \cal I$ is an undirected hyperedge, drawn as a
		square, connecting the vertices in the set $\iota(\alpha)$.  
	%	a bipartite graph $((\mathbf X, \cal I), \iota)$ with extra vertices (drawn as squares) corresponding to the factors. 

	%	\note{Though easier to define in terms of MRFs, and this obscures the relationship to BNs and MRFs; this  paper in particular is an attempt to claim that adding and removing nodes is not something to sweep under the rug.}



	%	The important thing about 
	A factor graph $\Phi = (\{\phi_\alpha\}_{\alpha \in \cal I})$ on $\mathbf X$ defines a probability distribution over $\V(\mathbf X)$ by 
	\begin{align*}
	%joe6: I have never seen the :\alpha notation before.  Unless it's
	%standard, just use \alpha.  It also doesn't make sense to have both
	%\alpha and =.  Let's just simplyify it
	%		\Pr\nolimits_\Phi(\vec x) &:\propto \prod_{\alpha \in
		  %                  \cal I} \phi_\alpha(\vec x_{\alpha})
	%          		~~= \frac{1}{Z_\Phi} \prod_{\alpha \in \cal I}
				\Pr\nolimits_\Phi(\vec x) 
		= \frac{1}{Z_\Phi} \prod_{\alpha \in \cal I}
								\phi_\alpha(\vec x_{\alpha}), 
	\end{align*}
	where $\vec{x}$ is a joint setting on all of the variables, $\vec{x}_\alpha$ is the restriction of $\vec{x}$ to only the variables selected by $\alpha$, and $Z_\Phi$ is the constant required to normalize the distribution. 
	%joe4
	%	There are several ways of parameterizing factor graphs; we
	%        start with the most explicit one. 
		
	%joe4*: I'm missing the big picture here.  What's the goal?  You've
	%defined factor graphs.  What more do you need.  (I don't mean to
	%imply that there isn't more that you might want/need, just that I
	%don't know what it is.)
	%joe4: I don't understand what it means that ``the particular setting of
	%which matter''.  And I'm not sure what global/local would mean here.
	%This is a fair complaint, this is not very well-explained.
	%even though the
	%        particular settings of which do matter, the interactions are
	%        global and it's hard to see how they will play out. Still,
	%joe4: I don't see how factor graphs are excellent desriptions of
	%independencies  This has to be explained better.
	%        they are excellent descriptions of independencies. 
	%	 David's score is independent of everything else in the picture, and though the other three are a clique, we can see different interactions

	%joe4
	%Any BN $\mathcal B = (\N, \Pa, \V, p)$ can be seen naturally
	A BN $\mathcal B = (\N, \Pa, \V, p)$ can be viewed
	as a factor graph, which we denote $\Phi(\mathcal B)$.
	%joe4: while the next line is true, do we need it?  Why not just
	%explain directly how a BN can be viewed as a factor graph.
	%oli5: I'm not sure what you're asking for. Writing down the global
	% semantics is a clear demonstration that it's a product of factors. 
	% I'm reinstating what I had, plus some minor modifications to clarify
	%joe6: are there also local semantics?
	%By the global semantics of BNs, we have that
	By the semantics of BNs, we have that
	\begin{align*}
	% joe4: to get the \B to be positions right relative to the \Pr.  But I think that we should cut this anyway
	  % \Pr\nolimits_{\cal B}(\vec x) = \prod_{N \in \N} p_N( \vec x_N
	  {\Pr}_{\cal B}(\vec x) = \prod_{N \in \N} p_N( \vec x_N
		 \mid \mathbf{Par}_N(\vec x)). 
	\end{align*} 
	%joe4: please fill in the blank below, and avoid using \iota
	%oli5: \iota is part of my definition, but it's a pretty obvious
	% identification so I can just not mention it if that's better...
	%	 Factors can be read off directly: set $\cal I = \N$, connect every
	% 	variable $X$, and all of its parents $\Pa_X$, to the factor
	% 	corresponding to $X$, by $\iota(X) := \{X\} \cup \Pa_X$. Finally,
	%	define the function $\phi_X(x, \vec{y}) := p_X( X \!\!=\!\! x \mid
	%	\Pa_X \!\!=\!\! \vec y)$ to simply be the cpd at $X$.
	%
	%joe6: Is this what you meant?
	%Factors can be read off directly: set $\cal I = \N$, connect every
	%variable $X$, and all of its parents $\Pa_X$, to the factor
	%corresponding to $X$%
	In the factor graph correponding to ${\cal B}$, 
	we set $\cal I = \N$, and have a factor for every variable $X$, which
	consists of $X$ and all its parents in ${\cal B}$.
	%oli5: it's still intelligable without this line, just not complete. 
	%, by $\iota(X) := \{X\} \cup \Pa_X$
	%joe6
	%Finally,
	%define the function $\phi_X(x, \vec{y}) := p_X( X \!\!=\!\! x \mid
	We define $\phi_X$ by taking
	$\phi_X(x, \vec{y}) := p_X( X \!\!=\!\! x \mid
	%joe6
	%\Pa_X \!\!=\!\! \vec y)$ to simply be the cpd at $X$.
	\Pa_X \!\!=\!\! \vec y)$; that is, $\phi_X$ is determined by the cpd of $X$.
	%joe6: this seems redundant
	%The corresponding factor graph has the same set of variables, and a
	%hyperege corresponding to each node, which connects a node $X$ to its
	%parents.
	The factor $\phi_X$ correspond to the node $X$ is given by the cpd of
	$X$; that is, $\phi_X(x,
	\vec{y}) := p_X( X \!\!=\!\! x \mid 
	\Pa_X \!\!=\!\! \vec y)$.
	%joe4:
	%Examples of this can be seen in the solid components of
	%\Cref{subfig:fg-gf,subfig:fg-smoking}, which correspond to the initial
	%
	\Cref{subfig:fg-gf,subfig:fg-ablate,subfig:fg-smoking} give the factor graphs corresponding  to
	the BNs in \Cref{ex:guns-and-floomps,ex:grok-ablate,ex:smoking}, respectively.  
	%\Cref{subfig:fg-gf,subfig:fg-smoking}, which correspond to the initial
	%to BNs in \Cref{ex:guns-and-floomps,ex:smoking}, respectively. 


	\begin{figure}[htb]
		\centering
		\begin{subfigure}[b]{0.22\linewidth}
			\scalebox{0.8}{
				\begin{tikzpicture}[center base]
					\node[fgnode] (F) at (-1.5,0) {$F$};
					\node[fgnode] (G) at (1.5,0) {$G$};
					\node[factor, above=0.5 of F] (f) {$\phi_F$};
					\node[factor, above=0.5 of G] (g) {$\phi_G$};
					
					\draw[thick] (F) -- (f) (G) -- (g);
					\draw[thick, dashed] (F) -- node[factor, fill=white]{$T$} (G);
			\end{tikzpicture} }
			\caption{}\label{subfig:fg-gf}
		\end{subfigure}%
		\hspace{1.5em}\vline\hspace{1.5em}%
		\begin{subfigure}[b]{0.3\linewidth}
			\scalebox{0.8}{
				\begin{tikzpicture}[center base, scale=0.9]
					\node[fgnode] (S) at (-0.4, 2) {$S$};
					\node[fgnode] (C) at (3, 2) {$C$};
					\node[fgnode] (L) at (1.3,0) {$L$};
					\node[fgnode, dashed] (W) at (-2,0) {$W$};
					
					\node[factor] (f1) at (1.3, 1.3){$\phi_1$};
					\node[factor, dashed] (f2) at (-0.3, 0){$\phi_2$};
					
					\draw[thick] (S) -- (f1) -- (C) (f1) -- (L);
					\draw[thick, dashed] (W) -- (f2) -- (L);
			\end{tikzpicture} }
			\caption{}\label{subfig:fg-ablate}
			
		\end{subfigure}%
		\hspace{1.5em}\vline\hspace{1.5em}%
		\begin{subfigure}[b]{0.3\linewidth}%
			%			\vspace{-1em}
			\scalebox{0.72}{
				\begin{tikzpicture}[center base, xscale=1.4,
					fgnode/.append style={minimum width=3em}]
					\node[factor] (prior) at (1.65,-1) {};
					\node[factor] (center) at (3.95, 0){};
					
					\node[fgnode] (PS) at (1.65,0.5) {$\mathit{PS}$};
					\node[fgnode] (S) at (3.3, 0.8) {$S$};
					\node[fgnode] (SH) at (3.0, -0.8) {$\mathit{SH}$};
					\node[fgnode] (C) at (4.8,0) {$C$};
					
					\draw[thick] (prior) -- (PS);
					\draw[thick] (PS) --node[factor, fill=white](pss){} (S);
					\draw[thick] (PS) --node[factor, fill=white](pssh){} (SH);
					\draw[thick] (S) -- (center) (center) -- (SH) (C) -- (center);
					
	%					\node[dpadded, fill=blue] (1) at (2.5,-2) {1};
	%					
	%					\draw[blue!50, arr] (1) -- (prior);
	%					\draw[blue!50, arr] (1) -- (center);
	%					\draw[blue!50, arr] (1) -- (pss);
	%					\draw[blue!50, arr] (1) -- (pssh);
					
					
					\node[fgnode, fill opacity=0.02,dashed] (T) at (4.8, -2) {$T$};
					\draw[thick,dashed] (T) -- node[factor, fill=white]{}  (C);	
			\end{tikzpicture}}
			\caption{}\label{subfig:fg-smoking}
		\end{subfigure}%
		\caption{Candidate factor graphs for \Cref{ex:guns-and-floomps,ex:grok-ablate,ex:smoking}.
 	%oli10: no light blue here anymore
%The light blue arrows illustrate \Cref{def:fg2PDG}.
		}
		\label{fig:fg-intro-examples}
	\end{figure}

	%joe6: I couldn't parse your English, so I wrote what I thought you meant:
	%	This suggests an obvious way to view an arbitrary collection
	%        of cpds in the form of a PDG $\dg M$ as a factor graph
	%        $\Phi(\dg M)$: just like for a BN, ignore the directions of the
		%        edges and use the cpds as factors.
	We can apply this way of viewing BNs as factor graphs to arbitrary
	PDGs: we take the factors to be defined by the cpds.
	\begin{defn}[PDG to factor graph]
		If $\dg M = \mnvars[]$ is a PDG, define 
	%joe6*: Sorry, I don't undestand this notation.  Is ((\Ed,\in), \mat
	%p)$ supposed to be a factor graph? So you're somehow identify \in
	%with \iota?  This shows that the use of \iota is making life worse
	%...  I think that you have to spell this out better.
		$ \Phi(\dg M) := ((\Ed,\in), \mat p)$
		to be the associated factor graph on the random
				variables $(\N, \V)$. 
	\end{defn}
	%joe6*: I didn't follow this remark.  I think that you want here is a
	%simple theorem about how the PDG and the associated factor graph
	%define the same distribution.  Is that even true?  If so according
	%to what semantics.  If not, then in what sense are the two related.
	%I an unhappy about this story.
	\begin{remark}
		It is easy to verify that this construction yields the 
				same product of factors, whether one thinks of a PDG
				as a hypergraph directly, or translates it to a graph
				first, as formalized in \Cref{sec:formal+syntax}. 
	\end{remark}

	%joe6*: Why do I care about inverting this process.  Again, I think
	%that this is not a good story.  I'm not reading the rest of this
	%carefully, since  I don't think we'll want to keep it.      
	In order to faithfully invert this process, converting a
		factor graph to a PDG, we would need to arbitrarily chose a
		direction for each edge and normalize; the different
		directions may result in wildly different distributions, none of
		which are necessarily related to the distribution determined
		by the original factor
		graph. We can do much better by giving up on creating
	%joe4*: I don't understand what the figure is showing me.  Ae we
	%getting different graphs?  It looks like we're getting just one PDG.
	%Moreover, Definition 4.4 also seems to get one PDG.
	%oli5: It is just one PDG. They're all one PDG. This paragraph was
	%intended to show that choosing a direction is NOT a tenable way to
	%construct a PDG from a factor graph, thereby explaining why we can't
	%totally invert the process we used to get there, and explaining why
	%they're all connected to \sf 1. 
		the \emph{same} graph, as illustrated in \Cref{fig:fg2PDG} and
		defined below.  

	\begin{figure}[htb]
		\centering
	%		\begin{subfigure}{0.5\linewidth}\centering
	%			\scalebox{0.72}{
	%				\begin{tikzpicture}[center base, xscale=1.4,
	%					fgnode/.append style={minimum width=3em}]
	%					\node[factor] (prior) at (1.65,-1) {};
	%					\node[factor] (center) at (3.95, 0){};
	%					
	%					\node[fgnode] (PS) at (1.65,0.5) {$\mathit{PS}$};
	%					\node[fgnode] (S) at (3.3, 0.8) {$S$};
	%					\node[fgnode] (SH) at (3.0, -0.8) {$\mathit{SH}$};
	%					\node[fgnode] (C) at (4.8,0) {$C$};
	%					
	%					\draw[thick] (prior) -- (PS);
	%					\draw[thick] (PS) --node[factor, fill=white](pss){} (S);
	%					\draw[thick] (PS) --node[factor, fill=white](pssh){} (SH);
	%					\draw[thick] (S) -- (center) (center) -- (SH) (C) -- (center);
	%					
	%					%					\node[dpadded, fill=blue] (1) at (2.5,-2) {1};
	%					%					
	%					%					\draw[blue!50, arr] (1) -- (prior);
	%					%					\draw[blue!50, arr] (1) -- (center);
	%					%					\draw[blue!50, arr] (1) -- (pss);
	%					%					\draw[blue!50, arr] (1) -- (pssh);
	%					
	%					
	%					\node[fgnode, fill opacity=0.02,dashed] (T) at (4.8, -2) {$T$};
	%					\draw[thick,dashed] (T) -- node[factor, fill=white]{}  (C);	
	%			\end{tikzpicture}}
	%		\end{subfigure}
	%		\begin{subfigure}{0.5\linewidth}\centering
			\scalebox{1}{
				\begin{tikzpicture}[center base, xscale=1.6,
					fgnode/.append style={minimum width=3em}]
					\node[dpadded] (prior) at (1.65,-1) {};
					\node[dpadded] (center) at (4.05, 0.2){};
					
					\node[fgnode] (PS) at (1.65,0.5) {$\mathit{PS}$};
					\node[fgnode] (S) at (3.3, 0.8) {$S$};
					\node[fgnode] (SH) at (3.3, -0.8) {$\mathit{SH}$};
					\node[fgnode] (C) at (4.9,0.5) {$C$};
					
					\draw[arr, <<-] (prior) -- (PS);
					\draw[arr, <<->>] (PS) --node[factor, fill=white](pss){} (S);
					\draw[arr, <<->>] (PS) --node[factor, fill=white](pssh){} (SH);
					\draw[arr, <<-] (S) -- (center); 
					\draw[arr, <<-] (SH)-- (center); 
					\draw[arr, <<-] (C) -- (center);
					
					\node[dpadded, fill=blue] (1) at (2.5,-2) {1};
					
					\draw[blue!50, arr] (1) -- (prior);
					\draw[blue!50, arr] (1) to[bend right=30] (center);
					\draw[blue!50, arr] (1) to[bend right = 10] (pss);
					\draw[blue!50, arr] (1) to[bend left = 10] (pssh);

					
					\node[fgnode] (T) at (4.8, -2) {$T$};
					\draw[arr, <<->>] (T) -- node[factor, fill=white](tc){}  (C);	

					\draw[blue!50, arr] (1) to[bend right = 10] (tc);
			\end{tikzpicture}}
	%		\end{subfigure}
		
		\caption{A graphical illustration of the conversion from a factor graph (the one shown in \Cref{subfig:fg-smoking}) to a PDG, as defined in \Cref{def:fg2PDG}. The blue edges carry the cpds corresponding to the original factors, and the structure is turned into the double headed deterministic black arrows.}
		\label{fig:fg2PDG}
	\end{figure}

	\begin{defn}[factor graph to PDG] \label{def:fg2PDG}
		If $(\{\phi_\alpha\}_{\alpha \in \cal I})$ is a factor graph, then let $\PDGof{\Phi}$ be the PDG generated by inserting joint variable node $X_\alpha = \prod_{j \in \iota(\alpha)} X_j$ for every factor node $\alpha \in \mathcal I$ (as done in \Cref{def:bn2PDG}), and an edge $\sf 1 \to X_\alpha$ whose associated cpd $\bp[\alpha]$ is the joint distribution on the variables corresponding to $\alpha$ obtained by normalizing $\phi_\alpha$ across all of their possible values.%
	\end{defn}


	\begin{prop}\label{prop:fg-pdg-lossless}
		$\Phi \circ \PDGof = \mathrm{Id}_{\text{FG}}$. That is, if $F$ is a factor graph, then $\Phi(\PDGof{F}) = F$.
	\end{prop}
	\begin{proof}
	%joe4: what's a local normalization?      
	%oli5: we are required to normalize each cpd 1->X because they are
	%distributions. It's local because it's done for each cpd, and these
	%normalizations are unlikely to ultimately be compatible with the
	%joint distributions on these variables.    
		Because each local normalization results in a local joint
				distribution $\bp[\alpha] = \frac{1}{Z\alpha}
	%joe4*: I'm confused.  What differs from what?  is this what you meant
	%                \phi_\alpha$, which only differs by a multiplicative
	%               constant, their product will only differ by a
	%oli5: You're right, this was super unclear. I rewrote to clarify.
				\phi_\alpha$ on the variables associated with $\alpha$, and these distributions differ from the original factors $\phi_\alpha$ by only a multiplicative 
			   constant, the product of these locally normalized factors differs from the product of the factors by only a constant, and so 
		\[ \Pr_F(\vec x) \propto \prod_\alpha \phi_\alpha(\vec x) \propto \prod_\alpha \left(\frac{\phi_\alpha(\vec x)}{Z_\alpha}\right) \propto \Pr_{\Phi(\PDGof{F})}(\vec x) \]
		and since the two distributions are normalized, they must be equal.
	\end{proof}

	%oli6: expanding a lot here to tell the story better.
	% 	This suggests that the PDG has all of the information we need
	% 	to interpret the factor graph.
	% However, the distribution $\UD{\PDGof{F}}$ that PDG semantics prescribe may
	% look nothing like $\Pr_F$ (cf.~\Cref{ex:fg-exam}). 

	It may be surprising that a factor graph can be \emph{losslessly} converted to a PDG in a reasonable way, given that PDGs are directed models.
	This fact suggests that $\PDGof{F}$ contains all of the same information as $F$. Knowing also that BNs are a kind of factor graph, it is natural to wonder if the unique distribution $\UD{\PDGof{F}}$ given by our semantics, is the same as $\Pr_F$.		
		It is not. 


	% For the time being, it is important just to note that the
	% distribution $\UD{\PDGof{F}}$ that PDG semantics prescribe may
	% look nothing like $\Pr_F$ (cf.~\Cref{ex:fg-exam}). 
		
	However, it is closely related---it turns out that by merely providing different weights for the terms in \Cref{eqn:full-score}, we can recover the probability distribution. Better still, this construction will litterally match the factor graph of equivalent of the scoring function $\mathcal U_\gamma(\dg M)$ (called the free energy, for reasons detailed in \Cref{sec:thermo}).
	%oli6: When should I get people thinking about U being a free energy? 
	% Ideally I want to point to the connection between them here, and I want to get people thinking
	% thinking of "free energy" this way as soon as possible without losing them. 
	%oli6: end of expansion, remove paragraph break.
	%
	%oli6: completely rewrote paragraph.
	% In \Cref{sec:fg-expfam}, we break down the full semantics $\bbr{-}$
	% of a PDG to show exactly how
	% the factor graph can be given by a different weighting of the
	% terms we have already given.
	% they differ from factor graphs, how one could add a parameter
		In \Cref{sec:fg-expfam}, we further analyze the PDG scoring semantics to explain how this works.
	%oli6: This rhetorical question... good framing or a waste of space? 
		If this is the case, why is there no clear choice of $\alpha,\beta,\gamma$ which results in the factor graph distribution?  
		We have made a deliberate choice to \emph{not} reproduce the semantics of general factor graphs, to avoid the their drawbacks, which 
		we now examine.

	%	\begin{coro}
	%		$\Pr_{\mathcal B}  = \Pr_{\Phi(\mathcal B)} = \Pr_{\Phi(\PDGof{{\mathcal B}}}$
	%	\end{coro}




	%	\begin{example}\label{ex:planet-fg}
	%		In our planet example, we treat each edge as a factor, the product of which gives the correct relative likelihoods for each of $S \times C \times W \times L$. Our initial knowledge, consisting only of the cpd, we have 
	%		\[ \Pr(s, c, w, l) \propto \phi_1(s,c,l)  \]
	%		where $\phi_1(s,c,l) = p(l \mid s,c)$, and no normalization is required.
	%		
	%		
	%		In contrast with BNs, there is no structural barrier to adding a new node, and factor $\phi_2(w,l) \!=\! \Pr(L\!=\!l\mid W\!=\!w)$ --- though to make sense of this as a probability we have to re-compute the normalization constant. The combination of the two factors is represented graphically in \Cref{subfig:fg-planet}, in which circles represent variables, and the boxes represent factors that depend on variables they connect to. 
	%		\todo{compute two different distributions}
	%	\end{example}

		
	%	\[ \Pr{} (\vec x)  = \frac{1}{Z(\vec\theta)} \exp \left\{ \sum_\alpha \theta_\alpha \varphi_\alpha(\vec x) \right\} \] 

	%joe6*: Yet again, I think that this is the wrong story.  We're not
	%writing a paper about factor graphs, but about PDGs.  You could
	%perhaps talk about the advantages of PDGs over factor graphs, but I'm
	%not sure that that's what we should be focused on.
			\subsubsection{Shortcomings of Factor Graphs}\label{sec:fg-issues}
%joe7*: while this is inappropriate -- we are not wriiting a critique
%of factor graphs -- it would be good to have in the main part of the
%paper a few sentnces about why PDGs are better than factor graphs in
%some improtant respects
	                While factor graphs are powerful statistical models, 
	we argue that they are not well suited to 
	%oli6:
	% modeling for epistemic state, for several reasons. 
	modeling a bounded agent's belief state, for the following reasons. 

	\begin{enumerate}
		\item They are undirected, making causal modeling, and intuitions about functions
			 impossible to capture. This is partially resolved \cite{frey2012extending} by directed factor graphs. 
		\label{fgproblem:undirected}
		\item The global normalization process is over-eager in sweeping all inconsistencies. As a result, a local view of a few factors may not provide any information about the distribution. For instance, in  \Cref{ex:fg-exam}, $\phi_2$ suggests a qualitatively different joint distribution on $A,B,C$ than the one obtained after incorporating $\phi_3$. \label{fgproblem:global}
		\item Factors cannot be re-weighted by importance while still preserving the ratios of likelihoods between alternatives\footnote{The absolute scale is irrelevant, as used in the proof of \Cref{prop:fg-pdg-lossless}, while the weight parameters of the corresponding exponential family used to control importance do so by imposing distortions (\Cref{sec:fg-expfam}).}. \label{fgproblem:reweight}
	%oli6: we're already over, no space for this :(
		% \item There is no possibility of corroborating evidence \label{fgproblem:corrob}
		
	%oli6: modified heavily but forgot to comment out the original.
		\item They are volatile: the addition of a new node can invalidate and arbitrarily distort the semantics \label{fgproblem:volatile} (\Cref{ex:fg-volatile,ex:fg-volatile-2} below). In fact, given a subgraph $F' \subseteq F$ of a factor graph $F$, it is impossible know anything about the semantics $\Pr_F$ of $F$ except that must assign zero mass to anything to any joint setting $w$ where $\Pr_{F'}(w) = 0$. 
		% We call this a security vulnerability.
	\end{enumerate}

	\begin{example}\label{ex:fg-volatile}
		Add a new factor, not connected to any variable, with $\phi() = 0$. Now the product of the factors is uniformly zero, and so the distribution is not defined. This is not even salvageable locally, and the factor cannot be found by tracing paths.  
	\end{example}


	In \Cref{ex:fg-volatile}, the designer is lucky in a sense: it is obvious that the model is broken, and the fix is to delete a single suspicious-looking factor.
	%Of course, without trying the NP hard normalization, there's no way to tell that anything is wrong.
	%
	In general, things could be  worse: the failure to normalize could be spread across multiple nodes, in a distributed way; and ruling out this possibility is NP-hard in the number of factors. 
	In addition to causing corruption, a single additional factor can precisely construted to exactly determine the semantics of the entire graph% \Cref{ex:fg-volatile-2}
	.

	\begin{example}\label{ex:fg-volatile-2}
		Let $\Phi$ be any factor graph whose factors all take strictly positive values, and distribution $\mu$ on the same variables. add a new factor $\phi$, connected to every variable, such that $\phi(\vec x) = {\mu(\vec x)}/{\Pr_\Phi(\vec x)}$. Then $\Pr_{\Phi \cup \phi} = \mu$. 
	\end{example}

	%oli6: rewritten. I'm actually proud of this paragraph.
	% This global normalization process in some sense is a catch-all fix that ensures that the factor graph is well-defined, but does not preserve any local meanings whatsoever, making it a poor tool for modeling local beliefs.
	If we think of a factor as an assertion of local relative likelihood, as in \Cref{ex:fg-exam}, the global normalization can be seen as a blunt agregation of the data presented by a set of seemingly inconsistent factors into a single consistent probability distribution (and can fail, as in \Cref{ex:fg-volatile})). The price of this consistency is arbitrary distortion of local relative liklihood constraints, making factor graphs also a poor tool for modeling modular beliefs (at least not about relative liklihood). 
		%over worlds which are truly random.
	%. and even worse for inconsistent ones.

	By contrast, PDGs
	\begin{vfull}
	(see \Cref{sec:pdg-operations})
	\end{vfull}
	are unaffected by any data that is not connected to the rest of the graph. Raise red flags when something is wrong, and do not have the security vulnerability that their the entire state is precisely controllable by a single new added piece of knowledge.
	% NOTE: This is not 
	\begin{vfull}
		A PDG clearly encodes more information than just the
                distribution: this is true for both Bayesian Networks
                and Factor Graphs as well. In both cases, this is
                sometimes cast as a flaw, as this makes them poor
                choices as canonical descriptions of distributions,
                which is why so much attention is given to I-maps in
                \cite{koller2009probabilistic}.  
		
		However, meaning beyond the distribution has not been empirically damaging. Despite being less expressive and obscuring independence relations, BNs continue to be a more popular modeling tool. The causal picture they can provide, beyond anything in the distribution, is evidently worth a lot.
	\end{vfull}
	%oli6: added. Also, the above two paragraphs need more trimming.
	It follows that it is only possible to avoid such issues with PDGs, if the class of factor graphs cannot be efficiently represented as PDGs in a way that preserves semantics.
	In the next section we see that the only reason PDGs do not naturally encompass factor graphs is an intentional coupling of two information theoretic quantities with the same parameter $\beta_L$. 
		% failing to keep an embedding 
	%
	%	\subsubsection\
	%	If we restrict the factors to have binary output $\phi_\alpha(x_\alpha$ of a constraint graph

	\subsubsection{Specifying Factors Directly}
%
%	How does one design a distribution with the factors? One way
%is to specify each $\phi$ directly, reasoning roughly as in
%\Cref{ex:fg-exam}. 

%	A factor graph is really just an exponential family \cite{wainwright2008graphical}, 

%joe4: this is a useful example even without the preceding story.
%Moved first sentence out of the example
%	\begin{example}\label{ex:fg-exam}
		To contrast with our other examples, which mostly correspond to directed models, we present a more general factor graph that displays some of the stranger features of factor graphs.

	\begin{example}\label{ex:fg-exam}		
			  Suppose that Alice, Bob, Clara, and David each had a
				take-home exam; let $\mathbf X = \{A, B, C, D\}$ be
				binary random variables taking $\{1,0\}$,
				corresponding to whether or not each person passed the
				exam.  
		We want a joint distribution over possible outcomes; our knowledge, depicted graphically in \Cref{fig:fg-exam}, is as follows:	

		\begin{figure}[H]
			\centering
			\scalebox{0.8}{
				\begin{tikzpicture}[scale=0.75]
					%			\node[bpt={a1|$a_1$}] at (0,0.2){};
					%			\node[tpt={a2|$a_2$}, below=.4 of a1] {};
					%			\node[bDom={$A$ (A) around \lab{a1}\lab{a2}},fgnode] {};
					%
					%			\node[bpt={b1|$b_1$}] at (4,0.2){};
					%			\node[tpt={b2|$bow=.4 of b1] {};
					%			\node[bDom={$B$ (B) around \lab{b1}\lab{b2}}] {};
					
					\node[fgnode] (A) at (0, 0) {$A$};
					\node[fgnode] (B) at (3, -1) {$B$};
					\node[fgnode] (C) at (3.5, 1.4) {$C$};
					\node[fgnode] (D) at (6, -1) {$D$};
					
					
					
					\node[factor] (f1) at (-2, 0){$\phi_1$};
					\node[factor] (f2) at (1.8,.4){$\phi_2$};
					\node[factor] (f3) at (1.3, -1.3){$\phi_3$};
					\node[factor] (f4) at (6, 1){$\phi_4$};
					
					
					\draw[thick] (f1) -- (A) -- (f2) -- (B) -- (f3) -- (A);
					\draw[thick] (C) -- (f2);
			\end{tikzpicture} }
			\caption{Factor Graph: exam scores}
			\label{fig:fg-exam}
		\end{figure}
		
		
		\begin{enumerate}[nosep]
%joe4
				  %		\item[$\phi_1$.] A. priori., Alice is 4 times
%                  as likely to pass as not, and so $\phi_1(a)
		\item[$\phi_1$.] \emph{A priori}, Alice is 4 times
				  as likely to pass as not, so $\phi_1(a)
						  = 4$ if $a = 1$, and 1 otherwise. 
\item[$\phi_2$.] Alice, Bob, and Clara
						  collaborated. Clara is very persuasive, and
						  Alice trusts her, so an outcome in which
						  everyone gets the same score is (a priori) 8 times more
%joe4: more likely than what?  One where they don't all get the same
%score?  If so, then Alice and Clara getting the same score can't be 4
%times more likely than them getting different scores.  I'm confused
%oli5: we discussed this in our meeting, but the correct interpetation is via energies. The relative likelihood holds locally and works if it doesn't interat with other factors---but the one I'm presenting is the only analogy to local graphical models we have.
%oli5: addressing "more likely than what"--added                          
						  likely than when each score is distinct,
%   
						  one in which only Alice and Clara
						  share a score is 4 times as likely, and one
						  in which only Bob and Clara share a score is
						  twice as likely. 
			\[ \phi_2(a,b,c) = \left\{\begin{aligned}
%joe4: redid layout to make it more standard
%oli5: That was a premature optimization to save space on my part, but this example is too bulky to make it into the short paper anyway.
				  %			8 &~~ \text{if~} a = b = c; 
							8 &~~ \text{if~} a = b = c;\\
										4 &~~ \text{if~}c = a \neq b; \\
				2 &~~ \text{if~}c = b \neq a;\\
				1 &~~ \text{otherwise.}
			\end{aligned}\right. \]
%joe4*: This seems inconsistent with the claim above that they're all
%more like to get the same score as not.  If this is intentional, you
%need to say someting about it.
%oli5: Perhaps I should do a better job of this earlier, but I describe below.
%joe5: I'm not reading that carefully at this point, because I
%currently think that this is the wrong story.
			  \item[$\phi_3$.] Alice thinks very poorly of
						  Bob, and ultimately reverses the answers to
						  all his questions; she's guarantee to fail
						  if he passes, and vice versa. $\phi_3(a,b) =
						  1$ if $a \neq b$ and 0 if $a=b$.
				%oli5: added. 
						  Note that this is
												  incompatible with
												  $\phi_2$, and so the
												  factor graph cannot
												  satisfy both
												  constraints
												  exactly. 
%joe4: I don't undersatnd the meaning of the isolated box \phi_4.  
%oli5: It's just a factor connected to zero variables. It has a value,
%but doesn't matter. Part of the point is that factor graphs encode a
%lot of useless information.
%joe5: If that's the point, you need to make it.
												\item[$\phi_4$.] The test is on factor graphs, which was unlikely, so $\phi_4() = 0.25$. This is true independent of anyone's scores, and doesn't bear on the distribution, so it will get normalized out.
		\end{enumerate}
		We don't know anything about David. The resulting distribution is given in \Cref{tab:fg-exam-dist}
		
		\begin{table}[h!]
			\renewcommand{\arraystretch}{1.15} 
			\centering
			\begin{tabular}{c|cc|cc}
				\multicolumn{1}{c}{}&\multicolumn{2}{c}{$a_0$} & \multicolumn{2}{c}{$a_1$} \\[-0.3em]
				&$c_0$ & $c_1$ & $c_0$ & $c_1$ \\\hline
				$b_0$&0 & 0 & .2667 & .5333 \\
				$b_1$&.1333 & .0667 & 0 & 0
			\end{tabular}
			
			\caption{The resulting distribution from \Cref{ex:fg-exam}}
			\label{tab:fg-exam-dist}
		\end{table}
		
		
		Note some features of this example:
		\begin{enumerate}[nosep]
%joe4: this should be mntion earlier (when you define \phi_3)
%joe4: this too should be mentioned earlier.  I noted it and was onfused.
%oli5: done.
				\item $\phi_3$ totally overrides the first case of $\phi_2$: The
  directions of an individual factor are just suggestions that are
  resolved globally. 
			%The intuition of relative likelihoods, only works locally.
\item Although $\phi_3$ was symmetric, our
						  story is not: Alice doesn't trust Bob, and
						  not the other way around. There is an
						  important distinction in the story (this
						  changes Alice's score, and not Bob's), but
						  this cannot be captured. 
%			To capture a conditional probability distributions, you need to impose \emph{local} normalization constraints \cite{frey2012extending}. In this case, this means insisting that  $\sum_{a} \phi_3(a,b) = 1$
			\item To get any marginal distribution such as $\Pr(B)$, you have to take into account every factor, including those such as $\phi_1$ that are not connected to $B$.
			\item To emphasize that a factor is more important, we cannot simply scale it, as the scaling will be normalized out; the only control available is to changing the variance of its items: setting things (close to) zero is the only way to ensure that the factor matters more than others.
		\end{enumerate}
	\end{example}
%joe4: this may be true, but it's irrelevant        
%	Generally, factor graphs are learned from data or translated
%        from another model, rather than specified by
		%        hand.  \Cref{ex:fg-exam} should make it clear why: there is a
As \Cref{ex:fg-exam} shows, there is a lot of freedom in specifying the factors, 
%oli5: added
	and very little in the way of locally interpretable semantics. 


	\subsection{DIRECTED FACTOR GRAPHS}
	
	One solution, by \cite{frey2012extending} is to also enforce some local constraints, in the form of a local normalization.  While this indeed solves issues \cref{fgproblem:undirected,fgproblem:global}, directed factor graphs still leave some bits of issues \cref{%fgproblem:corrob,
		fgproblem:reweight,fgproblem:volatile} unaddressed.
	
	Directed factor graphs are much more explicit with their factorizations than BNs, are as expected, even more closely related to PDGs. However, they too cannot capture scenarios such as \cref{ex:randomvars}. Consider example \ref{ex:directedfg}
	
	\begin{example}\label{ex:directedfg}
		\todo{Choose a different directed factor graph example that doesn't rely on sub-stochasticity}
	\end{example}
	
	

	
	
	\section{Structure-editing PDG Operations}
	
	While both PDGs and \MNH s are equivalent, and despite the fact that dealing with sets of variables is standard, we chose PDGs over \MNH s as the face of the paper. One of the primary reasons to do this is that it puts products on equal footing with other equally valid structural modifications we could have done instead, rather than specializing the definitions for products.
	
	\begin{enumerate}
		\item Latent variable nodes, e.g., through VAEs. Useful for representation learning and modeling bounded agents that just remember the gists of things.
		
		\item Sums nodes. For when one is being forced to chose between two options which might otherwise be unrelated, and the basic constructor for variables from points.
		
		\item Exponential nodes. Any positive temperature arrow can be reasoned about through expansion into its parameters.
		
		\item Compression nodes: e.g., truncation nodes for propositions. It may not matter exactly what proof you have so long as you've proved one exists. That a variable takes a value may be just as important as it.
	\end{enumerate}
	
	
	\section{More Examples}\label{sec:more-examples}
	
	\begin{example}
		\label{ex:corrob}
	\end{example}
	
	\begin{example}[Maximum Entropy with cpds is not the BN distribution]\label{ex:counterexample}
		Consider the Bayesian network 
		\begin{tikzcd}[cramped, sep=small]
			A \ar[r] & C & B \ar[l]
	 	\end{tikzcd}
		where $A$ and $B$ are binary, and $C$ can take $2^k$ values, including $c_0$. We now give the associated tables: both $A$ and $B$ get prior unconditional probabilities of $\nicefrac12$ apiece, and set $C$'s cpd to be
		\[
			\begin{idxmat}{{$a$,$b$},{$\bar a$, $b$},{$a$, $\bar b$},{$\bar a$, $\bar b$}}{$\Delta C$}
				\mathit{Uniform} \\ \delta_{c,c_0 }\\ \delta_{c,c_0} \\ \mathit{Uniform} \\
			\end{idxmat}
		\]
		where $\delta_{c,c_0}$ is the degenerate distribution that puts all mass on $c_0$. Looking at entropy, the uniform distribution on $C$ gets $k$ bits, and each of $A$ and $B$ we know each give one bit. 
		The semantics of a BN require that $A$ and $B$ are independent, since neither is a descendent of the other and neither has parents.  However, doing so results in a distribution of entropy $H(p) = 2 + k/2$ (one for each of the independent bits, and an expected k/2 bits from getting the uniform distribution on $C$ half the time), whereas if we correlate $A$ and $B$ so that they are always equal, we get $1 + k$ bits, one total bit from $A$ and $B$, and $k$ from $C | A,B$. For any finite $k$, this is still not the maximum entropy distribution, but it is much higher entropy than the one the BN suggests.
		
		Therefore the maximum entropy distribution consistent with the tables does not encode the independece assumption that a BN does. 
	\end{example}
	
	\begin{example}\label{ex:randomvars}
		Consider random variables $X_1$, $X_2$  on a set
                $\Omega$ of outcomes (distributed according to $p$),
                taking values in the set $\mathcal X$. This can be
                represented as the PDG below. 
		\begin{center}
			\scalebox{0.8}{
				\begin{tikzpicture}
				\node[dpadded] (1) at (0,0) {$\sf 1$};
				\node[dpadded] (W) at (2.5,0) {$\Omega$};
				\node[dpadded] (X1) at (5,1) {$X_1$};
				\node[dpadded] (X2) at (5,-1) {$X_2$};
				
				\draw[arr] (1) to node[fill=white]{$p$} (W);
				\draw[arr, ->>] (W) to node[fill=white]{$X_1$} (X1);
				\draw[arr, ->>] (W) to node[fill=white]{$X_2$} (X2);
				\draw[arr, gray] (X1) to node[right] {$p$} (X2);
				\end{tikzpicture}}
		\end{center}
		The setup so far, in black above, can be captured with a BN, but it is impossible to also articulate conditional probabilistic relations amongst the variables in the same time: in a BN, once we add a variable $\Omega$ which caracterizes all possible worlds as a parent of a variable (e.g., $X_2$), any other dependences will be irrelevant. Given a world $\omega$ and values of other variables, the cpd associated to $X_2$ would simply deterministically return the value of $X_2$ in $\omega$. 
		
		As a result, a BN has to choose between encoding conditional probabilistic information, and the knowledge of the complete information from $\Omega$. This is not true with a PDG, which makes it possible to simultaneously model the structure of the random variables around an agent's beliefs, in addition to the beliefs themselves.
	\end{example}

	
	\begin{vcat}
		\section{Categorical Presentation}
		\note{I will not put any time into this, as it's not going in the paper, but it's here as a placeholder, and I'll list some reasons why this is worth thinking about.}
		One reason this works out so nicely is every construction is universal. We can in fact give a simpler categorical presentation of PDGs for those who already know category theory. The highlights are as follows:
		\begin{enumerate}
			\item A PDG is an attention-shaped diagram in the Markov category. That is, functor from the free category generated by the graph $(\mathcal N, \Ed)$ representing attention, to the Markov category. Indeed $\mathcal V$ is the action on objects, assigning each $\mathcal N$ to a measurable set, $\bmu$ is the action on morphisms, sending edges in $\Ed$ to Markov kernels between their associated objects. 
			\begin{enumerate}
				\item Composition works out in general as we place no restrictions on anything, but
				\item If every edge in $\Ed$ represents the causal structure of their relationship, then the image of the resulting diagram will be flat, and so effectively there will only be at most one, belief, and no possibility of conflict.
				\item Interpreting with a different model of uncertainty (such as the powerset, giving us non-deterministic possibility) is simply an exchange of interpretation. However, for nice interaction with deterministic functions and logic, this notion of uncertainty must be a monad.
			\end{enumerate}
			
			\item This highlights the role of the ``qualitative'' and ``quantitative'' versions of this framework (which work out much more cleanly than for BNs in a categorical sense)
			
			\item A limit of this diagram is a space of worlds and all of the random variables as functions. A colimit is a the strongest thing that must be true according to the model (suspicion: this is somehow related to common knowledge). There is some strangeness about how samples work that I have not yet figured out.
		\end{enumerate}
		
		
		\section{Algebra}\label{sec:algebra}
		\begin{defn}
			If $\sigma$ is a signature, a $\sigma$-PDG $M'$ on a PDG $M=(\mathcal N, \Ed, \mathcal V, \mu)$ is a \modelname\ $(\mathcal N', \Ed', \mathcal V', \mu')$ such that
			\begin{itemize}
				\item $\mathcal N':= T_\sigma(\mathcal N)$ is the term algebra for the signature $\sigma$ over the alphabet $\Sigma = \mathcal N$.
				\item $\Ed' = \Ed \cup \Ed^\sigma$ is $\Ed$ extended with extra edges for operations that are 
			\end{itemize}
		\end{defn}
		
		\begin{example}
			content
		\end{example}		
	\end{vcat}
	
}
%joe7: \end{commentout}
% \end{notfocus}
\end{document}
