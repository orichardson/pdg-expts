% Created 2019-10-23 Wed 10:06
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
%\usepackage{rotating}
%\usepackage[normalem]{ulem}
\usepackage{amsmath}
%\usepackage{textcomp}
\usepackage{amssymb}
%\usepackage{capt-of}
\usepackage{hyperref}
\author{Oliver}
\date{\today}
\title{}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=magenta}
\hypersetup{
 pdfauthor={Oliver},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.2.5)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{Motivating Examples}
\label{sec:orgc34e51f}

\subsection{For Interpreting Arrows By Themselves, not guaranteeing independence}
\label{sec:org4e68bd2}
\subsubsection{Framing Problems?}
\label{sec:org79f8c21}
These are a better motivating example for preferences?

\subsubsection{Separate Sources of Information which merge}
\label{sec:orga594586}
\begin{enumerate}
\item Newcomb Problem
\label{sec:org8be0808}
\item Sun and Tanning Beds
\label{sec:orgf02b7ec}
\end{enumerate}

\subsection{For adding new variables}
\label{sec:orge4c6517}

\subsubsection{Keeping around old model while knowing the specific case}
\label{sec:org9c823b4}
Rain and office example? Or find a more convincing one?

\subsubsection{Discovering new correlations}
\label{sec:orgd8b07e8}
\subsubsection{For forgetting least important things}
\label{sec:orged40235}


\section{Why EDGs are good}
\label{sec:org5c3d212}

\subsection{Problem: sometimes you don't have a distribution. Sometimes overconstrained; other times under-constrained}
\label{sec:org2099cb0}

Mention: under-constrained = "discriminitive model" in ML; 
single distribution = "generative model". 
Over-constrained is a run-time error.

\subsection{Other graphical models represent single distributions, arguably not the best form for an epistemic state.}
\label{sec:org8af2731}

\subsection{We make it easy to add and remove nodes from the graph.}
\label{sec:orgf8dd297}

You can do this for a BN, but this is not normally done. We do everything by adding additional nodes to graphs and compressing / forgetting. 

Examples
\begin{itemize}
\item Update beliefs with new data by adding new node w / observation link, then using dempster's rule to combine
\item Alternatively, no need to ever throw out anything. Just get new data.
\item Model new correlations you're aware of by adding new links or variables
\item Abstraction: merging, compressing, and splitting nodes
\item This allows for new concepts that an agent can spontaneously form
\item \ldots{} in addition to those imposed by a modeler
\end{itemize}

\subsubsection{{\bfseries\sffamily TODO} (future: Connection to Single Static Assignment?)}
\label{sec:orgfe614e5}

\subsection{Can simulate BNs}
\label{sec:orge596a2a}
Max Entropy Center of consistent distribution. 

\section{Definitions and Semantics}
\label{sec:orgfc52c0d}
\subsection{Define an EDG:}
\label{sec:org992e627}
A directed graph with some type information in the nodes (e.g., products, internal homs), which force certain constraints to be true

\subsection{Interpretation of EDGs}
\label{sec:org01d23c4}
Interpretation = assignments of data to edges and nodes. Edge data: normally conditional (sub)probabilities;  

This lines up with the definition of a functor, a denotational semantics for programming languages, categorical diagrams, and the interpretations of logics / algebras, which is why I want to do it this way, rather than closer to the BN approach
\subsubsection{Example}
\label{sec:org207475a}

\subsubsection{Interpretation with sub-probability measures}
\label{sec:orge06a106}
\begin{enumerate}
\item Motivation: escaping, increasing expressive power by allowing disjunction, conditioning, making it possible to embed a logic
\label{sec:orge411fd3}
\item Additional effect: can now represent general factor graphs!!!
\label{sec:org553d1c7}
\item Mention relation to a particular (uninteresting) class of Dempster-Shafer Belief functions
\label{sec:org186f14f}
\end{enumerate}


\subsubsection{Example Dinky Interpretation:}
\label{sec:orgbacbca4}
Give edges (+ / -), show this is equivalent to the formalism of Epistemic Graphis in Hunter, Polberg, Poltia's paper.

Note: Many fewer parameters than a full conditional table, and just like 1/0 neurons.

Note: same structure as protien / ligand interaction graphs in endocrinology

Note: can also simulate this by giving binary variables and restricting to symmetric matrices on edges.


\subsubsection{Interpreting nodes as more than sets}
\label{sec:org45c4840}
(And then the arrows can be interpreted as \textasciitilde{}structure preserving operations, but I promise not to say the word functor). 

\begin{enumerate}
\item Ordered Sets
\label{sec:orgaea4330}
For things like preferences, relative liklihood, 

\item Convex / mixture spaces
\label{sec:orgeed01aa}
For things like utility domains, resource counters of other kinds, etc.

\item Weighted Points
\label{sec:org0911714}
For things like utilities, probabilities.
Note: these can be externalized in links the most obviously

\item Kernel-reproducing Hilbert Spaces
\label{sec:org09b19b4}
Places you want to use: need a stronger similarity metric for kernel learning
\end{enumerate}

\subsubsection{Externalization: most of these can be moved into their own nodes, with their own edges.}
\label{sec:org106e882}
Examples: utilities, probabilities are obvious. Ordered sets work like plausibility / preferences.
\ldots{} but then you still need at least one ordered node, one node with an RKHS, etc.,?

\subsubsection{Maybe for later: figure out what exactly needs to be true of the the target domain of interpretation for useful results to follow}
\label{sec:orgb68e1be}

\subsection{Partially Interpreted Models}
\label{sec:orgd0a09fb}
With part of the model filled in you can determine it to be inconsistent.


\subsection{Consistency Semantics:}
\label{sec:orga9b078b}
\subsubsection{Binary version of consistency results in set of distributions}
\label{sec:orgad1c4d5}
\subsubsection{Continuous one is a weighted set of distributions}
\label{sec:orga20c7d4}
\subsubsection{Taking the one with the highest consistency-entropy score interprets it as a single distribution.}
\label{sec:org716991c}
\begin{enumerate}
\item Conjecture: this results in normalizing a factor graph, and
\label{sec:orgd5f515e}
\end{enumerate}

\section{Reducing Inconsistency}
\label{sec:orgc9b4e8b}
\subsection{Discussion of Inconsistency}
\label{sec:org3240f42}
\begin{itemize}
\item Inconsistency is still bad but now we can model it.
\item It can happen simply by the world changing in subtle ways under your feet, so long as your model isn't 100\% causal
\end{itemize}

Consistency between node data is the local algorithm; if links can communicate through data, this is a message passing  / belief propagation algorithm; Global consistency between all links is hard to compute. 

Different degrees of inconsistency:

\begin{enumerate}
\item Exact Match, maximum entropy
\item Exact match, non-trivial correlations are not captured (\textasciitilde{}incomplete)
\item Have to change some things to match, but all correlations modeled (\textasciitilde{}unsound)
\item Infinite distance to match, but all correlationsm modeled (\textasciitilde{}very unsound)
\item Every link must be changed an infinite amount, totally incompatible with all. Also unmodeled correlations exist.
\end{enumerate}

\subsubsection{Examples for all of these are not too hard to produce, maybe not the best use of space.}
\label{sec:org662bb04}

\subsection{Consistency of node structure}
\label{sec:orga530a3b}
(this is degenerate for nodes that are just sets)

\subsubsection{Pairwise Consistency (local)}
\label{sec:org43131b8}
The original measure I presented: minimize distortion from chanels for all of the variables locally, but at once. 

\begin{enumerate}
\item {\bfseries\sffamily TODO} some analysis of this
\label{sec:org4a5f13d}
I suspect there's a strong connection when internal node data is expressed as an edge to another node
\end{enumerate}

\subsection{Consistency for Edges}
\label{sec:org5c73b11}
Given a joint distribution, and a model, how well does the distribution fit the model?

\begin{enumerate}
\item Pairwise Consistency (local)
\label{sec:org9e91757}
The original measure I presented: maximize information capacity of chanels locally. Can do this stochastically or get the full gradient at each time step.

\item Binary Consistency
\label{sec:org6ac07e9}
Is there a joint distribution consistent with constraints?

\begin{enumerate}
\item Note: hard constraint satisfaction problem with continuous probabilities illustrates that "soft" and "probabilistic" generalizations of CSPs are different.
\label{sec:org534d4d4}
\end{enumerate}

\item Continuous Consistency
\label{sec:orgea76b32}
Minimize some distance from metric. Relative entropy makes sure hard constraints don't move. 

\begin{enumerate}
\item Examples: show how the ones in the intro are resolved.
\label{sec:orgef89864}

\item {\bfseries\sffamily TODO} investigate whether variational distance does anything.
\label{sec:org69d2ce2}
\end{enumerate}
\end{enumerate}



\section{Simulating Other Things}
\label{sec:orgccdc494}
\subsection{Other Descriptions of Uncertainty}
\label{sec:orgff64b63}
\subsubsection{Probability (obviously)}
\label{sec:org94d0e30}
\subsubsection{Belief Functions}
\label{sec:orga1f5254}
\begin{enumerate}
\item {\bfseries\sffamily TODO} Can enforcment of belief function properties be done cleanly? Or am I just forcing everything I need to be true?
\label{sec:orgc39a807}
\end{enumerate}

\subsubsection{More generally, lower probability distributions}
\label{sec:orgbabe319}
(but requres a huge number of nodes to keep the whole thing, or alternatively a single node that explicitly tracks them)

\subsection{Expected Utility}
\label{sec:org46ae7cb}
(more in other paper. This is an excellent illustration of why composition is important, but maybe better left to the other paper?)

\subsection{Simulation of BNs with max entropy}
\label{sec:org08ab9ef}
\subsubsection{Conversion from BN to EDG in constant space}
\label{sec:org5c2e7da}
\subsubsection{Theorem: Center of this EDG is the distribution encoded}
\label{sec:org6c9d1d4}

\subsection{Belief Updating}
\label{sec:org1db0534}
\subsubsection{Regular Conditioning}
\label{sec:org698f068}
\subsubsection{Jeffrey's Rule by minimizing inconsistency}
\label{sec:org0fa57a0}
Reference and analyze more carefully Dietrich List Bradley paper
\subsubsection{Pearl's Rule by adding nodes and then minimizing inconsistency}
\label{sec:orgf68dfca}

\subsection{Constraint Satisfaction Problems}
\label{sec:orgc245b4d}
Factor Graphs are generalizations of them, except they encode good heuristics as well. However, people think of them as representing distributions, which is only a small part of what they represent. 

\subsubsection{Problem with Factor Graphs: Normalization done globally so you can't control things that happen.}
\label{sec:orga557052}
\begin{enumerate}
\item Security / voting analogy:
\label{sec:org4c02452}
anyone can throw off and totally change your normalization to an arbitrary value if they go after you. The most recent factor can make anything happen to non-zero probabilities. 

\item Voting Example
\label{sec:orgf70c2f8}
\end{enumerate}

\subsubsection{Richer picture of inconsistency than factor graphs because I can actually see how far off each constraint is off and assign blame properly.}
\label{sec:orgdaf476c}

\begin{enumerate}
\item Example: anything with nesting constraints
\label{sec:orga95eee7}
such as any example that's compelling for a  Dempster-Shafer belief function that's not also a probability
\end{enumerate}

\section{Learning Problems}
\label{sec:org46cafc1}

\subsection{Learning your BN online from observations}
\label{sec:org23eb6d8}
Worse fit than learning directly, better fit than 

Related: Learning by fitting a BN, and then marginalizing

\subsection{Supervised Learning in this framework, with losses}
\label{sec:orge5e1462}
\subsubsection{Illustration that the addition of additional losses outside can be done exactly once,}
\label{sec:org547f324}
And for the log liklihood loss, adding it explicitly as a node (internalizing it) does not change the outside-level loss (I think. TODO: make sure this is true)

\subsection{View DNNs as instances of this model}
\label{sec:orgb2e6b41}
At multiple abstractions:
\begin{enumerate}
\item Each neuron is a node
\item Each layer is a node
\item Whole network can be put together
\end{enumerate}

For non feed-forward architectures, things get interesting: non-dense connections encode lack of known dependence (though obviously there are dependences) 

Note: skip-layer connections result in merges! They are resolved with a sum or product + renormalization

\textbf{Conjecture}: this is the minimization of local inconsistency in some sense

But note: this is not even close to the semantics of a BN.

\subsubsection{{\bfseries\sffamily TODO} ? Does recognizing this inconsistency rather than normalizing it away protect from going off of}
\label{sec:org21d2f48}
This could be tested empirically.

\subsection{Abstraction}
\label{sec:org3f3e7a6}
\subsubsection{Talk about information compression}
\label{sec:org941d81c}
reducing the complexity of the interpretation of a variable, dropping the variable completely, dropping links, etc. (to be within some resource bound?)

\subsubsection{{\bfseries\sffamily TODO} Relate to Fixing a Broken Elbo paper}
\label{sec:orgaf2510c}
Compression by looking at rate-distortion can be thought of as a bounded best approximation to a path of two elements, and the minimization of the appropriate relative entropies seems simililar to my metric of inconsistency.

\section{Properties}
\label{sec:orgc9a0782}

\subsection{Theorem: NP-hard to minimize consistency in general.}
\label{sec:org04b205d}

\subsection{Belief Propagation}
\label{sec:orgd277982}
\subsubsection{can be done just like in a BN.}
\label{sec:orgc7ce18a}
\subsubsection{The sum-product or max-product algorithms can be implemented without additional space if we enrich our sets to be weighted.}
\label{sec:orgac96580}
\subsubsection{Conjecture: has a guaranteed convergence rate for sub-distributions.}
\label{sec:org15fe9ba}

\subsection{Information Theoretical View}
\label{sec:orga00c3c1}
Beliefs as noisy information chanels connecting concepts. Can now look at information capacity (informs whether you want to keep it), encoding / decoding problems, rate / distortion, etc.

\subsection{Thermodynamics Analogy}
\label{sec:org913b80c}

\subsubsection{Minimizing Lexicographical Free Energy gives a distribution.}
\label{sec:org26f2bbc}
\begin{enumerate}
\item In particular, this computes normalization constant and hence the distribution for factor graphs
\label{sec:org5d0bbf8}
\end{enumerate}

\subsubsection{Setting a positive temperature allows for a trade-off between inconsistency and entropy.}
\label{sec:org28ea09e}
This means you are now allowed to change your beliefs in order to maintain consistency and a smaller description size.

Note that if you're not allowed to pay in consistency, then the best fit distribution will marginalize out to whatever you have. In this case, the best fit could be different from your current beliefs. 

\begin{enumerate}
\item Conjecture: there are stable points to this process that are not globally maximum entropy
\label{sec:orge38c5cf}
\end{enumerate}




\section{Background (write at end)}
\label{sec:orgb093ab4}
\subsection{Bayesian Networks}
\label{sec:org3cb17cd}
\subsubsection{Belief Propogation}
\label{sec:org5ec6591}
\subsection{Markov Networks (MRFs)}
\label{sec:org34e6552}
\subsubsection{Relation to Gibbs Random Fields, Hammersly Clifford Theorem}
\label{sec:org3976383}
\subsubsection{Normalization NP-hard}
\label{sec:orgaded80e}
\subsection{Factor Graphs (alternative characterization of most MRFs)}
\label{sec:orge53720c}
\end{document}