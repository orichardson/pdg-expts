
	
%%% ATTEMPT 1
%As we will see, one reason these inconsistencies are difficult to avoid is our conceptual modularity: features of the world can be noticed, forgotten, and fragments of beliefs can be locally recombined long before they crystallize into global distributions or theorems.	
	
%%% ATTEMPT 2
%Separately, people enjoy conceptual modularity: features of the world (like the random variables which generate a graphical model) can be noticed, forgotten, and beliefs about their dependencies can be incorporated into our mental states without all of one's distantly unrelated beliefs.
%These two features are in fact related, and we present here a representation of uncertainty which models both.






%	Standard notions of uncertainty, including probability \cite{halpern2017reasoning}}, take for granted a set of possible worlds; approaches such as language-based games, and
%	We can exploit the absence of a global consistency mandate to entirely remove the computational overhead and assumptions required to make structural changes to the graph (sometimes incurring inconsistency). 
%	Despite their flexibility, \MNs\ perhaps counter-intuitively offer a way to recognize when something has gone wrong, and 


\subsection{Useful Avenues of Empirical study}

\begin{enumerate}
	\item Figure out how to empirically measure some kind of inconsistency, and lots of imagined correlates, such as amount of indecision, other people taking advantage of you, environments that would encourage ``double-think''. If there's a robust, multi-feature correlation between IQ and inconsistency, one concludes that additional mental power does not lead to coherence, and therefore some logical limits may not be as relevant as previously thought.
	\item 
\end{enumerate}



\subsection{HOW TO THINK OF PDGS}
\begin{itemize}
	\item A bayesian network with explicit higher order edges
	\item A vectorized / bundled version of conditional probability spaces that includes torsion
	\item An attention-shaped diagram into the Markov Category
	\item A second-order constraint on worlds that allows you to modify free energies.
\end{itemize}
\subsection{A LIST OF PDG BENEFITS}\label{sec:list-of-benefits}
\todo{Remove and refactor into appendix}
\begin{enumerate}[nosep]
	\item PDGs can represent both over-constrained and under-constrained mental states. 
	\item In particular, they may be inconsistent, which gives agents using PDGs the qualitatively new kind of `epistemic modesty': the possibility of realizing that something is wrong with their beliefs.
	\item Many standard algorithms, including as belief propogation, conditioning, and belief revision, can be regarded as resolution of inconsistency.
	\item PDGs can emulate the functionality of other graphical graphical models.
	\item PDGs are more modular, making it much less invasive to combine, reduce, or partially interpret parts of the model, compared to alternatives.
	\item The modularity enables type-forming rules which can be used to implement deductive inference.
	\item The many standard ways of adding and eliminating variables provides an answer to the question, ``why these possible worlds?''
	\item Compared with a standard constraint satisfaction problem, individual components have of have limited impact on the semantics.
	\item The class of free energies defined by PDGs is strictly more expressive than those given by alternative graphical models.
\end{enumerate} % trade-off: harder to analyze.
