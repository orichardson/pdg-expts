% !TeX TXS-program:bibliography = txs:///bibtex
%oli8: this line allows me to use plain BibTex without reconfiguring
%my own settings. 

%BEGIN_FOLD  AAAI-preamble
\def\year{2021}\relax
% File: formatting-instruction.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai21} % DO NOT CHANGE THIS
\usepackage{times} % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier} % DO NOT CHANGE THIS
\usepackage[hyphens]{url} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{natbib} % DO NOT CHANGE THIS OR ADD OPTIONS
\usepackage{caption} % DO NOT CHANGE THIS OR ADD OPTIONS
\frenchspacing % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case.
% No accents or commands. Retain the parentheses.
\pdfinfo{
/Title (Probabilisti Dependency Graphs)
/Author (Oliver Richardson, Joseph Halpern)
/TemplateVersion (2021.1)
}
%oli21: enabling subsection numbers (0: no numbers, 1: section, 2: subsection)
\setcounter{secnumdepth}{2} 
%END_FOLD   AAAI-preamble
%BEGIN_FOLD Tikz Stylings.
\usepackage{tikz}
	\usetikzlibrary{positioning,fit,calc, decorations, arrows, shapes, shapes.geometric}
	\usetikzlibrary{backgrounds}
	\usetikzlibrary{patterns}
	\usetikzlibrary{cd}
	
	\pgfdeclaredecoration{arrows}{draw}{
		\state{draw}[width=\pgfdecoratedinputsegmentlength]{%
			\path [every arrow subpath/.try] \pgfextra{%
				\pgfpathmoveto{\pgfpointdecoratedinputsegmentfirst}%
				\pgfpathlineto{\pgfpointdecoratedinputsegmentlast}%
			};
	}}
	%%%%%%%%%%%%
	\tikzset{AmpRep/.style={ampersand replacement=\&}}
	\tikzset{center base/.style={baseline={([yshift=-.8ex]current bounding box.center)}}}
	\tikzset{paperfig/.style={center base,scale=0.9, every node/.style={transform shape}}}

	\tikzset{is bn/.style={background rectangle/.style={fill=blue!35,opacity=0.3, rounded corners=5},show background rectangle}}
	% Node Stylings
	\tikzset{dpadded/.style={rounded corners=2, inner sep=0.7em, draw, outer sep=0.3em, fill={black!50}, fill opacity=0.08, text opacity=1}}
	\tikzset{dpad0/.style={outer sep=0.05em, inner sep=0.3em, draw=gray!75, rounded corners=4, fill=black!08, fill opacity=1}}
	\tikzset{dpad/.style args={#1}{every matrix/.append style={nodes={dpadded, #1}}}}
	\tikzset{light pad/.style={outer sep=0.2em, inner sep=0.5em, draw=gray!50}}
		
	\tikzset{arr/.style={draw, ->, thick, shorten <=3pt, shorten >=3pt}}
	\tikzset{arr0/.style={draw, ->, thick, shorten <=0pt, shorten >=0pt}}
	\tikzset{arr1/.style={draw, ->, thick, shorten <=1pt, shorten >=1pt}}
	\tikzset{arr2/.style={draw, ->, thick, shorten <=2pt, shorten >=2pt}}
	\tikzset{archain/.style args={#1}{arr, every arrow subpath/.style={draw,arr, #1}, decoration=arrows, decorate}}


	\tikzset{fgnode/.style={dpadded,inner sep=0.6em, circle},
	factor/.style={light pad, fill=black}}	
	
	
	\newcommand\cmergearr[4]{
		\draw[arr,-] (#1) -- (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\mergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1,#2=1,#3=1.2);
		\cmergearr{#1}{#2}{#3}{center-#1#2#3}
	}
	\newcommand\cunmergearr[4]{
		\draw[arr,-, , shorten >=0] (#1) -- (#4);
		\draw[arr, shorten <=0] (#4) -- (#2);
		\draw[arr, shorten <=0] (#4) -- (#3);
	}
	\newcommand\unmergearr[3]{
		\coordinate (center-#1#2#3) at (barycentric cs:#1=1.2,#2=1,#3=1);
		\cunmergearr{#1}{#2}{#3}{center-#1#2#3}
	}

	
	\usetikzlibrary{matrix}
	\tikzset{toprule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] 
	            (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east);%
	        }
	    },
	    bottomrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    leftrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north west) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south west);%
	        }
	    },
	    rightrule/.style={%
	        execute at end cell={%
	            \draw [line cap=rect,#1] (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.north east) -- (\tikzmatrixname-\the\pgfmatrixcurrentrow-\the\pgfmatrixcurrentcolumn.south east);%
	        }
	    },
	    table with head/.style={
		    matrix of nodes,
		    row sep=-\pgflinewidth,
		    column sep=-\pgflinewidth,
		    nodes={rectangle,minimum width=2.5em, outer sep=0pt},
		    row 1/.style={toprule=thick, bottomrule},
  	    }
	}
    %oli8: disable tikz externalization for now, dependence on etoolbox
\newif\ifprecompilefigs
\precompilefigstrue


\ifprecompilefigs\else
	\usetikzlibrary{external}
	\tikzexternalize[prefix=tikz/]  % activate!
	% \usepackage{etoolbox}
	%  \AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
	%  \AtEndEnvironment{tikzcd}{\tikzexternalenable}
\fi
%END_FOLD


%BEGIN_FOLD: Theorems and Tools

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}		%also loads amsmath
\usepackage{amssymb, bbm}

%oli20: oops, this is vorboten :(
% \usepackage[format=plain,
%             labelfont={sl},
%             textfont={it,small}]{caption}

\usepackage{relsize}
\usepackage{environ} % http://ctan.org/pkg/environ; for capturing body as a parameter for idxmats

\usepackage{color}
%\usepackage{stmaryrd}

\usepackage{amsthm}
\usepackage{thmtools}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{coro}{Corollary}[theorem]
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}

\theoremstyle{definition}

% no section numbers for theorems in AAAI style ... 
%joe17: can you reinstate this?
%oli20:done
\declaretheorem[name=Definition,qed=$\square$,numberwithin=section]{defn} %
\declaretheorem[name=Construction,qed=$\square$,sibling=defn]{constr}
\declaretheorem[qed=$\square$]{example}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{xstring}
\usepackage{enumitem}

\input{labelmatrix.tex}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%oli20: apparently this is not allowed in AAAI style.
%oli21: but it helps me edit so I'm reenabling it until later
%oli24: the time has come... goodbye links and colors :(
% \usepackage{hyperref}
% \definecolor{deepgreen}{rgb}{0,0.5,0}
% \hypersetup{colorlinks=true, linkcolor=blue!50!black, urlcolor=magenta, citecolor=deepgreen}

\usepackage[noabbrev,nameinlink,capitalize]{cleveref}
\crefname{example}{Example}{Examples}
\crefname{defn}{Definition}{Definitions}
\crefname{prop}{Proposition}{Propositions}
\crefname{constr}{Construction}{Constructions}
\crefname{conj}{Conjecture}{Conjectures}
\crefname{fact}{Fact}{Facts}
%\crefname{section}{\S\!}{\S\!}


\usepackage{float}
% \usepackage{subcaption}
\newcounter{subfigure}
	% \captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
	\renewcommand\thesubfigure{\thefigure(\alph{subfigure})}
    
\newenvironment{old}[1]{\par\noindent{\bf \Cref{#1}.} \em \noindent}{\par\medskip}

\usepackage{xpatch}
\makeatletter
\xpatchcmd{\thmt@restatable}% Edit \thmt@restatable
   {\csname #2\@xa\endcsname\ifx\@nx#1\@nx\else[{#1}]\fi}% Replace this code
   % {\ifthmt@thisistheone\csname #2\@xa\endcsname\typeout{oiii[#1;#2\@xa;#3;\csname thmt@stored@#3\endcsname]}\ifx\@nx#1\@nx\else[#1]\fi\else\csname #2\@xa\endcsname\fi}% with this code
   {\ifthmt@thisistheone\csname #2\@xa\endcsname\ifx\@nx#1\@nx\else[{#1}]\fi
   \else\fi}
   {\typeout{oii Success1?}}{\typeout{oiii failure1?}} % execute code for success/failure instances
\xpatchcmd{\thmt@restatable}% Edit \thmt@restatable
   {\csname end#2\endcsname}
   {\ifthmt@thisistheone\csname end#2\endcsname\else\fi}
   {\typeout{oii Success2?}}{\typeout{oiii failure2?}}
\newcommand{\recall}[1]{\medskip\par\noindent{\bf \expandarg\Cref{thmt@@#1}.} \begingroup\em \noindent
   \expandafter\csname#1\endcsname* \endgroup\par\smallskip}
\makeatother
%oli16: The extra space was because there was extra space in the paragraph, not
%because this length was too big. By breaking arrays, everything will be better.
\allowdisplaybreaks

\newcommand{\begthm}[3][]{\begin{#2}[{name=#1},restate=#3,label=#3]}

%TODO
\newcommand{\createversion}[2][{gray}{0.75}]{
	\definecolor{v#2color}#1\relax
    \expandafter\xdef\csname v#2on\endcsname{%
		% \xdef\gamma{\tau}%
		% \expandafter\renewcommand\csname v#2\endcsname{ONN}%
		% \expandafter\xdef{\csname v#2on\endcsname}##{{\color{v##2color} #1}}
	}
	\expandafter\xdef\csname v#2off\endcsname{
	% 	\expandafter\newcommand\csname v #2\endcsname[1]{{\color{v ##2 color} #1}}
	}
}
\createversion{test}
% \vteston
%END_FOLD

%BEGIN_FOLD   %%%% Version knobs %%%%%. 
%oli20: your commenting system is better than the one based on comment package, 
% which is way more problematic than I thought.
% I'm killing it and refactoring all comments to be like yours. I'm not annotating
% everything I'm doing here but the result will be way clearer and less problematic.

\definecolor{vfullcolor}{gray}{0.7}
\newcommand\vfull[1]{{\color{vfullcolor} #1}}
\renewcommand\vfull[1]{} % disable vfull

\definecolor{vleftoverscolor}{gray}{0.85}
\newcommand{\vleftovers}[1]{{\color{vleftoverscolor} #1}} 
\renewcommand{\vleftovers}[1]{} %disable vleftovers

\definecolor{notationcolor}{rgb}{0.9,0.9,.9} 
\newcommand{\notation}[1]{{\color{notationcolor} #1}}
\renewcommand{\notation}[1]{\ignorespaces} % disable notation

\definecolor{contentiouscolor}{rgb}{0.7,0.3,.1} 
\newcommand{\commentout}[1]{\ignorespaces} 

% \newcommand{\contentious}[1]{
% 	\noindent\colorbox{red!10!white}{\parbox{\linewidth-3pt}{\color{red!10!black}#1}}}
% \newcommand{\valpha}[1]{%
% 	% \colorbox{red!10!white}
% 	{\color{red!10!black}{#1}}%
% }
% \newcommand{\valpha}[1]{{\color{red!80!black}#1}}
\newcommand{\valpha}[1]{#1}

%END_FOLD


%BEGIN_FOLD definitions
%BEGIN_FOLD %%%%%   general shorthand I use   %%%%%%%%%%%%%%%%%

%\usepackage{stmaryrd}
%\DeclarePairedDelimiter{\ccbr}{\lBrace}{\rBrace}
%\DeclarePairedDelimiter{\bbr}{\llbracket}{\rrbracket}
%\DeclarePairedDelimiter{\ppr}{\llparenthesis}{\rrparenthesis}

\DeclarePairedDelimiterX{\bbr}[1]{[}{]}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\let\Horig\H
\let\H\relax
\DeclareMathOperator{\H}{\mathrm{H}} % Entropy
\DeclareMathOperator{\I}{\mathrm{I}} % Information
\DeclareMathOperator*{\Ex}{\mathbb{E}} % Expectation
\DeclareMathOperator*{\argmin}{arg\;min}
\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}} % Conditional Independence
\newcommand\mat[1]{\mathbf{#1}}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
	#1\;\delimsize\|\;#2%
}
\newcommand{\thickD}{I\mkern-8muD}
\newcommand{\kldiv}{\thickD\infdivx}


\newcommand{\todo}[1]{{\color{red}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{todo:} #1}\ \!\smash{\textbf{]}}}}
\newcommand{\note}[1]{{\color{blue}\ \!\Large\smash{\textbf{[}}{\normalsize\textsc{note:} #1}\ \!\smash{\textbf{]}}}}



% SPACES
\newcommand\Set{\mathbb{S}\mathrm{et}}
\newcommand\FinSet{\mathbb{F}\mathrm{in}\mathrm{S}\mathrm{et}}
\newcommand\Meas{\mathbb{M}\mathrm{eas}}
\newcommand\two{\mathbbm 2}

%END_FOLD

%BEGIN_FOLD %%%%%    PDG-specific macros     %%%%%%%%%%%%%%%%
\DeclarePairedDelimiterXPP{\SD}[1]{}{[}{]}{_{\text{sd}}}{\mspace{-3.5mu}\delimsize[#1\delimsize]\mspace{-3.5mu}}
		
%\usepackage{stmaryrd}
%\newcommand{\none}{\varobslash}
\newcommand{\none}{\bullet}

\def\sheq{\!=\!}
\DeclareMathOperator\dcap{\mathop{\dot\cap}}
\newcommand{\tto}{\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}

\newcommand{\bp}[1][L]{\mat{p}_{\!_{#1}\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\pdgvars}[1][]{(\N#1, \Ed#1, \V#1, \mat p#1, \beta#1)}


\DeclareMathAlphabet{\mathdcal}{U}{dutchcal}{m}{n}
\DeclareMathAlphabet{\mathbdcal}{U}{dutchcal}{b}{n}
%joe1:out of curiousity, why not use \mathcal?  That's what you use
%for BNs.  Why do PDG use a different font?
\newcommand{\dg}[1]{\mathbdcal{#1}}
\newcommand{\var}[1]{\mathsf{#1}}
\newcommand\Pa{\mathbf{Pa}}

%oli20: better spacing
% \newcommand{\IDef}[1]{\mathit{IDef}_{#1}}
\newcommand{\IDef}[1]{\mathit{IDef}_{\!#1}}

\newcommand\Inc{\mathit{Inc}}
\newcommand{\PDGof}[1]{{\dg M}_{#1}}
%oli22: a macro for unweighted PDGs
\newcommand{\UPDGof}[1]{{\dg N}_{#1}}
\newcommand{\WFGof}[1]{\Psi_{{#1}}}
%oli22: and for unweighted ones
\newcommand{\FGof}[1]{\Phi_{{#1}}}
%oli22: want to refer to the variable graph
\newcommand{\Gr}{\mathcal G}
%oli22: Gibbs Free Energy conflicts with \mathcal G. Now it lives in 
% this macro.
\newcommand\GFE{\mathit{G\mkern-4mu F\mkern-4.5mu E}}
%oli22: Right now we're using (\N, \V) to refer to variables of a
% PDG. I think it's important to keep both \N and \V but am happy to change
% the way we combine them. Factored into a macro so it's easier to change.
\newcommand{\varsNV}[1][\N,\V]{(#1)}


% \makeatletter %Arguments: L, X, Y, \scriptscriptstyle, -1pt (for raisebox)
% \newcommand{\ed@helper}[5]{#2\!%
%   \overset{\smash{\mskip-5mu\raisebox{-1pt}{$\scriptscriptstyle
%         #1$}}}{\rightarrow}\! #3} 
% \makeatother

%oli22: the edge notation is now uniform. Choose between the following
% Default: display as "X -L-> Y" (leave uncommented).
\newcommand{\ed}[3]{#2\!%
  \overset{\smash{\mskip-5mu\raisebox{-1pt}{$\scriptscriptstyle
        #1$}}}{\rightarrow}\! #3} 
% Option: uncomment to display as  "L = (X,Y,\ell)" instead.
% \renewcommand{\ed}[3]{#2 = (#1,#3,\ell)} 
\newcommand{\alle}[1][L]{_{ \ed {#1}XY}}
%END_FOLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%\addbibresource{../refs.bib}
%\addbibresource{../uncertainty.bib}
%\addbibresource{../maths.bib}
%\addbibresource{graphical-models.bib}

\title{Probabilistic Dependency Graphs}
\author{
	Oliver Richardson, Joseph Halpern \\
}
\affiliations {
	Computer Science Dept. \\
	Cornell University
}

%END_FOLD
\begin{document}
\maketitle
\begin{abstract}
We introduce Probabilistic Dependency Graphs (PDGs), a new class of
directed graphical models.   PDGs can capture inconsistent beliefs in a
natural way and are more modular than Bayesian Networks (BNs), in that
they make it easier to incorporate new information and restructure the  
representation.    We show by example how PDGs are an especially natural
modeling tool.
We provide three semantics for PDGs, each of which can be derived from a
%oli22: inserted "parameterized"; 
%joe20: cut; we don't give a hint here as to how it is paramterized,
%so why introduce the word and confuse the reader
%parameterized
scoring function (on joint distributions over the
variables in the network) that can be viewed as representing a
distribution's incompatibility with the PDG.
%joe17*: why did you rewrite this?  I think that the intuition of
%incompatibility is useful.  Also note while the first semantics  can
%be viewed as being derived from a scoring function, I don't think
%it's the best way to think about it.  I didn't change it back to
%avoid a series of back and forths, but unless you have a compelling
%reason for the change, I would prefer to do that
%oli20: I initially had more dramatic changes which I evidently walked back before sending you; after re-reading, I'm happy to reinstate the original.
%oli19: rewritten:
% We provide three semantics for PDGs, each derived from an
% information-theoretically motivated scoring function (on joint
% distributions over the network's variables)
For the PDG corresponding
to a BN, this function is uniquely minimized by the distribution the
BN represents, showing that PDG semantics extend BN semantics.  
We show further that factor graphs
%oli24: inserted
and their exponential families
can also be faithfully represented as PDGs
%oli24: important
%.
, and explain why, in general, they are different.
%oli22: added last part. You said you didn't like this before,
% but now the document supports this claim to a much larger degree.
%joe20: Cut; I didn't undersatnd what this meant before, and I still
%don't understand it now.
%but only when the trade-off parameter has been fixed, so that the qualitative 
%and quantitative scores cannot be separated.
%oli22: TODO: add something about DN's; the below is what I suggested before
%joe20: only if we have solid proofs to back up the claim
% Finally, we relate PDGs to Dependency Networks, and point out that the PDGs 
% provide many of the same benefits, but are much more expressive. 
\end{abstract}

\section{Introduction}

In this paper we introduce yet another graphical for modeling beliefs,
\emph{Probabilistic Dependency Graphs} (PDGs). There are already many
such models in the literature, including Bayesian networks (BNs) and
factor graphs. (For an overview, see \citeauthor{KF09}.)
Why does the world need one more?  

Our original motivation for introducing PDGs was to be able capture
inconsistency. We want to be able to model the process of resolving
inconsistency; to do so, we have to model the inconsistency itself. But our
approach to modeling inconsistency has many other advantages. In particular,
PDGs are significantly more modular than other directed graphical models:
operations like restriction and union that are easily done with PDGs are
difficult or impossible to do with other representations.

We start with some examples to motivate PDGs and illustrate some of these properties.  

\begin{example} \label{ex:guns-and-floomps}
Grok is visiting a neighboring district. From prior reading, she thinks it likely (probability
.95) that guns are illegal here. Some brief conversations with locals lead her to believe believe with
probility .1, that the law prohibits floomps.

% The obvious way to represent this as a BN involves two binary random variables,
% $F$ (taking values $\{f, \overline f\}$), indicating the legality of floomps,
% and $G$ (taking values $g, \overline g$) indicating the legality of guns. 
The obvious way to represent this as a BN is to use two random variables
$F$ and $G$ (respectively taking values $\{f, \smash{\overline f}\}$ and $g, \overline g$), indicating the respective legalities of owning floomps and guns.
%oli12 no paragraph break here.
The semantics of a 
%oli12
% Bayes Net
BN
offer her two choices: either assume that $F$ and $G$
% are independent and give (unconditional) probabilities of $F$ and $G$, or we
to be independent and give (unconditional) probabilities of $F$ and $G$, or
choose a direction of dependency, and give one of the two unconditional
probabilities and a conditional probability distribution. 
%oli12:
% As there is no reason
% to believe that either variable depends on the other, 
As there is no reason to choose either direction of dependence, the
natural choice is to 
assume independence, giving her the 
%oli12: combining figures
BN on the left of \Cref{fig:gun-floomp-diagram}.
%following BN

\begin{figure}[htb]
%joe11*: why are some parts of the figure in light gray?  I would prefer
%to make it all black.  If we use a different color, we have to
%explain why.  
  \centering
\ifprecompilefigs
	\raisebox{-0.5\height}{\includegraphics[scale=0.8]{figure-pdfs/fg-BN}}
	~\vrule~
	\raisebox{-0.5\height}{\includegraphics[scale=0.8]{figure-pdfs/fg-PDG}}
\else
	\scalebox{0.8}{
    \begin{tikzpicture}[center base, scale=0.7, AmpRep]
        % \def\figtabledist{1.4}
        % \def\fignodedist{1.2}
        % \def\figtableheight{0.22}
        \def\figtabledist{0.2}
        \def\fignodedist{1.4}
        \def\figtableheight{0.41} 

        \matrix [table with head, column 1/.style={leftrule}, anchor=south east,
             column 2/.style={rightrule}, row 2/.style={bottomrule}] at (-\figtabledist,\figtableheight) {
            \vphantom{$\overline fg$} $f$ \& \vphantom{$\overline fg$}$\overline f$\\
            .9 \& .1\\
        };
        \matrix [table with head, column 1/.style={leftrule}, anchor=south west,
             column 2/.style={rightrule}, row 2/.style={bottomrule}] at (\figtabledist,\figtableheight) {
             \vphantom{$\overline fg$}$g$ \& \vphantom{$\overline fg$}$\overline g$\\
             .05 \& .95\\
        };
        \node[dpadded, circle, fill=black!08, fill opacity=1] (floomp) at (-\fignodedist,0) {$F$};
        \node[dpadded, circle, fill=black!08, fill opacity=1] (gun) at (\fignodedist,0) {$G$};
    \end{tikzpicture}
    ~~\vrule~~
	\begin{tikzpicture}[center base]

        \def\fignodedist{2.1}
        \def\fignodeheight{1.1}
        \def\newcptX{-0.3}
        \def\newcptY{-0.1}
                     
		\node[dpadded, fill=white, draw=gray] (true)  at (0,1.8) {$\var 1$};
		\node[dpadded] (floomp) at (-\fignodedist,\fignodeheight) {$F$};
		\node[dpadded] (gun) at (\fignodedist,\fignodeheight) {$G$};			
		
		\draw[arr] (true) to[bend left=0] coordinate(A) (floomp);
		\draw[arr] (true) to[bend right=0] coordinate(B) (gun);

		\node[above left=2.0em and 1.5em of A, anchor=center] {
        %oli14 fix gray.
			% \begin{idxmat}{\!\!\!$\star$\;\;\;}{$f$, $\overline f$}
			\begin{idxmat}[\color{black}\smalltext]{\!\!\!$\star$\;\;\;}{$f$, $\overline f$}
				.90 & .10 \\
			\end{idxmat}
		};
		\node[above right=2.0em and 1.3em of B, anchor=center] {
        %oli14
        % \begin{idxmat}{\!\!\!$\star$}{$g$, $\overline g$}
			\begin{idxmat}[\color{black}\smalltext]{\!\!\!$\star$}{$g$, $\overline g$}
				.05 & .95 \\
			\end{idxmat}
		};
		\definecolor{heldout}{rgb}{0.6, 0.6, .6}	
		\draw[heldout, dashed, arr] (floomp.-30) to[bend right=7] node[pos=0.65, fill=white, inner sep=2pt] (C) {$\smash{p}\vphantom{v}$} (gun.210);
        %oli12: addes reverse arrow, edited the above line with a yshift.
        \draw[heldout, dashed, arr] (gun.190) to[bend left=5] node[pos=0.668, fill=white, inner sep=2pt] {$\smash{p'}\vphantom{v}$} (floomp.-10);
		\node[anchor=center] (newcpd) at (\newcptX,\newcptY) {
			\color{heldout}
			$\mat p =\!\!\!$\begin{idxmat}[\color{heldout}\smalltext]{$f$,$\overline f$}{$g$, $\overline g$}
%joe12
%			  .92 & 0.08 \\ .08 & .92 \\
			  .92 & .08 \\ .08 & .92 \\
          \end{idxmat}$~=~{\mat p'^{\textsf T}}$
		};
        % \node[below=3pt of newcpd] {\color{heldout}$\mat p' = \mat p^{\textsf T}$};
	\end{tikzpicture}
	}
\fi
    %oli12 update caption accordingly.
    %oli12: note before editing caption: making it fit on one line is not easy.
	% \caption{An inconsistent PDG, requiring resolution}
%joe11
%        \caption{A BN (left), and respective PDG (right), which can
%oli19: added word "simple" BN
%joe17: why did you add it?  What is a simple BN?
%oli20: Nothing technical. It doesn't look very much like a BN and I
%wanted to assure  
% readers that nothing strange is going on here. I trust your judgement and gether you
% think it's negative, and so am pre-emptively reverting it.
        \caption{A BN (left) and corresponding PDG (right), which can
        include more cpds; $p$ or $p'$ make it inconsistent.} 
    \label{fig:gun-floomp-diagram}
\end{figure}

%oli12
% Now suppose that you later discover that
A traumatic experience a few hours later leaves Grok believing that
%joe11
%``floomp'' is likely (92\%) to be another word for gun.
``floomp'' is likely (probability .92) to be another word for gun.
%oli12: I've removed the directionality by adding arrows in both directions.
%, and come to believe that if floomps are legal (resp., illegal), then
% there's a  chance guns are as well, and vice versa. 
%joe7: r seems like a atrange letter to use, although it's not a big deal
%oli12: I don't care about the letter. let's use p? Also this notation, while 
% you might not like it, is the consensus, especially in the conference we're 
% submitting to. We're already giving them something quite out of the ordinary;
% I don't want  to push it too far.  It's also expedient here as it lets us 
% immediately indicate the direction.
Let $p(G \mid F)$ be the \emph conditional \emph probability \emph
distribution (cpd) that describes 
the belief that if floomps are legal (resp., illegal),
%joe11
%then with 92\% probability, guns are as well, and $p'(F \mid G)$ be
then with probability .92, guns are as well, and $p'(F \mid G)$ be
the reverse. 
%oli12:
% A first reaction might be to
Starting with $p$, Grok's first instinct is to
simply incorporate the conditional information by adding $F$ as a parent of
$G$, and then associating
the cpd
$p$ with $G$. But then what should she do
with the original probability she had for $G$?  Should she just discard it?
It is easy to check that there is no 
%oli12
% probability distribution 
joint distribution
that is consistent with
%oli12 inserted
both
the two original priors on $F$ and $G$ and also 
%oli12: already lots of commas in this sentence. saying "the cpd" too often also gets to be a lot....
%the cpd $\mat r$, so if she
%joe11
%$p$---so if she
$p$.  So if she
is to represent the information with a BN, which always represents a consistent
distribution, she must resolve the inconsistency. 





However,
%oli12: rewrote paragraph.
% it may be better not to sort this out right away. 
% How to resolve it may be clearer 
% if you can get confirmation that guns are indeed floomps, or read the
% laws more carefully.
sorting this out immediately may not be ideal.
For instance, if the inconsistency arises from a conflation between
two definitions 
of ``gun'', a resolution will have destroyed the original cpds. A
better use of computation may be to notice the inconsistency and look
up the actual law. 

By way of contrast, consider the corresponding PDG. In a PDG, the cpds are
attached to edges, rather than nodes of the graph.
%oli12: this discussion is a distraction; it has nothing to do with PDGs over BNs.
 % we don't mention matrices anywhere else anymore, and the matrix representation
 % in the figure is both a common and inutuitive way of describing this
%
% The cpd associated with an
% edge $e$ from $X$ to $Y$ is a matrix $\mat e$, where the element $\mat e_{x,y}$
% at row $x$ and column $y$ is the conditional probability $\Pr(Y \!\!=\!\!y \mid
% X \!\!=\!\! x)$. 
In order to represent unconditional probabilities, we introduce
a \emph{unit variable} $\var 1$ which 
%oli12: no reason to be too verbose here; more important stuff is coming.
% takes on only one possible value, which we denote
takes only one value, denoted
$\star$. 
%oli12: Thus, we have
This leads Grok to 
the PDG depicted in \Cref{fig:gun-floomp-diagram},
where the edges from $\var 1$ to $F$ and $G$ are associated with the
unconditional probabilities of $F$ and $G$, and the 
%oli12
%edge from $F$ to $G$ is associated with the cpd $p$. 
edges between $F$ and $G$ are associated with $p$ and $p'$.



%joe11: if we make everything black, we should get rid of ``black'' in
%the next line.
The original state of knowledge consists of all three nodes and the two
%oli13: the important bit is that they're solid. I'm trying to
%linguistically  exclude the blue/ dashed lines.
% black
solid
edges from $\var 1$. This is like Bayes Net that we considered above,
%joe11
%except we
except that we 
no longer
%oli12! 
explicitly
%joe11
%oli13:  :(  I think "to be" sounds way better. It's shorter, it doesn't expend
% our limited supply of "and", "that" and "are", which tiring quickly. It sounds
% cooloer. That's also definitely how I would say it in person; I
% think the "that 
% ... are" sounds like you're talking to someone you only trust to know simple 
% grammar --- but in fact, "to be" often taught earlier when people learn English
% as a foreign language, so this form is shorter and without an accesibility 
% cost.
% I believe the infinitive also strengthens the statement by not implying a 
% present tense (how is time relevant here?). I'm changing it back. If you have 
% a reason for your aesthetic preference that you think objectively outweighs this
% consideration to a significant degree, you can change it back and I will 
% accept it without argument, but ask you why later.
%
% assume that $F$ and $G$ are independent; we merely record the constraints
%joe12: if you must have ``to be''
%assume $F$ and $G$ to be independent; we merely record the constraints
take  $F$ and $G$ to be independent; we merely record the constraints
imposed by the given probabilities.  
	
The key point is that we can incorporate the new information into our original
representation (the graph in \Cref{fig:gun-floomp-diagram} without the edge from
$F$ to $G$) simply  by adding the edge from $F$ to $G$ and the associated cpd
%joe12: I could accept that the new information is in gray but then
%why are f and \overline{f}, g nad \overline{g}, and * in gray?
%oli14: Fixed. The reason is because I didn't want to draw focus towards the labels.
%$\mat r$. Doing so does not change the meaning of the original edges.  
%oli14:
% $\mat p$ (the new infromation is shown in gray). 
$p$ (the new infromation is shown in blue).
Doing so does not change the meaning of the original edges.   
%oli12: redundant.
% This
% presentation lets us simply include information, and resolve inconsistencies
% later.
Unlike a Bayesian update, the operation is even reversible: all we need
to do recover our original belief state is delete the new edge, 
%oli12: no need for 'effectively'
%effectively
making it possible to mull over and then reject an observation.
%
\end{example}


The ability of PDGs to model inconsistency, as illustrated in
\Cref{ex:guns-and-floomps}, appears to have come at a significant cost. We seem
to have lost a key benefit of BNs: the ease with which they can
capture
%joe20: it seems strange to say ``Pearl has argued'', and then
%reference a paper by Pearl, Geiger, and Verma
%(conditional) independencies, which, as Pearl \cite{pearl1989conditional} has
(conditional) independencies, which, as Pearl (\citeyear{pearl}) has
argued forcefully, are omnipresent.
%oli12*: it seems like we should add a sentence fragment here, along
%the lines of "but we will be able to easily recover them".  Also, the
%above is kind of redundant, so I keep looking at it trying to figure
%out how to re-word, but it's so well written that I can't figure out
%what I want to do to it.
%joe11: how about:
%As we shall see, we will be able to recover this information.
%oli13: Most anything we add without cutting down the text before will
%ultimately cost a line. I'm not sure this particular phrase is worth
%it, I've commented it out. 
% Counterproposal:
%joe12: Looks like you didn't finish this here
%joe13*: you still didn't finish this sentence.  I'm cutting it.
%And yet:
%oli15: The intention was to lead directly to the example. It's a
%clever but maybe  
% too-cute transition; it is free (fits on on the line) if you remove a comma. 


% most of the time, we do not make the independence
% assumption in a bn because we know for certain that the
% variables are independent; rather, we just suspect that the
% identified edges are by much more important than the
% others. determining for sure that smoking  and second hand
% smoke are independent, controlling for parents' smoking
% habits, would extremely difficult, and would require
% empiricism to validate. 


\begin{example}[emulating a BN]\label{ex:smoking}

We now consider the classic (quantitative) Bayesian network $\cal B$, which has
four binary variables indicating whether a person ($C$) develops cancer, ($S$)
smokes, ($\mathit{SH}$) is exposed to second-hand smoke, and ($\mathit{PS}$) has
parents who smoke, presented graphically in \Cref{subfig:smoking-bn}. We now
walk through what is required to represent $\cal B$ as a PDG, which we call
$\PDGof{{\mathcal B}}$, shown as the solid nodes and edges in
\Cref{subfig:smoking-pdg}. 


%oli24: To make the figure appear in the rigght place, we have to move it to
% be way earlier. Also, some magic \hfils to center it more appropriately...
\begin{figure}[ht!]
\addtocounter{figure}{1}
\centering
\hfill
%oli24: tikzexternalize doesn't work on these...
% \begin{tikzcd}[center base, column sep=1.0em, row sep=0em, dpad={fill opacity=1,fill=black!08, circle, inner sep=3pt, minimum size=2.3em, draw=gray}, 
% 	ampersand replacement=\&]
% \& S \ar[dr] \\
% PS \ar[ur]\ar[dr] \&\& C \\
% \& SH \ar[ur]
% \end{tikzcd}
% }
% \caption{The Bayesian network $\cal B$}
\ifprecompilefigs
	\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-BN}}
\else
\begin{tikzpicture}[paperfig]
	\begin{scope}[every node/.style={dpadded, fill opacity=1,fill=black!08, circle, inner sep=2pt, minimum size=2em, draw=gray}]
		\node (PS) at (0,1.1) {$\mathit{PS}$};
		\node (SH) at (-0.6,0) {$\mathit{SH}$};
		\node (S) at (0.6,0) {$\mathit{S}$};
		\node (C) at (0,-1.1) {$\mathit{C}$};
	\end{scope}
	\draw[->] (PS) to (S);
	\draw[->] (PS) to (SH);
	\draw[->] (SH) to (C);
	\draw[->] (S) to (C);
\end{tikzpicture}
\fi
\refstepcounter{subfigure}
\label{subfig:smoking-bn}
~~\vline~~
\ifprecompilefigs
	\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-PDG}}
\else
\begin{tikzpicture}[paperfig]
	\fill[fill opacity=0.1, blue!80!black, draw, draw opacity=0.5] (2.73,1.35) rectangle (6.7, -1.35);
	
	%oli24: modifying positions to fit things...
	% \node[dpadded] (1) at (0,0) {$\var 1$};
	% \node[dpadded] (PS) at (1.65,0) {$\mathit{PS}$};
	\node[dpadded] (1) at (1.65,1) {$\var 1$};
	\node[dpadded] (PS) at (1.65,-0.4) {$\mathit{PS}$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (S) at (3.2, 0.8) {$S$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (SH) at (3.35, -0.8) {$\mathit{SH}$};
	\node[dpadded, fill=black!.16, fill opacity=0.9] (C) at (4.8,0) {$C$};
	
	\draw[arr1] (1) -- (PS);
	\draw[arr2] (PS) -- (S);
	\draw[arr2] (PS) -- (SH);
	\mergearr{SH}{S}{C}
	
	\node[dpadded, fill=black!.16, fill opacity=0.35, dashed] (T) at (6.15,0) {$T$};
	\draw[arr1,dashed] (T) -- (C);	

	\draw[very thick, |-|, color=blue!50!black,text=black] (2.7, 1.35) --coordinate(Q) (6.73,1.35);%(7.13,1.35);
	\fill[white] (2.6, 1.36) rectangle (6.9,1.55);
	% \useasboundingbox (current bounding box);
	\node[above=0.05em of Q]{\small Restricted PDG in \cref{ex:grok-ablate,ex:grok-union}};
\end{tikzpicture}
\fi
	\hfill~
%oli12: I wasn't really sure what to do with this caption given that it really needs to be 2/3 of a line for the figure to look right.
% \caption{The PDG $\PDGof{{\mathcal B}}$ corresponding to ${\mathcal B}$, and a restriction of it.} 
% \caption{The PDG $\PDGof{{\mathcal B}}$, and two alterations of it.} 
	\refstepcounter{subfigure}
	\label{subfig:smoking-pdg}
\addtocounter{figure}{-1}
% \end{subfigure}
%oli24*: merging captions together + updating them. SUbfigures are a bother with AAAI...
% \caption{Graphical models representing conditional relationships in \Cref{ex:smoking,ex:grok-ablate,ex:grok-union}}
\caption{ (a) The Bayesian Network $\mathcal B$ in \cref{ex:smoking} (left), and
(b) $\PDGof{\mathcal B}$, its corresponding PDG (right). The shaded box
indicates a restriction of $\PDGof{\mathcal B}$ to only the nodes and edges it
contains, and the dashed node $T$ and its arrow to $C$ can be added in the PDG,
without taking into account $S$ and $SH$.}
\label{fig:smoking-bn+pdg}
\end{figure}

We start with the nodes corresponding to the variables in $\cal B$, together
with the special node $\sf 1$ from \Cref{ex:guns-and-floomps}; we add an edge
from ${\sf 1}$ to $\mathit{PS}$, to which we associate the unconditional
probability given by the cpd for $\mathit{PS}$ in $\cal B$. We can also re-use
the cpds for $S$ and $\mathit{SH}$, assigning them, respectively, to the edges
$PS \to S$ and $PS \to SH$ in $\PDGof{{\mathcal B}}$.
There are two remaining problems: (1) modeling the remaining table in $\cal B$,
which corresponds to the conditional probability of $C$ given $S$ and $SH$; and
(2) recovering the additional
%oli12 added
conditional
independence assumptions in the BN. 

For (1), we cannot just add the edges $S \to C$ and $SH \to C$ that are present
%joe11: line shaving
%in $\cal B$, because, as we saw in \Cref{ex:guns-and-floomps}, this would mean
in $\cal B$. As we saw in \Cref{ex:guns-and-floomps}, this would mean
supplying two \emph{separate} tables, one indicating the probability of $C$
given $S$, and the other indicating the probability of $C$ given
%joe11: more line shaving
%$\mathit{SH}$. Doing this would lose significant information that is
$\mathit{SH}$.  We would lose significant information that is
present in $\cal B$  about 
how $C$ depends jointly on $S$ and $SH$. To distinguish the joint dependence on
$S$ and $\mathit{SH}$, for now, we draw an edge with two tails---a
\emph{hyperedge}---that completes the diagram in \Cref{subfig:smoking-pdg}. 
%
With regard to (2), there are many distributions consistent with the conditional
marginal probabilities in the cpds, and the independences presumed by $\cal B$
need not hold for them. 
%oli12
% Rather than encoding the extra probabilistic information as cpds,
Rather than trying to distinguish between them with additional constraints,
we develop a a scoring-function semantics for PDGs
%oli12: hmm, the consistency is part of the scoring function. 
%    , and show that, among all distributions consistent with
%    $\PDGof{{\mathcal B}}$,
%joe11: so we're encoding the constraints in the semantics of the
%scoring function, rather than directly in the PDG.  This makes it a
%``soft'' constraint.  We could say that somewhere, but this seems
%like the wrong place.
%joe11: I don't know what ``emphasis on matching (potentially
%arbitrary) cpds'' means 
%which, despite an emphasis on matching (potentially arbitrary) cpds,
which 
is in this case uniquely minimized by the distribution 
%joe8*: I think we need to throw out hints about how we're going to
%use scoring functions.  I view this as critical
%oli12: I think the BN result is strong enough that this is too much hedging.
% for the appropriate scoring function, 
% the unique distribution with a minimum
% score is the one
%
specified by ${\mathcal B}$ (\Cref{thm:bns-are-pdgs}).
This allows us to recover the semantics of Bayesian networks without requiring the independencies that they assume.

%But now suppose that we get information beyond that captured by the
Next suppose that we get information beyond that captured by the original BN.
Specifically, we read a thorough empirical study demonstrating that people who
use tanning beds have a 10\% incidence of cancer, compared with 1\% in the
%joe7: \mat p comes out as a strange symbol in my pdf file.  Why do
%you need to use nonstandard fonts like \mat?
%oli12: It's just \mathbf. I'm surprised it comes out strange. In any case, no
% more \mat for cpds unless they're matrices.
%joe11: this was an old problem, that got fixed by the other changes
%you made.  But I prefer the current notation in an case.
control (call the cpd for this $p$); we would like to add this information to
$\cal B$. The first step is clearly to add a new node labeled $T$, for ``tanning
bed use''.  But simply making $T$ a parent of $C$ (as clearly seems appropriate,
given that the incidence of cancer depends on tanning bed use) requires a
substantial expansion of the cpd; in particular, it requires us to make
assumptions about the interactions between tanning beds and smoking.  
%
The corresponding PDG, $\PDGof{{\mathcal B}}$, on the other hand, has no
trouble: We can simply add the node $T$ with an edge to $C$ that is associated
with $\mat p$.  But note that doing this makes it possible for our knowledge to
be inconsistent. To take a simple example, if the distribution on $C$ given $S$
and $H$ encoded in the original cpd was always deterministically ``has cancer''
for every possible value of $S$ and $H$, but the distribution according to the
new cpd from $T$ was deterministically ``no cancer'', the resulting PDG would be
inconsistent.  
%
\end{example}


We have seen that we can easily add information to PDGs; removing information is
equally painless.   

\begin{example}[restriction]\label{ex:grok-ablate}
%oli12
% After the communist uprising, 
%joe11
%  After the communist party came to power,
  After the Communist party came to power,
  children were raised communally, and so parents' smoking habits no longer had any impact on them. Grok is reading her favorite book on graphical models, and she realizes that while the node $\mathit{PS}$ in \Cref{subfig:smoking-bn} has lost its usefulness, and nodes $S$ and $\mathit{SH}$ no longer ought to have $\mathit{PS}$ as a parent, the other half of the diagram---that is, the node $C$ and its dependence on $S$ and $\mathit{SH}$---should apply as before.
%oli4: this next sentence is less useful, and can be
    %removed; its purpose is to pre-emptively push against
    %a desire to margnialize and get a new BN.  
%joe4: let's remove it
% \begin{edge} 
% 	The rise of the communist party also came with changes in smoking habits, so a new unconditional distribution on $S$ could not be obtained by eliminating the variable $PS$. 
% \end{edge}
Grok has identified two obstacles to modeling deletion of information from a BN
by simply deleting nodes and their associated cpds. First, this restricted model
is technically no longer a BN (which in this case would require unconditional
distributions on $S$ and $\mathit{SH}$), but rather a \emph{conditional} BN
\cite{KF09}, which allows for these nodes to be marked as observations;
observation nodes do not have associated beliefs. Second, even regarded as a
conditional BN, the result of deleting a node may introduce \emph{new}
independence information, incompatible with the original BN. For instance, by
deleting the node $B$ in a chain $A \rightarrow B \rightarrow C$, one concludes
that $A$ and $C$ are independent, a conclusion incompatible with the original BN
containing all three nodes.   
%joe7*: shortened significantly.  I don't think it's
   %worth agonizing this over this.
%oli12: Your shortening is excellent :)
%joe11: :-)
PDGs do not suffer from either problem.  We can easily delete the
nodes labeled 1 and $PS$ in \Cref{subfig:smoking-pdg} to get the
restricted PDG shown in the figure, which captures Grok's updated information.
%oli12: I want to keep some of the material  underneath it for the
%full paper though. 
% I have rewritten a lot of it.
%joe17: I can live with this in the paper
%\begin{vfull}
The resulting PDG has no edges leading to $S$ or $\mathit{SH}$, and hence no
distributions specified on them; no special modeling distinction between
observation nodes and other nodes are required. Because PDGs do not directly
make independence assumptions, the information in this fragment is truly a
subset of the information in the whole PDG. 	
%\end{vfull}
% 
\end{example}

Being able to form a well-behaved local picture and restrict knowledge is
useful, but an even more compelling reason to use PDGs is their ability to
aggregate information. 
	
\begin{example}\label{ex:grok-union}
Grok dreams of becoming Supreme Leader ($\it SL$), and has come up with a plan.
She has noticed that people who use tanning beds have significantly more power
and than those who don't. Unfortunately, her mom has always told her that
%joe11
%tanning beds cause cancer%
tanning beds cause cancer;
%oli12 
%. In particular,
%joe11: I'm not a fan of dashes; there are better punctuation
%oli13: I've noticed. I'm not a purist, except about maximizing the entropy
% of my punctuation :P
%---specifically, that
specifically, that
15\% of people who use tanning beds
get it, compared to the baseline of 2\%.
%oli12: shortening
% Let $q$ be the cpd associated with this belief.
Call this cpd $q$.
Grok thinks people will make fun of her if she uses a tanning bed and
gets cancer, making becoming Supreme Leader impossible. This mental state is
depicted as  a PDG on the left of \Cref{fig:grok-combine}.
%oli12 we haven't left out anything more than we have earlier.
% (where we have left out the cpds, to avoid clutter).


Grok is reading about graphical models because she vaguely remembers that the
variables in \Cref{ex:smoking} match the ones she already knows about. When she
finishes reading the statistics on smoking and the original study on tanning
beds (associated to a cpd $\mat p$ in \Cref{ex:smoking}), but before she has
time to reflect, we can represent her (conflicted) knowledge state as the union
of the two graphs, depicted graphically on the right of \Cref{fig:grok-combine}.  


\begin{figure}
	\hfill
	\ifprecompilefigs
		\raisebox{-0.5\height}{\includegraphics{figure-pdfs/grok-pre}}
		\hspace{1.2em}\vline\hspace{1.2em}
		\raisebox{-0.5\height}{\includegraphics{figure-pdfs/grok-post}}
	\else
	\colorlet{colorsmoking}{blue!50!black}
	\colorlet{colororiginal}{orange!80!black}
	\tikzset{hybrid/.style={postaction={draw,colorsmoking,dash pattern= on 5pt off 8pt,dash phase=6.5pt,thick},
		draw=colororiginal,dash pattern= on 5pt off 8pt,thick}}
	\centering
	\begin{tikzpicture}[paperfig, thick, draw=colororiginal, text=black]
		\node[dpadded] (C) at (0,0) {$C$};
		\node[dpadded] (T) at (2,0){$T$};
		\node[dpadded] (SL) at (1,-1.5){$\it SL$};
		
		\draw[arr] (T) to[bend right] node[above]{$q$} (C);
		\mergearr{C}{T}{SL}
	\end{tikzpicture}
	\hspace{1.6em}\vline\hspace{1.6em}
	\begin{tikzpicture}[paperfig]
		\begin{scope}[postaction={draw,colorsmoking,dash pattern= on 3pt off 5pt,dash phase=4pt,thick}]
			
			\node[dpadded,hybrid] (C) at (0,0) {$C$};
			\node[dpadded,hybrid] (T) at (2,0){$T$};
		\end{scope}
		
		\begin{scope}[thick, draw=colororiginal, text=black]
			\node[dpadded] (SL) at (1,-1.5){$\it SL$};
			\draw[arr] (T) to[bend right] node[above]{$q$} (C);
			\mergearr{C}{T}{SL}
		\end{scope}


		\begin{scope}[thick, draw=colorsmoking, text=black]
			\node[dpadded] (S) at (-1.4, 0.8) {$S$};
			\node[dpadded] (SH) at (-1.45, -0.8) {$\mathit{SH}$};
			\draw[arr] (T) to node[fill=white, fill opacity=1,text opacity=1,inner sep=1pt]{$p$} (C);
			\mergearr{S}{SH}{C}
		\end{scope}
	\end{tikzpicture}
	\fi
	\hfill~
	\caption{Grok's prior (left) and combined (right) knowledge.}
	\label{fig:grok-combine}
\end{figure}

The union of the two PDGs, even with overlapping 
%oli24:
% nodes and is still a PDG.
nodes, is still a PDG.
This is not the case in general
%oli24:
% with a BN.
for BNs.
Note that the PDG that Grok used to
represent her two different sources of information (the mother's wisdom and the
study) regarding the distribution of $C$ is a \emph{multigraph}: there are two
edges from $T$ to $C$, with inconsistent information. Had we not not allowed
multigraphs, we would need to choose between the two edges, or represent the
information some other (arguably less natural) way. As we are already allowing
inconsistency, merely recording both is much more in keeping with the way we
have handled other types of uncertainty. 
%		
%TODO: I should not say this yet. This is a related story that I haven't told yet. 
%Moreover, if Grok were to later discover that her mother had been faithfully transmitting the results of an unrelated study, she would be justified in increasing her certainty that a cpd roughly like $\mat p$ and $\mat q$ were correct.
% This suggests a result that is perhaps obvious in retrospect: the mere \emph{possibility} of inconisistency increases the value of consistency. For an agent that is guaranteed to be consistent by design, corroborating evidence has no value. 
\end{example}

Not all inconsistencies are equally egregious. For example, even though the cpds
$p$ and $q$ are different, they are numerically close, so, intuitively, the PDG on the right in
\Cref{fig:grok-combine} is not very inconsistent.
Making this precise 
%oli12:  
% will be
is
the focus of \Cref{sec:scoring-semantics}.


%joe4*: While I don't have an intrinsic problem with this paragraph,
%I'm not sure it belongs in the introduction.  Do we discuss this in
%more detail elsewhere in the paper?   If so, we have to say more
%about it.  As it stands, it seems like a letdown, after quite a
%compelling introduction.  I cut it for now.
%
%oli5: I agree with your assessment that it either needs to be followed
% up by something, or removed---although I'm not sure I agree there needs
% to be more text here. I strongly prefer to follow it up with something;
% I think path composition is one of the most important selling point of PDGs, 
% on par with the ability represent inconsistency, and showcases
% modularity in a useful, compositional way. To reflect this preference,
% I'm uncommenting this, but you're welcome to re-comment it in the next iteration.
%joe5*: commenting out, until you come up with a story for it that
%fits in the paper.  I strongly suspect that it won't make it into a
%NIPS submission, so by commenting it out, we'll be able to better
%judge space.
\commentout{
While a PDG is in some sense merely a set of constraints (the cpds), these constraints themselves have a useful computational meaning. Regarding cpds as stochastic matrices, we can get cpds corresponding to paths by multiplying them; equivalently, thought of as probabilistic functions, we can compose them.
	For instance, in \Cref{ex:grok-union}, if we were to give Grok
        unconditional probabilities in the form of vectors
        $\smash{(\vec s, \vec h, \vec t)}$ over the possible values of
        $\mathit{S, SH}$ and $\mathit T$ respectively, she could
        compute three distinct estimates for $\mathit{SL}$. This is
        perhaps clearest visually, but for clarity, if $\mat S$ is
        the cpd for the orange hyperedge that computes $C$ from
        $\mathit{S, SH}$, and $\mat L$ is the cpd for the
%joe4: the colors may not come across for some people, so you may want
%to use some other way of distinguishing them
%oli5*: Can I rely on colors to distinguish things in general? I've been using it throughout the document. I've seen papers that do this, but I can see why it might be poor taste (e.g., black and white printers). I can add letters to the hyper-edges here.
%        blue hyper edge, which computes $\mathit{SL}$ from $\mathit{C, T}$, and
%        $[\vec a; \vec b]$ is a vertical stacking of the vectors $\vec
    blue hyperedge that computes $\mathit{SL}$ from $\mathit{C, T}$, and we 
%oli5:
% use the notation 
		write
        $[\vec a; \vec b]$ for the matrix with rows $\vec
        a$ and $\vec b$, then 
	\[ \mat L \Big[\mat p \vec t; \vec t\ \Big],
		\qquad \mat L \Big[\mat q \vec t; \vec t\ \Big], \quad\text{and}
		\quad \mat L \Big[\mat S \big[\vec s; \vec h\big], \vec t\ \Big]  \]
	        are all probabilistic estimates of $\mathit{SL}$, which
                can be used in different circumstances: the first two are
        applicable even if given only $\vec t$, and the last requires
        all three values. 
	This property gives PDGs more useful structure than most
        collections of constraints.  
}
%joe5: \end{commentout}
        
These examples give a taste of the power of PDGs.  In the coming sections, we formalize PDGs and relate them to other approaches.		
% \begin{notfocus}
%	\begin{enumerate}[nosep]
%		\item This representation more naturally matches what humans are aware of, encoding small locally consistent models rather than one giant probability distribution
%		\item It is a strictly more general representation--- we can easily convert BNs to these diagrams (section \ref{sec:convert2bn})
%		\item This allows composition of arrows to be defined, and gives meanings to paths (section \ref{sec:composition}).
%		\item Allowing variables to be added and removed makes
%		\item Changing and partially determining arrows is more reasonable.
%		\item We can now represent inconsistency, which will allow us to capture mental states which, and . While we agree with the classical picture in that inconsistency is bad, now we can talk about it
%	\end{enumerate}
% Redundency is important: types in programming languages, more data in ML systems.
% Puts gurads
% Makes it possible to combine knowledge without destroying old knowledge.
% preference updating
	
	
\section{Syntax}\label{sec:formal+syntax}
We now provide formal definitions for PDGs.        
Although it is possible to formalize PDGS with hyperedges directly,
    we opt for a different approach here, in which PDGs have only regular edges,
and hyperedges are captured using a simple construction
that involves adding an extra node.

\vfull{\footnote{In the factor graph literature,
          especially with regard to loopy belief propagation
          \cite{wainwright2007graphical}, it is common to
          call a collection of marginals that are not
          necessarily all compatible with a distribution
          \emph{pseudomarginals}, making a PDG in some sense a
          collection of `conditional' pseudomarginals. This
          gives an alternate, more technically precise
          expansion of PDG as ``Pseudomarginal Dependency Graph''.}}

%oli22: removing label for line shave
% \begin{defn}[PDG]\label{def:model}
\begin{defn}\label{def:model}
%oli22: editing so that I can explicitly get graph, modified from 
% DN-and-WFG.tex as per our discussion.
% A \emph{Probabilistic Dependency Graph} is a tuple $\pdgvars[]$ where
%joe20*: reverted back to previous version.  I tried to salvage what
%you did below, but I'm really unhappy about this
%A \emph{Probabilistic Dependency Graph} is a tuple $\dg M = (\Gr,\mat
A \emph{Probabilistic Dependency Graph}
%oli24: \E was expectation (changed to \Ex to avoid confusion) and \Ed is edges. 
%oli24: also, the \alpha, \beta didn't get swapped. 
% is a tuple $\dg M = (\N,\E,\V,\mat p, \beta,\alpha)$, where 
is a tuple $\dg M = (\N,\Ed,\V,\mat p, \alpha, \beta)$, where 

%oli22: I still think ((\N,\V), E) is better notation, because it 
% is of the form ( VERT , E ) where VERT = (\N,\V) is the data necessary 
% to specify the variables (the vertices of the graph), and  E is a set of edges
% like usual. In other points we also want to refer to the set of variables, 
% which I have been writing (\N, \V). Do you think what I have below is better?
%joe20*: I *strongly* disagree.  A multigraph is a standard notion.
%You shouldn't change standard definitions.  We had agreed that you
%wouldn't do this.  Please do not do what we had agreed you wouldn't
%do without discsussing it.  It creates extra work for me and extra
%work for you.  I find this completely unacceptable.
%$\Gr = (\N,\Ed,\V)$ is a multi-graph whose nodes are variables, while
%$\Gr = (\N,\Ed)$ is a multi-graph whose nodes are variables, while
%$\Gr = (\N,\Ed)$ is a multi-graph whose nodes are variables, while
%$\mat p,\beta,\alpha$ respectively give a cpd and two weights
% its reliability,
%for each edge in $\Ed$. More precisely,
%
\begin{description}%[nosep]
	\item[$\N$] \notation{$:\Set$}%
		is a finite set of nodes, corresponding to variables;
	\item[$\Ed$] \notation{$\subseteq \N \times \N \times \mathit{Label}$}%
%oli22: clarify "arbitrary", introduce notation, and pull focus away from 
% the label, which is a necessary technicality. 
%Question: With the arrow,  "source", and "target" is the word "directed" positive or negative?
		% is a set of directed edges, each with a source and target in $\N$, 
		% as well as an arbitrary label;
		is a set of labeled edges $\{ \ed LXY \}$, each with a source 
%oli24:
		% $X$ and target $Y$ in $\N$, 
		$X$ and target $Y$ in $\N$;
	\item[$\V$] \notation{$\N \to \mathbf{Set}$}%
		associates each variable $N \in \N$ with a set $\V(N)$ of values that the variable $N$ can take;
  	\item[$\mat p$] \notation{$:\big(\!({A,B,\ell})\colon\!\Ed \big) \to \V(A) \to \Delta\V(B)$}%
	% HYPERGRAPH \mat p TYPE: $\colon\!\big(\!({\bf A,B})\colon \! \Ed \big) \to \prod\limits_{A\in \bf A} \!\! \V(A) \to \underline\Delta\left[\prod\limits_{B \in \bf B}\!\!\V(B)\right]$
%oli22: introducing the name of L, use notation. The first commented line below is 
% the orignial, and the second is an option that does not use the arrow notation.
	% associates to each edge $(X,Y,\ell) \in \Ed$
	% associates to each edge $L \!=\! (X,Y,\ell) \in \Ed$
	associates to each edge $\ed LXY \in \Ed$
	a distribution $\bp(x)$ on $Y$ for each $x \in \V(X)$; 

%oli22: \alpha used to be here and I moved it below \beta.
%joe20: undid change; it's strange to have \beta before \alpha.  We
%had agreed that you wouldn't make such changes.  
\item[$\alpha$] \notation{$:\Ed \to [0,1]$}
%oli22: feel free to reinstate; see corresponding comment on \beta above
% is a function that
%oli22: replacing paragraph to correct the following:
% (1) \alpha can be zero; 
% (2) removing the semicolon and re-introduced subject for flow.
% (3) bind "X" and "Y" before reference.
% (4) remove the "agent" which we've tried so hard to avoid in the
% description of IDef, as with \beta above.
%
%  associates a non-negative real number $\alpha_L$ with each edge $L$;
%  roughly speaking, $\alpha_L$ is the agent's subjective confidence in
%  the qualitative functional dependence of $Y$ on $X$ implicit in $L$. 
%
associates to each edge $\ed LXY$ a non-negative number $\alpha_L$ which,
%joe20
%roughly speaking, is a subjective confidence in the functional
%oli24: the L all by itself is bothering me a lot visually.
% roughly speaking, is the modeler's subjective confidence in the functional
roughly speaking, is the modeler's confidence in the functional
dependence of $Y$ on $X$ implicit in $L$; 
\item[$\beta$] \notation{$:\Ed \to \mathbb R^+$}
%oli22: [should we keep "is a function"?] 
% In the body, we refer to \beta as a "vector" instead of a "function";
% also, in the definition of \mat p above, we simply say "asssociates" without
% giving either --- though perhaps it's justifiable because it's dependently 
% typed whereas this is simply a function? I have commented out "is a function"
% here and also for \alpha above, but I defer to your judgement and won't 
% follow up on this.
%joe19
% is a function that 
%oli22: swap order for consistency with other items
% associates a positive real number $\beta_L$ with each edge $L$,
associates to each edge $L$ a positive real number $\beta_L$,
%oli22: removing "agent's" to clean up definition; also removed "indicating"
% purely as a line shave.
% indicating an agent's subjective confidence in the reliability of
%joe20:
the modeler's 
subjective confidence in the reliability of
%oli22:
% the cpd $\bp$. 
%joe21
%the cpd $\bp$;
the cpd $\bp$. 
\end{description}
%joe20: added, to try to salvage what you did.
Note that we allow multiple edges in $\Ed$ with the same source and
target; thus $(\N,\Ed)$ is a multigraph.  We occasionally write a PDG
%joe20
%as $\dg M = (\Gr,\mat p, \beta,\alpha)$, where $\Gr = (\N,\E,\V)$, and
%joe21: Note that we have three deifferent font in G-(N,E,V)
as $\dg M = (\Gr,\mat p, \alpha,\beta)$, where $\Gr = (\N,\Ed,\V)$, and
%joe21
%abuse terminology by referring to $\Gr$ as $\Gr$ as a multigraph.
abuse terminology by referring to $\Gr$ as a multigraph.
%oli22: added
%joe20: it's not a partial spefication.  
%We refer to a partial specification
We refer to 
%joe21: I think we have four different fonts here.  This is really not
%good, although I'm not going to worry about it now
%oli24: thanks for not worrying; it was an easily identifiable
% macro confusion that arose when you rewrote this bit.
${\dg N} = (\Gr, \mat p)$ as an \emph{unweighted} PDG,
%oli24: 
% pulled up from where you said this, with slight modification.
and give it semantics as though it were the (weighted) PDG $(\Gr, \mat p, \mat 1, \mat 1)$, where
$\bf 1$ is the constant function (i.e., so that $\alpha_L = \beta_L = 1$ for all $L$). 
%oli22*: We have to add \alpha_L to IDef or else the alternate
% representation of the semantics does not follow. Thus $\alph_L$ appears
% in all the places one would expect; we just don't fully discuss 
% the implications (mostly because we don't agree what they are). As a result,
% I think making too big a deal out of its lack of presense is a mistake.
%oli22: Original: 
%	Throughout most of this paper, we do not mention $\alpha_L$,
%	implicitly taking $\alpha_L\!=1$ for all edges $L$.
%oli22: this last statement seems out of place in the definition itself;
% is it important enough to re-insert afterwards?
%	(The one place where we use $\alpha_L$ is in our discussion of ...)  
%
% < various rewrites >
% In this paper, we try to avoid mentioning we implicitly take $\alpha_L=1$ for all edges $L$,  
% and abbreviate $\dg M = (\Gr, \mat p, \beta)$.
%
%joe21
%In this paper, we generally take $\alpha_L$ to equal 1 for all edges
%oli24
% In this paper, we generally take $\alpha = {\bf 1}$,
In this paper, with the exception of \cref{sec:expfam},  we implicitly take $\alpha = {\bf 1}$
% In this paper, we take $\alpha = {\bf 1}$ except in \cref{sec:expfam},
%oli24: I've pulled this parenthetical above
% (i.e., the constant function ${\bf 1}$, so that $\alpha_L = 1$ for all edges $L$) 
% unless we state otherwise,
and abbreviate $\dg M = (\Gr, \mat p, \beta)$.%
	\footnote{The appendix gives the analogs of other results for arbitrary $\alpha$.}
% 
% We refer to 
% %joe21: I think we have four different fonts here.  This is really not
% %good, although I'm not going to worry about it now
% %oli24: thanks for not worrying; it was an easily identifiable
% % macro confusion that arose when you rewrote this bit.
% ${\dg N} = (\Gr, \mat p)$ as an \emph{unweighted} PDG,
% %oli24: 
% % pulled up from where you said this, with slight modification.
% and give it semantics as though it were the (weighted) PDG $(\Gr, \mat p, \mat 1, \mat 1)$,
\end{defn}
If $\dg M$ is a PDG, we reserve the names 
$\N^{\dg M}, \Ed^{\dg M}, \ldots$,
for the components of $\dg M$, so that we may reference one without naming them
all explicitly. We write $\V(S)$ for the set of possible joint settings of a set
$S$ of variables, and write
%oli21: unnecessarily complex:
% $\V(\dg M) := \prod_{N \in \N^\dg M} \V^{\dg M}(N)$
%joe19: I can't parse this
%$\V(\dg M) :=\V^{\dg M}(\N^{\dg M})$
%for all settings of the variables $(\N^\dg M, \V^\dg M)$%
%oli21: introduce for clarity
%which we also refer to as ``worlds''.
$\V(\dg M)$ for all settings of the variables in $\N^{\dg M}$; we
refer  to these settings as ``worlds''.
While the definition above is sufficient to represent the class of all legal
PDGs, we often use two additional bits of syntax to represent common
constraints:  
    	
\begin{itemize}
    \item A special variable $\sf 1$ whose range consists of only element, which
    we denote $\star$. It is used to represent unconditional distributions, as
    in \Cref{ex:guns-and-floomps,ex:smoking}.  
%joe17* I would cut this.  As I said above, it breaks the flow and is
%unnecesary; the examles sffice
%oli20: oops, did not intend to reinsert this. 
\vleftovers{
		\begin{example}\label{ex:worldsonly}
			A probability distribution $p$ over a measurable set $W$ of possible worlds is represented as 
			\begin{center}
				\scalebox{0.8}{
				\begin{tikzpicture}
					\node[dpadded] (1) at (0,0) {$\sf 1$};
					\node[dpadded] (W) at (3,0) {$W$};

					\draw[arr] (1) to node[fill=white]{$p$} (W);
				\end{tikzpicture}}
			\end{center}
		\end{example}
}%joe17 \end{vleftovers}

		\item Double-headed arrows, $A \tto
                  B$, which visually indicate the degenerate special
                  case of a cpd that assigns probability 1 to $f(a)$
                  for each $a \in A$ (corresponding to a deterministic
                  function $f : A \to B$). 
\end{itemize}


\begin{constr}\label{constr:hyperedge-reducton}
We can now explain how we capture   the multi-tailed edges that 
were used in 
\Crefrange{ex:smoking}{ex:grok-union}. 
That notation can be viewed as shorthand for the graph that results by adding a new node at the junction representing the joint value of the nodes at the tails, with projections going back.  For instance,
% the diagram of the PDG in the shaded box of \Cref{subfig:smoking-pdg}
the diagram displaying Grok's prior knowledge in \Cref{ex:grok-union}, on the left of \Cref{fig:grok-combine}
%joe7: moved up from below, to save a line
%is really shorthand for the following PDG:
is really shorthand for the following PDG, where
where we insert a node labeled $C \times T$ at the junction:
\smallskip
	\begin{center}
	\ifprecompilefigs
		\raisebox{-0.5\height}{\includegraphics{figure-pdfs/widget}}
	\else
		\begin{tikzpicture}[paperfig]
			\node[dpadded] (SL) at (-1.0,0) {$\mathit{SL}$};
			
			\node[dpadded,light pad] (CT) at (-2.9, 0){$\scriptstyle C \times T$};
			\node[dpadded] (C) at (-4.8, -0.6) {$C$};
			\node[dpadded] (T) at (-4.8, 0.6) {$T$};
			
	%				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
	%				\draw[arr, color=violet] (X) -- (S);
	%				\draw[arr, color=violet] (X) -- (C);
	%				\draw[arr, dashed, color=violet] (X) -- (SC);
			
			\draw[arr, ->>] (CT) -- (C);
			\draw[arr, ->>] (CT) -- (T);
			\draw[arr] (CT) -- (SL);
			\draw[arr] (T) to [bend right=90, looseness=2] (C);
	\end{tikzpicture}
	\fi
	%%%%%%%%%%%%%%%%%  smoking fragment: %%%%%%%%%%%%%%%%%%%%%%
% 		\scalebox{0.8}{
% 			\begin{tikzpicture}
% 				\node[dpadded] (C) at (-1.0,0) {$C$};
% 				\node[dpadded] (T) at (0.5,0) {$T$};
% 
% 				\node[dpadded,light pad] (SSH) at (-2.9, 0){$\scriptsize \mathit{SH} \times S$};
% 				\node[dpadded] (S) at (-4.8, 0.6) {$S$};
% 				\node[dpadded] (SH) at (-5.0, -0.6) {$\mathit{SH}$};
% 
% %				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
% %				\draw[arr, color=violet] (X) -- (S);
% %				\draw[arr, color=violet] (X) -- (C);
% %				\draw[arr, dashed, color=violet] (X) -- (SC);
% 
% 				\draw[arr, ->>] (SSH) -- (S);
% 				\draw[arr, ->>] (SSH) -- (SH);
% 				\draw[arr] (SSH) -- (C);
% 				\draw[arr] (T) -- (C);
% 		\end{tikzpicture}}
	\end{center}
	\smallskip

% That is, we inserted a node labeled $SH \times S$ at the junction.  As
% the notation suggests, $\V( \mathit{SH} \times S) = \V(\mathit{SH}) \times \V(S)$.
% The cpd for $(h,s) \in \V(\mathit{SH} \times S)$  associated with 
% the edge from $\mathit{SH} \times S$ to $\mathit{SH}$ gives probability 1 to $h$;
% similarly, the cpd for $(s,c)$  associated with 
% the edge from $ C \times C$ to $S$ gives probability 1 to $s$.
%joe7
%        That is, we inserted a node labeled $C \times T$ at the junction.
As the notation suggests, $\V( C \times T) = \V(C) \times \V(T)$.
%joe2: this is not the time to start talking about matri\mathit{SL}es
%Thus, $\V(S \times \mathit{SL}) = \V(S) \times \V(\mathit{SL})$; the matrix asso\mathit{SL}iated with
For any joint setting $(c,t) \in \V(C \times T)$ of both variables, the cpd for
the edge from $C \times T$ to $C$ gives probability 1 to $c$;
similarly, the cpd for the edge from $ C \times T$ to $T$ gives probability 1 to $t$.
\end{constr}

%oli11: Quickly hint about why this works syntactically, so we can
%better summarize this contribution
%joe10*: NO!!!  This ha nothing to do with our story, It's a complete
%distraction.  I don't see the connection between a failure of
%consistency and a failure to commute (and I have a Ph.D. in
%mathematics and no exactly what the words mean).  I *strongly* object
%to including this.  If you have some interesting theorems to prove,
%you can write another paper on it.  You can also talk about it in
%your thesis if you really care.  But not in the paper!  Even if we
%could afford the space (which we can't) I would strongly object.
%oli12*: Are you serious that you don't see the connection?? We should
%talk about this.
%joe11: Yes, I'm serious.  We can discuss it after the deadline.
% I really do think it's a massive oversight not to at least mention
% this. I can see a world where mentioning this is the most important
% thing we do in the paper. I also think you hugely underestimate how
% many people care about commutative diagrams, what this buys you, and
% the implications for mathematical reasonsing.
%joe11: There are certainly people who care about this material.  They
%are an extremely small subset of the NeurIPS community.  We're
%submitting this paper to NeurIPS.  You're more than welcome to write
%a paper about this for a conference where there are substantially more pepole
%who care about commutative diagrms.
%joe17: We *definitely* should not be talking about commuatative
%diagrams at this point in the paper (frankly, I don't think we should
%talk about them at all in the paper
%oli20: agreed, did not intend for this to resurface.
\vleftovers{
    We have stressed that edges are interpreted differently in PDGs than
    in Bayesian Networks. Readers should be aware that this approach is
    closely related to the notion of a \emph{commutative diagram}, a
    common representation in pure mathematics. For those familiar with
    them, the potential failure of a PDG to be consistent is analogous to
    the possibility that a diagram could fail to commute. 
}

%joe4*: cut this paragraph.  It's a distraction.  Moreover, BN's don't
%need this ``trick'', because of the way they interpret the edges.  
\section{Semantics}\label{sec:semantics}
%oli12: I dislike the sentence below. Primarily, it seems like
%cheating to come up 
% with totally different semantics depending on what you want to capture, and 
% we're really not doing that --- our three semantics are intimately
% related, and 
% can all be represented naturally in terms of the scoring function. 
%   There is more than one way of giving semantics to a PDG.  
%oli12: I think something along these lines has the additional benefit of 
% quickly speaking to why we're doing this (people write down cpds all the time). 
Although the meaning of an individual cpd is clear, we have not yet given 
%joe11: you seem to like using ``local'' and ``global'' a lot.
%Although I understand what you intend here, I'm still uncomfortable
%about the usage, because it suggests that there's a local semantics.
%oli13: There is a local semantics: the cpd itself already is a local
%probabilistic object. It is already useful on its own. You can even compose
%them, etc,... But there's ambiguity in stitching them together. In some sense
%this is the problem we address.
%joe12: but we have never talked about local semanatics or local
%probabilistic objects.  This is not the time to start!
%oli14: Anecdotally, this is the point that convinced me that semantics was 
% actually worthwhile. Many people just use cpds by themselves; I'd guess many
% ML people will be wondering why we need anything else if we already have
% the cpds that we need. Can't you just do math with those? In any
% case, "global" 
% is good enough for me.
%joe11: I left it in with the quotation marks.
%oli13: The "local" and "global" is going to also help make sure these terms
% don't come out of the blue later in the paper.
%joe12: but they come out of the blue here.  Do we really need them?
%Are they adding something useful to the discussion?  (As you can
%guess, I don't think that they do.)
%oli14: I think they are. I think this "global" here is already worth it
% even if we don't follow up. I have plenty of following up to do, but
% probably anything substantive won't make it. I have just a few
% characters on the final page. 
%PDGs any global semantics. We discuss three related approaches. The first
%joe14
%PDGs a ``global'' semantics. We discuss three related approaches. The
PDGs a ``global'' semantics. We discuss three related approaches to doing so.
The first is the simplest: we associate with a PDG the set of distributions that
are consistent with it. This set will be empty if the PDG is inconsistent.
%
The second approach associates a PDG with a scoring function, indicating the fit
of an arbitrary distribution $\mu$, and can be thought of as a \emph{weighted}
set of distributions \cite{HL12}. This approach allows us to distinguish
inconsistent PDGs, while the first approach does not. The third approach chooses
the distributions with the best score, typically associating with a PDG a unique
distribution.

%joe18*: Oliver, can you reinstate subsection numbers?
%oli21: Ah, only section numbers. Changed.   
\subsection{PDGs As Sets Of Distributions}\label{sec:set-of-distribution-semantics} 
We have been thinking of a PDG as a collection of constraints on distributions,
specified by matching cpds. From this perspective, it is natural to consider the
set of all distributions that are consistent with the constraints.

\begin{defn} \label{def:set-semantics} 
%oli22: minor expansion to clarify that this works for both unweighted and
% weighted PDGs.
% If $\dg M\sheq\pdgvars[]$ is a PDG,
%joe20
%If $\dg M$ is an PDG (weighted or unweighted) with edges $\Ed$ and
If $\dg M$ is a PDG (weighted or unweighted) with edges $\Ed$ and
cpds $\mat p$, 
let $\SD{\dg M}$ be the \emph{s}et of
\emph{d}istributions over the variables in $\dg M$ 
%oli12: "the conditional probabilities" is ambiguous, I thought it was reffering
%to p. For a joint distribution p, it's the conditional _marginal_ on the
%appropriate variables that's of interest.
% for which the conditional probabilities are exactly 
%joe11
%whose conditional marginals on are exactly those given by $\mat p$.
whose conditional marginals are exactly those given by $\mat p$.
%oli20: edges have labels (but no need to draw attention to them)
% That is, $\mu \in \SD{\dg M}$ iff, for all edges $L = (X,Y) \in \Ed$,  $x \in
That is, $\mu \in \SD{\dg M}$ iff, for all edges $L \in \Ed$ from $X$
to $Y$,  $x \in 
\V(X)$,  and $y \in \V(Y)$, we have that $\mu(Y \!=\! y \mid X\sheq x) = \bp(x)$.
\notation{Formally,		
    \[ \SD[\Big]{\dg M} = \!\left\{\mu \!\in\! \Delta \V_\none (\dg M) \middle|\!
        \begin{array}{l}
        \mu(B\!\! =\!\! b \mid A\!\!=\!\! a) \geq \bp(b \mid a) \\[0.1em]
        ~\text{$\forall (A, B,\ell) \!\in\! \Ed$, $a \!\in\!\V(A)$, $b \!\in\! \V(B)$} 
   		\end{array}\!\!\! \right\}\]
    }
% $\dg M$ is \emph{consistent} if $\SD{\dg M} \ne \emptyset$, and inconsistent otherwise.
% $\dg M$ is \emph{inconsistent} if $\SD{\dg M} = \emptyset$, and consistent otherwise.
$\dg M$ is \emph{inconsistent} if $\SD{\dg M} = \emptyset$, and \emph{consistent} otherwise.
\end{defn}
%joe20
%%oli22: I still want to say this 
%joe20: cut; this is the wrong place to mention convexity
%\begin{remark}
%	$\SD{\dg M}$ is a convex set of distributions,
%	and independent of the weights $\alpha$ and $\beta$.
%	
%\end{remark}
Note that $\SD{\dg M}$ is independent of the weights $\alpha$ and $\beta$.

%joe17*: This breaks teh flow here.  If it were go anywhere, it should
%be in the appendix
%oli20: oops, I don't want it in the AAAI submission either.
\vleftovers{
	It turns out that this semantics only results in convex sets. This may provide useful intuition, and we will prove a stronger version of this statement that corresponds to our second semantics.
	\begin{lemma}[restate=thmsetconvex] 
		\label{prop:convex}
		$\SD{\dg M}$ is convex, for any PDG $\dg M$.
	\end{lemma}

	Note that being inconsistent is not the same things as \emph{over-constrained}: 	
	\begin{defn}
		$\dg M = \pdgvars[]$ is over-constrained if there exists
		  \emph{some $\mat p'$} assigning cpds to the same edges as
		  $\mat p$, such that $(\N, \Ed, \V, \mat p')$ is inconsistent
		  \notation{(i.e., $\SD{\N^\dg M, \Ed^\dg M, \V^\dg M, \mat p}
			= \emptyset$)}, and under-constrained if there are
		  multiple distributions in $(\N, \Ed, \V, \mat p')$ for
		  \emph{every such $\mat p'$}, making this a property of the
		  qualitative PDG $(\N, \Ed, \V)$.  
	\end{defn}

	We know that an under-constrained PDG is consistent without even looking at the tables. However if a we know that an \emph{over-constrained} PDG is actually consistent (when it could have easily contradicted itself), the information provides corroborating evidence, and one can take this as support in favor of the beliefs. 
}

\subsection{PDGs As Distribution Scoring Functions} \label{sec:scoring-semantics}   

We now generalize the previous semantics by viewing a PDG $\dg M$ as a
\emph{scoring function} that, given an arbitrary distribution $\mu$ on $\V(\dg
M)$, returns a real-valued score indicating how well $\mu$ fits $\dg M$.
Distributions with the lowest (best) scores are those that most closely match
the cpds in $\dg M$, and contain the fewest unspecified correlations.
% We now make this precise.

We start with the first component of the score, which assigns higher scores to
distributions that require a larger perturbation in order to be consistent with
$\dg M$.  
%
We measure the magnitude of this perturbation with relative entropy. In
%oli22: hiding the label \ell. 
%original:
% particular, for each edge $L = (X,Y, \ell)$, and each $x \in \V(X)$, we measure
%option 1: with words
% particular, for each edge $L$ from $X$ to $Y$, and each $x \in \V(X)$, we measure
%option 2: with the same notation I put in the definition
%joe20
%particular, for each edge $\ed LXY$, and each $x \in \V(X)$, we measure
%the relative entropy from $\bp(x)$ to $\mu(Y \!= \cdot\mid X=x)$, take the
particular, for an edge $\ed LXY$ and $x \in \V(X)$, we measure
the relative entropy from $\bp(x)$ to $\mu(Y \!= \cdot\mid X=x)$, and take the
expectation over $\mu_X$ (that is, the marginal of $\mu$ on $X$). We then sum
over all the edges $L$ in the PDG, weighted by their reliability.


\begin{defn}\label{def:inc}
%oli19: swapping \mu and {\dg M} to shift foucs.
	% The \emph{incompatibility} of a PDG $\dg M = \pdgvars[]$ with
	% a joint distribution $\mu$, denoted $\Inc_{\dg M}(\mu)$, is  
    For a PDG $\dg M$, the \emph{incompatibility} of a
    a joint distribution $\mu$ over $\V(\dg M)$, is given by
    \[
%oli19: removed \pdgvars above, putting ^{\dg M} subscripts here instead.
	\Inc_{\dg M }( \mu) := 
		\!\!\!\!\!\sum_{\ed L{X\,}{\,Y} \in \Ed^{\dg M}} \!\!\beta_L^{\dg M} \Ex_{x \sim \mu_{_X}}
%oli19: no \cdot to make equation fit on one line. The fact that it is a 
% distribution should be clear because both that this is standard notation, and
% because of the definition of KL divergence that follows. 
% \left[\kldiv[\Big]{ \mu(Y\!= \cdot\mid X \sheq x) }{\bp(x) } \right] ,
\left[\kldiv[\Big]{ \mu(Y \mid X \sheq x) }{\bp^{\dg M}(x) } \right] ,
	\]
	where $\kldiv{\mu}{\nu} = \sum_{w \in \text{Supp($\mu$)}} \mu(w) \log\frac{\mu(w)}{\nu(w)}$ is the 
	relative entropy from $\nu$ to $\mu$.
%joe17: If we use this at all, it should be defined where it's used,
%not here
%oli20: oops, fixed.
\vfull{
	The \emph{inconsistency of PDG $\dg M = \pdgvars[]$}, denoted $\Inc(\dg M)$, is the minimum possible incompatibility of $\dg M$ with any distribution $\mu$,  
	\[ \Inc(\dg M) = \inf_{ \mu \in \Delta [W_{\cal V}]} \Inc_{\dg M}(\mu) . \]
}
\end{defn}

%joe17*: cutting this.  This might be useful if we used it somewhere
%(in which case it should probably go where it's used, not here), but we don't.
%oli20: agreed, updating comment.
\vfull{
    The idea behind this definition of inconsistency is that we want to choose a
    distribution $\mu$ that minimizes the total number of bits required to
    encode all of the relevant conditional marginals. More precisely, fix a
    distribution $\mu$. For each edge $L = (X, Y, \ell) \in \Ed$ and $x \in
    \V(X)$, we are given a code for $Y$ optimized for the distribution $\bp(x)$,
    and asked to transmit data from $\mu(Y\mid x)$; we incur a cost for each bit
    required beyond what we would have used had we used a code optimized for the
    actual distribution $\mu(Y\mid X=x)$. To obtain the cost for $L$, we take a
    weighted average of these costs, where the weight for the value $x$ is the
    probability $\mu_X(x)$. We do this for every edge $L \in \Ed$, summing the
    cost.

    For even more intuition, imagine two agents ($A$ and $B$) with identical
    beliefs described by a PDG $\dg M$ about a set of variables that are in fact
    distributed according to $\mu$. For each edge $L = (X,Y, \ell) \in \Ed^\dg
    M$, values $x,y \in \V(X)$ are chosen according to $\mu_{_{XY}}$ and $x$ is
    given to both agents. 

    At this point, the agents, having the same conditional beliefs, and the same
    information about $Y$, agree on the optimal encoding of the possible values
    of $Y$ as sequences of bits, so that if $y$ were drawn from $\bp(x)$, the
    fewest number of bits would be needed to communicate it in expectation. The
    value of $y$---which is distributed not according to $\bp(x)$, but $\mu(Y
    \mid X=x)$---is now given to agent A. The agents pay a cost equal the number
    of bits needed to encode $y$ according to the agreed-upon optimal code, but
    reimbursed the (smaller) cost that would have been paid, had the agents
    beliefs lined up with the true distribution $\mu$.

    Repeating for each edge and summing the expectations of these costs, we can view
    $\Inc_{\dg M}(\mu)$ as the total number of \emph{additional} expected
    bits required to communicate $y$ with a code optimized for
    $\bp$ instead of the true conditional distribution   $\mu(Y \mid X=x)$. 

    If $\dg M$ is inconsistent, then there will be a cost no matter what
    distribution $\mu$ is the true distribution. Conversely, if $\dg M$ is
    consistent, then any distribution $\mu \in \SD{\dg M}$ will have $\Inc_{\dg
    M}( \mu) = 0$.  

	\begin{example}[continues=ex:worldsonly]
	    Recall our simplest example, which directly encodes an entire distribution $p$
	    over the set $W$. In this case, there is only one edge, the expectation is over
	    a single element, and the marginal on $W$ is the entire distribution. Therefore,
	    $\Inc(\dg M; \mu) = \kldiv{\mu}{\mu}$, so the inconsistency is just the
	    information $\mu$ and $p$, so is minimized uniquely when $\mu$ is $p$
	\end{example}
}

%oli22: remove "only" to reclaim my line
$\SD{\dg M}$ and $\Inc_{\dg M}$ distinguish %only
between distributions based on their compatibility with
$\dg M$, but even among distributions that match the
marginals, some more closely match the qualitative structure
of the graph than others.  
\commentout{
    Suppose an agent has a PDG $\dg M$ in mind, and imagines that all sample
    variation in a joint distribution $\mu$ over $\V(\dg M)$ arises as a result
    %joe9*: In the next line, did you mean ``some'' or ``each''? See my
    %next joe9*
    %oli11: Both words are ambiguous because of the sentence structure,
    %but I meant to say that we imagine the distribution is generated by
    %rolling the dice in each link. What we do is clear from the
    %formula. I think this is why I prefer putting the formula before the
    %explanation --- people read the explanation already knowing what
    %you're going to do, and then learn why you do it.
    %joe10: there are reasons to put intuition before the formula, and
    %reasons to put after.  I generally prefer giving a high-level
    %intuition first, and then following up with more intuition after the
    %formula if it's would help                
    of sampling the value of a target variable $Y$ of some edge $\ed LXY$, given the
    value of $X$. If this is the case, one would expect the total amount of
    information required to communicate a sample of $\mu$ to be the same as the
    total amount of the information required to separately encode, for
    each edge $\ed LXY$, the randomness of $Y$ given $X$.

    For an arbitrary PDG and $\mu$, these two quantities will differ; if the amount
    of uncertainty in the distribution is lower than one would expect from the
    edges, the distribution has an \emph{information deficiency}, indicating that there
    are additional correlations between variables that are not captured by the
    graph. The higher the information deficiency, the worse the qualitative fit between
    the PDG and $\mu$.
    %joe9*: Now you lost me.  What is ``surplus uncertainty''? This needs
    %tob e given *much* more intuition.
    On the other hand, if a distribution has surplus uncertainty, using it confers a
    benefit. For instance, a PDG containing only the variable $X$
    %joe9* I strenously object to the use of ``safest'' in the way you've
    %used it below.  What is safe depends on utilities, which we haven't
    %mentioned. 
    %oli11: I think this is false. While to express "safe" in general, you
    %would need a utility function, I claim that there is not *any*
    %utility function that, over the joint space of random variables here,
    %would change this calculus at all (except to make it so that you
                    %don't care).
    %joe11: I dsagree; you're too wedded to maxent.  But this is not the
    %time to have this battle                
    % there's 
    %oli13: (This part is still commented out.)
    %joe12: good; let's keep it that way :-)
    and no
    edges encodes awareness about $X$ but no beliefs about it. Given that $X$
    could be distributed in any way (even chosen adversarially), it is safest to act
    as though you believed $X$ to be uniform.  
    %oli11: irrelevant because commented out anyway, but I think what I
    %wrote is more coherent than the below.  and no edges leading to $X$
    %means that the agent is  aware of $X$, but has no  information
    %regarding the probability of various values of $X$ but no 
    We measure the surplus/deficit with a quantity we call
    the \emph{information fit} ($\IDef$) (between ${\dg M}$ and $\mu$):
}
%oli11: end comment. The summary at the end of our meeting was,
%" Given a distribution, compare the cost of specifying \mu directly
%to the cost of specifying the information on the edges for \mu.". You
%also said not to say anything about samples, but it's information
%required to specify a sample of the distribution, given the
%distribution. I'll try a slightly different appraoch, but I don't
%have a good idea about what's going to stick. 
%Let $G$ be a (multi-)graph whose nodes correspond to random variables. 
%joe10
%Each edge $\ed LXY$ of a PDG $\dg M$ represents a qualitative claim
%oli22: `of a PDG'' is clear from context.
% We think of each edge $\ed LXY$ of a PDG $\dg M$ as representing a
We think of each edge $\ed LXY$ as representing a
qualitative claim
%oli22: we need to add this so \alpha doesn't come out of nowhere
% in the definition below, and we need it in that definition so that
% Prop 4.4 is not just a new definition with \alpha inserted. I hope it's
% not too controvertial.
(with confidence $\alpha_L$)
%joe20: the computation isn't noisy; the output perhaps is. I don't
%think it adds to use the word
%that the value of $Y$ can be (noisily) computed from
that the value of $Y$ can be computed from
$X$ alone.  
%
%joe10*: I'm not happy about his, because ``amount of variation'' is
%not well defined. Whateever intuitions you're suggesting here is (in
%my opinion) too closely tied to entroyp.  As a technical matter, I
%think you want to talk about \mu matching \dg M.  Also, in fact, it
%does not match the definition, since it doesn't say what to do with
%parallel edges
%oli13: We already talk about matching in the sentence above.
%oli13: ALso the parallel edges is the reverse of what I'm talking about here.
%How do I people from getting this mixed up? If you think about it carefully.
%With parallel edges (b) > (a) and as we've said, there are additional
%correlations that allow for a more  compact representation. That's the case
%that parallel edges push towards ( and of course the total score is just in
%net). We still need to give intuition for the other case though; here we need
%to say something about the case when (for instance) there are no edges at all.
%joe12: I didn't understand what you wrote above
%   A joint distribution that qualitatively matches $\mu$ will intuitively
%   have as much variation as possible, except where we have claimed that
%   one variable depends on another.
%oli13*: Ok, but we need to impart slightly more detailed intuition instead of
% saying this. I know you don't like entroyp, but given that our formua uses
% entropy (Sorry), and we need to  say something of intermediate formality here
% that gives the right intuition --- we need to say something about variation, 
% or something else that's equivalent with different words. That's the 
% concept we need to communicate here. It doesn't matter if it's undefined, we
% are about to define it. You do a good job explaining it. We'll explain it 
% eve more carefully in the appendix. I promise you nobody at Neurips will care.
% People take so much information theory (and whatever else) for granted; for 
% the full paper, I think we can say enough to make you happy, because I belive
% it really is well-motivated.
%joe12: sometimes less is more.  If you can impart useful intuition.
%oli14: THe reason I keep changing your words in this particular
%section is not 
% because I don't like the way they sound, but because I think they either say
% almost nothing, or they are slightly wrong / misleading. Considering the 
% (perhaps regretably) standard % culture of talking in a cavalier way about
% information, I think I have a bit better sense of how to impart useful
% intuition about entropy.
%I'd *strongly* prefer to say something well in the appendix and nothing
%here rather than saying something not-so-good or unclear here.   
%that's good.  If what you impart is not so useful, then not so good.
%oli13: The current sentence does not serve the correct purpose (and its purpose has
% alredy been fulfilled by the first sentence in the paragraph).
% Thus, given ${\dg M}$, for each measure $\mu$, we consider how well
% the structure of ${\dg M}$ matches that of $\mu$.
%oli14*: I actually really like this sentence, save for the word "anything" 
% (which may be replacable but I could not do easily without either
% being more vague or 
% technical than is your standard preference)
% Think about it carefully before you change it. 
    % The best description of an outcome drawn from a joint distribution that 
%oli16: deleting entire sentence, starting over below.
%joe13
%qualitatively matches $G$, should intuitively both be efficiently
    % qualitatively matches $G$ should intuitively both be efficiently
%oli16
    % described by $G$
    % wherever $G$ has an edge,
%oli16: I'm not super happy with the two lines above either.  line below added
% and then removed.
% described by the conditional outcomes of each edge of $G$,
%joe13
%while also encoding uncertainty about
%oli15: it's not just dependence--- it's technically not accurate
%because there's also the univariate case! 
% while also not having dependences between variables where there is
% no edge in $G$.
%joe14*: Sorry; I do not understand ``maximal information'' in the next
%line.   %This has to be rewritten.  My preference would be for
%reinstating what I wrote.   I don't understand the issue with the
%univariate case, and evenif there is an issue with the univariate
%case, it doesn't prevent what I wrote from being good intuition.
%oli16: the problem is that   
% while also being requiring maximal information to encode relationships
% not in $G$ [[REWRITE]]. 
%oli16: It's so very difficult to write this sentence without making
%it technical, talking 
% about information, or lying....
%oli16: What I had before:
    % The best description of an outcomedrawn from a joint
    % distribution that qualitatively matches $G$ should intuitively
    % both be efficiently described by $G$ wherever $G$ has an edge,
    % while also requiring maximal information to encode relationships
    % not in $G$. 
%oli16: I did a bunch of rewrites, and kept the last 4 in the comments below.
%oli16:rewrite 1:
    % A distribution $\mu$  qualitatively matches $G$ well if it is
    % efficient to describe the outcomes along the edges of $G$, but
    % hard to describe features of $\mu$ that are not encoded by the
    % edges of $\mu$.  
%oli16:rewrite 2:
% pros: no lies, short, should be reasonable intuition. 
% cons: "respect" and "relationships" are vague.
%joe15: this didn't help at all.  I don't understand what it means to
%be ``inefficient to describe''.  It's OK to lie a little when giving
%intuition.  We said above than an think of edge from X to Y as
%''representing a qualitative claim that the value of $Y$ can be
%(noisily) computed from $X$ alone.'' If you want to say something about
%non-edges, it should be that there is no edge from X to Y if we can't
%compute X from Y alone.  If you think that's not quite right, then
%it's better to say nothing than to say something which is likely to
%confuse the reader, especially given how we've described the meaning
%of an edge.
%oli17: I don't think that saying that helps bridge the gap between this idea
%and the next one. I am willing to settle with the document in its current
%state. However, if you read these two paragraphs again closely, you will see
%a rather large chasm between the "qualitatively described just by X" bit, and
%the "information requird to draw a sample" bit. We do not explain why they are
%related. We might modify the first part, but we cannot delete it because we
%need to talk about the qualitative nature of a link; I think that changing the
%secong part significantly is an even bigger mistake, as this is the best
%intuition we give for the equation.
%
%$G$ should be inefficient to describe in every respect except those
%relationships that are encoded by the edges of $G$.  
%oli16:rewrite 3:
% pros: technically accurate
% cons: not the clearest why this is desirable. 
    % We judge a distribution $\mu$ to qualitatively fit $G$ better
    % when the features of an outcome $\mat w \sim \mu$ that follow
    % from the conditional outcomes of $G$ are easy to describe (given
    % $\mu$), and worse when any other features (i.e., those not
    % described by $G$) of $\mat w$ are easy to describe (given
    % $\mu$). 
%oli16:rewrite 4:
% pros: still accurate, more explicit about why it works
% cons: not very eloquent, and might require some of the figures to
% really see why this is the same as the formalism. 
    % Intuitively, a distribution is most likely to have been
    % generated by the edges of $G$ if the information required to
    % specify a the target variables of $G$ directly is no more than
    % the information required to sample separately along the edges of
    % $G$. If $\mu$ makes it easy to describe other aspects of a
    % sample, that $G$ does not articulate, this counts against the
    % match, because we imagine that $G$ would have included these
    % edges if they were easy to communicate qualitatively. 
%oli13: This "anything" is intentionally vague. We cannot clarify this with 
% just the variables it says nothing about, because cycles are possible. 
%joe12*: NO!  I don't know what it means, and I won't accept vagueness
%here.  It's better to cut it!  I need to go now; I'll come back to
%this around 2.
%oli14: Ok, I'll try something slightly less vague.
% anything
%joe13: cut
%\todo{anything}
%that is not described by $G$.
%oli21:
% To formalize this, we require only the underlying multigraph
%joe19: undid; I have no idea what a ``variable multigraph'' is
%To formalize this, we require only the underlying variable multigraph
To formalize this, we require only the 
%oli23 line shave
%underlying 
%oli22: we need alpha, unfortunately... which in a sense is still the
% still just the graph, since it's just a relaxation of the graph's adjacency
% matrix to have weights. 
%oli23 back again for here.
% (weighted) multigraph
multigraph
%oli21: don't want to carry the subscript. Need the values.
% $G^{\dg M} := (\N^{\dg M}, \Ed^{\dg M})$ of $\dg M$. 
%oli22: our new notation comes in handy, + we need the qualitative weights 
% $G := ((\N^{\dg M},\V^{\dg M}), \Ed^{\dg M})$.
%joe20: For what it's worth, the standard mathematical convention is
%to make objects with simpler fonts components of objects with more
%``complicated'' fonts.  Thus, it's strange to see G in italics taking
%G in fraktur as an argument.  
%oli23: way simpler for now.
% $G \!:=\! (\Gr^{\dg M},\alpha^{\dg M})$ of $\dg M$.
$\Gr^{\dg M}$.
%joe10: I'm not ure where there ``are known'' is coming from
%oli12: I don't think this is strictly necessary, but it could be useful
% clarification. It seems you (and sometimes I) sometimes get
% mixed up about what exactly this information means.
% The point is that it's the information needed to specify
% the outcome if you already know it is drawn from \mu. 
%joe11: the sentence with ``are known'' doesn't even parse.  Let's
%just leave it.
%oli13: Oops, this parses though (I don't think the "we thus" buys us anything except fluff):
%joe13: whether or not \mu is known is irrelevant!  Why empahsize it
%For a fixed $G$ and known $\mu$, contrast the amount
%oli15: very false. \mu has to be common knowledge! I won't change
%joe14: knowledge has nothing to do with anything here!  There are no
%agents in the picture in this definition.
%oli16: We're now heavily leaning on the term "description", which I argue
% is effectively knowledge. As a technical matter, the way we
% calculate the length of this  
% description assumes that the distribution \mu is known to both the
% sender and reiever of the description.
%joe15: I don't see why.  A calculation is a mathematical expression.
%At best you can say that they way you *use* the result of the calculation is
%meaningful only if something is common knowledge,  but that would
%depend on the usage.   The calculation itself does not depend on
%knowledge. There is no sender and receiver anywhere in our formalism.
%oli17: Ok no more agentive terminology. For the  "description" metaphor to make
%sense, it's necessary that the description gets to exploit the distribution.
%Therefore both the construction of the description, and the interpretation of
%the description, must depend on \mu. Are we now on the same page?
%joe16: I'm afraid not.  I'm not sure what you mean by ``the
%interpretation of the description'' and how ``the construction of the
%desription'' differs from ``the description''.  I'm happy to agree
%that everything (whatever everything is) depends on \mu.

%oli23:
% Given $G$ and $\mu$,
Given a multigraph $G$ and distribution $\mu$ on its variables,
contrast the amount of
%For an arbitrary  $G$ and $\mu$ are known, contrast the amount of
% We thus consider the difference between the
information required to 
%oli21: I globally replaced ``(a)'' and  ``(b)'' with the appropriate \ref.
%joe19: Please undo.  You should start with (1), (2), then go to (a)
%and (b), and then to (i) and (ii).  I could live with using (1) and
%(2), but would prefer not to have (i) and (ii).  In any case, why are
%you wasting our time on this??  I explicitly asked you to focus on
%the new material and not make changss to the old text!
%oli22: ^ Because (1) and (2) are used for the equations immediately below this, 
% I'm reverting to (a)/(b).
% \begin{enumerate}[label=(\roman*)]
\begin{enumerate}[label=(\alph*)]
%oli21: added labels for both items.
\item directly describe a joint outcome  \label{item:globalinfo}
%oli20: line shave
%$\mat w ~ \sim \mu$ drawn from $\mu$, and 
$\mat w$ drawn from $\mu$, and 
\item separately specify, for each edge $\ed LXY$, the value
    $\mat w_Y$ (of $Y$ in world $\mat w$) 
	given the value $\mat w_X$, in expectation.
%oli22: continue minimal mentioning of \alpha so that the argument still works
%oli24: I think the intuition is better here but I've come this far...
	% (weighted by the confidence $\alpha_L$ in this dependence).
	\label{item:localinfo}
\end{enumerate}
%oli21: restructure to break things up (see my comments below):
%joe19*: Oliver, why are you doing this??  It means that I have to
%spend time reading this and undoing many of your changes.  So rather
%than saving me time, you're costing me a nontrivial amount of time.
%Moreover, this is giving you an excuse not to do what you and I both
%agreed that you shoudl be doing.  PLEASE DON'T CHANGE THE MAIN TEXT.
%Largely reverting back.   I don't know what ``noise'' you're talking
%about, since we've never mentioned noise.  This is not the place to
%do so. I'll leave it to you to deal with the (i)/(a) mismatch. 
%oli22: fixed the (i)/(a) mismatch. I'm doing this because I still think it's
% incredibly important to get this right. I know you're furstrated but I'm
% totally convinced we're not doing this justice. In some cases what you've
% written is wrong. I can't just leave it like this.
	%According to $G$, \ref{item:localinfo} should be small (it would be
	%zero were it not for the allowance for noise),
	%while  \ref{item:globalinfo} is the information required to eliminate
		%all noise.
%oli22: I've merged your version (under "old version below:" a few hudred lines
% down), with mine (immediately above), to produce the following:
%joe20*: Cut; I still don't understand this.  Why should (b) be small
%according to G?  G has nothing to say about (b).  More importantly,
%WE AGREED THAT YOU WOULDN'T DO THIS.  This is a waste of my time and yours.
%According to $G$, knowledge of $\mu$ and the source variables $X$ should be
%sufficient to compute the target variables $Y$, and so \ref{item:localinfo}
%should be small. 
%oli22: cut again; I still think this is lazy b/c (b) is not expanded, and misleading
% because it's only in expectation, and  "description of the world" isn't accurate.
% Moreover, my version below has almost exactly the same content, modulo
% the 
	 % When these two quantities are equal, a specification of (b) has
	 % exactly the same length as a full desciption of the world. 
%joe18: Why did you cut this line?  I thought it was useful for the reader
%oli21: it seems like a weaker restatement of the definition of (a) without expanding (b) to give any insight. I was hoping I could replace it with a clear example of where it intuitively feels obvious that the two should be the same. 
%oli21: This sentence also makes me slightly uncomfortable because "description of the world"
% must be interpreted relative to $\mu$, which might be a point of confusion.
% minor rewrite. 
	 % When these two quantities are equal, a specification of (b) has
	 % exactly the same length as a full desciption of the world. 
%joe19: cut; I don't know what a specification ``along an edge'' is.
%oli21: reinstated with clarification; this is a more technically accurate 
% and version of what you keep reinstating above;
If \ref{item:globalinfo} $=$ \ref{item:localinfo},
%oli22: to bring this closer to what we had, I'll cut this line again.
 	%as occurs for instance when $\mu$ has no uncertanty,
%oli22: clarified "along each edge"
%joe20: reverting back; this doesn't help.  
%	 a specification along each edge of $G$ (as in  \ref{item:localinfo})
a specification of (b) has
 exactly the same length as a full desciption of the world. 
%joe18*: I couldn't parse the next sentence.   I think you mean to be
%talking about distributions that are point masses, but why focus on them?
%oli21: Yeah, by degenerate I mean point masses. The intent was to give a clear
% example of where the information deficiency is zero. Somehow it seems 
% geometrically relevant that the term goes to zero on the vertices of the simplex.
% --- any individual world, regarded as a distribution, has no info deficiency.
%oli20:
%For degenerate distribution $\mu$, every variable is known zero
%further information, (a) = (b) = 0, and there is no information
%deficiency. 
%joe10: misplaced  
%In particular, this is true if
%$\mu$ is a degenerate distribution, in which case the agent, who knows
%$\mu$, requires no descriptions at all.  
%
%If (b) $>$ (a), then there are extra correlations in $\mu$ that are
% If (b) $>$ (a), then there are dependencies suggested by $G$ that are
% not present in $\mu$
If \ref{item:localinfo} $>$ \ref{item:globalinfo}, then there are
correlations in $\mu$ that allow for a more compact representation
than $G$ provides. 
% The larger the difference, the more information is needed to determine
% targets $Y$ beyond their sources $X$ (which according to $G$ should be
% sufficient to compute them)
%joe18*: I found this hard to follow, so rewrote slightly.   But, more
%importantly, I'm really confused.  I would have thought that if (b) > (a)
%then the information given in G might be enough to determine \mu, 
	%oli20: In retrospect I may have writen this in a misleading way.
	% It is true (and basically just a restatement of the defns of a and b)
	% that if (b) > (a), and if the bits in (b) were used more intelligently,
	% the number of bits used for (b) could determine an outcome drawn from \mu.
	% However, it is NOT true that the data of (b) determines (a). It could be
	% that there are just a ton of parallel edges which make (b) expensive
	% without saying anything about the rest of the distribution. Do not take
	% this as saying something about G; recall that for the purposes of this
	% definition, we take G as truth, and evaluate the quality of \mu relative
	% to it. Many parallel edges in G, amounts to an assertion that for \mu to be 
	% any good, it must be such that that Y is almost entirely determined by knowing X.
	%(intuitively: you're willing to compute it in all these different ways, which agree.)
	% 
%joe18*[continued]:... but it can be described more compactly.  You're suggesting
% that it's not enough, which would be the case if (a) > (b), not if (b) > (a).  
%oli21*:
% The quantity (b) is the total amount of information REQUIRED (in expectation),
% to determine Y, given the value of X. So if (b) is large, this is an intuitive
% violation  of G's qualitative assertion that Y can be computed from X --- 
%  --- because if this were the case, then not very much information is required
% to determine Y if X is known. 
%
% I think what you wrote is misleading for two reasons:
% (1) we're not using the conditional probabilities associated to edges; we're 
% only using data of \mu, and
%joe19*: I don't understand this point.  We looing at the entropy of
%the conditional probability determined by \mu relative to the
%conditional probability associated with the edge.
%oli22*: We're NOT doing that here! (though that is what we do for Inc.)
% I keep rephrasing it in part to minimize the probability that readers
% interpret it this way. We're looking at the conditional probability 
% determined by $\mu$ on every edge, relative to the global probability of $\mu$.
% *The cpds associated to the edges do not enter the calculation*.
% (2) it doesn't mention the value of the source X, which is critical. 
%joe19: I don't think this is a problem; we said ``conditional
%probabilities associated with the edge X -> Y.
%joe19: reverted back
%oli22: This is technically wrong still. I've altered my version 
% below to address the point you brought up.
% old version below:
%joe20*: I don't see why it's technically wrong.  We can discuss it
%AFTER THE DEADLINE
% The larger the difference, the more information is needed to determine
	% targets $Y$ beyond the conditional probabilities associated with the
	% edges $X \rightarrow Y$ leading to $Y$
	% (which according to $G$ should be sufficient to compute them)
%.
%oli21: Below is a technically accurate version which is quite hard to parse,
% for the reading I intended ---  but feel free to ignore if you like something
% along the lines of my rewrite, which is a little different.
%      The larger the difference, the more the additional information needed (in
% expectation) to determine, for each link from $X$ to $Y$, (in a world $\mat
% w\sim \mu$, knowing $\mu$, but not $\mat w$) the value of $Y$ ($\mat w_Y$)
% already knowing the value of $X$  ($\mat w_X$), which according to $G$ should be
% sufficient to compute $Y$ with some uncertainty,  exceeds the total amount
% uncertainty there is to be had in $\mu$ ($\H(\mu)$).  
%oli21: I've rewritten this sentence probably 50 times. I think
%finally found something 
% that's not too long and ties together the metaphrs a little better 
%joe19*: I think that this was a TERRIBLE use of your time, and
%something I explicitly asked you not to do.  Despite your concerns, I
%strongly prefer the original version; I don't know what ''noise along
%G'' means. I undid all your changes.  Please don't spend time on this now.
%oli22: that's fair. one more attempt...
%jeo20: reverting back.  It seems to me that what you wrote is going
%the wrong way in any case.  If (b) > (a), this says that \mu gives a
%more compact description that G does, so I find what you wrote really
%confusing.  I kept the last phrase
The larger the difference,  the more information is needed to determine
targets $Y$ beyond the conditional probabilities associated with the
edges $X \rightarrow Y$ leading to $Y$
(which according to $G$ should be sufficient to compute them), 
%%the shorter $\mu$ falls in acounting for all of its uncertainty along each edge of $G$ (as in \ref{item:localinfo}),
% the shorter $\mu$ falls in acounting for all of its noise along $G$,
% the more $\mu$'s correlations undermine the qualitative picture painted by $G$,
% the more such correlations undermine a view of \cref{item:locclinfo} as noise,
% the less reasonable it is to view \cref{item:localinfo} as noise, 
% the more the noise along $G$ exceeds all noise in the system, 
% the more the variation that $G$ attributes to noise exceeds the total entropy in the system, 
% the more additional information beyond a of each edge target given its source exceeds the information required to determine a sample drawn from $\mu$,
%
%oli22: these lines were in the previous version and did not get reverted.
%and so the greater the $G$-information deficiency of $\mu$, 
and the poorer the qualitative fit of $\mu$ to $G$.
%oli21: third hand
% On the other hand, 
Finally,
if \ref{item:globalinfo} $>$ \ref{item:localinfo}, then 
%oli21: (new line) add the clause for intuition on this half also
%joe19: I don't understand the clause that you added; I cut it.
%$\mu$ must also display further uncertainty on variables $G$ does not
%constrain, because a world requires 
% we give $\mu$ bonus credit for displaying uncertainty about random
% variables $G$ does not constrain, and requires 
% $\mu$ must also align with $G$'s indeterminateness by displaying
% uncertainty about random variables $G$ does not constrain, because
$\mu$ requires
%oli21: comma
% additional information to specify beyond
additional information to specify, beyond
%oli20: minor style edits + trying to continue style pattern in other direction
% what could be used to encode outcomes of the marginals selected by $G$.
%oli21: make it easier to parse by swapping out a "specify"
% what is necessary to specify outcomes of the marginals selected by $G$.
what is necessary to determine outcomes of the marginals selected by $G$.
%joe18: cut this; I don't think it helps.  I prefer what we had
%before.  More importantly, I think what we had before will be more
%meaningful to more readers.
%oli21: Ok I agree now.
%---what might be called a $G$-information excess.

% $\mu$ now has a $G$-information \emph{surplus}.
%joe11*: I don't like this, and don't really understand it.  Why are
%we suddenly making things  unspecified by G constants.  I cut it.
%joe13* :I don't understand why this should be true, and don't
%understand why yu want ot make things a contsnt.  Again, I feel
%strongly that this hurts far more than it helps.
%oli15*: You wanted to know what zero means. I'm motivating negative
%numbers. In any case we still need the punchline. partially
%reinstated. 
%joe14*: I still don't understand.  What does it mean to fix things
%that are unkonwn, and why should we want to do this?  If you can't
%explain this better, then I feel strongly we should cut the next line.
%oli16: A distribution fixes something that is unknown, if there's a
%feature that has no presence in $G$, and according to $\mu$ this
%feature is a constant.
%joe15: I simply did not understand the sentence above at all.  I
%wrote a whole book on uncertainty.  I'm considered an expert in the
%area, yet I find the language in your preceding sentence completely
%mysterious.  I feel like we're speaking different languages.  
%We don't want to fix things that are unknown. You've got it backwards: I'm
%saying that such a distribution DOES in fact match all of the independencies of
%$G$, so it would not be fair to ding it. But another distribution that did all
%of this and also made these unknown features in $G$ also difficult to describe
%--- this distribution deserves extra credit.
%Such a $\mu$ is arguably a \emph{better} fit to $G$, than a
    % Such a $\mu$ is arguably a \emph{better} fit to $G$ than a
    % distribution that that merely obeys the dependencies (and fixes things
    % that are unknown) [[REWRITE]]. 
%oli16:rewrite.
%joe15*: I give up.  I don't understand this at all, so it clearly is
%not helping my intuition.  I have no clue why we're comparing to a
%distribution that just exhibits the dependence structure of G (which
%I assume means that \mu has all the conditional independencies
%expressed by the edges of G.  Where did that come from?  I have no
%idea what it means for G to to articular features of an outcome.  My
%best guess is that you're trying to say something about non-edges,
%but that's a guess.
%oli17: IDef has two effects:
% (Here "wherever" means for every region of the info diagram, i.e., subset
% of variables that excludes some other subset.)
% (1) to give a max-entropy result wherever G does not specify something
% (think: no edges, or a cycle that allows multiple solutions), and 
% (2) to give a min-entropy result whenever it is over-specified. For regions
% with exactly 2 overlapping variables, this amounts to a statement of
% independence.
%joe16: This may be true; I don't understand IDef well enough.  I
%suspect it's not true, but just describes extreme cases.  In any case, it
%is definitely *not* what we should be writing here.  It's far too
%dependent on the details of this scoring function, and doesn't get at
%the essence of what we hope the scoring function is trying to do.
%You keep saying ``the scoring function does the right thing''.  What
%we should be saying here is what the ``right thing'' is, not
%describing what the scoring function does.
%oli17 Examples. In [X -> Y], X is qualitatively under-specified, and in 
% [ X <-> Y ], the mutual data between X and Y is qualitatively under-specified.
% On the other hand, in [X <- 1 -> Y], the area shared in both X and Y is
% over-specified (as it is specified by both edges), and so the resulting IDef
% term penalizes mutual information, giving a min-entropy result.
%
%oli17: now, with regard to the above, I'm trying to hint at (1). I'm trying to 
% that if G has nothing to say about a variable Z (maybe no edges to Z), or about the mutual information between X and Y (maybe there's a cycle, with many soultions), then $\mu$ gets bonus points for not being certain about it. 
%
%oli17: I can articulate this more clearly with the information diagrams---which I think
% I am going to write in a separate document for you, and not press here right now.
%joe16: I will say again that this discussion doesn't belong at this
%point in the paper (and is probably best saved for a meeting).  
%joe15:  I view the next sentence as a significant net
%negative.  The likelihood that it will hep a reader are far lower
%than the likelihood that it will confuse the reader.  I'm cutting it.
%oli17*: If we do not at least state the polarity, there is another big gap
% in our story here---why do we even bother saying this sentence? It doesn't 
% even lead to a vague intuition about IDef. We need to discuss this in person.
%joe16: Yes; let's discuss it in person
%Such a $\mu$ is arguably a \emph{better} fit to $G$ than a
%distribution that that merely exhibits the dependence structure of
%$G$, because it expresses uncertainty about features  
%%oli16 next line optional
%(of an outcome) 
%that $G$ does not articulate.
%is silent on.
%oli16: I notice you don't like the "is silent" on. Why is that?
%oli16 I'm also using "features" to avoid specific mention of 
% variables here, when in fact, it could involve any relationship
% between any number of variables, which is too many words and
% distracts from the point.
%joe15: This indirect way of talking about things requires the reader
%to read your mind.  I obviusly don't understand the point.  
% than one in which anything
%unspecified by $G$ were a constant (and thus would be known
%immediately even before 
%drawing a sample)---even though this would still be perfectly
%consistent with each 
%edge determining its target.
% Such a $\mu$ is arguably a \emph{better} description
% of the agent's knowledge than a description in which anything
% unspecified by $G$ were a constant according to $\mu$ (and could be
% known once we are handed $\mu$, without the need to communicate any
% information about a sample)---even though this would still be
% perfectly consistent.  
%joe10: worse fit than what?
%not suggested by the graph structure, which makes a $\mu$ a worse fit
%if you think the graph is correct.
%not suggested by the graph structure.
%joe19
%If (a) $<$ (b), then there is something in the distribution that $G$
%is silent on; distributions that are more uncertain in these places
%fit the graph better. 

\begin{defn}\label{def:info-deficiency}
%oli21: technically we also need \V. 
% For a multi-graph $G = (\N, \Ed)$ over a set $\N$ of variables,
%joe19
%For a multi-graph $G = ((\N,\V), \Ed)$ over a set $\N$ of variables,
%oli22: no change, but pulling this also into the variable macro.
% For a multigraph $G = ((\N,\V), \Ed)$ over a set $\N$ of variables,
For a multigraph $G = (\varsNV, \Ed)$ over a set $\N$ of variables,
define the \emph{$G$-information deficiency}
of distribution $\mu$, denoted $\IDef{G}(\mu)$,
by considering the difference between (a) and (b), 
where we measure the amount of information needed for a description
using entropy: 
\begin{equation}
%oli22: this is where \alpha needs to be introduced, or else our alternate
% representation is an alternate definition, not a proposition.
%oli23: change back for here. 
	\IDef{G}(\mu) := \sum_{(X,Y) \in \Ed} \H_\mu(Y\mid X) - \H(\mu). 
	% \IDef{G}(\mu) := \sum_{\ed LXY \in \Ed} \alpha_L \H_\mu(Y\mid X) - \H(\mu). 
	\label{eqn:alt-extra}
\end{equation}
%oli11: I've rewritten the commented out material below.
%		This approach neatly combines the benefits of choosing
%                the maximum-entropy distribution consistent with
%                constraints \cite{Jaynes57}, with the ability to
		%                articulate qualitative independences.
%oli11:
% where, just as in the case of incompatibility, we are measuring the amount of
% surplus/deficit using entropy.%
%\footnote{Recall that $H_\mu(Y\mid X)$, the
(Recall that $H_\mu(Y\mid X)$, the
%oli19: shortening "with respect to" (\mu is implicit anyway in the literature)
% so that the text justification doesn't make the spacing strange
% \emph{conditional entropy of $Y$ given $X$} with respect to $\mu$, is
($\mu$-)\emph{conditional entropy of $Y$ given $X$}, is
defined as $- \sum_{x,y \in \V(X,Y)} \mu(x,y) \log \mu(y\mid x)$.)
%oli21: this had \V, (I meant to type \N), but in fact it needs both.
% For a PDG ${\dg M}$, we take $\IDef{\dg M} = \IDef{(\V^{\dg M}, \Ed^{\dg M})}$. 
%oli22: using our new notation
% For a PDG ${\dg M}$, we take $\IDef{\dg M} = \IDef{((\N^{\dg M}, \V^{\dg M}), \Ed^{\dg M})}$.   
For a PDG ${\dg M}$, we take $\IDef{\dg M} = \IDef{\Gr^{\dg M}}$.   
\end{defn}

We illustrate $\IDef{\dg M}$ with some simple examples.  
%joe10*: I find this incredibly frustrating.  I'm sure you spent hours
%on this.  Unfortunately, these figures tell me NOTHING about the
%definition we have given.  
%
%They don't relate to our discussion abot
%dependency.  I don't know what ``shared information'' means, and it
%is not mentioned in our earlier discussion.  you can't bring these
%things up out of the blue.    There might be an interesting
%connection that would be worth making to BN's, but we haven't
%discussed it in the text, and I crtainly can't make sense of it from
%thee pictures.  I have no inuution for what the ``candidate area''
%means,
%oli12: ... I never said "candidate area".
%joe11: you did!  See the line immediately below, which is commented out
% and why variation in the candidate area makes things better,
%since you haven't related this to our discussion.
%I'm cutting the figure.  It may be worth adding a figure to the main
        %paper, but it won't look anything like this.
\commentout{
	%oli11*: This is much longer than it should eventually be, but I assume you will be happier cutting and rewriting than trying to come up with this yourself.
    %joe10: I basically scrapped it and started from scratch, since what
    %you wrote didn't really helallp my intuition at all (besides being too long)
    	
    %oli11: recycled from above with modifications.
    %oli14: Do you think it is possible to hint at any of this? In my
    %view, this is  
    % the payoff, and if we relegate it to the apendix (like we probably
    % need to) I want to hint at it. 
    %joe13: No!   It will be too unclear, so the prospect of having
    %someone disagree with it or get confused makes adding it a net negative
      This approach combines the benefits of choosing
    the maximum-entropy distribution consistent with
    constraints \cite{Jaynes57}, while also providing the ability to back off of this (e.g., by providing a universal graph structure), and simultaneously providing a way of articulating qualitative independences.
    There is no information required
}
%joe10: \end{commentout} followed by my rewrite
%oli21: adding a backslash to each H in this section so it is typeset correctly
% as an operator.
%joe19: This was not a good use of your time!
Suppose that $\dg M$ has two nodes, $X$ and $Y$.  If $\dg M$ has no edges, the
$\IDef{\dg M}(\mu) = - \H(\mu)$. There is no information required to specify, for
each edge in ${\dg M}$ from $X$ to $Y$, the value ${\mat w}_Y$ given ${\mat
w}_X$, since there are no edges. Since we view smaller numbers as representing a
better fit, $\IDef{\dg M}$ in this case will prefer the distribution that
maximizes entropy. If $\dg M$ has one edge from $X$ to $Y$, then since $\H(\mu) =
\H_{\mu}(Y \mid X) + \H_\mu(X)$
%oli21: this is not worth saying. My guess is over 99% of the people
%who  get this far into the paper will know this, there's no vagueness, 
% and it feels condescending to  point it out.
%joe19: undid; I guess that I'm in the remaining 1%
by the well known \emph{entropy chain rule} \cite{mackay2003information},
$\IDef{\dg   M}(\mu) = -\H_{\mu}(X)$. Intuitively,
while knowing the conditional probability $\mu(Y \mid X)$ is helpful, to
completely specify $\mu$ we also need $\mu(X)$. Thus, in this case, $\IDef{\dg
M}$ prefers distributions that maximize the entropy of the marginal
on $X$. 
If $\dg M$ has sufficiently many parallel edges
% What I don't like about "sufficiently many" is that we're varying the graph
% while keeping $\mu$ constant, and so $\H > 0$ is not particularly meaningful.
% I'm adding an edge 1 -> X so that this is correct and has the right emphasis.
%joe13*: Aargh!! You took a clean story with two nodes, and muddled it
%by adding a third node.  This is not helpful.  I reverted back.  
% an edge $1 \to X$, and also 
%oli15* AAhh This node doesn't even change the joint space! It's also
%not and  emphasizes the wrong things
%joe14*: but from a reader's perspective, it does (and you've never
%said that the node 1 doesn't count for the sample space.  If that's
%the cse, you need to say that somewhere!
%oli16*: I'm not even treating it like a special variable. It just has
%one value, so the sample space is naturally isomorphic to the sample
%space that does not include it.
%joe15: I will repeat.  These are things you haven't discussed.  Why
%complicated the reader's likfe by introducing it?
%oli17: another possibility: define IDef for hyper-graphs. It's more intuitive,
% we'll be able to use examples like this directly, and it will immediately
% apply to other directed graphical models.
%joe16: NO!!!  After saying that we're not going to deal with
%hypergraphs, we shouldn't bring them in now.
%oli16: When I last looked at this I didn't re-perform the changes you made here
%due to time pressure, but I still think the emphasis on a fixed \mu and a
%changing M is much less clear than the reverse. Besides, wasn't that our
%rationale for changing IDef to have M as a subscript? You're emphasizing this,
%and the number zero, both of which you have repeatedly told me make no sense. I
%do not understand why you wrote the example this way. 
 %oli14: fix spacing
 %more than one parallel edge
%joe13: this wasnn't worth the effort; I really don't want to
%introduce this notation out of the blue
 % $X \substack{\to\\[-1em]\to} Y$
 from $X$ to $Y$
%oli13:
%joe13: reverted
 and $\H_{\mu}(Y \mid X) > 0$ 
%while $H_{\mu}(Y \mid X) > 0$ 
%oli12: this is reversed for H, but the correct intuition for each edge.
% Because of the context, I'm assuming you meant the former and changing it.
% (so knowing $X$ tells you something about $Y$),
(so that $Y$ is not totally determined by $X$)
then we have $\IDef{\dg M}(\mu) > 0$, because the redundant edges add no
information, but there is still a cost to specifying them.
In this case, $\IDef{\dg M}$ prefers distributions that make $Y$ a
deterministic function of $X$ will maximizing the entropy of the
marginal on $X$.
Finally, if ${\dg M}$ has an edge from $X$ to $Y$ and another from $Y$
to $X$, then a distribution $\mu$ minimizes $\IDef{\dg M}$ when 
% $X$ and $Y$ depend on each other 
%joe11: I don't like ``have randomness'', and it's unneessary.  I
%don't see why ``vary together'' is better than ``depend on each
%other'', but I'll leave it
%$X$ and $Y$ both individually have randomness, but vary together
$X$ and $Y$  vary together (so that $\H_\mu(Y \mid X) = \H_\mu(X \mid Y) = 0$)
while maximizing $\H(\mu)$, for example, by taking $\mu(0,0)
= \mu(1,1) = 1/2$. 
%oli12: added.
%joe11
%More complicated examples and graphical illustrations can be found in
%joe16*; this for now, since I'm not sure that this wil be
%true.  If it is, we should reinstate this.
%More examples can be found in the appendix. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %I know you didn't ask for this, but it made it way easier to articulate
% % the dependency network stuff.
% Furthermore, $\IDef{}$ captures conditional independence.
% % , while also providing the ability to back off of this (e.g., by providing a universal graph structure), and simultaneously providing a way of articulating qualitative independences.
% 
% \begin{defn}
% For sets of variables $\mat X, \mat Y, \mat Z \sutbseteq \N$, the quantity%
% 	\footnote{where $\I(\mat X; \mat Y \mid \mat Z)(\mu)$ is the conditional information between $\mat X$ and $\mat Y$ given $\mat Z$, a non-negative quantity which is zero iff $\mat X \CI_{\mu} \mat Y \mid \mat Z$.
% }
% \[  \frac{\partial \IDef{G}}{\partial \I_{\mu}(\mat X; \mat Y \mid \mat Z)}(\mu) < 0 
% 	\text{for all $\mu$ iff} \mat X \CI_{G} \mat Y \mid \mat Z
% 	% \quad\text{iff}\quad \mat X \CI_\mu \mat Y \mid \mat Z
% 	 \]
% we say $ \mat X \CI_{\IDef{G}} \mat Y \mid \mat Z$ iff it is negative. 
% \end{defn}
% \begin{prop}
% 	$\mat X \CI_{\IDef{G}} \mat Y \mid \mat Z$ iff $\mat X \CI_{G} \mat Y \mid \mat Z$
% \end{prop}
% 
% %%%%%%%%%%%%%%%%%%%%%%%

$\Inc_{\dg M}(\mu)$ and $\IDef{\dg M}(\mu)$ give us two measures
of compatibility between ${\dg M}$ and a distribution $\mu$.
We take the score of interest to be their sum, with the tradeoff
controlled by a parameter $\gamma \ge 0$:

\begin{equation}
  	  \bbr{\dg M}_\gamma(\mu)
%joe8*: your formullation does not let us ignore Inc.  I don't see why
%we shouldn't allow ignoring Inc
%			 := \Inc(\dg M,\mu) + \gamma \extrainfo\mu
%oli10: This is a good point. I am actually sympathetic to this framing of it, 
% Howeer, it's not mathematically necessary becasue we already have
% scaling parameters for each $\beta$.  
% I actually now buy that convex combinations are the "right" thing to
% do, but it involves adding two extra scalar parameters that change
      % nothing to get the units right.
%joe9: I don't see why; let's discuss.          
% In the full paper I would prefer to do this properly as a convex combination.
%oli10: Because of our discussion and my rewrite above, I am reverting this. 
% I'm not trying to be stubborn though; my preference has weakened,
%and if you care you should change it back. Just be aware that the
%factor graph section will require a few factors of 1/(1-\gamma).  
%joe9*: OK; let's leave it as is  for the abstract; we have better
%things to focus on.  But since it seems that we agree that the convex
%combination is ``right'', let's do that for the full paper.
	  % := (1-\gamma) \Inc(\dg M,\mu) + \gamma \extrainfo\mu
	 := \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)  \label{eqn:full-score}
\end{equation}

The following just makes precise that the scoring semantics generalizes the first semantics.

\begthm{prop}{prop:sd-is-zeroset}
% For all PDGs $\dg M$, we have that $\SD{\dg M}=\{\mu : \bbr{\dg M}_0(\mu) = 0\}$. 
	$\SD{\dg M} \!= \{ \mu : \bbr{\dg M}_0(\mu) \!=\! 0\}$ for 
%oli22: line shave
	% every PDG $\dg M$.
	all $\dg M$.
\end{prop}
          
While we focus on this particular scoring function in the paper, 
in part because
%oli19
% it leads to interesting results and
% has deep connections to the free energy of a factor graph \cite{KF09},
%joe17: what does it mean that an anlogy is common in a model?  You
%can replace ``factor graph'' by ``undirected graphical model'' if
%it's correct
%doesn't parse in ENglish. Reinstated most of earlier text
%it replicates the thermostatistical analogy common in undirected
%graphical models, 
it has deep connections to the free energy of a factor graph \cite{KF09},
other scoring functions may well end up being of interest. 
%oli19*: You may appreciate this
%joe17*: I might appreciate it if I understood it.  is there a space
%of possible divergence functions?  If so, this needs to be explained
%*much better* before it can be included.  Specifically, you have to
%describe what the space is, and give a reference.  I cut it for now.
%oli20*: a divergence D on a space X := any D satisfying  { D(x,y) = 0
%iff x=y }.
%joe18: This may be a standad definition, but I've never seen it
%before.  You certainly can't assume that the reader knows it.
% To get the BN theorem, we only need Inc to be such that a distribution minimizing
% it is compatible with all CPTs.  (Please follow up if you still want a reference.)
%
%oli21: what do you think of adding the below with that definition?
%joe19: Please don't add it.   If you did, you would have to explain
%divergence.  Moreover, this is just one result that we have.
%In fact, \Cref{thm:bns-are-pdgs} would follow from \emph{any} choice
%of divergence, so the fact that PDGs extend Bayesian Network semantics
%does not hinge on this choice. 
        
%oli8: deleted. Higher value to talk about trade-off parameter
%	For $\gamma = 0$, the only thing that matters is consistency, as $\mathcal U_0 = \Inc$.
%joe7: I have no idea of what ``regularization strength'' means.
%The trade-off parameter $\gamma$ can be seen as a regularization strength,
%                trading off quantitative fit for
                %        qualitative accuracy.
%joe7**: why do we focus on this case?  We need to motivate this.
%joe8: I cut this from here: it's the wrong place.
%        In this paper, we focus on the case
%        where $\gamma$ is small, making $\Inc$ the more important term. 
%joe4*: cut; this is false, unless you're allowing infinitesimals.
%Why complicate things?
%	For $\gamma>0$ but arbitrarily small, the preference for
%        consistency dominates, and extra information is used only to
%        break ties. If an agent is very certain that their cpds are
%        well-informed, which we will call the `low ambient
        %        uncertainty' case, this may be appropriate.
        %joe4*: I don't understand this intuition, Oliver.  It needs to
        %be explained better.  In what way do you get more ``safety''
        %if \gamma is larger?

\commentout{
	\begin{example}
		Consider the PDG with variables $\sf 1$ and $X$, and two distributions $p, q$ on $X$, that only overlap with $\epsilon$ mass.
		\todo{This works out nicely but I actually don't think this can be done without eliding a row of a cpd (which I've been calling sub-stochasticity). This is the one discussed in meeting, where two experts disagree on their except for $\epsilon$ overlap.}
	\end{example}


    $\mathcal U_\gamma(\dg M, p)$ can be thought of as the energy state of
            a distribution (larger numbers indicate a worse fit) and can
            be anywhere between $-\infty$ and $\infty$.
            To get a semantics in the style of \cite{halpern2015weighted},
            we transform it so that it is non-negative and larger scores
            are better (making it distribution-like, by analogy to a
            Boltzmann distribution) and normalize so that the lowest
            energy distributions get weight 1. We introduce a scaling
            parameter $k > 0$ (the corresponding inverse temperature)
           which does not affect the rankings of distributions. 
    %joe4*: I would like to add a sentence relating this to Halpern and
    %Leung, but I don't undrestand the scaling parameter at all.   Why
    %complicate matters. We can just take it to be 1.
           \[ W^k_\gamma(\dg M, \mu) := \exp\Big(  -k \Big\{\mathcal U_{\gamma}(\dg M,\mu)- \inf_{\mu'} \mathcal U_\gamma(\dg M,\mu')\Big\} \Big).\]
    Now 
    $W^k_\gamma(\mu) \in  [0,1]$, for all $k$, and hence acts like a weight in the sense of  Halpern and Leung \citeyear{halpern2015weighted}.
    For now, we set $k=1$, but note that $k$ does not affect the ranking of distributions of scores according to $W^k_\gamma(\dg M, -)$, and so $\arg\max_{\mu'} W^k_\gamma(\dg M, \mu')  = \arg\min_{\mu'} \mathcal U_\gamma(\dg M,\mu')$ is independent of $k$. 
    %
    $W_\gamma$ and $\mathcal U_\gamma$ carry roughly equivalent information aside from the normalization, but the latter is slightly more useful and will play a larger role in our analysis, and so we notationally use this as our second semantics: $\bbr{\dg M}_\gamma(\mu) := \mathcal U_\gamma(\dg M, \mu)$.
}
%joe4: \end{commentout}

\commentout{        
	We now offer generalizations of some results found in in
        \Cref{sec:set-of-distribution-semantics}:
        \Cref{def:cont-inconsist} is the continuous version of
        inconsistency we were searching
        for. \Cref{prop:union-weight-semantics} shows that the
        weighted semantics $\bbr{-}$ also has the modularity
        properties we wanted, and \Cref{thm:zetaconvex} will let us
        define concrete distributions just like its more specific
        counterpart. 

	%	\Cref{prop:union-weight-semantics} shows that the weighted distributions also have the modularity properties we're interested in.
	\begin{prop}[name=\Cref{prop:union-set-semantics} analog]\label{prop:union-weight-semantics}
		$\bbr{\dg M \cup \dg M'} = \bbr{\dg M} + \bbr{\dg M'}$
	\end{prop}
	
	\begin{coro}\label{cor:u-convex}
		$\mathcal{U}_\alpha(\dg M, p)$ is convex in $p$ for $\alpha \geq 0$, and strictly so for $\alpha> 0$. Furthermore, $\bbr{-}$ is quasiconvex---that is, all of its level sets are convex sets.
	\end{coro}

	As a result of \Cref{cor:u-convex}, $\bbr{M}$ is a quasiconvex function 
	%, and hence every local minimum is a global minimum. 
	In particular, \Cref{prop:convex} is the special case for the level set $\mathcal U \leq 0$ for $\alpha = 0$. 
}

	% unhelpful?
\commentout{
	The semantics given in \Cref{sec:set-of-distribution-semantics} does not allow us to express independencies very effectively.
	For instance, in \Cref{ex:smoking}, to fully get the joint
        representation given by the BN, we also need to somehow assume
        that $SC \CI S \mid PS$. This is possible to do explicitly
        with an extra arrow meaningless arrow, but this solution doe
        not scale well, looks like it dependence (the opposite of our
        intention), and clutters the diagram.  
	We now show that by applying the principal of maximum entropy, we can recover all of a BN's independence assumptions at once. This should make intuitive sense: maximizing entropy tend to `make things as independent as possible'. 
	Those familiar with the view of graphical models as separable exponential families may have even seen a similar result in an undirected setting \cite[e.g..][pp. 37-39]{wainwright2008graphical}. 
	We proceed with the theorem, and discuss some consequences below.
}

\subsection{PDGs As Unique Distributions}\label{sec:uniq-dist-semantics}

Finally, we provide an interpretation of a PDG as a probability distribution.
Before we provide this semantics, we stress that this distribution does
\emph{not} capture all of the important information in the PDG---for example, a
PDG can represent inconsistent knowledge states.  Still, by giving a
distribution, we enable comparisons with other graphical models% 
%oli24:
% what is "it"? 
%. It also shows that PDGS are 
, and reveal PDGs to be
a surprisingly flexible tool for specifying distributions.  
The idea is to select the distributions with the best score. 
We thus define 
\begin{equation}
	\bbr{\dg M}_\gamma^* = \argmin_{\mu \in \Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu).
\end{equation}   

In general, $\bbr{\dg M}_\gamma^*$ does not give a unique distribution.  But if
$\gamma$ is sufficiently small, then it does:

\begthm{prop}{prop:sem3}
	If $\dg M$ is a PDG and $0 < \gamma \leq \min_L \beta_L^{\dg M}$, then
	$\bbr{\dg M}_\gamma^*$ is a singleton. 
\end{prop}

In this paper, we are interested in the case where $\gamma$ is small;
this amounts to emphasizing the accuracy of the probability
distribution as a description of probabilistic information,
%joe20: IDef does not talk directly about the independence structre.  
%rather than the independence structure of the PDG%.
rather than the graphical structure of the PDG%.  
%oli22: poorly worded.
% This is what was going on in all the examples in the introduction.
%joe20: I'm not sure what the ``this'' is, nor what ability you're
%talking about (since the previous lines don't talkabout ability).  I
%tried to put what you meant
%, an ability we exploited often in the introduction.
In the examples, that we considered in the introduction, our focus was
the probability, so this choice is compatible with the way we
presented the examples.
This motivates us to consider
what happens as $\gamma$ goes to 0.  If $S_\gamma$ is a set of
probability distributions for all $\gamma \in [0,1]$, we define $\lim_{\gamma
\rightarrow 0} S_\gamma$ to consist of all distributions $\mu$ such that there
is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and
$\mu_i \to \mu$ such that $\mu_i \in S_{\gamma_i}$ for all $i$. 
It can be further shown that 

\begthm{prop}{prop:limit-uniq}
    For all $\dg M$, $\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$ is a singleton.
\end{prop}
%
Let $\bbr{{\dg M}}^*$ be the unique element of $\smash{\lim\limits_{\gamma
	\rightarrow 0}} \bbr{{\dg M}}_\gamma^*$. 
%
%oli9: this prop is false for now
\commentout{
	There is a unique such distribution because, as we now
	show, the score is strongly convex
	which can be found efficiently \cite{strongconvexopt}.
    \begin{prop}\label{prop:u-convex}
      $\bbr{\dg M}_\gamma(\mu)$ is $\gamma$-strongly convex.% in $\mu$.
    \end{prop}
	%oli7: I can do a better job here; I was feeling rushed when I wrote it.
%joe7*: Is this result still true, now that we're going with the other
%definition of extra information?
%oli9: Only if \alpha\gamma < \beta for all links.
    \begin{proof}
      $\Inc_{\dg M}( \mu)$ is convex in $\mu$
      (\Cref{thm:inc-convex}), and $\gamma\sum\alle \E_{x\sim \mu_X}
      \H(\bp(x))$ is linear in $\mu$.  
		Negative entropy is $1$-strongly convex
		(\Cref{prop:neg-ent-convex}), so $- \gamma \H(\mu)$ is $\gamma$-strongly convex.
		The sum of a $\gamma$-strongly convex, linear, and
        convex functions must be $\gamma$-strongly convex. 
%		, and strongly so when the coefficient on $-\H$ ($\gamma$) is positive. 
		%(see \cite{Rockafellar1970ConvexA})
	\end{proof}
}%oli9: end commentout
 %oli8: change text
% We can now define the unique distribution semantics:
%joe7
    %    We use this to get our desired semantics
%oli9 added alternate proposition for Extra.
%joe8*: I simplified the wording.  In any case, this needs a proof and
%can go to the appendix.  We'll need the space.
\commentout{
    \begin{prop}\label{prop:convex-if-gamma-small}
      If $\dg M$ is a PDG and
    %joe8*
      %  $\beta_0$ is a constant less than any
      %        $\beta_L \in \beta^{\dg M}$, then for any $\gamma < \beta_0$,
      $\gamma < \min_L \beta_L^{\dg M}$, then
      $\bbr{\dg M}_\gamma$ is a strictly convex function of $\mu$.%
    %  		\footnote{All proofs can be found in \Cref{sec:proofs}.}
    \end{prop}

                              
                              
    %oli9: expanded this, added footnote.
    % Proposition~\ref{prop:u-convex} allows us to define our desired
    \Cref{prop:convex-if-gamma-small} allows us to define our desired
    semantics by ensuring the limit%
    	\footnote{$\mu$ is in this limit iff there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in \bbr{\dg M}_{\gamma_i}$ for all $i$.}
     in \eqref{eq:uniqdist} is well-defined.
	
     %oli8: reformat with equation, added the limit.
	\begin{equation}
		 \bbr{\dg M}_* := \lim_{\gamma\to 0^+}\argmin_{\mu \in
    %joe7
    %                   \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu) 
    %oli9: I missed this instance of \U when I eliminated it.
                       % \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu). 
				   \Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu). 
		   \label{eq:uniqdist}
	\end{equation}
}
%joe8: \end{commentout}
The semantics has an important property: 

\begthm{prop}{prop:consist}
	$\bbr{\dg M}^* \in \bbr{\dg M}_0^*$, so if $\dg M$ is consistent,
	then $\bbr{\dg M}^* \in \SD{\dg  M}$.
\end{prop}

%oli22: some more intersting material;
%joe20*: I would be happy to keep this if I could make sense of it.  I
%looked up ``information projection'' in KF, and couldn't find it.
%There is a notion of I-projection, of a distribution P onto a set of
%distributions \Q; it's the best approximation to P in \Q as measured
%by relative entropy.  That seems to be what you have in mind, but I
%don't have a clue of what the product of the P_l's is, or what set of
%distribution you have in mind.  
\commentout{
In fact, it can be shown that $\bbr{\dg M}^*$ is the \emph{information
projection} 
%oli24: double parens
% (again, see \cite{KF09}) of the product of every $\bp$ into
\cite{KF09} of the product of every $\bp$ into
$\bbr{\dg M}_0^*$,  
which provides an alternate justification for many of the coming results.
}
    % \begin{defn}
    % 	For $\gamma > 0$,
    % 	$\bbr{\dg M}^*_\gamma := \arg\min_{\mu \in \Delta\V(\dg M)} \mathcal U_\gamma(\dg M;\mu)$
    % \end{defn}

        %oli8:
        %joe7
\commentout{        
	\begin{remark}
		If $\dg M$ is a consistent PDG, then 
		$\bbr{\dg M}'_* = \bbr{\dg M}_*$
	 	where $\bbr{\dg M}'_*$ is the variant of \eqref{eq:uniqdist} which uses the alternate formulation $\IBal'$ of the extra information in place of $\IDef$.
	\end{remark}
}

%joe4*: this comes out of the blue, since you haven't discussed union
%for the other two semantics %(which is as it should be; it's a distraction)   

%joe17*: this is absolutely the wrong place for this paragraph.  If we
%havea discussion of union in this paper (which I could actually
%imagine in the full paper) it should go there
\vleftovers{
      In contrast with the other two semantics, $\UD{\dg M_1 \cup
          \dg M_2}$ cannot be calculated from $\UD{\dg M_1}$ and
        $\UD{\dg M_2}$. However, it is effectively the only semantics
        offered by alternative graphical models, which contributes to
        their relative lack of modularity. We return to this after a
        more careful treatment of unions in
        \Cref{sec:pdg-operations}.
}%\end{vleftovers}
%joe4*: This is a distraction, althogh it is cute.  If I were writing
%a text on this material, it woudl be an exericse.
\commentout{
	\subsection{Recovering Other Semantics As Special Cases of Distribution Scores}

	We have just seen that $\gamma >0$, $\mathcal U_\gamma$ has a unique minimum,
        $\UD{\dg M}$. By increasing $k$ in $W^k_\gamma$, the weights of any distribution that is not optimal go to zero. Therefore, taking a limit as $k \to \infty$, we see that $W^k_\gamma$ converges to the weighted
        distribution that puts weight 1 on $\UD{\dg M}$ and weight 0 on all
                        other distributions, as shown below:
%joe3*: I have no idea why this should put weight 1 on a single
%distribution.  I'm lost!  I think we should just give a third
%semantics that amounts to picking the distribution of higest weight, 
%after stating a theorem that says that there always is one.  (Your
%results on convexity will be used in the proof of the theorem, but
%don't need to be stated separately.
%oli3*: I'm just using the standard trick to achieve the maximum without taking 
% a supremum. A supremum is not continuous, but we can get it for these strongly
% convex spaces by taking the associated temperature of the Boltzmann distribution to
% zero.  We're just doing this one level up, for the weighted distributions...
%oli4: I've made a bunch of changes so it's more readable. It still probably won't make it into the short paper, but I think it is useful intuition and verification for the naturality of the semantics.
	\[ \lim_{k \to \infty} W_\gamma^k(\dg M, \mu)
        = \lim_{k \to \infty} \exp\Big\{-k \Big(\mathcal
        U_\gamma(\dg M;\mu) - \mathcal
        U_\gamma(\dg M;\UD\dg M)\Big) \Big\} = \begin{cases}
        	1 & \UD\dg M = \mu \\
        	0 & \text{otherwise}
        \end{cases} \] 
\vleftovers{
   		A weighted distribution is closely related to a second order
    	distribution: a distribution over distributions, which can be
    	naturally collapsed to a single distribution by taking an expectation. 
   		\begin{align*}
    		\Big(\bbr{\dg M}_{\alpha_0}^*\Big)(w) &:= \lim_{k \to \infty} 
    		\E_{\mu \sim W_{\gamma}^k(\dg M,-)} [\mu(w)] \numberthis\label{eqn:higher-expectation} \\
    		&= \lim_{k \to \infty}  \frac{1}{Z} \int_{\Delta\V(\dg M)} W_\gamma^k(\dg M;\mu) \mu(w) d\mu
    	\end{align*}
    	defined where the sum is finite; $Z$ is a normalization constant across all worlds $w \in \V(\dg M)$. 
}%\end{vleftovers}

For $\gamma = 0$, and $\dg M$ is consistent
%joe10: undefined notation
% (i.e., $\SD{\dg M} \ne \emptyset$), 
%oli12: This is in the full paper, where this notation actually is
%defined. Reverting.
%joe11: this will have to be rewritten in any case ...
($i.e., \Inc(\dg   M) = 0$)
 we recover the set-of-distribution semantics with
          the same trick: 
%oli4:
%	\note{thereby providing an alternate notational justification for $^*$ as a limit as $k\to \infty$}:
	\begin{align*}
		 \lim_{k \to \infty} W_0^k(\dg M, \mu)
		&= \lim_{k \to \infty} \exp\Big\{-k \Big(\mathcal U_0(\dg M;\mu) - \inf_{\mu'} \mathcal U_0(\dg M; \mu')\Big) \Big\} \\
		&= \lim_{k \to \infty} \exp\Big\{-k \Big(\Inc_{\dg
                 M})\mu) - \Inc(\dg M)\Big) \Big\}  
		= \begin{cases}
			1 & \mu \in \SD\dg M \\
			0 & \text{otherwise}
		\end{cases} 
	\end{align*}

	In this sense, a weighted distribution provides a much more expressive semantics for a PDG than a single probability distribution, or set of them.
}%end{commentout}

%joe1*: I cut this.  You haven't explained in what sense a conditional
%distribuiton is a program, you havne't motivated it, it takes us too
%far afield ...	
\vleftovers{
	\subsection{As Probabilisitic Programs}\label{sec:prog-semantics}
	
	One final way of viewing PDGs is as a set of probabilistic programs, corresponding to the edges. 
	Conditional distributions can be thought of as probabilistic programs. As a result, we can compose and run them: paths from $A$ to $B$ correspond to noisy estimates of $B$ from $A$.
	
	Specifically, if $f(b \mid a) : \mathcal V_A \to \Delta \mathcal V_B$ and $g(c \mid b) : \mathcal V_B \to \Delta \mathcal V_C$ are conditional distributions, then the probabilistic composition $g\circ f : \mathcal V_A \to \Delta\mathcal V_C$ is
	\begin{align*}
		(g\circ  f) (c \mid a) :=  \sum_{b \in \mathcal V B}\!\! f (b \mid a)\ g(c \mid b)
	\end{align*}
	
	This can be recognized as a matrix multiplication $f$ and $g$ regarded as sub-stochastic matrices.
	Thinking about graphical models this way makes thinking about chains of reasoning simpler, gives us a way out of storing probability tables, and suggests additional applications.
	
	More formally, we can define
	\[ \bbr{M}_\lambda = \left\{
			\begin{aligned}
				 \text{paths } p = N_0 \xrightarrow{p^1} N_1 \xrightarrow{p^2}\cdots\xrightarrow{p^n}N_n \\
				 \text{such that } (N_{i-1}, N_i, p^i) \in \Ed^\dg M
			\end{aligned}
		\right\} \]
	
	\begin{example}
		Composition of arrows in a tables in chain is simply an easy case of variable elimination. 
		
		\[
			\scalebox{0.8}{
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& C
			\end{tikzcd}\hspace{3em}
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& B \ar[r] \& C
			\end{tikzcd}}
		\]	

		Conversely, factorization of a table $A \to C$ into tables $A \to B$ and $B \to C$ (i.e., a stochastic matrix factorization) corresponds to splitting a program into two steps, and the data necessary to describe it will be smaller if $|B|$ is small.
	\end{example}	
	
	% It also has not escaped us that PDGs have a particularly nice description in categorical terms, which we do not pursue further here.
	
	Furthermore, thinking about the mental state of an agent as a collection of programs you could run from any concept gives our first natural interpretation of a sub-distribution (more in \Cref{sec:full-model}): probability mass assigned to $\none$ by a edge $p$ has not terminated yet (if at all). 
	Even given infinite time, some paths in $\bbr{\dg M}_\lambda$ may be infinite.
	
	\begin{defn}
		A PDG $\dg M$ is \emph{strongly consistent} if every collection of paths $P \subseteq \bbr{\dg M}_\lambda$ is compatible, in that 
		$$\bigcap_{p \in P}\ \SD*{\vphantom{\Big|}p^0\circ \cdots\circ p^k} \neq \varnothing$$
	\end{defn}

	\begin{example}
		Bayesian Networks and conditional Bayesian Networks are strongly consistent.
	\end{example}

	\begin{prop}
%		$ \text{strongly consistent}  \subsetneq \quad 
%		\text{strictly consistent}  \subsetneq  \text{consistent} $
		Any PDG $M$ that is strongly consistent is also consistent, but some strongly consistent PDGs are not strictly consistent.
	\end{prop}
}
%joe9
%\section{Relations to Other Graphical
%          Models}\label{sec:other-graphical-models}
%oli21: updating title
% \section{Relations to BNs and Factor Graphs}
\section{Relationships to Other Graphical Models}
\label{sec:other-graphical-models} 
%oli21: no need to mention DNs here, right?
%joe19: right
%We now relate 
We start by relating
PDGs to two of the most popular graphical models: BNs and factor
graphs. PDGs are strictly more general than BNs, and can emulate factor graphs
for a particular value of $\gamma$. 
%oli8: Unecessary, I'll get to it. Would require updating anyway. Deleted.
	% More concretely, we will see
    %     that we can get the standard free energy of factor graphs, and
    %     more generally, of the full exponential family that it
    %     corresponds to, by setting each $\alpha$ to zero, and removing
    %     an implicit  `local regularization' term in $\mathcal U$. 
%	; for others, consult \Cref{fig:model-transformations} and its explanation in \Cref{sec:many-relations-graphical-models}.
%joe10: can cut this to save space if needed to save space
%oli12: done
%joe11: reinstated, since we have the space, but I don't mind cutting
%it again.  But it actually doesn't seem to save space in practice
%joe17: OK; I think we have room.  But we shoudl definitely reinstate
%the section numbers I'm pretty sure they're allowed
%\vfull{
\subsection{Bayesian Networks} 
%}%\end{vfull}
\label{sec:bn-convert}
\commentout{	
	A (quantitative) Bayesian Network $(G, f)$ consists of two parts: its qualitative graphical structure $G$, indicating a set of
	variables and
	conditional independencies, and its quantitative data $f$, an assignment of 
	a cpd $p_i(X_i \mid \Pa(X_i))$ to each variable $X_i$.
    %
    %joe4: this may be true, but why bother saying it?
    %oli5: I guess I really have a terrible model of what you view as
    %worth saying. This might not be the most efficient use of space, but
    %I think provides useful historical background, explains why the
    %problem hasn't been solved yet, provides a great deal more intuititon
    %about how this solution works than the proof. It also tells a story.  
    %joe5: My model is ``have a clear conception of the story and
    %ruthlessly restructure things so as to bring it out''
            %oli5: I've changed it to vfull, as I understand we're short on space,
    %but I remain confused about why you don't view it as worth
    %saying---especially in contrast to the verbose expansions of
    %sentences you employ when you rewrite my texts, and reitterations of
    %previous points with "as we've said". 
    %joe5: HOw does it fit the story?  We are not telling a story about
    %BNs, but about PDGs.  Even in the full paper, it doesn't belong.
            %oli5: I also think the narrative and reasons for dropping the independences are important for discussing BNs, which have historically had that focus.
    % I've therefore reinstated this paragraph, and promoted the rest of the comment to the full version.
    	The first is usually seen as more fundamental
    %oli8: Updated to reflect new understanding of \alpha, though could use further editing
    %	; one can think of the corresponding PDG as keeping only the second. 
    %joe7
    %	, but equation \eqref{eq:uniqdist}, specifically the limit as
    %        $\gamma \to 0$, can be thought of as elevating the
    %        quantitative data above the independence assumptions.  
            but the third semantics ($\bbr{\dg M}^*$) can be
            can be understood as viewing the quantitative information as
    %joe7*: But this begs the question.  Why are we doing this?
    %oli9: Because with BN's it's impossible to break the independece assumptions.  Worse, there's no way to sepcify constaints ---even constraints consistent with the independence assumptions--- unless they lie on one of the edges of the graph. 
    % In a BN, independence is primary. But I think it's really easy to argue that those independencies aught to take a back seat to the data. This way you can do both at once.
            more important that the qualitative independence assumptions.  
    %oli5: I can do without this sentence though:
    %	Fortunately, there is an intuitive way to recover the
    %independencies by optimizing for a natural information-theoretic
    %quantity: the extra information (\Cref{sec:extra}). 
    %joe5*: Kept the sentence above. Cut the rest.  I don't even know what
    %contravariant means in this context.
    %oli6: I find this a useful explanation of why keeping track of 
    % independences messes up modularity. I explain what I mean by contravarient
    % immediately below. It can be cut for space but I'm marking it for
    % the full paper 
    %joe6: No!  This is a distraction.  We are not writing a paper on BNs,
    %or what is the right way to interpret things to get modularity. Focus
    %on the story!
    \vfull{
    	This is the more desirable option if one cares about
    	modularity, because the two components are contravariant: a subset of
    	the graph, and hence of the cpds, results in a \emph{super-set} of
    	the independencies, and vice versa. It is in part for this reason
    	that a BN does not say monotonically less when edges are deleted, or
    	more when edges are added. 
    }

    %joe7: I don't understand the net paragraph, so I'm just cutting it.
    \commentout{
    To do without the independence assumptions, one might hope
            that maximizing entropy would recover the conditional
            independencies, as maximizing entropy tends to make things as
            independent as possible given the constraints --- but
            maximizing entropy alone is not enough
            (\Cref{ex:counterexample}).
    } %joe7: \end{commentout}
    \vfull{	
    	In response, some \cite{williamson2001foundations}\cite{holmes2001independence} have added alternate constraints of a causal flavor, which are perhaps smaller and more palatable than the full set of conditional independencies.  Williamson, for instance, introduces what he calls the \emph{principle of causal irrelevance}--- that extending a BN with variables $\{C_i\}$ with children $\{D_j\}$ where no $C_i$ depends on a $D_j$, restricts to the same distribution as the original.  However, these constraints are also overkill: by merely maximizing entropy one can already get the BN distribution for rooted trees, disconnected graphs, and even graphs that have nodes with multiple incoming edges, so long as every row in each such target node's cpd has the same entropy---none of which are reflected as a weakening of assumptions in a Williamson's principle of causal irrelevance.
    	
        %	\begin{enumerate}
        %		\item (1) In contrast with a Bayesian Network, in which each node has a set of parents, each node of a PDG has possibly many sets of parents, where each set of parents corresponds to a different constraint, associated to a different table, and (2) We no longer require conditional independence of non-descendants given children
        %		\item A BN is just a PDG where every cycle commutes		
        %		% \item A tree. 
        %	\end{enumerate}
    }
    %oli5: I've rewritten this more dramatically and pulled it out of the comment, as a transition
    %joe5
    %The key insight%
    %joe8: cut this too.  it's no longer consistent with what we do (and I
    %never undrstaood it anyway).
    The key insight is that we can recover the BN distribution if we control for
    %joe5: sorry; I don't understand this.  What does it mean to control
    %for the counterfactual nature of the cpd?  For that matter, what's
    %counterfactult about it?
    %oli6: This is the motivation for the extra information. We
    %acknowledge that a cpd 
    % results in a distribution at its target, whose entropy depends on
    % the distribution at its 
    % source. Therefore, the cpd results in a different constraint,
    % depending on what the distirbution 
    % at the source is (the cpd counterfactually contains information
    % about the distribution at $Y$, even if the distribution at X were to
    % be completely different). In minimizing the information we know
    % about the distribution, we have to control for the fact that cpds
    % have this property, making them very unlike the standard constraints
    % that are used (e.g., for exponential families). The resulting
            % correction gives us the extra information.
    %joe6*: If you want to keep this, you need to slow down.  Look at a BN
    %of the form X -> Y and point out that the cpd for X gives us the
    %actual distribution on X, but the cpd lets us detemine the marginal
    %probabilty of Y for all distributions on X.  In that sense, its
    %giving us counterfactual information.  As I said in an earlier joe6*
    %comment, you probably should say this earlier.
            %Then exlain (slowly) how your definition does account for it.
    %This will be a mysterious definition to many readers, so you have to
    %motivate it much better.         
            the counterfactual nature of the cpd as a constraint, as we
            do in \Cref{sec:scoring-semantics}, allowing us to recover the
    %joe6
            %        independences without assuming them.
            independencies without assuming them.
    %}        
    Nevertheless, as we shall show, our third semantics still allows us to
    recover the independencies.
}%joe8: \end{commentout}

\Cref{constr:hyperedge-reducton} can be generalized to convert arbitrary Bayesian Networks into PDGs.
%oli19: don't need to wrap in defn
% \begin{defn}[BN to PDG]
%oli21:
% Given a BN $\mathcal B$, and a positive number $\beta_X$ for
%         each variable $X$ of $\cal B$,
%joe19
%Given a BN $\mathcal B$, and a positive confidence $\beta_X$ for
Given a BN $\mathcal B$ and a positive confidence $\beta_X$ for
the cpd of each variable $X$ of $\cal B$,
%joe10
%oli120: reinstating your %joe10 .
let $\PDGof{\mathcal B, \beta}$
%oli21:
% be the PDG containing of the cpds of $\cal B$
be the PDG comprising the cpds of $\cal B$
%oli20: ... with this extra text:
%oli21:
% in this way, the straightforward formal details of which we defer to the appendix. 
in this way; we defer the straightforward formal details to the appendix. 
%oli20: here's what we had before:
% we denote its corresponding PDG $\PDGof{\mathcal B, \beta}$.  
% We defer the
% straightforward formal details
% to the appendix. % (\Cref{def:bn2PDG}).
% \end{defn}

	
	% \begin{theorem}[restate=thmbnsRpdgs]\label{thm:bns-are-pdgs}
    % \begin{restatable}{theorem}{bnsRpdgs}\label{thm:bns-are-pdgs}
\begthm{theorem}{thm:bns-are-pdgs}
 	  If $\cal B$ is a Bayesian network
%oli15:
    % and specifies the distribution $\Pr_{\cal B}$, then 
%joe14: for consistency
%          and $\Pr_{\cal B}$ is the distribution it specifies, then
          and $\Pr_{\cal B}$ is the distribution it specifies, then
          %oli11: insert \betas, and reword because one semantics distribution is provably unique.
	% for all $\gamma > 0$,
	% we have $\bbr{\PDGof{{\mathcal B}}}_\gamma^* =
	% \bbr{\PDGof{{\mathcal B}}}^*$.  Moreover, the unique probability distribution in
	% $\bbr{\PDGof{{\mathcal B}}}^*$ is the distribution specified by
	%             ${\mathcal B}$.
%joe14: not all vectors \beta
%oli16*: Currently, \beta > 0 by definition; there's no need for this
% if we keep our current definition. Therefore, reverted for now.
%joe15*: You missed my point.  Do you want to rquire that all entries
%are strictly positive? If so, you have to say it.  This does not
%follow from \beta -> 0. 
%oli17: Oh, I see what you're saying. I just assuemed "all vectors beta" was
% valid shorthand for "all valid vectors of positive numbers, like in our
% definition"---but you're probably right to state this
% explicitly. I'm reinstating 
% your version.
%it does in one of our definitions
          % Benefits of mandating \beta > 0:
%  - it means a PDG always represents a unique distribtion as \gamma -> 0
%  - we don't have to keep mentioning this condition (and we're short on space).
% Benefits of allowing \beta=0
%  - Can articulate an edge qualitatively without supplying a cpt (ideally
%       we would articulate how this works better before doing this. Ideally,
%       we could get to the point where people buy the qualitative picture in 
%       well enough that they understand the diagrams and feel like they know
%       what a qualitative edge does)
%  - [works better with \alpha]: \alpha = 0 makes a lot of sense, and so 
%       the symmetry probably worth it when we include \alpha.
        % for all $\gamma > 0$ and all vectors $\beta$,
%oli16: reinstated the above and commented out the below:
        for all $\gamma > 0$ and all vectors $\beta$ such
        that $\beta_L > 0$ for all edges $L$,
%joe15: I will not change this, but if you don't change it back to my
%version, then you have to weaken the requirement  \beta_X > 0 in
%Definition 4.1.
        %joe14
    %    $\bbr{\PDGof{\mathcal B, \beta}}_\gamma^* = \{ \Pr_{\cal B}\}$.
        $\bbr{\PDGof{\mathcal B, \beta}}_\gamma^* = \{ \Pr_{\cal B}\}$, 
    %oli15: added
    %joe14: It's not ``in particular'', since it's not a special case
    %of the above, although it does follow from the above.
    %oli16: does it not make sense to you to say "these two functions
    %    are the same, so in particular, their minima are the same?" 
%joe15: what you wrote below is certainly a logical consequence of
%what you wrote above, but it's not a special case.   I would not say
%(in English) ``in particular, their minima are the same''.   I would
%say ``and so their minima are the same''.  We're arguing about
%English here, not mathematics.
    %oli16: whether or not this is an obvious special case might depend on your 
    % representation of the function (it's natural to represent a convex
    % function as a taylor expansion around its critical points, for instance).
        %    In particular, $\bbr{\PDGof{\mathcal B, \beta}}^* = \Pr_{\cal B}$.
and thus $\bbr{\PDGof{\mathcal B, \beta}}^* = \Pr_{\cal B}$.    
\end{theorem}
%oli24: disclaimer. 
\Cref{thm:bns-are-pdgs} is quite robust to parameter choices: it holds for any
choice of $\gamma$. However, it is our only result that does
not have an analog for $\alpha \ne \mathbf 1$.

%oli11: For the full paper, once we add restriction & combination, add the following result for conditional BNs.
%joe10*: If we include this, we'll need a *much* better story.  The
%goal is not to overwhelm the reader with theorems, but to tell a
%story.  My guess is that if this belongs anywhere, it belongs in a
%section on  modularity, where we have a more general discussion of
%modularity   This will be an example there.
%joe17*: See my comment above
%oli20: oops, agreed.
\vleftovers{
    \begin{theorem}
    If $\mathcal B_1, \mathcal B_2, \ldots$ are a conditional Bayesian
    networks containing whose sets of variables they condition on are
    pairwise disjoint, then they can be combined into one conditional BN
    $\cal B$, and  PDG union 
        %	\[ \dg M} := \bigoplus_i \mathcal
 	\[ {\dg M} := \bigoplus_i \PDGof{\mathcal B_i} \]  
			satisfies
			$\bbr{\dg M} = \bbr{\PDGof{\cal B}}$.
	\end{theorem}
}


%joe6*: Note for the Nuerips submission, nad we don't want to talk
%about \alpha_L yet
\commentout{
    %joe4*: I don't understand the corollary (or even the notation)
	%oli5: I've introduced all of the notation, but I can explain
	%it better. This is the more powerful version of the theorem,
	%and in my opinion, is the real payout of this appraoch: not
	%only is it true for some settings of the parameters, but you
	%get th BN distribution out for ALMOST EVERY setting of the
	%parameters. 
    \begin{coro}
		Let $\cal B$ be a BN. For any $\gamma > 0$, and vector of positive numbers $\vec \beta > \vec 0$, 
		\[ \UD[\Big]{\PDGof{\mathcal B}, \vec{1}, \vec
                  \beta}_\gamma = \Pr\nolimits_{\cal B} \]
%oli5: added. I did this because you say you don't understand the
%statement above, but I have all of this explanation below as well. We
                %should probably remove one of the two.
%joe5: I didn't read this; I don't want to think about \alpha yet (and
%I still don't understand it).                  
		That is, so long as we take each $\alpha_L = 1$, the
                distribution defined by the cpds of $\PDGof{\mathcal
                B}$ with \emph{any weights} is precisely the one given
                by $\cal B$. 
	\end{coro}
	The corollary extends \Cref{thm:bns-are-pdgs} through the other two semantics, showing that the result is not sensitive to the additional parameters ($\beta, \gamma$), and works for the default value of $\alpha = 1$.
	This is true for PDGs which are structurally just subsets of BNs, where every node has at most one incoming edge. In such a structure, every cpd can be simultaneously attained perfectly regardless of how little you are attached to them ($\beta$) and the strength of the bias towards uncertainty $(\gamma)$.
	However, not all PDGs have the particularly nice structure,
        and these parameters will be important once there starts to be
        possible conflict between beliefs.  
}
%joe6: \end{commentout}
% d-separation? I don't have a lot to say but it the specialness of the ``colider'' or head-to-head nodes in determining connectedness  is related to the difference in interpretations I think.
%oli12: done
%joe11
%joe17
%\vfull{
\subsection{Factor Graphs} 
%}%\end{vfull}
\label{sec:factor-graphs}
%oli8: moved all of the original material to the appendix, this section is new.
%joe7
%	Factor graphs \cite{koller2009probabilistic}, make some
%        similar promises to PDGs. They generalize BNs, the barrier to
%        adding observations is extremely low, and their failure to
%        normalize in general may be viewed as a kind of inconsistency
%        in a very similar fashion \cite{wainwright2008graphical}.
%joe18: we may want to cite the original paper on factor graphs here,
%along with (probably more accessible) KF09; I on'd tfeel strongly
%about this though.
%oli21: added refernece; is easy to remove. I've also subsituted
% the wainwright reference because it's much more focused on factor graphs
% and the authors more strongly take the "factor graphs generalize BNs" position.
% Factor graphs \cite{KF09},
Factor graphs 
%oli22: The original paper is actually really persuasive and arguably
%a better introduction
%joe20: you need to send me an updated bib file
%\cite{wainwright2008graphical,kschischang2001sumproduct},
\cite{kschischang2001sumproduct},
%	like PDGs, generalize BNs and have a low barrier to adding observations.
%joe19: we never discuss adding observations to a factor graph
%	like PDGs, generalize BNs.
%oli22: fair. On the other hand, they are clearly less strict and
% anyone who knows about them will identify that they solve the "we can't 
% legally add this information" problem.
%oli22: also, I want to soften this b/c  while they can represent the 
% distribution of any quantitative BN, they don't capture the 
% indepencencies of a qualitative BN  (though directed factor graphs do) and
% do not contain the counterfactual information that a BN does. 
% This may seem like a technicality, but it is actually a core part of the
% story: the factor graph representation does not exactly capture the BN; its
% sensitivity to future additions &  depenednce on \gamma,\beta are features
% that the BN does not have.
%Rewording:
%joe20: why is it just ``claim''.  They clearly do, in a precise
%sense.  In what sense are they more modular?  I've never seen the
%modularity of factor graphs discussed; rather, the claim is that
%having the factors makes this more efficent computationally.  Why
%make statements that may be controversial or unclar, that we don't
%make se of anywhere?  This is not a paper on factor graphs.
%like PDGs, claim to generalize BNs, and are also much more modular.
like PDGs, generalize BNs.
%oli22: I still would defend this, but let's not go there right now. Most
%people would agree with
%*%
%joe20: maybe, but what about those that don't?  why introduce this
%when we don't need it.
% moreover, their failure to
%         normalize in general may be viewed as a way of representing
%         some inconsistency.
%oli22: introduce one more acronym
% In this section, we consider the relationship between factor graphs and PDGs.
In this section, we consider the relationship between factor graphs (FGs) and PDGs.
\begin{defn}
%oli22: if "set" is preferable to "collection" for indexed sets when
% we define PDGs, it's certainly prefereable here also. 
 % A \emph{factor graph} $\Phi$ is a collection of random variables
 A \emph{factor graph} $\Phi$ is a set of random variables
        $\mathcal X = \{X_i\}$ and \emph{factors}
       $\{\phi_J\colon \V(X_J) \to \mathbb R_{\geq0}\}_{J \in
%joe19
%\mathcal J }$ %where each $J \in \mathcal J$ is associated
\mathcal J }$,
%joe19
%where each $X_J \subseteq \mathcal X$.
%more precisely, each factor $\phi_J$ is associated with a subset
where $X_J \subseteq \mathcal X$.  
More precisely, each factor $\phi_J$ is associated with a subset
$X_J\subseteq \mathcal{X}$ of variables, and maps
joint settings of $X_J$ to non-negative real numbers.
%
%oli23*:moving this material inside the definition
$\Phi$ specifies a distribution
\[ {\Pr}_{\Phi}(\vec x) = \frac{1}{Z_{\Phi}}
 	\prod_{J \in \cal J} \phi_J(\vec x_J), \]
%joe21
%where $\vec{x}$ is a joint setting on all of the variables,
where $\vec{x}$ is a joint setting of all of the variables,
 $\vec{x}_J$ is the restriction of $\vec{x}$ to only the
 variables $X_J$, and $Z_{\Phi}$ is the constant required to
 normalize the distribution.  
\end{defn}

%oli23*: moved all of this material below
% We take a \emph{weighted factor graph} $\Psi$ to be a pair $(\Phi,\theta)$ consisting of a factor graph $\Phi$  together with a vector of non-negative weights  $\{ \theta_J \}_{J \in \mathcal J}$.
% $\Psi$ specifies a distribution 
%oli23: refactor, using \Phi here, no weights, \Psi later.
% \[ {\Pr}_{\Psi}(\vec x) = \frac{1}{Z_{\Psi}}
% \[ {\Pr}_{(\Phi,\theta)}(\vec x) = \frac{1}{Z_{\Phi,\theta}}
% 		\prod_{J \in \cal J} \phi_J(\vec x_J)^{\theta_J} \]


%oli23: eliminating this is a benefit of the refactor
	% A factor graph $\Phi$ defines a distribution $\Pr_\Phi$ and scoring function 
	% $\GFE_{\Phi}$ by implicitly taking every $\theta_J = 1$.

%joe21: this isn't a good story
%The cpds of a PDG straightforwardly constitute the data of a factor graph.
We can associate with each PDG a unique factor graph and vice versa.
The map from PDGs to factor graphs takes the cpds of the PDG to be the
facotors of the factor graph.  

%joe21
%\begin{defn}[PDG to factor graph]\label{def:PDG2fg}
\begin{defn}[unweighted PDG to factor graph]\label{def:PDG2fg}
%oli22: first do this for an unweighted PDG.
% If $\dg M$ is a PDG, define   
If $\dg N = (\Gr, \mat p)$ is an unweighted PDG, define   
%oli21: I'm envisioning some disagreement about this notation; putting
%this in a macro to make this smoother
%joe19: I think the notation is OK, but as I said above, we might as
%well use \Psi everywhere.
%oli22: unweighted first clarifies this.
% the associated factor graph $(\Phi, \theta)_{\dg M}$ on the
% the associated WFG $\WFGof{\dg M} = (\Phi,\theta)$ on the 
the associated FG $\FGof{\dg N}$ on the 
%oli22: Note: here's another place where we refer to variables by (N,V). I 
% like making it clear that a variable is determined by both \N and \V, but
% again am open to alternate notation, so long as we keep both \N,\V. Factoring
% this also out into a macro.
	% variables $(\N, \V)$ by
variables $\varsNV$ by
%oli22: "factors given by the edges" is not quite accurate; it's the index that
% is given by the edges $\Ed$, and the factors are given by $\bp$. 
	% taking the factors to be given by the edges in $\Ed^{\dg M}$, 
%joe21: I couldn't parse this
%taking $\mathcal J := \Ed^{\dg M}$ to be the set of edges,
taking $\mathcal J$ to be the set to be the set of edges, 
%oli22: also changing "X" to "Z" because of possible name conflict; in our
% presentation X_{--} is already a specific variable. Also slowing down and
% describing the translation more carefully.
% and for an edge $L$ from $X$ to $Y$, taking $\phi_L(x,y)$ to be $(\bp^{\dg M}(x))(y)$,
and for an edge $L$ from $Z$ to $Y$, taking $X_{L} = \{Z,Y\}$, and $\phi_L(z,y)$ to be $(\bp^{\dg M}(z))(y)$.
%oli22: now we do the weighted case by re-using the above. 
	% and taking the weight $\theta_L = \beta_L$.
%joe20
%We extend transformation to one that takes a (weighted) PDG $\dg M =
%oli23*: moving the extension to below.
% We extend this transformation to one that takes a (weighted) PDG $\dg M =
% (\dg N, \beta)$  
% to a WFG $\WFGof{\dg M} := (\FGof{\dg N}, \beta)$ by setting $\theta_L = \beta_L$.
\end{defn}


%oli19: new text, some storytelling
%joe19: on't call it a a trick
%Using essentially the same trick as \cref{constr:hyperedge-reducton},
%oli22: I prefer idea singular.
% Using essentially the same ideas as in \cref{constr:hyperedge-reducton},
Using essentially the same idea as in \cref{constr:hyperedge-reducton},
we can encode a factor graph as an assertion about the unconditional
probability distribution over the variables associated to each
factor.  

%joe21
%\begin{defn}[factor graph to PDG] \label{def:fg2PDG}
\begin{defn}[factor graph to unweighted PDG] \label{def:fg2PDG}
%oli20: shuffle + add \theta (2 lines)
% If $\Phi=(\{\phi_J\}_{J \in \cal J})$ is a factor graph, then $\PDGof{\Phi}$ is
%oli21: this is fine but using WFG terminology instead:
	% For a factor graph $\Phi=(\{\phi_J\}_{J \in \cal J})$ and 
	% non-negative vector $\theta$ over $\cal J$,  let $\PDGof{\Phi,\theta}$ be
%oli22: unweighted case first.
% For a WFG $\Psi = (\Phi,\theta)$, let $\PDGof{\Psi}$ be
For a FG $\Phi$, let $\UPDGof{\Phi}$ be
%oli22: was extremely tricky to read and edit in its fragle
%run-on-sentence form.  
% split it into the bullets as we discussed.
% the PDG whose variables are the variables in $\Phi$ together with $\sf 1$ and a
% variable $X_J := \prod_{j \in J} X_j$ for every factor $J \in \mathcal J$%
% , whose edges consist of projections $X_J \tto X_j$ for each $X_j \in X_J$ and
% unconditional joint distributions ${\mathsf 1} \to X_J$ with
% associated cpd $\bp[J]$ equal to the joint distribution on $X_J$ obtained by
% %normalizing $\phi_J$; 
the unweighted PDG consisting of
\begin{itemize}
	\item the variables in $\Phi$ together
   with $\var 1$ and a variable $X_{\!J} := \prod_{j \in J} X_j$ for every factor $J \in \mathcal J$%
   , and
   \item edges ${\var 1} \!\to\! X_{\!J}$ for each $J$ and $X_{\!J} \!\tto\! X_j$ for each $X_j \in \mat X_J$,
\end{itemize}
where the edges $ X_{\!J} \!\tto\! X_j$ are associated with the appropriate projections, and each ${\var 1} \!\to\! X_{\!J}$ is associated with the unconditional joint distribution on $X_J$ obtained by normalizing $\phi_J$.
%joe19*: Are you claiming that you get the same
%result with \alpha_L = 1 and \alpha_L = \theta?  That's strange (and
%inconsistent with what you wrote later
%oli22: They don't give the same result and I didn't intend to suggest
% that they did. Hopefully this presentation is a lot clearer.
%joe19: no \alphas here.
%% finally, let \valpha{$\alpha_L = $}$\beta_L:= \theta_L$.
%oli22: why remove the colon? Others (and programming languages)
% have told me to go out of my way to distinguish between construction and
% assertion.  Also, I thought you wanted me to put an \alpha 
% in the translation to a factor graph? 
% finally, let $\beta_L = \theta_L$. 
The process is illustrated in \cref{fig:fg2PDG}.
%oli22: now the weighted case.
%oli23: moved below.
\commentout{
	We extend the transformation to weighted objects by mapping a WFG 
	%joe20*: going back to the special case of \alpha=1, AS WE HAD AGREED.   What
	%do we do for edges not in \J.   I also don't think that this is quite
	%right, sonce you haven't defined \beta for edges not in \J.  I now do
	%so, although you should check 
	%$\Psi=(\Phi, \theta)$ to a PDG $\PDGof{\Psi} = (\UPDGof{\Phi},\theta, \theta)$ 
	$\Psi=(\Phi, \theta)$ to a PDG $\PDGof{\Psi} = (\UPDGof{\Phi},\beta_{\theta})$ 
	%by taking both $\alpha$ and $\beta$ to be $\theta$.
	by taking $\beta_J = \theta_J$ for the edge $1  \rightarrow X_J$ and
	taking $\beta_L = 1$ for the projections $X_J \!\tto\! X_j$.
}
\end{defn}



%joe18
%\begin{figure*}
\begin{figure*}[htb]
	\centering
	\hfill
	\ifprecompilefigs
		\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-FG}}
	\else
	\begin{tikzpicture}[center base, xscale=1.4,
		fgnode/.append style={minimum width=2.7em, inner sep=0.3em}]
		\node[factor] (prior) at (1.65,-1) {};
		\node[factor] (center) at (3.75, 0.1){};
		
		\node[fgnode] (PS) at (1.65,0.5) {$\mathit{PS}$};
		\node[fgnode] (S) at (3.1, 0.8) {$S$};
		\node[fgnode] (SH) at (3.0, -0.8) {$\mathit{SH}$};
		\node[fgnode] (C) at (4.8,0.5) {$C$};
		
		\draw[thick] (prior) -- (PS);
		\draw[thick] (PS) --node[factor](pss){} (S);
		\draw[thick] (PS) --node[factor](pssh){} (SH);
		\draw[thick] (S) -- (center) (center) -- (SH) (C) -- (center);

%		\node[dpadded, fill=blue] (1) at (2.5,-2) {1};
%					
%		\draw[blue!50, arr] (1) -- (prior);
%		\draw[blue!50, arr] (1) -- (center);
%		\draw[blue!50, arr] (1) -- (pss);
%		\draw[blue!50, arr] (1) -- (pssh);
		
		%oli24:
		% \node[fgnode, fill opacity=0.02,dashed] (T) at (4.8, -1.3) {$T$};
		\node[fgnode] (T) at (4.8, -1.3) {$T$};
		\draw[thick] (T) -- node[factor]{}  (C);	
		% \node[factor, draw=black, pattern=north east hatch] at (Q){};
	\end{tikzpicture}
	\fi
	%oli22:improving spacing.
        % ~\vrule~
	\hfill\vrule\hfill
		% \end{subfigure}
		% \begin{subfigure}{0.5\linewidth}\centering
	\ifprecompilefigs
		\raisebox{-0.5\height}{\includegraphics{figure-pdfs/smoking-convert}}
	\else
	\begin{tikzpicture}[center base, xscale=1.6,
        newnode/.style={rectangle, inner sep=5pt, fill=gray!30, rounded corners=3, thick,draw}]
		\node[newnode] (prior) at (1.65,-1) {};
		\node[newnode] (center) at (4.1, 0.25){};
		
		\node[dpadded] (PS) at (1.65,0.5) {$\mathit{PS}$};
		\node[dpadded] (S) at (3.3, 0.8) {$S$};
		\node[dpadded] (SH) at (3.3, -0.6) {$\mathit{SH}$};
		\node[dpadded] (C) at (4.9,0.5) {$C$};
		
		\draw[arr, ->>, shorten <=0pt] (prior) -- (PS);
		\draw[arr, <<->>] (PS) --node[newnode](pss){} (S);
		\draw[arr, <<->>] (PS) --node[newnode](pssh){} (SH);
		\draw[arr, <<-, shorten >=0pt] (S) -- (center); 
		\draw[arr, <<-, shorten >=0pt] (SH)-- (center); 
		\draw[arr, <<-, shorten >=0pt] (C) -- (center);
		
		\node[dpadded, fill=blue] (1) at (2.7,-1.8) {1};
		
		\draw[blue!50, arr] (1) -- (prior);
		\draw[blue!50, arr] (1) to[bend right=30] (center);
		\draw[blue!50, arr] (1) to[bend right = 5] (pss);
		\draw[blue!50, arr] (1) to[bend left = 10] (pssh);

		
		\node[dpadded] (T) at (4.8, -1.7) {$T$};
		\draw[arr, <<->>] (T) -- node[newnode](tc){}  (C);	

		\draw[blue!50, arr] (1) to[bend right = 10] (tc);
	\end{tikzpicture}
	\fi
		% \end{subfigure}
	\hfill~
	\caption{
%oli20: oops garbled. Fixing.
% The conversion from a PDG to a factor graph to factor
% graph, and vice versa, as defined in \Cref{def:fg2PDG}. The
%joe18: still garbled
%Conversion of the PDG in \cref{ex:smoking} a PDG to a factor graph
Conversion of the PDG in \cref{ex:smoking} to a factor graph
according to \cref{def:PDG2fg} (left), and from that factor graph back
to a PDG by \cref{def:fg2PDG} (right). 
%joe17
%blue edges carry the (renormalized) cpds corresponding to the
%joe18: what does ``renormalized'' mean here?  Why did the cpds have
%to be renoormalized
%oli21: because they're now being regarded as unconditional distributions. To
%illustrate: both a cpt X -> Y and an unconditional distribution 1 -> XY are
%matrices. In the first case, each row sums to 1, whereas in the second, the
%whole matrix sums to 1. We're renormalizing the cpd so that they are
%unconditional distributions. I think your original edit introduced
%this ambiguity.
%joe19*: I'm lost.  Why are they now being regarded as unconditional
%distributions?  I'm OK with the current caption.
%oli21: let me try again to write this clearly.
	% In the latter, blue edges are associated with the cpds corresponding to the
	% original factors, each leading to a new node $X_J$ (displayed as a
	% smaller darker rectangle) whose values are joint settings of the
	% variables connected to the factor $J$. 
%
%joe19
%In the latter, for each $J$ we intorduce a new node $X_J$ (displayed as a
%smaller darker rectangle) whose values are joint settings of the
In the latter, for each $J$ we introduce a new variable $X_J$ (displayed as a
smaller darker rectangle), whose values are joint settings of the
variables connected it, and also an edge $1 \to X_J$ 
%joe19
%(blue)
(shown in blue),
%oli21: AAAI only begrudgingly accepts color I will make sure all the 
% figures, etc. look good in black and white later.
%FIXME
%joe19
%to which we associate with the unconditional
to which we associate the unconditional 
distribution given by normalizing $\phi_J$.
} 
	\label{fig:fg2PDG}
\end{figure*}


%joe1: rewrote
%Surprisingly, despite garbling the structure (see
%\Cref{fig:fg2PDG,fig:fg-intro-examples}), when we fix $\gamma=1$, the
%two operations preserve most of their semantics.
PDGs are directed graphs, while factors graphs are undirected. The
map from PDGs to factor graphs thus loses some important structure.
As shown in
%joe17: I get problems when I latex this.  It says ``As shown in
%Figures 8 and 9 in Figure 9''. Moreover, the actual figure is Figure 4.
%oli20: another problem with comments... should be fixed now. 
\vfull{
    \Cref{fig:fg2PDG,fig:fg-intro-examples}
}%\end{vfull}
\Cref{fig:fg2PDG},
%joe11
%the mappings can change the graphical structure signfiicantly.
the mappings can change the graphical structure significantly.
%oli12: no \alphas, so we have something slightly different is true.
% Nevertheless, if we take $\gamma=1$,
%oli22: As you say in a %joe18* below, let's just state the theorems,
% and then describe useful corolaries, instead of building up to them with
% the theorem statements below, which are evidently quite confusing. 
	% Nevertheless, in the case where every weight is the same,
Nevertheless, 
%oli23: not quite true; removing together with the theorem below.
% if we start with an unweighted factor graph, then
% applying the two conversions take us back to the same factor graph, so
% each is the inverse of the other.  Moreover, 
%oli23: we can substantially strengthen this claim now.
% in the case where all the
% weights are the same, then 
%joe21
%both conversions preserve the semantics.
both conversions preserve the semantics, 
%oli24: I find this comes off as way less ad-hoc if we don't
% make this definition last-minute. I'm putting this convention with the \alhpa
% in the definition.
% if we associate the
% unweighted PDG $\dg N$ with the (weighted) PDG $(\dg N,{\bf 1})$
% (i.e., we take $\beta$ to be the constant function {\bf 1}).



%oli22: commenting out the less general theorems.
%joe20*: reinstasting, as we agreed
%oli9: this holds more generally, illustrating that PDGs can carry factor graph data regardless of \theta, \beta.
%oli23: removing because to make it correct we either need to 
% weaken it by taking the probabilities, as was done in the version
% that the %oli9 comment above was arguing for, or state
% that it holds up to the equivalence of removing the extra coherence
% structure we've introduced --- it is merely that the collection of factors
% is the same.
\commentout{
	\begthm{prop}{prop:fg-pdg-lossless}
	%joe20*:
	%$\Phi \circ \PDGof = \mathrm{Id}_{\text{FG}}$. That
	%		is, if $F$ is a factor graph, then
	%		$\Pr_{\Phi(\PDGof{F})} = \Pr_F$.
	Given an unweighted FG $\Phi$, $\Phi_{\UPDGof{\Phi}} = \Phi$ 
	\end{prop}
}

%oli23*: Preserving this discussion, rewriting the unweighted versions from scratch
\commentout{
	% \begthm{theorem}[restate=thmpdgisfg]\label{thm:pdg-is-fg}
	\begthm{theorem}{thm:pdg-is-fg}
	%oli20: ok let's be more careful. What is 'gamma'?
	% If $\dg M$ is a PDG with $\beta_L = \gamma$ for all 
	%joe18: I actually thought it was fine before, but I'm OK with the
	%following slight reweroding
	%If $\dg M$ is a PDG, and $\gamma$ is a number such that $\beta_L\!
		If $\dg M$ is a PDG such that for some $\gamma >0$, we have that $\beta_L\!
		= \gamma$ for all  $L$, then
	%oli21: your reword (above) has the structure:
	% if [exists \gamma ... ] then  [\phrase involving \gamma], which 
	% muddles quantifiers, but I don't like my fix (below) so I'll leave yours.
		% If $\dg M$ is a PDG and $\gamma > 0$ satisfies $\beta_L\!
		% = \gamma$ for all  $L$, then
	$\bbr{\dg M}_{\gamma} = \gamma\,\GFE_{ \WFGof{\dg M} }$ and
	$\bbr{\dg M}_{\gamma}^* = \{\Pr_{ \WFGof{\dg M}} \}$.
	%oli21: to do it directly in the old notation (I find it 
	%cluttered and difficult to parse, which is why I changed it)
	% $\bbr{\dg M}_{\gamma} = \gamma\,\GFE_{ \WFGof{\dg M}, \beta^{\dg M}}$ and
	% $\bbr{\dg M}_{\gamma}^* = \{\Pr_{ \Phi_{{\dg M}},  \beta^{\dg M}} \}$.
	\end{theorem}
	\begthm{theorem}{thm:fg-is-pdg}
	%joe19*: You should work with WFGs everywhere, and use \Psi
	Given a factor graph $\Phi$, 
	and a constant vector $\kappa = \{ k \}_{J \in \cal J}$ for some fixed $k$, then
	we have that
	$\GFE_{\Phi, \kappa} = \nicefrac{1}{k}\bbr{\PDGof{\Phi,\kappa}}_{k} + C$  
	for some constant $C$, so $\Pr_{\Phi, \kappa}$ is the unique element of 
	$\bbr{\PDGof{\Phi,\kappa}}_{k}^*$. 
	\end{theorem}
}%oli23: endcommentout

%oli23*: here are the unweighted analogs.
\begthm{theorem}{thm:fg-is-pdg}
%oli24: I placed the comment in the beginning; this is really not as ad-hoc
% as this notation makes it seem.
$\Pr_{\Phi} = \bbr{\UPDGof{\Phi}}_{1}^*\;$ for all factor graphs $\Phi$.
%joe21
% $\Pr_{\Phi} = \bbr{(\UPDGof{\Phi},{\bf 1})}_{1}^*\;$ for all factor graphs $\Phi$.
\end{theorem}
\begthm{theorem}{thm:pdg-is-fg}
%oli24: and again
$\bbr{\dg N}_{1}^* = \Pr_{\FGof{\dg N}}\;$ for all unweighted
%joe21*
% $\bbr{(\dg N, {\bf 1})}_{1}^* = \Pr_{\FGof{\dg N}}\;$ for all unweighted
	PDGs $\dg N$.  
\end{theorem}
%oli23*: added important discussion.
The correspondence hinges on the fact that we take $\gamma=1$, so that $\Inc$ and $\IDef{}$ are weighted equally.

%joe21
%What about weighted PDGs, of the form $(\Gr, \mat p, \beta)$?
%Factor graphs, too, have a standard notion of weightedness, 
%but so long as we stick with our convention of setting every $\alpha_L = 1$, 
%we cannot say much about them.
What about weighted PDGs $(\Gr, \mat p, \beta)$ where $\beta \ne {\bf 1}$?
There is also a standard notion of weighted factor graph,
but as long as we stick with our convention of taking  $\alpha = {\bf 1}$, 
we cannot relate them to weighted PDGs.  As we show in the next
section, once we drop this convention, we can do much more.

%joe18*: rewrote.  Assuming we keep this, the real question is where
%we introduce \alpha.  If we introduce it at the beginning, then we
%need to include it in the BN theorem and were really should give more
%intuition (which we'll have a hard time agreeing on, based on past
%history), although perhaps we can get away with saying that we'll
%just use it in a technical way.  Obviously, if we include it, we
%shold get rid of Theorems 4.2 and 4.3, and just use the stronger version. 
%oli22:  agree with the sentiment above,
% but was not able to reorganize enough of the document to make that happen 
% last time. I'm attempting it this time, which is why the present section
% is commented out.
%These theorems above are actually quite restrictive:
%	both \cref{thm:pdg-is-fg,thm:fg-is-pdg} require that the
%	confidences be uniform,  
	% and even then only offer a correspondence for a specific value of $\gamma$, which makes it impossible to even consider our limit.
%oli23*:
% This is not the right story. The theorems are restrictive
% because PDGs are not
\commentout{
	These theorems are quite restrictive:  Theorem~\ref{thm:pdg-is-fg}
	applies only to PDGs where all the uncertainties $\beta_L$ are
	identical, while Theorem~\ref{thm:fg-is-pdg} applies only to factor
	graphs where all weights $\Theta_J$ are identical.  We can generalize
	%joe19*: 
	%Theorem~\ref{thm:pdg-is-fg} slightly to the case where the ratio of
	Theorem~\ref{thm:pdg-is-fg} slightly by allowing $\alpha_L$ to have
%joe21
%values other than 1.
values other than ${\bf 1}$.
	%joe20*: added, since you never said this
	That is, we now consider PDGs of the form $(\Gr,\mat
	p, \alpha,\beta)$, where $\alpha_L$ can have arbitrary non-negative values.
	We generalize the translation given in Definition~\ref{def:fg2PDG} by
	allowing a family of translations, parameterized by $\gamma$.
	Specifically, given a WFG $\Psi$, we take $\PDGof{\Psi,\gamma} =
	(\UPDGof{\Phi},\beta_{\theta}/\gamma, \beta_{\theta})$. 
}


\subsection{Factored Exponential Families}\label{sec:expfam}
%oli23* insertion




%li23: acronym? did not make change.
% We take a \emph{weighted factor graph}  $\Psi$ to be a pair
%joe21
%We take a \emph{weighted factor graph} $\Psi$ to be a pair
A \emph{weighted factor graph (WFG)} $\Psi$ is a pair
$(\Phi,\theta)$ consisting of a factor graph $\Phi$ 
together with a vector of non-negative weights
$\{ \theta_J \}_{J \in \mathcal J}$.
$\Psi$ specifies a canonical scoring function 
\begin{equation}
\GFE_{\Psi}(\mu)
%   \GFE_{(\Phi,\theta)}(\mu)
	 := \!\Ex_{\vec x\sim\mu}\left[  \sum_{J \in
           \cal J} \theta_J \log\frac1{\phi_J(\vec
               x_J)}\right] - \H(\mu)  , 
			   \label{eqn:free-energy}
\end{equation}
%joe20
%which $\Pr_{(\Phi,\theta)}$ minimizes, called the \emph{variational
%oli23:
% which $\Pr_{\Psi}$ minimizes,
called the \emph{variational
Gibbs free energy} \cite{mezard2009information}. 
%oli23:
$\GFE_{\Psi}$ is uniquely minimized by the distribution
${\Pr}_{\Psi}(\vec x) = \frac{1}{Z_{\Psi}}
 	\prod_{J \in \cal J} \phi_J(\vec x_J)$, 
which matches the unweighted case when every $\theta_J = 1$.
The mapping $\theta \mapsto \Pr_{(\Phi,\theta)}$ is known as 
%joe20
%$\Phi$'s exponential family and is a central tool in the analysis
$\Phi$'s \emph{exponential family} and is a central tool in the analysis  
and development of many algorithms for graphical models \cite{wainwright2008graphical}.


PDGs can in fact capture the full exponential family of a factor graph, but only
%oli24
by allowing values of $\alpha$ other than ${\bf 1}$. In this case, the only definition 
% by allowing values of $\alpha$ other than 1. In this case, the only definition 
that requires alteration is $\IDef{}$, which now depends on the \emph{weighted multigraph}
$(\Gr^{\dg M}, \alpha^{\dg M})$, and is given by
\begin{equation}
%oli24: Let's just define it for M;
	% \IDef{G}(\mu) := \sum_{\ed LXY \in \Ed} \alpha_L \H_\mu(Y\mid X) - \H(\mu). 
	\IDef{\dg M}(\mu) := \sum_{\ed LXY \in \Ed} \alpha_L \H_\mu(Y\mid X) - \H(\mu). 
	\label{eqn:alt-extra2}
\end{equation}
%joe21: rewrote.  The reader won't know what (b) is
%In ths case, each specification in~\ref{item:localinfo} may be
%weighted by differently, so that some edges are more qualitatively
%certain than others, and correspondingly it may be more or less
%important to describe them properly.
Thus, the conditional entropy $\H_\mu(Y\mid X)$ associated with the
edge $\ed LXY$ is multiplied by the weight $\alpha_L$ of that edge.

%oli23: taken from your text.
%oli24: Softening slightly
% The key benefit of using $\alpha$ is that we can
One key benefit of using $\alpha$ is that we can
capture arbitrary WFGs, not just ones with a constant weight
vector.    All we have to do is to ensure that in our translation from
factor graphs to PDGs, the ratio $\alpha_L/\beta_L$ is a
constant.  (Of course, if we allow arbitrary weights, we cannot hope
to do this if $\alpha_L = 1$ for all edges $L$.)  
%oli23: new
%joe21
%We therefore define a family of translations.
We therefore define a family of translations, parameterized by the
ratio of $\alpha_L$ to $\beta_L$.
\begin{defn}[WFG to PDG]\label{def:wfg2pdg}
Given a WFG
%joe20*: going back to the special case of \alpha=1, AS WE HAD AGREED.   What
%do we do for edges not in \J.   I also don't think that this is quite
%right, sonce you haven't defined \beta for edges not in \J.  I now do
%so, although you should check 
%$\Psi=(\Phi, \theta)$ to a PDG $\PDGof{\Psi} = (\UPDGof{\Phi},\theta, \theta)$ 
$\Psi=(\Phi, \theta)$,
and postive number $k$, 
we define the corresponding PDG $\PDGof{\Psi,k} = (\UPDGof{\Phi},\alpha_{\theta}, \beta_{\theta})$ 
%by taking both $\alpha$ and $\beta$ to be $\theta$.
by taking $\beta_J = k \theta_J$ and $\alpha_J = \theta_J$ for the edge $1  \rightarrow X_J$, and
%oli24:
taking $\beta_L = k$ and $\alpha_L = 1$ for the projections $X_J \!\tto\! X_j$.
% taking $\alpha_L = 1$, $\beta_L = k$ for the projections $X_J \!\tto\! X_j$.
\end{defn}

%joe21*: Cut  I don't like the notion of capturing data.  More
%importantlly,
%We now turn to extend \cref{def:PDG2fg}.  
%Since PDGs have two sets of weights, and WFGs only have one,
%we will not be able to capture all the data.
We now extend Definitions~\ref{def:PDG2fg} and \ref{def:fg2PDG} to
(weighted) PDGs and WFGs.  In
%going
translating from PDGs to WFGs, 
%oli24: the mismatch isn't just a problem for this direction
% note that we have somewhat of a mismatch: PDGs have two sets of weights, and WFGs
we will have to lose some information: PDGs have two sets, while WFGs have 
%oli24:
% only have one, So in our translation, we ingore $\alpha$, and consider
only have one. Here we throw out $\alpha$ and keep $\beta$, 
%oli24: added
though to provide a left inverse of \cref{def:wfg2pdg}, either set would suffice.

%there are more PDGs than WFGs, and so there is not a perfect way to
%translate back. 
%We have chosen here to preserve $\beta$.

\begin{defn}[PDG to WFG]
Given a (weighted) PDG $\dg M =
(\dg N, \beta)$, we take its corresponding WFG to be $\WFGof{\dg M} :=
%joe21
%(\FGof{\dg N}, \beta)$ by setting $\theta_L := \beta_L$.
(\FGof{\dg N}, \beta)$; that is, $\theta_L := \beta_L$ for all edges $L$.
\end{defn}



%joe19*: 
%oli22: blank %joe19*.

%oli22: I'm actually also cutting the current presentation of these theorems
% and re-stating them below, 
% using un-weighted objects + weights, which I think is far less confusing.
% allows the most general theorems to be put right away without special
% conversions and specific discussion about what to do with \alpha.
\commentout{
\begthm{theorem}{thm:pdg-is-fg2}
%joe18
%If $\dg M$ is a PDG, and $\gamma$ is a number such that
If $\dg M$ is a PDG such that for some $\gamma >0$, we have that
	$\beta_L = \alpha_L \gamma$ for all  
	%oli12 line shave
	%edges
	$L$, then
	$\bbr{\dg M}_{\gamma} = \gamma\,\GFE_{\Phi_{\dg M}} $ and
	$\bbr{\dg M}_{\gamma}^* = \{\Pr_{\Phi_{\dg M}} \}$.
	 	% $\kldiv{\mu}{\Pr_{\Phi(\dg M)}} = \bbr{\dg M}_{1}(\mu)$
		% In particular, $\Pr_{\Phi(\dg M)} = \bbr{\dg M}_*^{\gamma := 1}$
\end{theorem}

%joe19

% \begin{theorem}[restate=thmfgispdg]\label{thm:fg-is-pdg}

%joe18* unless I'm missing something, you need to redo Definition 4.3
%to explain how to add the \alphas.  Will you add them in such a way
%that the ratio of \beta_L to \alpha_L is constant?  If so, we can say
%that because the construction made it a constant, the following result holds.
%If the updated Definition 4.3 allows some flexibility in
%choosing \alpha and \beta, then this would reqire more work.
%oli21: Oops, I forgot to add those two characters; we set \alpha_L = \beta_L. 
% In any case, I think the true things you say above can be sold better:
%joe19: As I said, I don't believe that you want \alpha_L = \beta_L
%oli22**: Since in general we cannot know gamma (because the translation meerely
% supplies the data of the PDG, including alpha and beta, but a user can
% laterquery the semantics for multiple different values of gamma),
% we can't do any better than to set them to be proportional. 1 is a nice
% nice multiplicative constant generally, and also is speial because that's the 
% one we need to capture unweighted factor graphs with unweighted PDGs.

%joe19*: AARGH!  I was hoping that you would discuss the more general
%translation here, where \alpha could be arbitrar.  I don't believe
%that the reslt will hold if \alpha_L = \beta_L.  As you ptoint out,
%we need \beta_L = \alpha_L \gamma not \alpha_L = \beta_L.  So
%although you spent a lot of time doing what I explicitly asked you to
%do, you didn't address my main concerns.  You absolutely did not
%explain how to do  the translation when you have an abtrary weight vector.
%specify how you do the translatin here from factor graphs to PDGs.
%I put in what I think you should have put in.
%oli22: As stated in emails, I believe your conversion is correct but 
% slightly deceptive. I have therefore recycled 
The key benefit in using $\alpha$ is that we can
capture arbitrary WFGs, not just ones with a constant weight
vector.    All we have to do is to ensure that in our translation from
factor graphs to PDGs, the ratio ratio $\alpha_L/\beta_L$ is a
constant.  (Of course, if we allow arbitrary weights, we cannot hope
to do this if $\alpha_L = 1$ for all edges $L$.)

Specifically, given
$0 < \gamma \le 1$ and a WFG $\Psi = (\Phi,\theta)$, we take the PDG
${\dg M}_{\Psi,\gamma}$ to be defined just like the PDG ${\dg M}_\Psi$ of
Definition~\ref{def:fg2PDG}, except that instead of having $\alpha_L =
1$, we have $\alpha_L = \gamma\beta_L$. 


	\begthm{theorem}{thm:fg-is-pdg2}
	%oli20: we got to remove the condition!
%joe19
%For every factor graph $\Phi$, and EVERY vector $\theta$ over $\cal J$
For all WFGs $\Psi = (\Phi,\theta)$ and all $\gamma$ with $0
< \gamma \le 1$
	%, and EVERY $\gamma >0$
	we have that
	% for any joint distribution $\mu$ on $\V(\mathcal X)$, we
	        % have $\kldiv{\mu}{\Pr_\Phi} = \bbr{\PDGof{(\Phi)}}_{1}(\mu)$ 
	%joe8: again
	%oli10: adding back in
	%joe9
	        %	$\gamma \GFE_\Phi = \bbr{\PDGof{(\Phi)}}_{\gamma} + k$
	        %        where $k$ is a constant, and in particular, 
%joe19*
%$\GFE_\Phi = \nicefrac1{\gamma} \bbr{\PDGof{\Phi,\theta}}_{\gamma} + C$  
$\GFE_\Psi = \nicefrac1{\gamma} \bbr{\PDGof{\Psi,\gamma}}_{\gamma} + C$  
for some constant $C$, so
%joe19*
%$\Pr_{\Phi, \theta}$ is the unique element of
%$\bbr{\PDGof{\Phi,\theta}}_{\gamma}^*$.
$\Pr_{\Psi}$ is the unique element of
$\bbr{\PDGof{\Psi,\gamma}}_{\gamma}^*$.  
	   % Moreover, $\Pr_{\Phi, \theta} = \bbr{\PDGof{\Phi,\theta}}^*$.
	\end{theorem}
}%oli22: \end{commentout}

%oli22**: big insertion: both of the above theorems + recycled material
% from your discussion. 
%joe20: note that I changed the label
%\begthm{theorem}{thm:pdg-is-fg}We can can now generalize our earlier results 


%oli23: canabalized for the above.
% %joe20*: resinstated
% The key benefit of using $\alpha$ is that we can
% capture arbitrary WFGs, not just ones with a constant weight
% vector.    All we have to do is to ensure that in our translation from
% factor graphs to PDGs, the ratio $\alpha_L/\beta_L$ is a
% constant.  (Of course, if we allow arbitrary weights, we cannot hope
% to do this if $\alpha_L = 1$ for all edges $L$.)  Specifically, given
% $0 < \gamma \le 1$ and a WFG $\Psi = (\Phi,\theta)$, we take the PDG
% ${\dg M}_{\Psi,\gamma}$ to be defined just like the PDG ${\dg M}_\Psi$ of
% Definition~\ref{def:fg2PDG}, except that instead of having $\alpha_L =
% %joe20
% %1$, we have $\alpha_L = \gamma\beta_L$.
% 1$, we have $\alpha_L = \beta_L/\gamma$. 

We now show that we can now capture the entire exponential family of a factor graph,
%joe21
%but only for the value of $\gamma$ equal to the constant $k$ used in
but only for $\gamma$ equal to the constant $k$ used in
the translation.  


\begin{theorem}\label{thm:wfg-is-pdg}
For all WFGs $\Psi = (\Phi,\theta)$ and all $\gamma > 0$,
we have that
$\GFE_\Psi
%joe20*: using notation defined above
%= \nicefrac1{\gamma} \bbr{(\UPDGof{\Phi}, \theta, \nicefrac{1}{\!\gamma\,}\theta)}_{\gamma}
= \nicefrac1{\gamma} \bbr{{\dg M}_{\Psi,\gamma}}_{\gamma} 
+ C$   
for some constant $C$, so
$\Pr_{\Psi}$ is the unique element of
%joe20*: switching alpha and beta again, and using \beta_\theta
%instead of \theta
%$\bbr{(\UPDGof{\Phi}, \theta, \nicefrac{1}{\!\gamma\,}\theta)}_{\gamma}^*$.  
$\bbr{{\dg M}_{\Psi,\gamma}}_{\gamma}^*$.
\end{theorem}

%joe21
%In particular,for $k\!=\!1$, so that $\theta$ is used for both
In particular, for $k\!=\!1$, so that $\theta$ is used for both the functions
$\alpha$ and $\beta$ of the resulting PDG,
\cref{thm:wfg-is-pdg} strictly generalizes \cref{thm:fg-is-pdg}.
\begin{coro}
	For all weighted factor graphs $(\Phi, \theta)$,
	we have that
	$\Pr_{(\Phi,\theta)} = \bbr{(\UPDGof{\Phi}, \theta,\theta)}_1^*$
\end{coro}

%joe21
%Conversely, so long as the ratio of $\alpha_L$ to $\beta_L$ is constant, the
Conversely, as long as the ratio of $\alpha_L$ to $\beta_L$ is constant, the
reverse translation also preserves semantics.
%oli23: insertaion
% Conversely, we can generalize 
% to the case where the ratio of
% $\alpha_L$ to $\beta_L$ is a constant.
%joe20
%}%oli22 \end{commentout}
\begthm{theorem}{thm:pdg-is-wfg}
For all unweighted PDGs $\dg{N}$ and non-negative vectors $\mat v$
over $\Ed^{\dg N}$, and all $\gamma > 0$, we have that 
%joe20*: you're writing \beta,\alpha; switching it to \alpha \beta,
%and making \beta = v, not \alpha = v.  Does the equality still hold?
%$\bbr{(\dg N, \gamma  \mat v, \mat v)}_{\gamma}
$\bbr{(\dg N, \mat v/\gamma, \mat v)}_{\gamma} 
		= \gamma\,\GFE_{(\Phi_{\dg N}, \mat v)} $ and consequently
%joe20*
%$\bbr{(\dg N, \gamma \mat v, \mat v)}_{\gamma}^*
$\bbr{(\dg N, \mat v/\gamma, \mat v)}_{\gamma}^*
		= \{\Pr_{(\Phi_{\dg N}, \mat v)} \}$. 
\end{theorem}

% With these translations, factor graphs therefore define a particular subclass
% of PDGs in which the weights $\alpha$ and $\beta$ are proportional.

%oli23: removed
% \noindent Note that if $\beta_L = \gamma$ for all edges $L$, then
% 		$\alpha$ is the 
% constant function 1 in Theorem~\ref{thm:pdg-is-fg1}, so
% Theorem~\ref{thm:pdg-is-fg1} is a generalization of
% Theorem~\ref{thm:pdg-is-fg}. 
%oli23: nevermind 
% We interpret the fact that the correspondence only holds when the constant
% $k$ used in the translation equals $\gamma$, as a reflection of the fact
% that there is no way to articulate a 

%oli22: I've rephrased the statement of the theorem in a suggestive 
% (but kind of clunky) way below.
%I think this is kind of cool, and a neat story. Do you buy it?
%joe20: It's not so much that I don't buy it, but that I can't make
%sense of it.  Where did the product of cpts come from?  Surely you
%must have a factor graf in the picture here.  If you can explain it
%to me in a way that I can understand it, we may want to reinstate
%this. 
%misleading 
%Thus, PDGs in which the quantitative and qualitative certainties are
%fused, when evaluated in the semantics corresponding to the particular
%trade-off coresponding to their cofficient of proportionality,
%precisely generate the exponential family of the associated factor
%graph.
%In particular, our semantics regard an unweighted PDG as a product of
%its cpts, when $\gamma = 1$.
%\begin{coro}\label{coro:justafg} 
%% For all $\dg N \!=\! (\Gr,\mat p)$,
%% If $\dg M$ is an unweighted PDG, then 
%	$\displaystyle\bbr{(\Gr, \mat p)}_1^* 
%		= \frac{1}{Z_{\dg M}}\prod_{\ed LXY \in \Ed^{\dg M}} \bp^{\dg M}(Y \mid X)
%	\propto \!\!\prod_{\ed LXY \mathrlap{\in \Ed^{\dg M}}} \bp(Y \mid X)$.
%\end{coro}
%joe20
\commentout{
We have seen that only a subset of PDGs can be faithfully 
represented as WFGs; we now show the other side of the correspondence: any
factor graph be captured by more than one PDG (though again, only for a fixed $\gamma$).
}

%\begthm{theorem}{thm:fg-is-pdg}





%oli22: end big insertion.

%joe18: cut from here.  Once we fix the second theorem, the relevant
%discussion should go before the theorem
%	\cref{thm:pdg-is-fg} still has a retriction, but if we can
%        choose the values of $\alpha$ (or alternatively $\beta$), we
%        can get the result to apply for arbitrary values of $\gamma$
%        and $\beta$ (resp. $\alpha$). \Cref{thm:fg-is-pdg2} is
%        substantially stronger than its counterpart: it shows that
%        PDGs in which the quantitative and qualitative certainties are
%        fused ($\alpha_L = \beta_L$) completely adopt the semantics of
%        factor graphs and their exponential families, for $\gamma =
%        1$.\footnotemark 
%joe18*: This is the point; you have to explain how the tranlsation
%works in the presence of \alpha!
%\footnotetext{The result can be achieved for arbitrary
%	$\gamma \neq 1$, if we are willing to set $\alpha$ based on
%	$\gamma$ in our translation of factor graphs to PDGs.}}

%oli19:
% Justification for both theorems is provided by rewriting
%joe18*: We need to add a proof for this result in teh appendix.
%The truth of \Cref{thm:fg-is-pdg,thm:pdg-is-fg} may be seen
%intuitively by rewriting 
The key step in proving \Cref{thm:wfg-is-pdg,thm:pdg-is-wfg}
(and in the proofs of a number of other results) involves 
rewriting  
$\bbr{\dg M}_\gamma$ as follows: 
\begin{prop}[restate=prop:nice-score,label=prop:nice-score]% \label{}
% \begin{restatable}{prop}{propnicescore}\label{prop:nice-score}
 Letting $x^{\mat w}$ and $y^{\mat w}$ denote the values of
  $X$ and $Y$, respectively, in $\mat w \in \V(\dg M)$, 
we have 
\begin{equation}\label{eq:semantics-breakdown}
\begin{split}
%joe17*: Now that I look at it again, this is horrible notation, and
%it confused me for a whle.  When we write E(X), X is a random
%variable.  So what random variable are you taking the expectation of
%Look at the last term.  You wrote \mu(w), but that's not a random
%variable.  (I suspect that there's a typo here and it should
%be \mu(\mat w).  But, in any case, that's a value, not a random
%varaible (which is a function.  I just got rid of the E and corrected
%the typo, so it least now it's mathematically correct.
%oli20: Hmm. I'm willing to accept the change, but I don't buy the
%argument. Is there something wrong with regarding $\mu(\mat w)$ as a
%random variable which depends on \mu? It clearly is a function from
%worlds to reals. Convention, exepctation is quite common in such
%contexts in information theory, and quite intuitive (for instance,
%the entropy = "expected surprise" relies on this). As a matter of
%consistency, you should be aware that we do a very similar thing in
%the definition of Inc: there's an expected  divergence
%from \mu(Y|x). This kind of term is common, the expectation is the
%standard way to write it.
%joe18: you're right; we do it there, and it's a bad abuse of notation
%there too.  Probabilists have this unfortunate habit of using what I
%find terribly confusing notation.  I always feel forced to unpack
%it.  How many people who read this even realize that a random
%variable is not a variable and is not random, but is a function.  I
%would actually prefer to rewrite Definition 3.2 this way as well.  I
%think it would make it clearer to get rid of the expectation.  I
%think it make things clearer to get rid of it.
%oli20: I changed the spacing some so your version looks better. I
%still prefer the expectation but if you're not swayed by the above I
%won't say anything furhter. 
%joe17
%oli24: for consistency with the rest of the information theory literature
% and our definition of Inc, as well as personal ease of reading,
% I'm changing this back.
\bbr{\dg M}(\mu) =  \Ex_{\mat w \sim \mu}\! \Bigg\{
% \bbr{\dg M}(\mu) =  \!\!\!\sum_{\mat w \in \V(\dg M)} \!\!\! \mu(\mat w) \Bigg\{
 \sum_{ X \xrightarrow{\!\!L} Y  }
\bigg[\,
    \color{gray}\overbrace{\color{black}
      \!\beta_L \log \frac{1}{\bp(y^{\mat w} |x^{\mat w})}
%	}^{\color{gray}\smash{\text{log likelihood}}} + \\[-0.5em]
	}^{\color{gray}\smash{\mathclap{\text{log likelihood / cross entropy}}}} + \\[-0.5em]
    \color{gray}\underbrace{\color{black} 
(\valpha{\alpha_L}\gamma - \beta_L ) \log \frac{1}{\mu(y^{\mat w} |x^{\mat w})} 
	}_{\color{gray}\smash{\mathclap{\text{local regularization (if $\beta_L > \gamma$)}}}}\bigg] - \underbrace{\color{black}
%joe17
%\gamma \log \frac{1}{\mu(w)}
\gamma \log \frac{1}{\mu(\mat w)}
	}_{\color{gray}\smash{\mathclap{\text{global
        regularization}}}}\color{black} \Bigg\} .
\end{split}
\end{equation}
\end{prop}
%joe17*: sorry; this isn't claer to me at all.  
%assume that $\beta_L = \theta_L$.  But we did that, so that's not so
%bad (although you should say it). Second, we get, the last term
%becomes \gamma H(\mu), so it only matches the free energy if \gamma =
%-1.  Finally, why should p_L(y^W | x^w) = \phi_L(x_J) (even if we
%assume that \phi = p_L.
%  Part of the problem is that you wronte in
%the w  I think what you mean is the E(\gamme \log(1/\mu)
%= \sum_\oemga \gamma \mu(w)/log(\mu) = = H(\mu).  But then ou're
%still out by a factor of -\gamma.
%oli20: A factor of + \gamma, but point taken. I guess it's standard
%to effectively 
% fix \gamma=1 for factor graph because the ratio of the \theta's to the entropy term is
% the only thing that matters, so one can fix that scale to 1.
%joe18: it may be standard, but not everyone knows it (I didn't).
%  I've rewritten it so it's more correct. 
%	The first and last terms of \eqref{eq:semantics-breakdown} are precisely
%	$\GFE_\Phi$ for $\phi = \bp$.  
%joe18
%For any fixed $\gamma$, the first and last terms
For a fixed $\gamma$, the first and last terms
of \eqref{eq:semantics-breakdown} are equal to a scaled
%joe18: in what sense is it equivalent?  
%(which is essentially equivalent)
%joe18*: I'm confused.  This statement seems technically incorrect to
%me, on  two counts.  First, you would have to
%multiply the first term by \gamma (there is no \gamma factor
%currently in the first term); second, for the last term, the factor
%is -\gamma, not \gamma.  
%joe19*: Oliver, you did not address the joe18* immediately above,
%which points out that, unless I'm missing someting, your claim below
%is incorrect.    Please address this.  
%oli22: [as resolved in email]: the sign on the entropy is correct, and
% this conversion is not "the official one", a confusion which I've attempted
% to further eliminate with my edit below 
version of the free energy, $\gamma\GFE_\Phi$, 
%oli22:
% for $\phi_J = \bp$ and $\theta_J= \nicefrac{\beta_L}{\gamma}$
if we set $\phi_J := \bp$ and $\theta_J := \nicefrac{\beta_L}{\gamma}$.  
%oli19
% Thus, if we assume that $\phi = \bp$ and in addition assume that each
%oli20: edit for flow
% Furthermore, if each $\beta_L = \gamma$, the local regularization term disappears, so we get 
%joe18
%If in addition, each $\beta_L = \valpha{$\alpha_L$}\gamma$,
If, in addition, $\beta_L = \valpha{\alpha_L}\gamma$ for all
edges $L$, then
the local regularization term disappears, giving us
%oli20:
% an exact correspondence with $\GFE_\Phi$%
the desired correspondence. 

%oli20: paragraph break + signposting
%joe18*: cut this; see below for why
\commentout{
We now explain how the middle term may be viewed as a ``local'' regularization,
and why including such a term is necessary for the semantics to 
%joe18*: I find this statement very confusing.  Why shouldn't we take
%them seriously?  And they *are* assertions about probability, no
%matter how we  take them.  
%oli21*: I agree completely! But that is not the way the semantics work, without
% a local regularization term. I'm trying to motivate the need for the locaity.
%joe18*[continued]: Most importantly of all, I see no way in
%which this example tells me anything about how I shoudl view p.  It
%shows potential probablems with the factor graph (which is what I
%believe it was intended to do).  Some signposting would be useful, but I find
%this completely unhelpful.
%oli21*: I'm not sure exactly what you mean about "how you veiew p", but I assume
% you mean something like, whether to take the cpds seriously or think of them
% as energies. The example clearly shows that without a local regularization
% term, the semantics effectively treats them like energies, and the interpretation
% as a probability distribution is not preserved without the local term. As we
% point out, this local term  
%joe18*[continued]: We had some text here which you seem to
%have cut, rather than commenting it out.  I though the text was fine,
%and I'm reinstating it.  (Also, please don't just cut text.)
%oli21: This text was not removed; it's was just at the end of the document 
% serving a slightly different post-example analysis purpose. 
% To be honest I'm perplexed that so many of these stories, whose delivery I take
% quite seriously and that I edit several times before turning over to you, are
% still entirely lost in transit....
%take our cpds $\mat p$ seroiusly, as assertions about probability%
view our cpds $\mat p$ as assertions about probability%
% (instead of viwing them as each contributing to a badness, as factor
% graphs do) 
. 
%oli20: pulled example from the end of the document.
}

%joe18*: reinstating some material from before, with slight rewriting.
\Cref{eq:semantics-breakdown} also makes it clear that 
taking $\beta_L = \valpha{\alpha_L} \gamma$ for all edges $L$ is
essentially necessary to get \Cref{thm:pdg-is-fg,thm:fg-is-pdg}.
%oli21*: ! this is the opposite polarity of what we need to say.
%. The equality HOLDING is what gives us strange behavior.  Rewriting.
%	If this equality does not hold, then we can get some 
%\valpha{\cref{prop:consist} does not hold unless .}
%joe19: OK
%oli22: oops I hadn't finished the rewrite in this document, only in 
% DN-and-WFG. I'm updating to what I had there (2 lines).
% Because this equality must hold, we can get
%joe20
%Of course, fixed $\gamma$ precludes taking $\lim_{\gamma\to0}$, so
Of course, fixed $\gamma$ precludes taking the limit as $\gamma$ goes
to 0, so 
\cref{prop:consist} does apply. This is reflected in 
%
some strange
behavior in factor graphs trying to capture the same phenomena as
PDGs, as the following example shows.

\begin{example}\label{ex:overdet}
Consider the PDG $\dg M$ containing just $X$ and $1$, and two edges
$p, q: 1 \to X$.
%joe8:
(Recall that such a PDG can arise if we get different information about the
probability of $X$ from two different 
%oli19:added, otherwise this is a reasonable inference
% but not independent
%joe17: I don't understand.  Such a PDG can arise whether or not the
%sources are independent.  There is no inference going on here!  If
%you want to say something about non-independence, it should go
%somewhere else.
%oli20: I'm not trying to say anything about independence, but we want the
%example to be a clear illustration of what PDGs buy you. If I thought the
%sources were independent, I would be justified in combining 0.7 and 0.7 to get
%0.85. I find the example to be more powerful if we explicitly state that we
%don't know them to be independent, so we cannot make this inference. Thoughts?
%joe18: This is not the place to discuss these issues.  We're just
%trying to make a point about factor graphs having strange behavior.
%The issue of combining .7 and .7, and possibly getting .85 needs
%*much* more discussion; this is not the place to do it.
sources; this is a situation we
certainly want to be able to capture!)
Consider the simplest situation, where $p$ and $q$ are both associated
with the same distribution on $X$%
%oli20: added next line
%joe18:
%	, as the same certainty $\beta_p = \beta_q = 1$.
; further suppose that the agent is certain about the distribution, so
$\beta_p = \beta_q = 1$.
%joe18*: what abou \alpha?  If we introduce it, we have to say
%something about it here.
For definiteness, suppose that
$\V(X) = \{x_1,x_2\}$, and
that the distribution associated with both edges is $\mu_{.7}$, which ascribes
%joe9: slowing down.
%probability $.7$ to $x_1$, then  $\bbr{\dg M} = \{\mu_{.7}\}$,
%while it can be shown that 
probability $.7$ to $x_1$. Then, as we would hope  $\bbr{\dg M}^* =
\{\mu_{.7}\}$; after all, both sources agree on the information.
However, it can be shown that 
%oli20: some M's escaped. Added msising subscript.
% $\Pr_{\Phi{{\dg M')}}} = \mu_{.85}$, so  $\bbr{\dg M'} = \{\mu_{.85}\}$.
%joe18: there's a typo here: I wrote what I thought you intended
%$\Pr_{\Phi{{\dg M)}}} = \mu_{.85}$, so  $\bbr{\dg M}_1^* = \{\mu_{.85}\}$.
%oli21: 
% $\Pr_{\Phi_{{\dg M)},1)}} = \mu_{.85}$, so  $\bbr{\dg M}_1^* = \{\mu_{.85}\}$.
%joe19*: this is now inconsistent with the notation I introduced.  If
%you're happy with what I did, it should be corrected.
$\Pr_{\WFGof{\dg M}} = \mu_{.85}$, so  $\bbr{\dg M}_1^* = \{\mu_{.85}\}$.
%joe18*: this arguably also shows the problem with the \bbr{\dg M}_k^*
%semantics when k \ne 0.  We should say something about that.
%oli21: A more accurate takeaway is that if \gamma is strictly
%positive, then a messed 
% up qualitative picture will impact the semantics.
\end{example}

%joe11*: is this what you mean?  Why is there only one such world?
%optimal distribution would put all mass on the single best world; to
%oli13: It's not guaranteed to be unique, but all mass is concentrated
% on those worlds(s) which maximize the function. Added plural.
%oli19: rewrote
% If we had included only the likelihood term (the first one), 
% an optimal distribution would put all mass on the worlds of maximum likelihood. 
%joe17*: I don't undrstand why you made these changes.  How do you know
%that there's a unique world of maximum likelihood?
%oli20: it's not litterally true, but overwhelmingly likely and I don't want to
% complicate the story unnecessarily. See the below.
%the best distribution would put all mass on the highet likelihood world. 
%oli20: commenting out, see discussion + replacement below.
% the best distribution would put all mass on the worlds with maximum likelihood.
%joe17: I really don't like ``devoid of unceratinty''!  Moreover, if
%there isn't a unique world of maximum likelihood, why is there no
%uncertainty?  
%oli20*: while there may be multiple, any "uncertainty" is extremely fragile. 
% With any real data, there is a unique best world with probability 1.If we add
% epsilon noise to every cpt entry, there is a unique best world with probability 1.
% In the same vein: the only way it is possible for an interior point to have any probability at all, is if the product of the factors is is the constant function. 
%oli20: A simple example: a binary variable X, with an edge from 1 and an unconditional distribution % p(X=1) = 0.5 + \epsilon.  But X=1 is the only maximum likelihood world, so the optimal distribution is % the one that places all mass on X=1.  Taking this distribution seems hugely overconfident, and is totally in conflict with the uncertainty in the cpt. This is essentially what is hapepning in a factor graph. 
%oli20: trying again.
%joe18: this is not helpful  
%The log likelihood term
%%, which is as the cross entropy, 
%is the only term that makes use of the cpds.
%joe18*: in what sense is it the most important?
%so in some sense it is the most important.  
%oli21: (because it's the only term that depends on the cpds)
%joe18*: Sorry, I can't make any sense of the next sentence.  In what
%sense are the distributions miscalibrated?
%On its own, however, the distributions it selects are badly
%miscalibrated, placing all mass on worlds with the very highest
%likelihood.
%joe18*: the best distance by what metric?
%oli21: (metric=scoring function with only log likelihood term)
%For instance, the best distribution in \cref{ex:overdet}
%by this metric is $\mu_{1.0}$, even if only link $p$ were given ---
%which is in direct conflict with $p$ itself.  
%Such a distribution is devoid of uncertainty, which usually is in direct
%conflict with the data of the cpds $\mat p$.
%joe17: What issue is it resolving?  How is it resolving it?
%oli20: hopefully clearer now; I'm uncommenting and expanding.
%joe18*: I'm afraid it's not at all clear, and I still have no idea
%what issue is being resolved.
%A factor graph essentially resolves this issue by
%also including the final term,
%oli20: added
%joe18*: Sorry, I can't make any sense of this.  What does it mean to
%``properly balance'' something.  How can ou tell if it's ``properly
%balanced''.  
%oli21*: By "balanced", I am referring technically to the situation where they
% have the same coefficient. But saying this doesn't describe the effect:
% When the coefficient on a regularization term, log[ 1 / \mu(x) ], equals the
% coefficient on the energy term, log [ 1 / p(x)], for some fixed p, the
% distribution \mu that minimizes the total is p (because their sum is then the KL
% divergence from \mu to p).  If the regularization term
% had a smaller coefficient, the optimal \mu would be distorted to be more
% deterministic than p, and if it had a smaller one, it would be more spread
% out. I claim that if p is meant as a description of the actual probability and
% not just some measure of relative goodness, then balancing the two terms is
% important.  
%
%which imposes a cost for uncertainy. For the PDG consisting of only
%$p : 1 \to X$ from \cref{ex:overdet}, this results in $\bbr{1 \to
%p}^* = p$. As we have seen, adding $q$ . It seems that as long as our
%regularization term remains agnostic to the number of links in the
%graph, and which nodes they attach to, it cannot properly balance the
%likelihood so that the cpt is satisfied.    

%joe18*: The next sentence should go earlier, after we've talked about
%that first and last terms of (5) captring the free energy.
%oli21: I keep trying to tell a story about what the equation does by building
% up the terms, but I'm not getting through. I think without communicating the
% instability + overconfidence of the likelihood term, and its standard 
% resolution which involves the free energy, it's not really worth saying this anymore.
%The scoring function that we use for PDGs can thus be viewed as
%joe19: moved up from below, with minor changes to make it flow better.
Although both $\theta$ and $\beta$ are measures of confidence, 
%joe17
%the way $\theta$ that a factor graph varies with $\theta$
%joe19
%the way that a factor graph varies with $\theta$
%is quite different from the way a PDG
the way that the Gibbs free energy varies with $\theta$ 
is quite different from the way that the score of a PDG
varies with $\beta$. 
The scoring function that we use for PDGs can be viewed as
extending ${\GFE}_{\Phi,\theta}$ by including
the local regularization term.
As $\gamma$ approaches zero,
%joe18: it's not the strength, but the importance
%oli21: Huh, I've always heard the adjective "strength" used to
%describe the magnitude 
% of the coefficient of a regularization term.  
%the strenth of global regularization
%drops and the strength of local regularization increases. 
the importance of the global regularization terms decreases relative
%joe19
%to that of the local regularization term.
to that of the local regularization term, so the PDG scoring function
becomes quite different from Gibbs free energy.


%joe11*: why do we want to express uncertainty?   What is it
%uncertainty about.  I'm trusting you that ``regularization'' will be
%meaningful in this context to others.  It's certainly not meaningful
%to me. You have the space to add a sentence of clarification.
%oli13: "Uncertainty" was maybe not the best word choice. Let's try this:
% to capture $\bbr{\dg M}_\gamma(\mu)$, we need to include 
%joe13*: this doesn't make sense to me.  Where did overfitting come
%from?  We're not doing machine learning?  Since you haven't told me
%the objective function, so to speak, I find this worse than useless.
%It makes me think of things that ought to be irrelevant. At a
%minimum, to include them, you must explain the goal (i.e., give some
%intution!).  
%to avoid overfitting these particular distributions, we need to include 
%\emph{regularization terms} \todo{cite}.  
%oli15
% the regularization terms are needed to capture the inconsistency and
% information deficiency of $\mu$ relative to ${\dg M}$.
%joe14: I put ``regulararization'' in quotes
%the regularization terms are thus necessary to model uncertainty.
%You need to say something about how, although they're not
%regluarization terms as the term is used in the ML community (since
%we are *not* doing machine learning), they have some of the same spirit.
%joe15*: Why are regularization terms necessary to model uncertainty?
%All that you can say is here is that they're necessary to capture
%this particular socirng function
%oli17*: No, this is much weaker than what I want to say. These terms
%are  necessary to have an optimal distribution $\mu$ that is not a
%point mass on a particular world. 
%oli17: Unsure if helpful, but the thermodynamic analogy is
%temperature=0 Kelvin, 
% so that everything is in its very minimum energy state, and the distribution of possible
% configurations is a point mass on the single best one. 
%oli17: Again, the point is: if we want it to even be possible for the minimum
% of this function to be a non-degenerate distribution (i.e., one with uncertainty),
% then we need the regularization terms. 
%oli17: rewrote more explicitly, in the hopes that this was convincing.
%
%the ``regularization'' terms are thus necessary to model uncertainty.
% the ``regularization'' terms are thus necessary to capture $\bbr{\dg
  % M}_\gamma$. 
%joe16*: Cut.  I do not understand what this means, or why it's
%important.  This hurts far more than it helps.
%the ``regularization'' terms are thus necessary for these
%distributions to have any uncertainty. 
%oli19: rewriting.
% By contrasting equation
% \eqref{eq:semantics-breakdown} with the expression for $\GFE_\Phi$, we see
% that, although both $\theta$ and $\beta$ are measures of confidence, the way
% that a factor graph varies with $\theta$ is quite different from the way a PDG
% varies with $\beta$. 
%joe17: Cut this.  I 
%The fact that the local term varies with $\beta_L$ has important
%modeling consequences. 

%joe17*: I don't undersatnd teh rest of this.  I couldn't see anywhere
%in the example where these points were illustrated.  Indeed, the
%exampe doesn't mention \beta at all.  If you could rewrite the exmple
%to illustrate these points, we might consider reinstating this.
%oli20: fair enough, reinstated and added missing details. 
%joe18*: I still can't make sense out of any of this.  As an aside,
%this is *not* what I asked you to do.  I asked you just to focus on
%the technical material, which is all I was (and am) comfortable with.
%$\theta_J$ controls the strength of the potential associated to $J$;
%joe18*: I don't understand this.  Why does increasing one
%terms \theta_J for a specific J, make distributions more
%deterministic.  If you increase all of them, I can see how it wuld
%make distributions more determininstic, but only if all the ffactors 
%were different numbers.  But why do we are about this?  We're writing
%a paper about PDGs.  What does this tell us about PDGs.
%increasing it results in optimal distributions which are more
%deterministic.  
%oli21: You raise a good point about an individual $\theta_J$;
% thanks for adding the context.  
%oli20: added
%joe18*: I have no idea how a generaic atatement about properties. I
%would *strongly* prefer to return this to its previous state (modlo
%the technical concerns I've raised).
%of \theta_J could follow from an Example.  But even if you meant that
%Example 5 illustrates your point, I can't see any way in which you've
%argued that doubling J makes things more deterministic.  The next
%sentence just feels like a complete non sequitur.  
%This follows from \cref{ex:overdet} the fact that doubling $\theta_J$
%is equivalent to duplicating factor $J$. By contrast, $\beta_L$
%controls both the likelihood and the local uncertainty together: as
%described in our motivation for $\Inc$, increasing the reliability of
%$\beta$ in a PDG increases the cost of failing to match the given cpd.  
%\\\contentious{
%joe19*: Please do not spend time doing this, Oliver.  Put your
%efforts dealing with the issues that we agreed were the important
%issues.  I don't understand any of the discussion below. I don't know
%what it means for a factor graph to ``fuse'' things, nor why we have
%to have \alpha_L = \beta_L (indeed, as I pointed out above, we
%definitely do *not* want to assume this). Since we have no intuition
%for \alpha_L, we can make no claim about what is appropriate for it.
%I cut all this.
%The behavior in \cref{ex:overdet} also sheds some light on the nature
%of $\alpha$. 
%Because a factor graph fuses $\alpha_L = \beta_L$, adding a new edge
%effectively doubles both (it doubles $\theta_L$). If one believes
%that both $p$ and $q$ are from independent sources, and thus
%constitute separate qualitative determinations of $X$, then
%effectively doubling $\alpha_p$ is appropriate. If, on the other
%hand, you merely have two distinct sets of observations which agree,
%and have no reason to believe they are independent or have anything
%to do with the causal structure of the world, then effectively
%doubling $\alpha_p$ is not appropriate.   
% }
%
%joe13*: the equation says nothing abut \theta.  This is even more
%confusing because in the tranlsation you take \beta_L = \theta_L.  My
%preference would be to ut the rest of the paragraph, although I didn't
%do it.  I think it hurts more than helps.
%\eqref{eq:semantics-breakdown} shows further that, although
%oli15
% \eqref{eq:semantics-breakdown} helps show  that, although
%oli12: added the technical details and fused with next sentence
%joe11*: typo, I assume.  If not, then it's unclear
%For a factor graph, $\beta$ controls the importance of the likelihood
%term, while in a PDG it also increases the strength of the local
%oli13: you're right. This enables further simplification
% For a factor graph, $\theta$ controls the importance of the likelihood
% term, while in a PDG, $\beta$ it also increases the strength of the
%joe13: I don't know what ``jointly controls'' means.  After changing
%the next few lines, I cut them.   It's true that \beta affets both
%the likelihood term and the local regularaization term, but I don't
%wee why that makes it different from theta; there is no theta in the
%equation above
%oli15: I've reverted because the story makes more sense with this in than out. 
% It's true and I think it won't be too difficult to follow from the equation.
%%$\beta$ jointly controls the importance of the likelihood and local
%oli15 modified
%joe14*: In what sense does it balance them?  I'm lost.  If you say
%``affects'' it's fine.  If not, you must explain what ``balance'' means.
%oli16: the strength of the regularization must match the strength of
%the likelihood. Only then is the function minimized by the cpd on the
%edge, rather than a more deterministic one (if the former is
%stronger) or a more random one (if the latter is stronger). 
    % $\beta$ balances the likelihood and local
    % regularization terms [[REWRITE]], while 
    % $\theta$ affects only the former.
%oli16: rewritten so that now it is clear, but much too long. I like my shortening above,
% and do not think it needs anything else, but I'll let you shorten this one as you like. It also double-covers material from the next sentence.)
%joe15*: Sorry; it is *not* clear (at least, not to me).  I don't know
%what it means that they ``remain balanced''.  I also don't like the
%use of terms of agency (``controls'').  Are you saying anything other
%than \beta affects the difference between the terms.  Is that what
%balance is supposed to mean?  If so, it's an awfully complicated way
%of saying it.  If not, what is it saying that's different?  I cut
%most of it.  I think that what you wrote is a net negative; in
%expectation, it will hurt more than it will help.  Do NOT
%make further changes without discussion (other than correcting typos)
%$\beta$ simultaneously controls both the local likelihood and local
%regularization terms, so that the two remain balanced, and making a
%link more or less random never does better than exactly matching the
%cpt.  
%oli17: \beta preserves the ratio between the first and second 
% terms (in the limit as \gamma -> 0)
%joe16: As a technical matter, I don't see why this is true.  I don't
%see in what sense \beta preserves anything.  It's just a number that
%you miltiply the difference between the terms by.  
%oli17: and the cpds that that optimize it remain the same.
%joe16: The same as what?  I'm totally lost
%oli17: This is very different from just increasing the strength of the 
% likelihood because it makes things more determinisitc. I maintain that this
% is a critical point, but I will not modify the text here to make it until we
% agree...
%joe16: good.  I truly have no idea what you said above, and I
%strongly suspect that I would not understand anything you tried to
%say here.
%oli19: I don't need this anymore either
%joe17: reinstated, because I cut what you added
%oli20: removed again. I don't think this says very much, and I've tried
% to address your concerns again.
	% In (\ref{eq:semantics-breakdown}), 
	% $\beta$ affects the difference between the local likelihood and
	% local regularization terms; on the other hand, in $\GFE_\Phi$,
	% $\theta$ affects only the likliehood.
% %oli17
% $\theta$ affects only the likelihood.
%$\theta$ only controls the likliehood, and so as it
%increases the benefits of reporting an uncertain distribution fade as

%oli21: Here's the paragraph that I didn't cut before, but now it's above so I'm cutting it.
% \Cref{eq:semantics-breakdown} also makes it clear that 
% taking $\beta_L = \valpha{$\alpha_L$} \gamma$ for all edges $L$ is
% essentially necessary to get \Cref{thm:pdg-is-fg,thm:fg-is-pdg}.
% However, the analogue of \Cref{prop:consist} does not hold
% in general for such choices, leading to some arguably unacceptable
% behavior in factor graphs trying to capture the same phenomena as PDGs.

%joe9: cut; folded some material into the next section
%oli20: I rewrote this section and brought it up in case 
%joe18: I commented it out again, after making minor changes.  Nothing
%in what you've written explains the connection between PDGs and
%directed factor networks.  THere's no point in giving a (not
%particularly useful) introduction to directed factor graphs.  We're
%not writing a paper about factor graphs.  If you have something
%useful to say about the connection between diredcted factor graphs
%and PDGs (e.g, we can prove an analogue of THeorems 4.4 and 4.5 for
%them), then it would be worth saying.  Otherwise, this is a poor use
%of space.
\commentout{

	\subsection{Directed Factor Graphs}
	% Directed Factor graphs \cite{frey2012extending} impose constraints on factor graphs to resolve some of their issues and bring them more in line with BNs, but they have no semantics if the constraints are not satisfied, making them a way of visualizing factor graphs more than a novel modeling tool.
%joe18
%While PDGs can be thought as a loosening the restrictions on
While PDGs can be thought as a loosening of the restrictions on
	Bayesian Networks, 
%joe18
%Directed Factor Graphs \cite{frey2012extending} go the
\emph{directed Factor Graphs} \cite{frey2012extending} go in the
	opposite direction: they  
%joe18: I don't know what ``better-behaved'' means.  Better behaved
%than what?
%extend factor graphs with better-behaved directed edges,
are variant of factor graphs where edges are directed,
	allowing them to represent a larger class of independencies,
%joe18
%including those of Bayesian Networks. Such an directed edge
including those of Bayesian Networks. A directed edge
	indicates that the product of incoming edges normalize to 1,
	an invariant which must be enforced and maintained as the
	model is changed. As a result, directed factor graphs are
	well-suited to visually describing the structure of
	distributions, but less well suited to modelding beliefs that
	change quickly, or could be inconsistent. 
}

%oli22*: I'm still working on this section. THere's no need to edit it
% at this point, because I plan to reduce it massively myself.
% (also much of it has been heavily modified without marks) 
% I do have a question: is it worth dding a plots which emperically suggest
% that these results are true, even without theorems?
\commentout{
\subsection{Dependency Networks}

PDGs are closely related to Dependency Networks, or DNs
\cite{heckerman2000dependency}. The data of a DN is also an unstructured
collection of cpds, but the cpds are attached to nodes rather than edges, so
there must be exactly one per node.  \citeauthor{heckerman2000dependency} also
emphasize the benefit of being able to supply arbitrary data in the cpds,
without enforcing the consistency constraints.


\begin{defn}
%joe20
%A Dependency Network is a tuple $\mathcal D = (\Gr, \mat p) $, where
A \emph{dependency network} is a tuple $\mathcal D = (\Gr, \mat p) $, where
$\Gr$ is a 
%joe20
%directed graph of variables, and $\mat p$, gives for each $X \in \N$ a
directed graph of variables and $\mat p$ gives, for each $X \in \N$ a
%joe20*: misplaced footnote mark.  More importantly, does Heckerman
%require p to be positive in the defintion, or only n the theorem.
%The definitions make perfect sense even without this assmption.
%positive 
%	\footnote{that is, each cpd $\bp[X]$ has no zero entries}
%cpd $\bp[X](X \mid \Pa^{\Gr}(X))$,
positive cpd $\bp[X](X \mid \Pa^{\Gr}(X))$ (i.e., all entries in $\bp[X]$ are
psoitive),
where $\Pa^{\Gr}(X)$ are the parents of $X$ in $\Gr$.
Together with a total order $\prec$ on $\N$,
$\mathcal D$ defines a joint distribution $\Pr^\prec_{\mathcal D}$ on $\V(\N)$ via an \emph{ordered Gibbs sampler}. 
Concretely,
%joe20
%initialize $\mat X^{(0)}$ to any joint setting of variables, and
%for $t= 1,2, \ldots$, and for $i=1,2,\ldots$, we draw
initialize $\mat X^{(0)}$ to an arbitrary joint setting of variables, and
for $t= 1,2, \ldots$ and $i=1,2,\ldots$, define
\[
	 X_i^{(t)} \sim \bp[i]\left(X_i ~\big|~ x_1^{(t)}, \ldots,
%joe20
%x_{i-1}^{(t)}, x_{i+1}^{(t-1)}, \ldots, x^{(t-1)}_{n} \right)
x_{i-1}^{(t)}, x_{i+1}^{(t-1)}, \ldots, x^{(t-1)}_{n} \right).  
%		= \bp[i](X_i \mid \mat{pa}^{\Gr}(i)) 
\]
%joe20*: what does rotating through the variables have to do with it?
%After each full rotation through the variables, we get a new sample
%which depends only on $\mat X^{(t)}$> Thus we have a Markov chain of
Note that $\mat X^{(t+1)}$
depends only on $\mat X^{(t)}$. Thus, we have a Markov chain of
joint variable settings 
$ \mat X^{(0)} \to \mat X^{(1)} \to \ldots \to \mat X^{(t)} \to \ldots$.
%joe20
%that, so long as each $\bp[X]$ is positive, ensures that the chain is
Since each matrix $\bp[X]$ is positive,
the chain is
%joe20: you need a reference (perhaps Puterman's book), and need to
%explain ergodic. 
ergodic, 
%joe20
%and thus limits to a unique stationary distribution on $\mat X$,
and thus has a unique stationary distribution on $\mat X$ as its limit.
%joe20
%which we take to be the definition of $\Pr^\prec_{\mathcal D}$.
We define $\Pr^\prec_{\mathcal D}$ as this stationary distribution,
\end{defn}

% << This is what was written in your email. It's still more detailed
% than I have written but I don't want to finish integrating it until after
% I've proven some results. >>
%
%This is clearer than what's written, but it's different from what you 
%wrote in our writeup, as near as I can tell.  But I think I see what's 
%going on, and how it should be explained (at least for me to understand 
%it).  First, you have tell me how to initialize x_1, ..., x_n (i.e., 
%what is x_1^0, ..., x_n^0).  We then define X_n^t by induction on t.  We 
%take X_0^t = (x_1^0, ..., x_n^0) (which I'm assuming is given somehow).  
%If t = nt'+i, we assume that we have defined x_j^0, \ldots, x_j^{t'} for 
%j < i and x_j^0, \ldots, x_j^{t'-1} for j \ge i.  We then define x_i^t' 
%to be the result of drawing a value according to the distribution 
%p_i(.|x_1^{t'}, ..., x_{i-1}^{t'}, x_i^{t'-1}, ...,  x_n^{t'-1}).   Then 
%we define X^t = (x_1^{t'}, x_i^{t'}, x_{i+1}^{t'-1}, ..., x_n^{t'-1}).
%
%Notice how different this description is from years.  Now you can point 
%out that the sequence X^0, X^1, ... can be viewed as a Markov chain.  
%However, you can't just say "you take the limiting distribution", 
%because the limiting distribution does not exist in general.  This is 
%where the fact that all the cpds (not *local distibutions, which is an 
%undefined term) are positive comes in; it ensures that the Markove chain 
%is irreducible, which in turn guarantees that there's a unique 
%stationary distribution. This should be pointed out.

The dependence on the order is artificial in the case that the DN is ``consistent'', which in the authors parlance implies that it also has their favored independencies.\footnote{Interestingly, any BN $\mathcal B$ is an ``inconsistent'' dependency network, despite the fact that the Gibbs sampler whose order $\prec$ is a topological sort  of $\cal B$, generates $\Pr_{\mathcal B}$.}

\begthm[\citeauthor{heckerman2000dependency}]{theorem}{thm:dns-uniq}
%Theorem 1. (wrapped into definiton above)
%An ordered Gibbs sampler applied to a dependency network for $\mat X$, where each $X_i$ is discrete and each local distribution $\bp[X](x \mid \mat{pa}(X))$ is positive, has a unique stationary joint distribution for $\mat X$.  
If a dependency network $\mathcal D \!=\! (\Gr, \mat p)$ over variables $\N$ is consistent with a positive joint distribution $p$,
in that $p(X \mid \N\setminus \{X\}) = \bp[X](X \mid \Pa^{\Gr}(X))$ for all $X \in \N$,
 then $\Pr_{\mathcal D}^\prec = p$.
\end{theorem}

A dependency network $\mathcal D$, together with any a vector of confidences $\beta$ for the cpds on each variable
% (defaulting to $\beta_X=1$) 
can be naturally regarded as a PDG $\PDGof{\mathcal D, \beta}$ 
via the same translation used for a Bayesian Network.
Although on the surface PDG and DN semantics are quite
different,
the former subsumes the latter, as the next results show.


%\begin{lemma}
%	A distribution $\mu$ that locally minimizes $\IDef{\PDGof{\mathcal D, \beta}}$ satisfies all of the independencies of $\mathcal D$.
%\end{lemma}


\begthm{conj}{thm:dns-are-pdgs}
%oli22: updating theorem statement
% If $\mathcal D = (\N, \Ed, \V, \mathcal P)$ is a consistent dependency network with a positive stationary distribution $p^*$ of its sampling procedure, then the PDG $\PDGof{\mathcal D} = (\N, \mathit{merge}(\Ed), \V, $$\mathcal P, \mat 1\valpha{, $\mat 1$})$, then $\bbr{\PDGof{\mathcal D}} = p^*$.
If $\mathcal D$ is a consistent dependency network,
%with positive local distributions,
then for all positive vectors $\beta$, and all orders $\prec$, we have that
%$\SD{\PDGof{\mathcal D, \beta}} =  \{ \Pr_{\cal D}^\prec \}$.
$\bbr{\PDGof{\mathcal D, \beta}}^* =  \Pr_{\cal D}^\prec$.
%, where $\Pr_{\cal D}^\prec$ is the unique stationary distribution of the $\prec$-ordered Gibbs sampling procedure. 
\end{conj}

Inconsistent DNs still define a unique distribution, but we cannot hope that it 
will be the same one as generated by the PDG, owing to the fact that
the Gibbs sampler defined by \citeauthor{heckerman2000dependency} is dependent
on the (arbitrary) fixed order used in sampling.
A PDG can simulate the effect of the order with the reliability $\beta$.

\begin{conj}
If $\mathcal D$ is a dependency network over the variables
$X_1 \prec X_2  \ldots  \prec X_n$ is a total order on the variables, and $\beta$ is a vector over the edges
%oli22: to preemptively eliminate some confusion:
% (which have a 1-1 correspondence with variables in a DN) 
such that  $\beta_1 \ll\beta_2 \ll \ldots \ll \beta_n$, 
then $\Pr_{\cal D}^{\prec} = \bbr{\dg M; \beta}^*$
\end{conj}

A \emph{random scan} Gibbs sampler, rather than cycling though variables in a 
particular order, draws the next variable index to update randomly.
Given a distribution $d$ over $\N$,
let $\mathit{RScan}(d)$ be the random scan Gibbs sampling procedure that draws
the next variable to update from $d$. $\mathit{RScan}(d)$ also has a unique stationary distribution, $\Pr_{\mathcal D}^d$, which is equal to $\Pr_{\cal D}^{\prec}$ for any $d$ and $\prec$ if $\mathcal D$ is a consistent DN. This more symmetric procedure is captured nicely by PDG semantics.

\begin{conj}\label{thm:dns-are-completely-pdgs}
	Let $d(X) \propto \beta_X$. Then
	$\Pr_{\mathcal D}^d = \bbr{\PDGof{\mathcal D, \beta}}^*$, whether or not $\mathcal D$ is consistent.
\end{conj}

}
%joe9: \end{commentout}
	
	% \todo{more related work if this is the right place to put it.}

%joe8: This will not make it to the submission.  It's totally
%unrelated to the rest of the paper.  We should definitely write a
%paper on incosnsiency (the one we were originally going to write) but
        %this isn't it
{%\color{blue}        
%joe18*: Oliver, this is a bad idea.  We have a poorly presented
%example instead of a big story about how PDGs can represent
%inconsistency, with updating a special case.  What we want is a
%general theorem, not an example, and a much better discussion.  But
%this would be better placed in another paper.  Commenting it out
%again.
\commentout{
%\section{Using Inconsistency}
\section{Capturing Updating using PDGs vian Inconsistency}
%oli8: remove subsection
	% \subsection{BELIEF UPDATING} 
%oli8: I don't know if it's reasonable to keep this for the final version, but I think it's 
% one of the most important classes of example (where we add scaffolding and reason about a digram without altering its contents). It does not require any additional theory, and we keep cutting the things of this flavor.
	\label{sec:belief-update}
	%oli20: lots of edits in this section.
	Belief updating, through Bayes' or more generally Jeffrey's rule, can be thought of as the addition of a new marginal to a distribution, and then a resolution of inconsistency. In  \cite{dietrich2016belief}, for instance, a belief update $p \mapsto p_I$ takes a belief state $p$ to a new one consistent with the input $I$. We can do this with PDGs. 
	
	% For us, belief revision consists simply of the addition of a new edge to the picture, followed by a resolution of the resulting inconsistency. 
	\begin{example}
		Suppose that we have a variable $W$ of possible worlds, over which $B$ is a random  variable, and $p$ is an unconditional distribution. This data can be encoded in a PDG $\dg M$ as links $\ed{p}{1}{W}$ and $W \tto B$.
		Now, suppose we noisily observe $B$, giving a distribution $\pi$ over $B$. We add $\ed\pi1W$ to $\dg M$, which then becomes inconsistent (so long as $\pi \neq p(B)$). To resolve the inconsistency, we look to $\bbr{\dg M}^*$, the closest joint distribution on $\{W, B, 1\}$. Because $B$ is a function of $W$ and $1$ has only a single value, $\bbr{\dg M}$ is isomorphic to a distribution on $W$, which we adopt as $p'$.  $\bbr{\dg M}$ will likely be a compromise between $p$ and $\pi$, but if we trust the observation a lot more than the prior $(\beta_\pi \gg \beta_p)$, then $\bbr{\dg M}$ will be consistent with $\pi$. In this case, $\Inc_{\dg M}(\mu) = \kldiv{\mu}{p}$, which is is uniquely minimized among those $\mu$ consistent with $\pi$ by applying Jeffrey's rule \cite{halpern2017reasoning}. See \cref{fig:belief-update} for an illustration.
	\end{example}

	% With reference to \Cref{fig:belief-update}, an extension of \Cref{ex:randomvars}, consider the following update. 
	% Upon noisily observing the variable $B$ to with probabilities indexed by , Jeffrey's rule prescribes a posterior probability $p'$ of any event $E$ by:
	% \[ p'(E) = \sum_{b \in B} p(E \mid B\sheq b) \pi_b \]
	% Bayes Rule corresponds the particular case of Jeffrey's rule, in which the variable is binary and the outcome is certain.

% 
	\begin{figure}[htb]
		\centering
%		\scalebox{0.8}{
%			\begin{tikzpicture}[center base]
%				\node[dpadded] (1) at (0,3) {$\sf 1$};
%				\node[dpadded] (W) at (0,0) {$\cal W$};
%				\node[dpadded] (B) at (-2,1) {$B$};
%				
%				\draw[arr] (1) to node[fill=white]{$p$} (W);
%				\draw[arr] (1) to node[fill=white]{$\pi$} (B);
%				
%				\draw[arr, gray] (W) to[bend left=10] (B);
%				\draw[arr, dashed] (B) to[bend right=30] (W);	
%		\end{tikzpicture}}
		\scalebox{0.8}{
		\begin{tikzpicture}[center base]
			% \useasboundingbox (-3,-1) rectangle (3.5,4);
			\node[dpadded] (1) at (0,0.51) {$\sf 1$};
			\node[dpadded] (W) at (4,0.5) {$W$};
			\node[dpadded] (B) at (2,0) {$B$};
			% \node[dpadded] (E) at (6, 0.5 ){$E$};
			% \coordinate (Q) at (6,0); % to even out controls

			\draw[arr] (1) to[bend left=10] node[fill=white]{$p$} (W);
			\draw[arr] (1) to node[fill=white]{$\pi$} (B);

			\draw[arr, gray, ->>] (W) to[bend left=10] (B);
			% \draw[arr, dashed] (B) to[bend right=30] (W);	
			\draw[arr, dashed] (1) to[bend right=50] node[fill=white]{$p'$}(W);	
			% \draw[arr, ->>] (W) to (E);

			% \draw[arr,blue!50] (1) .. controls (2, -1) and (4,-1) .. node[fill=white]{$p'(E)$} (E);
			% \draw[arr,orange!70] (1) .. controls (2,2) and (4,2) .. node[fill=white]{$p(E)$} (E);
		\end{tikzpicture}}
		\caption{PDG Belief Updating via Inconsistency}
		\label{fig:belief-update}
	\end{figure}
}
%joe18: \end{commentout}
	% To understand the update visually in \Cref{fig:belief-update}, imagine the original distribution $p$ from $\sf 1$ to $W$ being replaced by the path $p' := p(W \mid B) \circ \pi$  on the left. The gray arrow on the bottom left is the definition of the random variable, as in \Cref{ex:randomvars}, and the dashed one is its inversion, which can be computed by Bayes' rule.  %that factors through $B$ via the new observation $\pi$.
	% To query the resulting distribution on, an arbitrary event $E$, with an indicator variable of the same name. Initially, we got a marginal on $E$ by going through $p$; we now use $p'$. Effectively, the orange path to $E$ has been replaced by the blue one.
	% 
	% 
	% To observe $\pi$, we simply view it as a cpd conditioned on $\star$ and add it to our collection. 
	% Although it is likely to be inconsistent, resolving this inconsistency in a way that retains $\pi$ is a belief update. 
	% Even failure retain $\pi$ entirely may not be a concern: so long as an they continue to observe or remember, an agent endures discomfort until $\pi$ is incorporated. This setting is arguably more natural than a standard one: without spending energy, it is easy to forget or partially reject implications of the observation.	
	% Once again, with a PDG, the resolution need not happen immediately. This makes the approach more convincing for cognitively bounded agents, who might have more pressing matters than sorting through beliefs, and who might do them out of order.
}%\end{commentout}
\vleftovers{
	\section{Algorithms} 
		\label{sec:algorithms}
	\subsection{Belief Propagation}
	
	
	\subsection{Sampling}
	
	One of the nice about directed graphical models is that the model itself is roughly a sampling algorithm. For instance, taking a Bayes Net $\cal B$ and generating samples according to the tables is an efficient way to sample $\Pr_{\mathcal B}$.

	This works because there is only one path, but more generally, for any conditional marginal $Y|X$, we can think of all of the different paths in the PDG different ways an agent with knowledge $\dg M$ can get probabilistic estimates of the conditional distribution $\bbr{\dg M}\MaxEnt(Y | X)$. The next result states that, in a precise sense, these various estimates bound the location of the marginal for this maximum entropy distribution, which suggests an efficient sampling algorithm for $\bbr{\dg M}\MaxEnt(Y | X)$, after learning some weights.
	
	\begin{conj}\label{thm:maxent-hull}
		For any PDG $\dg M = \pdgvars[]$ containing variables $X, Y$, the maximum entropy conditional marginal $\bbr{\dg M}\MaxEnt(y \mid x)$ is a convex mixture of the conditional marginals generated by the paths from $X$ to $Y$.  That is, there exist weights $\{\alpha_i \geq 0\}$ on the paths in $\dg M$ and a bias weight $\alpha_0$ with $\sum_i {\alpha_i} = 1$ and
		\[ \bbr{\dg M}\MaxEnt(Y \mid X) = \alpha_0 \  p^{\text{unif}}_Y \sum_{p \in \bbr{\dg M}_\lambda(X, Y)} \alpha_i (p_1 \circ \ldots \circ p_k) \]
		where $p^{\text{unif}}_Y$ is the uniform distribution on $Y$, and $\bbr{\dg M}_\lambda(X,Y)$ is the set of paths from $X$ to $Y$ generated by composition and Bayes Rule in $\dg M$. 
	\end{conj}

	One natural choice of these $\alpha$'s is the certainty scores for each edge, given by a weighted PDG, but we do not have any further formal results in this direction.
	Note that it is common for humans to make decisions in this way: to estimate whether something is realistic by following multiple chains of reasoning weighting them by strength of argument.
	
%	\begin{conj}
%		The conditional marginal of the maximum entropy distribution $\bbr{M}\MaxEnt(b \mid a)$ is in the convex hull of the compositions of paths $A \to B$. 
%	\end{conj}
}


	\section{Discussion}
        %oli8: entirely rewritten.
%joe7* It will need to be rewritten again, I'm afraid.  We should hint
%at what PDGs are good for: the fact that we can keep track of
%different, possibly conflict sources of information (we should make a
%bigger deal of that in the intro), the fact that we can capture lots
%of distributions (we hinted at that; can we say more?), the fact that
%they are modular (which lets us put together different sources of
%information) and the fact
%that we can model inconsistency and how people recover from
%inconsisency.  This will require us to get into the dynamics of PDGs
%(how people recover from inconsistency) and will require another
%paper, but we should mention it here.  A good discussion will easily
%fill up the 8th page.
%oli9: I agree, this plan sounds excellent. While you do your pass, I will write a draft of this in a separate document. 
%oli10        
\commentout{
	Given more computation, would your beliefs be more consistent? Or would you explore further, forming more extensive networks of them?
	The canonical picture of an idealized agent has always given the first answer, but this may not necessarily be the case.
}
 
 	% Do dogs have more or less consistent beliefs than we do? 
%oli8
	% They may just not have as many concepts.
	% It may be that they are no less consistent, but just have fewer concetpts.

%oli10: deleted the previous text, 
	% We have given the semantics of PDGs, a modeling framework that enables formal reasoning about this kind of mental state, and is strictly more expressive than the class of Bayesian Networks. The scoring semantics for fixed $\beta_L = \gamma = 1$ recovers the factor graph. The distribution given by a PDG, however is not generally this one, but rather one that is as consistent as possible with the supplied cpds.
	% 
	% The material covered in the present paper is only part of the picture. There are two very important generalizations that we plan to cover next. First, by minorly relaxing the definition of a cpd so that it doesn't need to provide a distribution for a special \texttt{null} value, we gain a huge amount of expressive power, allowing for simpler representations of events and partial knowledge.
 	% Second, while we have discussed what happens in the limit as $\gamma \to 0$, emphasizing the quantitative part of the PDG, there is also a rich story to be told about the qualitative half. Together with the modularity provided by the PDG, and the relaxation to strict PDGs, PDGs are able to function as causal models.
%oli10: new text.
%joe9*: rewrote completely.  You have a few lines to say more.  It
%would also be good to squeeze in somewhere something about related
%work (e.g., perhaps dependency networks).
\commentout{
PDGs are a powerful tool for representing local probabilistic information.
	Though represented by a graph, the edges are interpreted
        differently. Each edge alone determines a cpd, making PDGs
        formally analogous to a commutative diagram, instead of a
        flow-chart. A more familiar network can be obtained with the
        use of multi-tailed arrows. 

	PDGs have a parameterized semantics $[[ - ]]_\gamma$, which always generalizes Bayesian Networks, and precisely becomes a factor graph when $\gamma=1$.  This exposes an implicit trade-off between quantitative and qualitative data; the two behave very differently, but are unfortunately fused in a factor graph. Both qualitative and quantitative information can be inconsistent, although the former is less straightforward, this preliminary paper we focus on the quantitative limit.

	Incorporating new variables, data, or restricting to subgraphs
        of a PDG is simple, making it possible to construct one by
        simply throwing together some pre-trained statistical
        models. Moreover, in the quantitative limit, PDGs continue to
        have local meaning, in stark contrast with energy-based models
        such as factor graphs. As a result, PDGs are not only a
        flexible representation, but modular as well. 

	The most dramatic feature of PDGs is their ability to deal with inconsistency. A PDG can track conflicting information from different sources, and its semantics identify when this occurs, rather than quietly sweeping problems under the rug.
	As a result, a user may resolve inconsistency in multiple ways. Inconsistency can be dealt with by updating one or multiple tables, by introducing introducing or splitting variables, or even left unresolved as as one searches for clarification. The pain of inconsistency can be mitigated by expressing a decreased confidence, without altering any data. 
}
%joe9: \end{commenout}
We have introduced PDGs, a powerful tool for representing
probabilistic information. 
%oli11: trying again to integrate some of what I wrote above in
%clearer terms. Note that now the commutative diagram hint comes in
%the syntax section.
%joe10*  NO!!  Do NOT mention commutative diagrams. It's a distraction.
%It's also not true that PDGs are equivalent to BN's when both are
%rooted trees.  You also need to require that there are no parallel
%edges.  I just cut this sentence
%Although PDGs are visually similar to a BNs and equivalent when both
%are rooted trees, a slight change in the way edges are interpreted
%brings them in line with commutative diagrams, 
%and makes them
%oli11: I like this adjective more than you do; you're welcome to find a new one
%dramatically
%%
%more expressive. 
They have a number of advantages of other
graphical representations:
\begin{itemize}
  \item They allow us to capture inconsistency,
including conflict information from multiple sources, and
%oli11
% confidence about pieces of information.
express variable confidence in the information. 
\item 
They are much more
modular 
%oli11: If modularity = flexibility + ability to break into local components with meaning, then I buy this.
% If modularity = flexibility of adding things, it's only true for directed models; factor graphs are modular as hell but difficult to interpret and only have global meaning.
than other representations; 
for example, we can
combine information from two sources by simply taking the union of two
PDGs, and it is easy to add new information 
%oli11 added
%joe10: cut; I don't undrstand what this means
%(edges, including any statistical model you happen to have)
(edges)
%oli11: I want something simpler than "representational capacity" but
%I can't think of what to say instead 
%joe10: I odn't know what ``representational capacity'' means.
%and representational capacity (nodes)
and features (nodes)
%oli11
% regarding nodes and edges
without affecting previously-received information.
%oli11: I like what you have written, but my paragraph above says a
%lot more. I'm trying to merge them to make it clearer. 
%oli11: This local meaning is super important. It was in the equation,
%but I keep not being able to show it directly; I'll try to re-word.  
%joe10: we haven't even defined restrictions, nor have we explained
%this point.  You can't bring it up out of the blue now!  After
%starting to make changs, I just cut this sentence
%oli12: we have now explained the point. I'm inserting a part of this
%again.
%joe11*: we have *not* empahsized the point.  Nowhere do we talk about
%preserving local meanings of cpds, nor explained why this is important.
%In addition to their flexibility (and unlike similarly flexible models
%such factor graphs), PDGs are unique in also preserving the local
%meanings of their cpds.  
%oli13*: This is an important subtlety --- if by "modular" you mean
% flexiblle, then factor graphs are in some sense better than PDGs (you don't
% even need to normalize your factors!), but we have not time or space. 
% I guess I have to drop it..
\commentout{
%In stark contrast to factor graphs,
In contrast to factor graphs,
%joe10: I don't agree
%oli12*: Why not? You can just add a factor anywhere.
%which are equally flexible,
restrictions of PDGs continue to 
%joe10: you keep using words like ``local'' and ``global''; I don't
%know what they mean.  We can't introduce them out of the blue
retain the same local commitment to the meanings of cpds in the
restriction; this is  
As a result, PDGs exhibit both flexibility and locality, making them
uniquely modular. 
}
\item They allow for a clean separation between quantitiatve
  information (the cpds) and more qualitative information contained by
%joe10
%  the graph struture; this is captured by the terms $\Inc$ and $\IDef$
    the graph structure; this is captured by the terms $\Inc$ and $\IDef{}$
  in our scoring function.
\item PDGs have (several) natural semantics;
%oli11 added
%joe10: I don't understand this
  %, each of which paints a global probabilistic picture with
  %  constraints,
%oli12: this was clarified in the semantics introduction now. edited
%and reinstated. 
%joe11: this was *not* clarified in the introduction, and I don't like
%the terminology.  What does it mean to bear on 
%  each of which which bears on global properties of joint distributions,
%  making a PDG more useful than a mere collection of data. 
one of them (under some assumptions) allows us to pick out a unique
distribtuion specified by the PDG.   This, in turn, shows that PDGS 
%joe11: combined
%
%\end{itemize}
%We showed that PDGs
can capture BNs and factor graphs, in the latter case by choosing 
appropriate parameters in the scoring rule. However, they also avoid
%joe10*: let's make sure that we do this
some of the arguably unnatural properties of factor graphs.
%oli22: the AAAI appendix is only seen by the reviewers, so I don't think it's 
% worth it until we expand it to a full length journal paper.
%	 (a point discussed in more detail in the appendix).
\end{itemize}

We have only scratched the surface of what can be done with PDGs here. 
%oli11:added
%joe10: this sentence tailed off.  Feel free to add it again
%For simplicitly of presentation, we have only allowed PDGs to express 
%
One issue we hope to foucs on in the future is dynamics.  We believe
that PDGs will prove a particularly useful tool for examining how agents resolve
inconsistency.
%oli11: inefficient use of space
% , moving from an inconsistent state to a consistent state.
%oli11: added
%joe10: This is not ``Moreover''.  It's not quite the right story
%Moreover, it seems that the setups to a very large class of standard
%naturally encoded as PDGs, while their standard resolutions (resp.,
%naturally encoded as PDGs; the standard approaches to deal
%with them (resp.,
%variational inference, Bayesian updating) can be regarded as
%reductions of its inconsistency.   
Inconsistency resolution is a pervasive phenomenon.
%oli22: For example, belief updating can be naturally viewed as an instance of
% resolving inconsistency: this is not a good example, and duplicates the below.
% We now believe that something is true to
% which we earlier assigned probabiity less than 1.
For example, 
conditioning can be
viewed as a way of resolving that inconsistency (as can other more
general approaches to belief updating, such as Jeffrey's Rule
\cite{Jeffrey68};
%oli22:
we know that this generalization is cleanly captured with PDGs. 
Belief propogation, too, 
can be naturally regarded in this light: a collection of pseudomarginals are
that correspond to factor and variable beliefs may be inconsistent, and 
messsage passing algorithms are greedy mechanisms for reducing this inconsistency \cite{wainwright2008graphical}.
More distantly related tasks in other fields, such as variational inference in
machine learning, can also be framed as inconsistency reduction: a variational 
autoencoder is in some sense an assertion that both a distribution
and a variational approximation to it are both accurate; minimmizing
the inconsistency between them is the learning task \cite{kingma2013autoencoding}. In all three of these
examples, the objective even has a standard formulation as a minimization of KL divergence,
suggesting that we can apply PDGs without modification to capture an incredibly 
broad range of tasks and resolve them in a principled, uniform way.
We hope to report on this in the future.


Although we have not considered computational issues here, many
standard algorithms for inference on graphical models
seem directly applicable to PDGs; we plan on investigating their
behavior to see the extent to which they can be modified to yield
results consistent with the semantics that
we have provided.

\end{document}
