[Finite beings cannot actually represent real numbers]
	That's ok, we only need to store an approximation of them; this is not a problem in practice. Sure, it's some other field, but the important thing is that we can order things and roughly trade them off as though they had values that were in the real numbers. Also, to the extent that we fail to do this, it's not clear how much of a problem this is.

[The utility function has to be represented as part of the world configuration]
	Yeah, but we can approximate it, perhaps. Some parts of the world are more important than others to this agent, mabe. There will definitely be some lossiness, and enven though this is circular to some degree (i.e., a world contains a model of an agent which contains a model of a world, ..., etc. ), we can allow loss at each level of modelling so that the representation works out.

[No agent can actually observe the state of the world]
	That's ok, we can do Bayesian reasoning to figure out what likely states of the world are, by moving around and doing things.x
	

[The notionof "possible worlds" is misguided]
	There's really only one possible world in some sense. What's to stop an agent from having preferences over arbitrarily awful implausible worlds, if it's dumb? We really don't have preferences over worlds, but rather over our perceptions of them.
	


The world has some substructere
