\documentclass{article}

\input{prefs-commands.tex}
\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{A Simpler Explanation}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}

\begin{document}
	This document is intended to be clear, simple, and at a high level. % It exists as a safe haven away from most of the math and generality I want to later think about, aside from some footnotes and a section where I list some connections I suspect exist. 
	I will use no numbers, prove nothing here, and just make some arguments and provide examples.
	

	\section{Why This Is Worth Doing}
	\vspace{-1em}
	\textit{A few short motivations from several perspectives}
	
	
	% AI safety
	
	%	There are many ways of framing this.
	%		real preferences change. 
	%		Dual brittleness: certainty in preferences
	%		Computational boundedness

	
	% AI safety
	
%	There are many ways of framing this.
%		real preferences change. 
%		Dual brittleness: certainty in preferences
%		Computational boundedness
	

	\section{Longer Informal Description and Intuition}
	
	\subsection{Representation}
	We will consider preferences on small sets of alternatives (nodes), together with a graph of beliefs, about how one preference impacts another. For example, the nodes could include: the set of ice cream flavors, a collection of memorable experiences, freedom and control, and anything else that could conceivably be considered an exhaustive set of alternatives for a choice. On these sets, we require preferences of some form; while I would like to ultimately propose that they be thought of as semiring matrices, we will stick to utilities for each alternative in the examples here.
	
	In addition, we also need beliefs about the impact of $A$ on $B$. While we do not want to commit to one of these in general yet, conditional probabilities will do the trick to explain the examples which follow. A belief about how $A$ impacts $B$ then, for the remainder of this document, is a family of probability distributions over $B$, one for each alternative $a \in A$ --- an object which looks and acts like the conditional distribution $\Pr(B \mid A)$, with which it will intentionally be confused.
	
	The structure of a representation with both pieces might look something like this:
	\begin{center}
	\begin{tikzpicture}
		\node[bpt={d1|$d_1$}] at (0,1){};
		\node[tpt={d2|$d_2$}, below=1.5em of d1] {};
		\draw[arr,->, thin, gray] (d1) to[bend left=30](d2);
		\node[Dom={$\sf D$ (D) around \lab{d1}\lab{d2}}] {};
		
		
		\node[bpt={c1|$c_1$}] at (4,0){};
		\node[bpt={c2|$c_2$}, below=1.8em of c1] {};
		\node[bpt={c3|$c_3$}, below=1.8em of {c2}] {};
		\draw[arr,->, thin, gray] (c1) to[bend left=50](c2);
		\draw[arr,->, thin, gray] (c3) to[bend left=50](c1);
		\node[Dom={$\sf C$ (C) around \lab{c1}\lab{c3}}] {};
		
		\node[bpt={w1|$w_1$}] at (-4,0){};
		\node[bpt={w2|$w_2$}, below=1.8em of w1] {};
		\node[bpt={w3|$w_3$}, below=1.8em of w2] {};
		\draw[arr,->, thin, gray] (w2) to[bend left=50](w3);
		\draw[arr,->, thin, gray] (w1) to[bend left=50](w2);
		\node[Dom={$\sf W$ (W) around \lab{w1}\lab{w3}}] {};
		
		
		\foreach [evaluate=\x as \y using \x/2-1.75] \x in {0, 1, ...,7} {
			%					\pgfmathmacro{\tmp}{-1 + 0.2*\x}
			\node[bpt={e\x | $e_\x$}] (e\x) at (\y,-3) {};
		}
		%nonsense
		\draw[arr,->, thin, gray] (e0) to[bend right=30](e6);
		\draw[arr,->, thin, gray] (e6) to[bend left=20](e2);
		\draw[arr,->, thin, gray] (e2) to[bend left=20](e4);
		\draw[arr,->, thin, gray] (e4) to[bend left=10](e7);
		\draw[arr,->, thin, gray] (e7) to[bend left=40](e5);
		\draw[arr,->, thin, gray] (e5) to[bend left=20](e1);
		%end nonsense
		\coordinate (Q) at (0,-3.5);
		\node[bDom={$\sf E$ (E) around \lab{e0}\lab{e7}(Q)}] {};
		
		\draw[arr] (D) -- (C);
		\draw[arr] (D) -- (W);
		\draw[arr] (E) -- (C);
%		\draw[arr, dashed] (D) to[bend left] (E);
		\draw[arr] (E) -- (D);
	\end{tikzpicture}
	\end{center}
	There are alternatives in each of the domains, preferences among the alternatives (we draw $w_1 \to w_2$ to communicate that $w_1 \leqc w_2$, i.e., that $w_2$ has higher utility than $w_1$), and links (conditional probability distributions, whose values are not drawn here) between them. 
	
	\subsubsection{Representing Utilities, Probabilities}
	While I will explain more details of the correspondence between this presentation and other standard ones in section \ref{sec:emulation}, the examples may be more informative if we can see how utilities and credence enter the picture.
	
	
	\todo{Move this so the section isn't as long, this is starting to be out of the critical path}{\color{gray}
	
	So why is the standard to use the real numbers instead of some other ordered set? The biggest reason is that uncertainties need to trade off against value, and we use probabilities to track uncertainty. For example, suppose I think $a_1 \leqc a_2 \leqc a_3$. This is enough information to make choices between subsets of the values of $a$, but is not enough to know whether I prefer an $\alpha$ chance at $a_1$ and $(1-\alpha)$ chance at $a_3$, over a sure outcome of $a_2$. This is unsurprising --- after all, bets are different things altogether. In fact, we can also put the space of bets into our diagram to clarify what's going on here:
	
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (0,0){$\sf A$};
			\node[dpadded] (BA) at (-3, 0){${B}[\sf A]$};
			\node[dpadded] (U) at (3,0){$\mathbb R, \leq$};
			
			\draw[arr](A) to node[above]{$U_A $} (U);
			\draw [arr] (BA) to node[above]{$\mathrm{id}$} (A);
		\end{tikzpicture}
	\end{center}
	A choice of bets on $A$ (above represented $B[A]$) clearly impact $A$, and in this case our formulation is particularly simple: the conditional probability $\Pr(A \mid B[A])$ is just the bet itself, i.e., $\Pr(A = a \mid B[a] = \vec{p}) = \vec p_a$. But if we want the composition of these two links to be equal to the utility on bets (i.e., we want it to be consistent, as discussed in the dynamics section), then we have to be computing expected utility.
	}
	
	
	On the one hand, we can use these links to describe utility, ignoring the probability distribution over the output, and making full use of the dependence on input. We can also do the exact opposite, and ignore the dependence on input, making full use of the distribution as output. This can be used to give us a prior without any parameters, which is represented graphically as an arrow from the preference domain with only one object, denoted $\sf 1$.
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (0,0){$\sf A$};
			\node[dpadded] (1) at (-3,0){$\mathsf 1$};
			\draw[arr](1) to node[above]{$\Pr(A)$} (A);
		\end{tikzpicture}
	\end{center}	
	
	These two are dual in some sense. Intuitively, one of these choices is choice made by the agent, and the other one by the environment. A utility function uses only the dependence on the input, so it can be seen as a vector $u: A \to \mathbb R$; a probability distribution makes use only of the distribution over outputs, and can be represented as a vector. If the links are thought of as stochastic matrices, then one is the transpose of the other.
	
		
	In this way, we can think of passive expected utility calculations, across the values of a variable $X$, as a factorization of $\sf 1 \to U$ through the alternatives of $X$, i.e., $\sf 1 \to X \to U$.
%	\todo{restructure: move to end}
%	Think of the conditional probability $\Pr(B \mid A)$ as a right-stochastic matrix $P$: $P_{ij}$, 
	
%	if we consider all possible settings of the variables to be alternatives of a single preference domain $\cal W$, then we can 
	
	
	
	\subsubsection{Benefits of this Representation}
	It allows us:
	\begin{itemize}[nosep]
		\item to represent inconsistent and redundant preferences
		\item to introduce new choices, and to modify or delete preference information without needing to globally re-compile a joint preference
		\item to emulate other preference descriptions (see above, and section \ref{sec:emulation})
		\item to store preferences about common choices so they can be re-used and nudged without fully recomputing everything for each decision
	\end{itemize}
	The most important reason for representing preferences this way is to facilitate preference change, described in a bit more detail below.
%	It does not fully specify a joint preference on all of the nodes, but neither does a CP net, which only defines a partial order on the product of all variables. I argue that this complete preference on all possible outcomes is not a feature we need to represent, because we will never have a choice between them all. The only reason to do so is so that we can compute expected utility.
	
%	While we no longer always have an explicit answer to the question ``is this world better than that one'' (a choice we rarely need to make), we still have a decision making apparatus, and in the case where there's no inconsistency, we're still implicitly defining a joint utility function.
	
%	Moreover, maintaining it as more domains are added to the picture becomes very expensive.
	

	
	\subsection{Dynamics}
	
	Given a network representation of preferences, we can compute some measure of inconsistency, and then try to move preferences (or even beliefs) around so as to reduce the total inconsistency. This simple procedure has a number of interesting consequences, which we will go through in this section. 
	%
	Later, when we further specify the way this can be done, we will be able to explain several additional phenomena.
	% reasonable utility functions from strange starting places
	% value capture
	
	\subsubsection{Value Capture}
	Suppose you care about
	

	
%	Of course, this picture cannot capture interaction effects---rather it visually encodes the additive separability assumption---but we can handle this by finding 

	
	
	
	
%	One way of representing this is as a diffusion of computation across the network
	
%	Intuitively, a 
	
		
	% axioms are cycles
	% topology of the graph 
	
	
	\section{Insufficiency of Classical Model: Examples}
	
	
	\subsection{Framing Problems}
	Since inconsistency is driving preference changes in this formulation, we will start with the case where clearly this is what's going on. 
	%
	Suppose you prefer $a_1$ over $a_2$ (for variable $A$) and $b_2$ over $b_1$ (for variable $B$). Later, you come to believe that a choice of $a_1$ is really logically equivalent to $b_1$, and $a_2$ to $b_2$; in fact, $A$ and $B$ were the same variable. This happens all the time for humans, if $A$ and $B$ were given different descriptions. For example,
	
	\begin{quotation}
		\it\small
		You believe that the rich should not get a larger federal tax exemption (than the poor do) for having children.
		At the same time, you believe that if the default number of children were 2, and people paid a surcharge to the government for foregoing them, that the rich should pay a larger surcharge (than the poor do) for this. 
		However, the default number of children is not a feature of the world, and the same amount of money changes hands in both cases; the two preferences are logically in conflict.
		
%		You later come to realize that these preferences are logically in conflict; you then update your beliefs about both of them, causing yourself to be less sure, until the more entrenched one wins, and your viewpoint on the other issue has swapped.
	\end{quotation}
	
	This creates a conflict--- it is impossible to have all four of
	\[ \Big\{\text{consistency},\qquad a_1 \geqc a_2,\qquad b_1 \leqc b_2,\qquad\text{the belief that}~(a_1 \equiv b_1) \land( a_2 \equiv b_2)\Big\}\]
	
	Before we go any further, notice that there's not even an obvious, natural representation of this conundrum in terms of classical decision theory: if utilities are over outcomes, and the two descriptions pick out the same set of outcomes (without changing their relative probabilities), then you will never be in a situation like this. Let's try to salvage this from the classical point of view in a few ways.
	%Let's try a few. 
	
	\subsubsection{}
	One possible patch is to have utilities over \textit{descriptions} of worlds, dependent on more than just the worlds they pick out. Now, this ``conflict'' from before is not problematic: you prefer $a_1$ to $a_2$, $b_2$ over $b_1$. 
	
	We can use this to represent the state of the world, but now:
	\begin{itemize}%[nosep]
		\item Computation of expected utility now requires priors over descriptions of worlds, an object which is even more astronomically complex than a prior over worlds. %In addition to being large, these probabilities seems problematic in other ways: a change in the distribution of language you use might now change your beliefs 
		
		\item Crucially, this account does not denounce, let alone provide any mechanism for resolving the disagreement, and therefore we sacrifice all of the standard rationality guarantees, such as resistance to dutch booking%
			\footnote{If you have utilities $u_1, u_2, v_1, v_2$ for $a_1, a_2, b_1, b_2$, respectively, then you would be willing to pay $u_1 + v_2$ for a bet of $a_1$ or $b_2$ (which has probability 1), and $u_2 + v_1$ for a bet of $a_2$ or $b_1$, which again is probability 1. Since the difference in utility differs between the $a$ and $b$ descriptions, $u_1 - u_2 \neq v_1 - v_2$, and so  $u_1 + v_2 \neq u_2 + v_1$, and a bookie can make unbounded money off of you by selling you one bet and buying the other.}.
		Decisions consistent with this picture may have nothing to do with the world may be entirely based on the number of parentheses in the formula.
%		\item The premises, that we prefer $a_1$ to $a_2$ and $b_2$ to $b_1$, are 
	\end{itemize}

	Clearly, this is a flimsy model as it stands, but it's not clear how to fix it. If we add independence of description as an axiom, we're back to our original representation problem. 
	
	\subsubsection{}
	Another tactic we compatible with the classical decision theory perspective\footnote{the model described here actually corresponds to an influence diagram with two decision and two value nodes}, this time much closer to our own formulation, is to define utility by additively separable components, each arising from a variable (we have once again implicitly given ourselves utilities over descriptions of the world), relying on belief revision to effectively make some worlds impossible.
	
	The set of possible worlds which we have utilities over is once again all possible assignments to variables, and hence includes ``impossible possible worlds'', such as the one where $a_1$ and $b_2$ are simultaneously true. By belief updating, the likelihood of these worlds can be driven to zero, and when we say ``$a_1 \geqc a_2$'', we really mean that the variable $A$ has an additively separable component of our total utility function (resp. for $B$), and it is merely an unfortunate fact of life that good things are attached to bad ones. If the two preferences carry the same weight, then as the possibility of achieving impossible worlds vanish, the two given preferences annihilate one another and we are left indifferent, as far as the total utility function is concerned.
	
	This seems in many ways better, because the tension is resolved, it can happen continuously, the agent is only vulnerable to dutch booking as long as its beliefs are inaccurate, and we get a temporal reason for why the agent's choices change. In other ways, it is less satisfying:
	\begin{itemize}%[nosep]
		\item In order to define a classical agent, a modeler must specify a utility function, and in this case means that each additively separable component must have been there in the beginning. In particular, this means the modeler decides on the set of variables\footnote{If instead the modeler only described a way of generating these utilities, so that the set of variables could change, then there must be some additional structure to make these decisions, making the picture immediately very non-classical --- classical utilities do not depend on an agent's beliefs or other mental features}, and so agents can only be in this situation insofar as the modeler has given them representational redundancy: it is impossible for an agent to experience conflict in this way if the variables in fact determine unique states of the world.
		
		\item There can never be a time when the agent holds both the contradictory preferences, and also the belief that they are contradictory, even if belief updates happen slowly.		
		
		\item You can never truly change your opinion on either issue: you will always want $a_1$ over $a_2$ and $b_2$ over $b_1$ --- even if you now see how they are exactly the same decision.
		%By contrast, humans seem to take both into account and then decide on a joint picture of how the decision should be made.
		
		\item Because we define a fixed utility function, the two conflicting components on the same concept could not have been the result of some dynamics mechanism; they were in fact put there (explicitly) by the modeler. 
	\end{itemize}
	
	More briefly, our complaint is that the components of the utility function didn't actually change, and that the set of variables is necessarily fixed.
	
	\subsubsection{With Our Preference Network Model}
	
	We propose a slight modification of the previous model, where preferences actually flow along the beliefs that link them: In our case, this means we consider what it would look like to make a decision about variable $B$ looking only at what we care about in $A$, and vice versa. Since the correspondence is a logical one, the ``impact'' works both ways, so for us this means we'll use two beliefs, to distinguish it from the case where one causes the other.
	
	\begin{center}
		\begin{tikzpicture}
		\node[dpadded] (A) {$A$};
		\node[dpadded, right=3 of A] (B) {$B$};
		\draw[->, thick] (A.10) -- (B.170);
		\draw[->, thick] (B.190) -- (A.350);
		\end{tikzpicture}
	\end{center}

	%At this point, we should be a bit more concrete. 

	To keep this as simple as possible, let's consider the case where the beliefs linking the two domains form instantly, and consists of perfect correspondences, degenerate probability distributions with all of the mass on one point:
	\[ \Pr(B \mid A) = \begin{cases} a_1 \mapsto b_1\\ a_2 \mapsto b_2 \end{cases} \qquad\qquad
	   \Pr(A \mid B) = \begin{cases} b_1 \mapsto a_1\\ b_2 \mapsto a_2 \end{cases} \]
	
	With this, our preference ($a_1 \geqc a_2$) has an image of ($b_1 \geqc b_2$) under the link $B \to A$, which conflicts with our preference on $B$; similarly, ($b_2 \geqc b_1$) results in a preference ($a_2 \geqc a_1$) which conflicts with our preference on $A$. Changing the belief or either preference can resolve the conflict locally, but as we saw earlier, the actual inconsistency depends on the rest of the network. 
	
	
	
	
	% rather than being aggregated at the end, we conceptualize utility as through beliefs. For instance \todo{chipfiring}
	

	
	
%	\subsection{

	\section{More Examples}
	\todo{I have a list of some of these, commented out because I haven't put in the work to explain any of them. They're just place holders for me as I worked things out; I'll uncomment them as I get time to explain them, but they're low priority, merely serving to address concerns I had but didn't articulate}
%	\begin{example}[Converting game trees]
%		Suppose you flip a coin $C_1$: if it comes up heads, then you flip a coin $C_2$, and there is a 1/3 chance that you work, sleep, and play games. If it comes up tails, you have to work. You don't want to work.
%	\end{example}

	\section{Ties To Existing Models}
	
	\subsection{Emulating Other Structures}\label{sec:emulation}

	\subsection{Important Differences}
	
	\subsubsection{Departure from BNs and CP Nets}
	
	Although this representation looks like a Bayseian Network, and in fact any BN can be encoded in this way, the two formulations differ at face value in several important ways:
	\begin{enumerate}
		\item We encode preference information in each domains, 
		
		
		\item The existence of two arrows into $\sf C$ in the diagram above does not mean that there is a distribution $\Pr({\sf C \mid D, E})$, but rather two different ones $\Pr\sf( C \mid E)$ and $\Pr\sf(C \mid D)$. It is possible to recover this meaning if we are allowed to introduce new product nodes:
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (D) at (0,0) {$\sf D$};
			\node[dpadded] (E) at (0,-2) {$\sf E$};
			\node[dpadded] (DE) at (2.5,-1) {$\sf D \times E$};
			\node[dpadded] (C) at (5,-1) {$\sf C$};
			
			\draw[arr] (DE) -- (D);
			\draw[arr] (DE) -- (E);
			\draw[arr] (DE) -- (C);
			\end{tikzpicture}
		\end{center}
		
		\item Similarly, a node without any parents has a probability distribution on it in a BN; this is not the case for us (think of this as the zero-arity product, making this a special case of the above). Once again, this can be represented by explicitly adding an edge, from the singleton preference domain:
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {$\sf 1$};
			\node[dpadded] (D) at (4,0) {$\sf D$};
			
			
			\draw[arr] (1) to node[above]{$\Pr(D)$} (D);
			\end{tikzpicture}
		\end{center}
		
	\end{enumerate}
	As a result, a model like this does not always represent a factorization of the joint distribution on the product of the variables. In some sense, it does represent a factorization of joint preferences on the variables, but not in a way analogous to the BN approach (this is what CP Nets do).
	
	While it is true that I have implicitly defined a utility function, 
	
	
	\subsubsection{Static description in terms of Influence Diagrams}
	Our rounded nodes function somewhat like a hybrid of the three nodes used in influence diagrams: aleatory variables (circles), decisions (squares), and values (octagons). \todo{}
	
	\begin{center}
		\begin{tikzpicture}[baseline=(current bounding box.center)]
		\node[dpadded] (D) at (0,0) {$\sf D$};
		\node[dpadded] (C) at (2,0) {$\sf C$};
		\node[dpadded] (E) at (1,-1.6) {$\sf E$};
		\draw[arr] (D) -- (C); \draw[arr] (E) -- (C); \draw[arr] (E) -- (D);
		\end{tikzpicture}
		$\quad\stackrel{?}{\cong}\quad$
		\begin{tikzpicture}[lesspad/.style={inner sep=0.6em,outer sep=0.2em},baseline=(current bounding box.center)]
		\node[dpadded, square, lesspad] (dD) at (0,3) {$\sf D$};
		\node[dpadded, circle, lesspad] (aD) at (-0.8,1.5) {$\sf D$};
		\node[dpadded, octagon, lesspad] (vD) at (-1.6,0) {$\sf D$};
		\draw[arr] (dD) -- (aD); \draw[arr] (aD) -- (vD);
		
		\node[dpadded, square, lesspad] (dC) at (2,3.0) {$\sf C$};
		\node[dpadded, circle, lesspad] (aC) at (2.8,1.5) {$\sf C$};
		\node[dpadded, octagon, lesspad] (vC) at (3.6,0) {$\sf C$};
		\draw[arr] (dC) -- (aC); \draw[arr] (aC) -- (vC);
		
		\node[dpadded, square, lesspad] (dE) at (-0.5,-1) {$\sf E$};
		\node[dpadded, circle, lesspad] (aE) at (1,-1) {$\sf E$};
		\node[dpadded, octagon, lesspad] (vE) at (2.5,-1) {$\sf E$};
		\draw[arr] (dE) -- (aE); \draw[arr] (aE) -- (vE);
		
		\draw[arr] (dD) -- (aC);
		\draw[arr] (dE) -- (aD);
		\draw[arr] (dE) -- (aC);
		
		%		\draw[arr] (DE) -- (D);
		%		\draw[arr] (DE) -- (E);
		%		\draw[arr] (DE) -- (C);
		\end{tikzpicture}
	\end{center}
	
	
\end{document}