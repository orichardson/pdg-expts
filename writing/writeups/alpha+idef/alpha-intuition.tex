\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}

\renewcommand{\H}{\mathop{\mathrm H}}
\newcommand{\E}{\mathop{\mathbb E}}
\newcommand{\bp}[1][L]{\mathbf{p}_{\!_#1\!}}
%\newcommand{\V}{\mathcal V}
%\newcommand{\N}{\mathcal N}
%\newcommand{\Ed}{\mathcal E}
%\newcommand{\sfM}{\mathsf M}
%\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bp#1)}
%\def\Pa{\mathbf{Pa}}
%\newcommand\SD{_{\text{sd}}}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}


\begin{document}
We have a weighted PDG representing our beliefs; 
our third semantics gives us a way to score an arbitrary distribution $\mu$,
based on how well it matches the picture the PDG represents.  For context, the two terms of distribution-scoring semantics of a PDG are given by the \emph{extra information}, which can be defined by either of the two equations below:

%We also recently defined the \emph{extra information} of a distribution $\mu$ to be
\[ 	
	 \overbrace{\Bigg[\sum_{ X \xrightarrow{\!\!L} Y  } \alpha_L \H(Y \mid X) \Bigg] - \H(\mu) }^{\text{FORMULATION 1}}
		\qquad\qquad \text{or} \qquad\qquad
		\overbrace{\Bigg[\sum_{ X \xrightarrow{\!\!L} Y  } 
		\alpha_L \E_{x \sim \mu_X}  \H (\bp (x)) \Bigg] - \H(\mu)}^{\text{FORMULATION 2}}
\] 

The question we aim to tackle here is to give intuition and motivation for what $\alpha$ means, or more precisely, 

\begin{quote}\it
How does the weight parameter $\alpha$ impact which distributions are judged to be good?
\end{quote}

\section*{FORMULATION 1}
\vspace{-1em}
\hspace{1in}(\textit{$\alpha_L$ controls the term $+\H_\mu(Y \mid X)$} )
\vspace{1em}

By including the edge $L$ with positive corresponding value of $\alpha_L$, 
an agent is indicating that this edge is somehow an ``effective way of estimating the value of $Y$'' from the value of $X$. This notion is independent of the corresponding cpt: you can believe that $Y$ can be effectively determined by $X$ even if you do not know how to do this yourself.

For each edge $L$ from $X$ to $Y$, $\alpha_L$ is the loss mu incurs, per unit of residual uncertainty in $Y$ (in expectation over $\mu$) that remains even knowing the value of $X$ (drawn from $\mu$). \textit{Note this last parenthetical implies that the uncertainty of $Y$ given $X$ depends not only on the conditional probability of $Y$ given $X$, but also on the probability of $X$ itself}. In the general case, it can be increased or decreased independent of the cpt, by changing the relative probability of values of $X$. If $\mu$ makes a value $x_0$, from which $\mu(Y | X = x_0)$ is very uncertain, then in expectation the uncertainty in $Y$ given $X$ increases, even though the conditional probability of $Y$ given $X$ remains unchanged. A distribution is penalized for this in proportion to $\alpha_L$. 

While this technically is the full role $\alpha_L$ plays, the moral of the same equation can look different depending on the angle. We start by addressing a potential qualms one might have had with rewarding determinism, by looking at what happens when we take both terms of the extra information (the one controlled by $\alpha$, which penalizes uncertainty at a target given source, and the one which rewards the total global uncertainty in a distribution) together:

\subsection*{An Uncertainty Budget}

It may still seem as though something is morally wrong in rewarding $\mu$ for simply being deterministic in this way: uncertainty in $Y$ given $X$ may not always feel like a defect of $\mu$. It is possible that you know $Y$ to be 
noisy, for instance.

This is offset by the fact that any distribution $\mu$ also gets a bonus score equal to its global uncertainty. We can think of this bonus as a (partial) reimbursement for the total uncertainty, some of which $\mu$ needed to pay for even to match the tables. From this perspective, a distribution that does not result much global randomness, but according to which edge targets $Y$ uncertain even given the values their sources $X$ (for instance, by making $Y$ difficult to guess given $X$, by making $X$ more deterministic as above, or by having many variables share state, none of which is exploited with the edges, which all detail random correlations), will ultimately result in paying a much higher price than the bonus. 

For a Bayesian Network, the best a distribution can do is to break even.

\subsection*{A Penalty for Failure to be Independent}
As stressed in earlier presentations, and our meeting, we can re-write the fist term an order-dependent in terms of a mutual information, which appears order dependent. Before providing a more general sketch less entwined with our information theoretic presentation, we concretely show this equation for clarity in the case where $X_1$ and $X_2$ are two variables, both of which have an edge leading to $Y$, with corresponding causal weights $\alpha_1$ and $\alpha_2$, respectively. Here, $\alpha$-controlled terms for the diagram are:
\begin{align*}
	&\alpha_1 H(Y \mid X_1) + \alpha_2 \H(Y \mid X_2) \\
	&\quad = (\alpha_1+ \alpha_2) \H(Y \mid X_1, X_2) + \alpha_1 \mathrm{I}(Y; X_2 \mid X_1 )+ \alpha_2 \mathrm{I}(Y ; X_1 \mid X_2)
\end{align*}
where $\mathrm I(A ; B \mid C)$ is the mutual information of $A$ and $B$ given $C$, which is a non-negative, symmetric, measure of the degree to which $A$ and $B$ fail to be independent given $C$.

The first of these is the term that a hyper-edge that took both into account would pay, and is the same independent of the relative weightings of $\alpha_1 $ and $\alpha_2$. The other two controls which should intuitively be more painful: violating the goal of reliably estimate $Y$ from $X_1$ (an not using $X_2$) or violating the goal of reliably estimating $Y$ from $X_2$ alone, and not using $X_1$. 
These can often be chained together and re-arranged to also explicitly show independence from other another node $Z$, but this cannot be done in without the structure of the graph.


%\subsection*{A Blockade, preventing Randomness from Diffusing Backwards Across Causal Links}
%This final interpretation is still rough, but it might be helpful and I want to put my foot in the door just in case. Just as the maximumum entropy distribution associated with a set of constraints can be identified with the fixed point of a Markov chain which locally uniformly at random modifies coordinates so long as the result is still consistent with the constraints (a Gibbs Sampler), the $\alpha_L$-\emph{modified} 



%In many cases, the alpha parameter will not matter. For instance, if the distribution has no uncertainty, then $H(Y\mid X) = 0$, and so $\alpha$ has no effect. This is also true in less degenerate cases: in rooted Markov trees (i.e., BNs without merges), for instance, 


\section*{FORMULATION 2:}% (alpha attached to E_mu [ H(p_L(x)) ])
\vspace{-1em}
\hspace{1in}(\textit{$\alpha_L$ controls the term $\mathbb E_{x \sim \mu} [ \H (\mathbf p_L(x)) ]$ })
\vspace{1em}

In the second formulation, the CPT $\bp(x)$ makes an explicit appearance: rather tracking the uncertainty of $Y$ given $X$, completely according to $\mu$, we penalize $\mu$ for the uncertainty that the \emph{supplied cpt} would exhibit, if $X$ were distributed according to $\mu$. When combined with the global uncertainty term, this formulation has a clearer interpretation of ``maximizing entropy according to constraints'', as adding $\mathbb E_{x \sim \mu} [ \H (\mathbf p_L(x)) ]$ can be viewed as we controlling for the entropy already encoded in $\bp$. This is the intuition we originally used to recover BN semantics.

Scaling back $\alpha_L$, then, corresponds to discounting, rather than eliminating, the entropy benefit to distributions that put more mass on values of $X$ that are already known to have high entropy, according to $\bp$. 
This can be seen as a weakening of one's belief that $\bp$ holds causally: we are now willing to trade the off the idea that $X$ determines $Y$ if we can get enough generality (in the form of entropy) for it. 




\end{document}