\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.8}

\newcommand{\bp}[1][L]{\mat{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bp#1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}

\newcommand{\obrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbrace{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrace}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbrace{\color{black}#2}_{#3}}\endgroup}
\newcommand{\obrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\overbracket{\color{black}#2}^{#3}}\endgroup}
\newcommand{\ubrak}[3][blue]{\begingroup\color{#1} \vphantom{#2}\smash{\underbracket{\color{black}#2}_{#3}}\endgroup}



\begin{document}
    
    \section{A High Level Recap}
    Suppose we have the cpts of a BN, (i.e,. a PDG); we would like to recover its global semantics, effectively including the independence assumptions, as a semantics through which we can view the PDG (as well as more general PDGs, in a way that makes sense)---without forcing the agent, whose knowledge consists of the CPTs, to make any assumptions. Why?
    \begin{enumerate}
        \item Deciding whether two variables are (conditionally) inependent is difficult, and we would like to distinguish a truly clueless agent from one who knows CPTs to be independent.
            \footnote{While }
        \item PDGs cannot in general cannot afford in general to make such assumptions and still retain their modularity.
        \item Doing so will provide a new justification for making such independence assumptions in BNs---one that has more minimal assumptions than related attempts, 
    \end{enumerate}
    
    


    
    %
    % The picture one might have in mind is one in which there is that the way in which variables are determined is the average of many runs.
    
    
    \section{Different Parameterizations}
    \begin{align*}
		\bbr{\sfM}(\mu) :=&\sum_{ X \xrightarrow{\!\!L} Y  \in \Ed } \E_\mu  \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} - \beta_L \lambda \log \frac{1}{\mu(y \mid x)} + \alpha_L \gamma \log \frac{1}{\mu(y\mid x)}  \right] - \gamma \H(\mu) \\
			=&\E_\mu \Bigg\{   \sum_{ X \xrightarrow{\!\!L} Y  } \left[
			\beta_L \log \frac{1}{\bp(y\mid x)} + (\alpha_L \gamma - \beta_L \lambda) \log \frac{1}{\mu(y \mid x)} \right] - \gamma \log \frac{1}{\mu(w)} \Bigg\} 
	\end{align*}
    
    
    
    \section\
    
    \begin{conj}
        Running a randomized Gibbs sampler, in which each link is picked with probability $\alpha_L$
    \end{conj}
    
    Question: search?
    
    
    % TODO: WHERE ALPHA COMES FROM
    % TODO: WHY ALPHA IS NECESSARY WITH NEW LINKS
    % TODO: WHAT DOES ALPHA = 0.5 MEAN
    % TODO: Causality.
\end{document}
