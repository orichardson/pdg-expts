\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.1}

\newcommand{\bp}[1][L]{\mat{p}_{\!_{#1}\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \mat p #1)}
\def\Pa{\mathbf{Pa}}
\newcommand\SD{_{\text{sd}}}

\def\extrainfo{extra information}

\begin{document}
\subsection*{Preliminaries}

For brevity, we use the standard notation and write $p(x, y)$ instead of $p(X \!=\! x, Y \!=\! y)$, $p(x \mid y)$ instead of $p(X \!=\! x\mid Y \!=\! y)$, and so forth. So long as $x$ is bound solely as an element of $\V(X)$, the meaning is unambiguous. 


\begin{defn}[Conditional Entropy]
	If $p$ is a distribution over a set $\Omega$ of out comes, and $X$ and $Y$ are random variables on $\Omega$, then the \emph{conditional entropy}, $\H_p(X \mid Y)$, is defined as 
	\[ - \sum_{x \in \V(X), y \in \V(Y)} p(x,y) \log \frac{p(x,y)}{p(x)} \]
\end{defn}


\begin{defn}[PDG, short version and new notation]
	A PDG is given by $\sfM = \mnvars[]$, where $\N$ is a set of random variables, $\V(N)$ is the set of values a random variable $N \in \N$ can take, $\Ed$ is a set of labeled edges, and $\bp$ associates each edge $L$ from $X$ to $Y$ to a cpt $\bp$ such that $\bp(y \mid x)$ is the probability of the event $Y = y$ given that $X = x$. We will also write $\mat p^L$ when we want to think of $\bp[L]$ as a matrix, and access element $x,y$ by  $\mat p^L_{x,y}$.
\end{defn}

\begin{defn} \label{def:set-rv}
	Sets of random variables as random variables. If $S$ is a set of random variables $X_i : \Omega \to \V(X_i)$ on the same set of outcomes $\Omega$, we consider $S$ itself to be the random variable taking values $\V(X) = \{(x_1, \ldots, x_i \ldots) \}$ for $x_i \in \V(X_i)$. Formally, we define its value on a world $\omega$ to be $S(\omega) := (X_1(\omega), \ldots, X_i(\omega), \ldots)$. 
\end{defn}

\begin{defn}[Transformation of a BN to a PDG]\label{def:gamma}
	If $\cal B$ is a Bayesian network on random variables $X_1, \ldots, X_n$, we construct its corresponding PDG as follows: we take $\N := \{X_1, \ldots, X_n \} \cup \bigcup_{i=1}^n\{ \Pa(X_i) \}$ to be the set of all of the BN's variables, plus a new variable for each collection of parents, if not already in the collection. The values $\V(X_i)$ for a random variable $X_i$ are unchanged, and $\V(\Pa(X_i))$ is defind on sets as above.		
	We take the set of edges $\Ed := \{ (\Pa(X_i), X_i) : i = 1, \ldots, n \} \cup \{ (\Pa(X_i), Y) : Y \in \Pa(X_i)\}$ to be the set of links to a variable $X_i$ from its parents, plus also projection edges from the sets $\Pa(X_i)$ to their elements. 
	Finally, we set $\bp[(\Pa(X_i), X_i)]$ to be the cpt associated with $X_i$ in $\cal B$, and for the projections, we set $\bp[(\Pa(X_i), Y)](Y = y \mid \Pa(X_i) = (\ldots, y', \ldots)) := \delta_{y,y'}$, that is, given a setting $(\ldots, y', \ldots)$ of a set including the variable $Y$, we give a distribution on $Y$ by  1 if $y = y'$ and 0 otherwise.
\end{defn}

	% \footnote{Contrary to common assertion, this is \emph{not} an abuse of notation so long as $\mathcal V(X) \cap \mathcal V(Y) = \emptyset$, which is always possible by simply tagging values with type information, by $x \mapsto (x, X)$, for instance.}   
When we say a distribution $p$ ``satisfies the constraints given by a PDG $\sf M$'', we mean that for every edge from $X$ to $Y$ in $\sf M$, associated to the cpt $\mathbf e$, the table of conditional marginals $p(y \mid x)$ is equal to $\mathbf e$.

\subsection*{The Theorem}
We now start the process of showing that BNs can be captured by PDGs. 

\begin{defn}
	The \emph{\extrainfo}, $\H^{\sfM}$, given by a distribution $p$ that satisfies the constraints of a PDG $\sfM = \mnvars[]$ is given by
	\[ \H^{\sfM}(p) := \left(\sum_{L = (X,Y, \ell)\in \cal L} ~~\E_{x \sim p_X}  \H (\bp (x)) \right) - \H(p) \] 
	where $p_X$ is the marginal of $p$ on the variable $X$, and $\H(\bp(x))$ is the entropy of the distribution on $Y$ specified by the cpt $\bp$ when $X = x$. 
\end{defn}
% The cpts may not (in the entropy sense) 
We can think of the \extrainfo\ as the sum of the entropies that \emph{actually} result from each table, in the context of distribution $p$, minus the total entropy of the distribution.
Alternatively, we can think of the negation of the \extrainfo\, $-\H^\sfM(p)$ as the information in $p$, which has not already been specified by the cpts in $\sf M$.

To prove our theorem, we now present a helper lemma, which will do most of the work. For context, skip to its usage in the proof of Theorem~\ref{thm:bns-are-pdgs}.

\begin{lemma} \label{lem:bnmaxent-component}
	If $p$ is a probability distribution over a set of outcomes, and $X$, $Y$, $Z$ are random variables (or sets of random variables, by Definition~\ref{def:set-rv}), then 
	\[ \tilde H_p(X \mid Y; Z) := \E_{y \sim p_{_{Y}}} \Big[ \H_p(X \mid Y \!=\!y) \Big]  - \H_p( X \mid Y, Z)\]
	is (a) non-negative, and (b) equal to zero if and only if $X$ and $Z$ are indep. given $Y$.
\end{lemma}
\begin{proof}
	% We start by giving this quantity a name. Let's call it $\tilde H$.
	\begin{align*}
		\tilde H_p(X \mid Y; Z) &= \E_{y \sim p_{_{Y}}}  \Big[ \H_p(X \mid Y \!=\!y)\Big] - \H_p( X \mid Y, Z)  \\
		&=  \left[\sum_{y} p(y) \sum_x  p(x\mid y) \log \frac{1}{p(x \mid y)} \right]+ \left[\sum_{x,y, z} p(x, y, z) \log \frac{p(x,y,z)}{p(y, z)}\right] \\[0.5em]
		&= \left[\sum_{x,y} p(x,y) \log \frac{p(y)}{p(x,y)}
			% \cdot \left( {\color{red} \vphantom{\sum_{z}}\smash{\overbracket{\color{black} \sum_{z}~p(z \mid x, y)}^{=1}}}\right)
			\right] + {\color{inactive}\left[\sum_{x,y, z} p(x, y, z) \log \frac{p(x,y,z)}{p(y, z)} \right]} \\
		%(below is optional)
		% &= \left[\sum_{x,y, z} p(x,y) p(z \mid x, y) \log \frac{p(y)}{p(x,y)} \right] + {\color{inactive}\left[\sum_{x,y, z} p(x, y, z) \log \frac{p(x,y,z)}{p(y, z)} \right]} \\
		&= \left[\sum_{x,y, z} p(x,y ,z) \log \frac{p(y)}{p(x,y)}
			\right] + {\color{inactive}\left[\sum_{x,y, z} p(x, y, z) \log \frac{p(x,y,z)}{p(y, z)} \right]} \\
		&= \sum_{x,y, z} p(x,y ,z) \left[ \log \frac{p(y)}{p(x,y)} + \log \frac{p(x,y,z)}{p(y, z)} \right] \\
		&= \sum_{x,y, z}  p(x,y ,z) \log \left[\frac{p(y)\ p(x,y,z)}{p(x,y)\ p(y,z)} \right]  \\
	\end{align*}
	% \intertext{
	Define $q(x,z,y) := {p(x,y)\ p(y,z) }/{p(y)}$, wherever $p(y)\neq 0$, and $p(x,y,z) = 0$ otherwise. $q$ is in fact a distribution over the values of $X$, $Y$, and $Z$, since it 
	is clearly non-negative, and sums to 1, as we now show:
	\[
	 \sum_{x,y,z} q(x,y, z) = \sum_{x,y,z} \frac{p(x,y)\ p(y,z)}{p(y)}
		= \sum_{x,y,z} p(x \mid y) p(y,z)
		= \sum_{y,z} \left(\sum_x p(x \mid y)\right) p(y,z)
		= \sum_{y,z}  p(y,z)
		= 1
	\]	
		With this definition, we return to our computation of $\tilde H_p(X \mid Y; Z)$:
	% }
	\begin{align*}
		\tilde H_p(X \mid Y; Z) &= \sum_{x,y, z}  p(x,y ,z) \log \left[\frac{p(y)\ p(x,y,z)}{p(x,y)\ p(y,z)} \right]  \\ % this is a duplicate line, for readabilitz
		&= \sum_{x,y, z}  p(x,y ,z) \log \frac{p(x,y,z)}{q(x,y,z)}  \\
		&= \kldiv{p_{_{XYZ}}}{q}
	\end{align*}
	where $p_{_{XYZ}}$ is the marginal of $p$ on the settings of $XYZ$, and $\kldiv{p_{_{XYZ}}}{q}$ is the relative entropy from $p_{_{XYZ}}$ to $q$. By Gibbs' inequality (non-negativity of relative entropy), $\tilde H$ is  (1) non-negative, and (2) equal to zero if and only if $p_{_{XYZ}} = q$, meaning that 
	\[  p(x,y,z) =\begin{cases} \frac{p(x,y)\ p(y,z)}{p(y)} & \text{if }p(y) > 0\\ 0 & \text{otherwise} \end{cases} \qquad \implies \qquad p(x,y,z) p(y) = p(x,y) p(y, z) \] 
	and so $\tilde H_p(X \mid Y; Z)$ is (1) non-negative, and (2) equal to zero if and only if $X$ and $Z$ are independent given $Y$ according to $p$.
\end{proof}

\begin{theorem}\label{thm:bns-are-pdgs}
	If $\cal B$ is a Bayesian network, then there is a unique probability distribution $\mu^*$ consistent with the PDG $\Gamma(\cal B)$ with minimal \extrainfo\ $\H^{\Gamma(\mathcal B)}(p)$. Furthermore, $\mu^* = \Pr_{\cal B}$. That is to say, $\mu^*$ is the same distribution as the unique one that satisfies all of the conditional independences of the $\cal B$.	
\end{theorem}
\begin{proof}%[Proof of Theorem~\ref{thm:bns-are-pdgs}]
	% \label{proof:bns-are-pdgs}
	Choose an arbitrary distribution $p$ over the variables, subject to the sole constraint of being compatible with $\Gamma(\cal B)$ (which again means that each cpt in $\cal B$ must agree with the conditional marginals of $p$), and let $X_1, \ldots, X_n$ be any ordering of the variables in $\mathcal B$, such each node $X_i$ has parents $\Pa(X_i)$ with strictly smaller indices (we call such an ordering $\cal B$-topological). At least one $\cal B$-topological ordering is possible because the underlying graph of $\cal B$ is acyclic. 
	
	% We prepare to decompose $\H^{\Gamma(\cal B)}$ by recalling two facts. 
	We now put the following facts in equational form for ease of use:
	\begin{description}
	\item[Fact 1] (Entropy Chain Rule). using the chain rule for conditional entropy, we can write 
	\[ \H(p) = \sum_{i = 1}^n \H_p(X_i \mid X_1, \ldots X_{i-1}). \]
	%
	\item[Fact 2] (Only $\cal B$'s cpts have entropy).
	In $\cal B$, there is exactly one cpt for each node. Recall from Definition~\ref{def:gamma}, that $\Gamma(\cal B)$ contains all of these cpts, and possibly also degenerate cpts $\pi_{i,j}$, which, for a setting $(y_1, y_2, \ldots, y_m)$ of the new joint variable $\Pa(X_i)$, determines a (degenerate) distribution over a particular parent $Y_j \in \Pa(X_i)$, by putting all mass on $y_j$. This cpt is equal to $\delta_{y, y_j}$, and $\H(\delta_{y, y_j}) = 0$. Although the value of $y_j$ may change between settings of $\Pa(X_i)$, in each case it is a point mass, which always has no entropy. 
	Therefore, the projeections satisfy $H(\pi_{i,j}(y)) = 0$ for any value of $y \in \V(\Pa(X_i))$, and so the only cpts which could have non-zero expected entropy are the original ones from $\cal B$. As a result, we can write the sum of expected entropies in $\Gamma(\cal B)$ for all links can be expressed as
	\[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim p_Y}  \H (\bp ( y)) = \sum_{i=1}^n\E_{\vec y \sim p_{\Pa(X_i)}}  \H (\bp[(\Pa(X_i),X_i)] (\vec y))\]
	
	% since $\cal B$ is a BN, $\Gamma(\mathcal B)$ has $n$ cpts\footnote{exactly $n$ if no cpt is deterministic, otherwise at most $n$} whose target distributions (that is, the distribution that they give for $X_i$) could could have positive entropy, corresponding to the $n$ cpts describing the conditional probability of each variable given settings of its parents.% 
	%  	\footnote{Projections, of course, have zero entropy, and so this is true for both the hyper-graph and standard presentations of PDGs.}
	% Moreover, since $p$ is compatible with every cpt, $\bp[\Pa(X_i),X_i]$
	\item[Fact 3.] (Compatibility). Since $p$ is compatible with every cpt, $\bp[\Pa(X_i),X_i] = p(X_i \mid \Pa(X_i))$. Therefore, $\H_p(X_i \mid \Pa(X_i) = \vec y) $, which depends on only on the probability of $X_i$ given $\Pa(X_i)$, is equal to $\H(\bp[\Pa(X_i),X_i](\vec y))$. 
	\end{description}
	We can now calculate $\H^{\Gamma(\cal B)}$ directly.
		
	% Putting these two together, we expand the \extrainfo\ of $p$ with respect to $\Gamma(\cal B)$:
		
	\begin{align*}
		\H^{\Gamma(\mathcal B)}(p) &= \Bigg[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim p_Y}  \H (\bp (y)) \Bigg] - \H(p) \\
		&= {\color{inactive}\Bigg[\sum_{Y,X, \ell \in \cal L} ~~\E_{y \sim p_Y}  \H (\bp (y)) \Bigg]} - \sum_{i = 1}^n \H_p(X_i \mid X_1, \ldots X_{i-1}) & \text{Fact 1} \\
		&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim p_{\Pa(X_i)}} \H (\bp[\Pa(X_i), X_i] (\vec y)) \Bigg] {\color{inactive} - \sum_{i = 1}^n \H_p(X_i \mid X_1, \ldots X_{i-1})} & \text{Fact 2} \\
		&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim p_{\Pa(X_i)}}  \H_p (X_i \mid \Pa(X_i) \!=\! \vec y) \Bigg] 
		{\color{inactive} - \sum_{i = 1}^n \H_p(X_i \mid X_1, \ldots X_{i-1})} & \text{Fact 3} \\
		&= \sum_{i = 1}^n  \Bigg[ \E_{\vec y \sim p_{\Pa(X_i)}} \H_p (X_i \mid \Pa(X_i) \!=\! \vec y)  - \H_p(X_i \mid X_1, \ldots X_{i-1}) \Bigg]  \\
	\intertext{Applying the definition in Lemma~\ref{lem:bnmaxent-component},
	with $Y := \Pa(X_i)$,~$Z := \{X_1, \ldots, X_{i-1}\} \setminus \Pa(X_i)$, and $X := X_i$}
		&= \sum_{i = 1}^n  \Bigg[ \tilde H\Big(X_i ~\Big|~\Pa(X_i);~~\{X_1, \ldots, X_{i-1}\} \setminus \Pa(X_i)\Big) \Bigg]   \numberthis\label{eqn:maxentsum}
	\end{align*}%
		% \footnotetext{To do this, we need to think of sets of variables as variables themselves. Doing so is straightforward (the joint variable takes valeues which are tuples, with probabilities given by the joint distribution on the set of variables), but those that are worried can verify that nothing in the proof of the lemma changes by recognizing this explicitly and writing $x,y,z$ as vectors.}%
	Lemma~\ref{lem:bnmaxent-component} tells us that each individual term of the sum in \eqref{eqn:maxentsum} is non-negative, and equal to zero if and only if $X_i$ is independent of every previous (that is, $j < i$) non-parent variable $X_j$ for $j < i$, given its parents. 	
	Therefore $\H^{\Gamma(\mathcal B)}(p)$, is non-negative, and equal to zero if and only if \emph{every} variable is independent of all previous variables given its parents, according to $p$. 
	% As conditional independence is symmetric, we conclude that $\H^{\Gamma(\mathcal B)}(\mu) = 0$ iff $\mu$ causes every variable $X$ to be independent of any other $Z$ given $\Pa(X), \Pa(Y)$, which happens iff each varaible is independent of its non-descendants given its parents.
	% Here are two alternate ways of using this to conclude that if $\H^{\Gamma(\mathcal B)}(p) = 0$, then $p = \Pr_{\cal B}$.
	
	\textbf{Extending these independences to all variables.}
	% We claim that the following are equivalent:
	% \begin{enumerate}[label=(\alph*)]
	% 	\item $\H^{\Gamma(\cal B)} = 0$ \label{item:noextrainfo}
	% 	\item $X_i \CI X_j \mid \Pa(X_i)$  if $j  < i$ for some $\cal B$-topological ordering of the variables.\label{item:someorder}
	% 	\item $X_i \CI X_j \mid \Pa(X_i)$  if $j  < i$ for every $\cal B$-topological ordering of the variables.\label{item:allorders}
	% \end{enumerate}
	% We have just shown the equivalence of (\ref{item:noextrainfo}) and (\ref{item:someorder}). Now suppose 
	
	% The equivalence of \ref{item:noextrainfo} and \ref{item:someorder}
	%   easily follows, since if there were some topological sort for which the independence didn't hold, then your proof shows that $\H^{\Gamma(\cal B)}(p) \ne 0$.
	% 
	We have shown that, for any topological ordering on the variables of $\cal B$, $\H^{\Gamma(\cal B)}(p) = 0$ if and only if, according to $p$,  each $X_i \CI X_j \mid \Pa(X_i)$ for $j  < i$; we will refer to this as $(\star)$.
	
	Now, suppose $X_j$ were a non-descendent of $X_i$, with $j > i$. Because $X_j$ is not a descendent of $X_i$, we can construct a second toplogoical sort of the variables in $\cal B$, in which $\#(X_j) < \#(X_i)$, where $\#(X)$ is the index of $X$ in the new ordering. 
	We can obtain $\#$, for instance, by topologically sorting $X_j$ and its ancestors, and then adding the rest of the variables (which we call $\bf R$) in their original order. The concatination of these two is a valid topological sort because the ancestors of $X_j$ are topologicaly ordered, and the parents of each $X \in \bf R$ occur no later than before.
	
	
	With this new order, suppose that $\H^{\Gamma(\cal B)}(p) = 0$. By $(\star)$, since $\#(X_j) < \#(X_i)$, we know that $X_i \CI X_j \mid \Pa(X_i)$ according to $p$. Since this is true for an aribitrary $i$ and $j$ without changing the distribution $p$, we conclude that if $\H^{\Gamma(\cal B)}(p) = 0$, then $p$ makes \emph{every} variable $X_i$ independent of its non-descendents $X_j$, given its parents.
	Conversely, if every variable is independent of its non-descendents given its parents, then $p$ is the unique distribution determined by $\cal B$, and since each variable of $\cal B$ is independent of previous variables given the values of its parents,  we know by $(\star)$ that $\H^{\Gamma(\cal B)}(p) = 0$. Therefore, if $X_j$ is a non-descendent of $X_i$, 
	\[ \H^{\Gamma(\cal B)}(p) = 0 \qquad\iff\qquad X_i \CI X_j \mid \Pa(X_i) \] 
	% Conversely, if $\H^{\Gamma(\cal B)}(p) \neq 0$, then by $\star$ it cannot be the case that in some order, every variable is independent of all previous variables given its parents, and so in every order, some variable is not independent of all previous variables given its parents.  
	
	Because $\Pr_{\cal B}$ is the unique distribution that satisfies these independences, we conclude that $\H^{\Gamma(\cal B)}(p) = 0$ if and only if $p = \Pr_{\cal B}$. 	
	As $\H^{\Gamma(\cal B)}(p)$ is non-negative, $\Pr_{\cal B}$ is its unique minimizer. 


	% \textbf{v2. Uniqueness by strong convexity.}
	% Part (a) of Lemma~\ref{lem:bnmaxent-component} tells us that $\H^{\Gamma(\mathcal B)}$ is a sum of strongly convex functions, and hence strongly convex itself. Because the set of distributions that are compatible with $\Gamma(\cal B)$ is convex (Lemma~\ref{lem:convex}), $\H^{\Gamma(\mathcal B)}$ has a unique minimum $\mu^*$ on this set. At the same time, the distribution $\Pr_{\cal B}$ described by $\cal B$ satisfies the independences from Lemma~\ref{lem:bnmaxent-component}, so we must have $\H^{\Gamma(\mathcal B)}(\Pr_{\cal B}) = 0$, and since $\H^{\Gamma(\cal B)} \geq 0$ and  has a unique minimizer, $\Pr_{\cal B} = \mu^*$.
\end{proof}

\clearpage

\begin{lemma}
	\label{lem:convex}
	$\bbr{\sfM}\SD$ is convex, for any PDG  $\sfM$.
\end{lemma}%
\begin{proof}
	Choose any two distributions $p, q \in \bbr{M}\SD$ consistent with $M$, any mixture coefficient $\alpha \in [0,1]$, and any edge $E \in \Ed$. Let $\mat e$ be the cpt associated to $E$, regarded as a matrix. 
	
	By the definition of $\bbr{M}\SD$, we have $p(B = b \mid A = a) = q(B = b \mid A = a) = \mat e_{a,b}$.
	Once again, for brevity we use lower case letters ($a$) to denote events ($A = a$).
	Therefore, $p(a\land b) = (\mat e_{a,b})  p(a)$ and $q(ab) = (\mat e_{a,b})  q(a)$. Some algebra reveals:
	\begin{align*}
		\Big( \alpha p + (1-\alpha) q \Big) (B = b \mid A = a) &= 
		\frac{\Big( \alpha p + (1-\alpha) q \Big) (b \land a)}{\Big( \alpha p + (1-\alpha) q \Big) (a)} \\
		&= \frac{ \alpha p(b \land a) + (1-\alpha) q(b \land a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
		&= \frac{ \alpha\ (\mat e_{a,b})\  p(a) + (1-\alpha)\  (\mat e_{a,b})\  q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
		&=\mat e_{a,b} \left(\frac{ \alpha  p(a) + (1-\alpha) q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)}\right)\\
		&= \mat e_{a,b}
	\end{align*}
	and so the mixture $\Big(\alpha p + (1-\alpha) q \Big)$ is also contained in $\bbr{M}\SD$.
\end{proof}

\end{document}
