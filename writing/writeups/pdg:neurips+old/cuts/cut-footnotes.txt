------------------

Empirically, people remember old belief states, and sometimes recover the old ones rather than their updated ones \todo{find psych literature. All the time I find myself realizing that I already figured out or observed things, but the state didn't stick. It is also true that people forget that they ever held old positions, which is still expected after the resolution occurs.}

----------------

%			\item Suppose that floomps are indeed an exact synonym for guns. Although the initial, no-edge Bayes Net represents a valid probability distribution, it is one that assigns mass to worlds that are logically impossible. Upon validating the fact that they are two names for the same .
%				\label{item:impossible-world}
%	
%			% TODO why is this bad?
%			% it's not bad because subjectively these are possible. It's not logically impossible for floomps and guns a priori to be different things.
%			% In the context of a system with lots of runs, ....  still no problems. W_{w,i} can be arbitrary. 
%			% alternate example: 

--------------------

\footnote{Inability to represent \emph{under-constrained} belief states is common and addressed by many exiting representations of uncertainty, including sets of probability distributions, Dempster-Shafer belief functions, inner/outer measures, and lower probabilities.}


---------------------


	\footnote{%
		One might argue that you need to resolve the inconsistency in case you need to make decisions before you can discover the truth---that it is appropriate to soften all three beliefs as appropriate rather than leave the inconsistency. This is a reasonable point of view, and still consistent with using a \MN---but one should note that this is an eager rather than lazy approach, and will in general be more expensive.}%
	\footnote{
		One reason to be wary of resolving the inconsistency is that the process is not invertible: from a probabilistic perspective, resolution of the inconsistency is a projection (read: non-injective) into a feasible subspace. Thus this eager resolution loses information, which might be useful in the future.}
		
		
---------------
	\footnote{
		Models like these are well used to hardcore probabilists as well. In statistical learning theory, an `incomplete' model like this is a discriminative (or conditional) model, as compared to a generative one.
	}
----------------
	\footnote{
		The definition $\bmu$ is slightly over-simplified if not everything is measurable. More generally if $\mathcal V(A) = (X, \mathcal A)$ and $\mathcal V(B) = (Y, \mathcal B)$, then for any link $L \in \mathcal L$, we're really referring to a function $\bmu_L : X \times \mathcal B \to [0,1]$ such that $\bmu_L(x,-): \mathcal B \to [0,1]$ is a probability distribution and $\bmu_L(-, S)^{-1}(R) \in \mathcal A$ for any $S \in \mathcal B$, and measurable subset $R \subseteq [0,1]$, technically making $\bmu_{A,B}$ a \textit{Markov Kernel} from $A$ to $B$.
	}