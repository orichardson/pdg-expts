what they're good for
 > track info with completely different
 > capture lots of distributions
 > modularity
 Can model inconsistency and recovery.
---

Things left unmentioned in the paper that deserve recognition
	- Probabilized Commutative Diagrams
	- Causality, \alpha parameters, 
----


PDGs are a powerful tool for representing local probabilistic information. 
Though represented by a graph, the edges are interpreted differently. Each edge alone determines a cpd, making PDGs formally analogous to a commutative diagram, rather than a flow-chart. The familiar picture of a directed graphical model can be recovered with the use of multi-tailed arrows.

PDGs have a parameterized semantics $[[ - ]]_\gamma$, which always generalizes Bayesian Networks, and precisely becomes a factor graph when $\gamma=1$.  This exposes an implicit trade-off between quantitative and qualitative data; the two behave very differently, but are unfortunately fused in a factor graph. Both qualitative and quantitative information can be inconsistent, although the former is less straightforward, and so in this preliminary paper we focus on the quantitative limit. 

Incorporating new variables, data, or restricting to subgraphs of a PDG is simple, making it possible to construct one by simply throwing together some pre-trained statistical models. Moreover, in the quantitative limit, PDGs continue to have local meaning, in stark contrast with energy-based models such as factor graphs. As a result, PDGs are not only a flexible representation, but modular as well. 

The most dramatic feature of PDGs is their ability to deal with inconsistency. A PDG can track conflicting information from different sources, and its semantics identify when this occurs, rather than quietly sweeping problems under the rug.
As a result, a user may resolve inconsistency in multiple ways. Inconsistency can be dealt with by updating one or multiple tables, by introducing introducing or splitting variables, or even left unresolved as one looks for clarifying information. The pain of inconsistency can be mitigated by expressing a decreased confidence, without altering any data. 


The formal details of these procedures are still.
% For instance, an inconsistency in $1 -> X -> Y <-1$ may f
