IN EMAIL.

To summarize: relabel the "degree of causality", as described in examples and descriptions, as a different letter (c), whose sum bears no obvious relation to 1 as one expects intuitively. However, because the quantity it acts on obeys the inclusion-exclusion rule, it is like a belief function. We define the \alphas, (which we need to sum to 1 to make the semantics work), to be the corresponding mass function.   

[inside my definition]
... Rather than a distribution, we first define a belief function c, over S_Y, where c_{L_i} is the belief that edge L_i can identify Y from X_i regardless of the other variables, as per the information theoretic intuition. We set \alpha to be its corresponding mass function, which is a distribution on 2^{S_Y}; we add edges  We restrict c so that there is no mass on subsets of 

Suppose that we put \alpha = a on the edge X -> Y and \alpha=(1-a) on the edge X,Z -> Y.  No matter what, \mu2 will score at least as well as \mu1; for a=0, they are indistinguishable, and as a increases, \mu1 and \mu2 will both score worse, but \mu1 becomes worse faster, and gap between them widens in proportion to \alpha. 