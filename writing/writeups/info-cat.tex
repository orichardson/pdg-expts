\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{nicefrac}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\def\nf{\nicefrac}
\usepackage[margin=1in]{geometry}
\usepackage[backend=biber,
	style=authoryear,%alphabetic
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}



\input{../papers/model-commands.tex}

\definecolor{inactive}{gray}{0.1}

\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}
\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \bmu#1)}
\newcommand\SD{_{\text{sd}}}

\def\extrainfo{extra information}

\begin{document}

    \section{Information Theoretic Quantities}
    
    \subsection{Relative Entropy}
    
    \subsection{Information Content}
    
    \subsection{Conditional Entropies}
    \begin{center}
        \begin{tikzpicture}
            \node[dpadded](1) at (0,0) {$\sf 1$} ;
            \node[dpadded](X) at (2,0) {$X$} ;
            \node[dpadded](Y) at (4,0) {$Y$} ;
            
            \draw[arr] (1) --node[above]{$\mu$} (X);
            \draw[arr] (X) --node[above]{$p$} (Y);
        \end{tikzpicture}
    \end{center}
    
    \[ \H(Y \mid X) := \sum_{x,y}  \E_{x \sim \mu} \]
    
    
    
    \section{Facts to Explain}
    \begin{enumerate}
        \item $I(X ; Y) = \kldiv{p_{XY}}{p_X p_Y}$
        \item $I(X ; Y) = \E_Y [\kldiv{p_{X|Y}}{p_X}] $ 
        \item $\H(X \mid Y) = H(X) - I(X;Y)$
        \item $\H(X) = \E[ I_X(x) ]$
        \item $\H(X) = \log(N) - \kldiv{p_X}{\mathit{Unif}_X}$
    \end{enumerate}
    
    \section{Conjectures}
    
    \begin{conj}
        If $\sf M$ is consistent, and $p, q, r$ are paths from $X$ to $Y$, then 
        \[ \kldiv{p}{q} + \kldiv{q}{r} \leq \kldiv{p}{r} \]
        satisfies the triangle inequality.
    \end{conj}
    
\end{document}
