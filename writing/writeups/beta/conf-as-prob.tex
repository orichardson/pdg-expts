
\documentclass{article}

\input{pdg-preamble-v1}
% \usepackage{parskip}

\setlength\parskip{1ex}

\title{A Probabilistic Account of Confidence}

\begin{document}
\maketitle

Let's consider two variants of PDGs. The first, which we'll call an opdg, is just what we had before. 
Concretely, an opdg $\dg M = (\mat p, \bbeta)$ is collection of conditional probability distributions (cpds) $\mat p = \{p\ssub L \}_{L \in \Ed}$, with associated weights $\bbeta = \{\beta\ssub L \in \mathbb R \cup \{\infty\}\}_{L \in \Ed}$, typically positive, where $\beta\ssub L=0$ means the associated cpd $p\ssub L(Y|X)$ is totally ignored (and in particular is semantically equivalent to one in which $p\ssub L(Y|X)$ were replaced with a different cpd $p'(Y|X)$). 

While the scale makes sense at the extremes and is mathematiaclly well-motivated, the numerical value of $\beta \in [0,\infty]$ is in some ways hard to make sense of. 
How should we set $\beta$?  When, for instance, should we assign $\beta = 3$?

To make sense of these questions, we will contrast opdgs with a variant, which we refer to as a jpdg. A jpdg $\mathfrak M = (\mat p, \mat c)$ is again a collection of weighted cpds, but instead the weights $c \ssub L$ for each cpd $p\ssub L$ lie in the interval $[0,1]$, and we interpret them as the ``(independent) probability that the given edge is part of the PDG''.

In both cases, we can make sense of the ``set of joint distributions compatible with a pdg'' in the same way, as a function of $\mat p$ alone: 
\[
    \SD{\mat p, \bbeta} = \SD{\mat p, \mat c} = \SD{\mat p} 
        := \Big\{ \mu \in \Delta\! \V : \mu(X,Y) = \mu(X)p\ssub L(Y|X) \text{ for all } 
            \ed LXY  \in \Ed \Big\}.
\]


% So, while the scoring semantics of an opdg 
However, we don't imagine all cpds are equal. 
In the case of opdgs, recall that we remedy this by introducing a scoring function semantics $\bbr{-} : \Delta\!\V \to \overline{\mathbb R}$, which makes compatibility with matter of degree. 
Rather than simply being consistent or not, $\bbr{\dg M}(\mu)$ is a measure of \emph{how incompatible} $\dg M$ is with $\mu$.
 % The formula is given by

A jpdg represents a slightly different continuous generalization. 
Rather than simply be consistent with a PDG or not, $\Pr_{\mathfrak M}\mathrm{Comp}: \Delta\!\V \to [0,1]$ is a measure of \emph{how likely} the distribution $\mu$ is to be compatible with $\dg M$.

% To contrast the two explicitly, the generalization of $\SD{-}$ offered by an opdg answers the question 



\section{Theory of JPDGs}
Given a jpdg $\mathfrak M$, the probabilistic confidences $\mat c$ generate a probability distribution over unweighted sub-PDGs $\dg N$, by
\[ 
    \Pr_{\mathfrak M}( \dg N )  = \prod_{L \in \Ed}\Big( c\ssub L ~\text{ if }~ L \in \mat p  \text{ else }1 - c\ssub L \Big)
\]



We claim that
\[
    \Ex_{\dg N \sim \Pr_{\mathfrak M}}
\]



\begin{prop}
    
\end{prop}

\section{}

\[
    \begin{tikzcd}
        (\overline{\mathbb R})^{\Delta\!\V}
            & (\Delta \overline{\mathbb R})^{\Delta \!\V}
        \\  \{0,1\}^{\Delta\!\V}
            &\ [0,1]^{\Delta\!\V}
    \end{tikzcd}
\]

Of course, we have the natural inclusions 
\begin{align*}
    \iota &: \{0, 1\} \to [0,1] \\
\end{align*}


\end{document}
