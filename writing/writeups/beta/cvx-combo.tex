\documentclass{article}

\input{pdg-preamble-v1}
% \usepackage{parskip}

\title{Convex Combinations of PDGs}

\begin{document}
\maketitle

Joe suggested the following `convex combination' operation, and the following inductive extension of the class of PDGs.

\begin{defn}[extended syntax]
    Given PDGs $\dg M_1$ and $\dg M_2$, and an interpolation parameter $\alpha \in [0,1]$, we can introduce a new ``composite'' PDG that we denote $\dg M_1 {:} \alpha {:}\, \dg M_2$, which represents a convex combination of the two PDGs.
\end{defn}

Intuitively, we imagine that $\dg M_1 {:} \alpha {:}\, \dg M_2$ represents a probabilistic choice between two PDGs, in which one gets $\dg M_1$ with probability $1-\alpha$, and $\dg M_2$ with probability $\alpha$. 
Now we provide the semantics of these extended composite PDGs. 

\begin{defn}[set-of-distributions semantics]
    We say a joint distribution $\mu$ is in the set
    % \[
    $
        \SD[\big]{\dg M_1 {:} \alpha {:}\, \dg M_2}
    $
    of distributions consistent with the composite PDG 
    $\dg M_1 {:} \alpha {:}\, \dg M_2$
    iff 
    it can be written as $\mu = (1-\alpha)\mu_1 + \alpha \mu_2$ such that
    $\mu_1 \in \SD{\dg M_1}$ and
    $\mu_2 \in \SD{\dg M_2}$.
\end{defn}

This has the following nice properties:
\begin{enumerate}
    \item At the extremes, it reduces as expected to the set-of-distribution semantics of its end points. That is to say, 
    \[
        \SD[\big]{\dg M_1 {:} 0 {:}\, \dg M_2} = \SD[\big]{\dg M_1}
            \qquad\text{and}\qquad
        \SD[\big]{\dg M_1 {:} 1 {:}\, \dg M_2} = \SD[\big]{\dg M_2}.
    \]
    
    \item The boundary of the set $\SD[\big]{\dg M_1 {:} \alpha {:}\, \dg M_2}$ varies continuously with $\alpha$. 
\end{enumerate}


\begin{defn}[scoring-function semantics]
    Then we can define
    \[
        \bbr[\big]{\dg M_1 {:} \alpha {:}\, \dg M_2}_\gamma (\mu)
            := 
            \inf_{\substack{\mu_1, \mu_2 \text{ s.t.} \\ 
                (1-\alpha) \mu_1 +\alpha \mu_2 = \mu}}
            \bbr{\dg M_1}_\gamma(\mu_1) + \bbr{\dg M_2}_\gamma(\mu_2) 
    \]
\end{defn}


\begin{prop}
    If there are distributions consistent with $\dg M_1 {:} \alpha {:}\, \dg M_2$, (that is, $\SD{\dg M_1 {:} \alpha {:}\, \dg M_2} \ne \emptyset$), then they are the minimizers of the scoring function semantics. That is,
    % $\bbr{-}_0$. 
    \[
        \bbr[\big]{\dg M_1 {:} \alpha {:}\, \dg M_2}_0^*
            = \SD[\big]{\dg M_1 {:} \alpha {:}\, \dg M_2}
        \qquad\text{if the latter is non-empty}.
    \]
\end{prop}
\begin{proof}
    (see attached notes.)
\end{proof}    

By contrast, the convex combination of the scoring functions $\bbr{\dg M_1}_0 = \Inc_{\dg M_1}$ and $\bbr{\dg M_2}_0 = \Inc_{\dg M_2}$, corresponds to simply taking a convex combination of the confidence vectors $\bbeta^{\dg M_1}$ and $\bbeta^{\dg M_2}$.

This does not enjoy the property above, since a distribution minimizes that convex combination iff it is in the intersection of the sets of distributions that minimize both. 

\bigskip

Now, what can we do with this extended set of PDGs? 


\end{document}
