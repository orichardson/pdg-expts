\documentclass{article}

\usepackage{fullpage}
\usepackage{amsmath, amssymb}

%\usepackage[backend=biber]{biblatex}
%\addbibresource{refs.bib}



\begin{document}
	\section{Motivation}

	
	
	\section{Formalization}
%	At every time $t$ there is a distribution $D_t$ over inputs, which is impacted by the actions, continuously. So actions live in a space $\cal A$. Each action $A : \cal A$
	
	\begin{tabular}{cl}
		$\cal A$ & set of agents \\ 
		$\mathcal I_{a}$ & space of inputs for agent $a \in \cal A$\\
		$\mathsf{ACT}^a_{X}$ & set of actions for agent $a$ given input $X \in \cal I$ \\
			% worry: does the agent have to compute what actions are available? Is this handed for free? Is it like a video game?
	\end{tabular}


	
	% Goal: abstraction to be able to talk about low-level actions being combined for high-level ones.
	% Question: memory: should it be wiped, or implemented by the system itself?
	
	a
	If $\cal W$ is the class of all possible worlds, then 
	
	
	\section{Rewards}
	Many ways of modeling. Explicit lines up with reinforcement learning literature better, and provides perhaps a good way of asking certain kinds of questions, and explicitly emulating other problems by providing: loss functions, etc.  Makes it easier to evaluate ``optimal''
	
	However, implicit rewards are also attractive if possible: we could then 
	
	
	\section{Questions}
	What are the appropriate coherence axioms for agents to be well-defined? Can they be also parsed purely from information theoretic terms? Separability = agents? Want many compatible definitions.
	
	Need to motivate the ``death'', and give a satisfying way of putting together the fact that maybe ``you'' die all the time but as long as your impact on the future is strong enough and of a certain kind, you haven't actually died.
		- To do this, memory must be extremely important; unfortunately it is tied to agents and so the types of do not easily allow for memories to be looked at directly.

	
	\subsection{Examples}
	
	\begin{itemize}
		\setlength\itemsep{0pt}
		\item Staying in center of line, given random noise (specific case of standard RL problem)
		\item Supervised classification problem (specific case of standard RL)
		\item How much time thinking before doing big actions? How does it depend on what you know about the world?
	\end{itemize}

	\section{Hopes}
		\begin{itemize}
		\setlength\itemsep{0pt}
		\item A theory of how reward structures can be arise, and how it relates to IRL.
		\item A general framework to talk about how much computational power you give something.
		\item A way of connecting it with interpreted systems in Joe's land
		\item A general framework in which to talk about power and influence
		\item A way of making sense of scaled abstractions, and thereby fitting PL research into RL context.
		\item Different ways of gating computational power result in different optimal solutions
		\item A satisfying symmetry between simulated agents
		\item A mechanism for an agent from escaping from a box
		\item Explanations of organizational systems: acreting other materials to be a super-agent.
	\end{itemize}

	\section{TODO}
	\begin{itemize}
		\item death
		\item Describe as system wrt epistemic logic
		\item Macro / Micro time + space
		\item Describe ACT as a dependent type
		\item Reductions to various levels of idealization ($\infty$-computation, $\infty$-knowledge about world, $\infty$-self knowledge, knowing nature perfectly, etc.)
		\item Investigations of sub-programs and misaligned objectives
		\item Reward functions as agents that are trained together
		\item Relations to MAML, Active Learning
		\item How optimal things behave w.r.t. information collection vs action
		\item Description as causal models
		\item Power
		\item More Examples.
	\end{itemize}



		- Extensions of memory via extenral devices: how to combine and abstract agentive properties:
	
		- organizational systems: motion from planning systems to logs, and datastructures that allow efficient searches.
		- Related to gamification, (then value capture).
		
		
		- IRL and clarifying preferences for new things in the prsence of new language
		- Adversarial relationship with reward function. Potential thm: requirement for some notion of stability.
		-
		- Directed samples: how much energy should be spent on desigin experiments vs thought? How does it depend on costs?
		- Active Learning
		- How to know how informative a sample would be
		- How much would you be willing to pay for the sample over a random one
		- Given that fitting has a cost in addition to
		- Optimal Design [Statistics Literature]
		- Explain human cognitive biases as optimal solutions to certain tasks
		- Optimality across each subset of tasks
		- Want information theoretical account of how the particular common knowledge about the task determines the optimal strategy.  For example:
		
		For a very specific optimization problem over one variable, there will be one optimum. We as humans can find that.
		
		However, for a larger class of problems, we may not know what the specific problem is, and so we need to find the optimal way of finding an algorithm that will find the optimum.
		
		Use $(- \times A) \dashv (-^A)$ adjunction to reduce levels

	
%	\bibliography{refs.bib}
%	\printbibliography
	\bibliographystyle{plain}
%	\bibliography{refs.bib}
\end{document}