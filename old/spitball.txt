Hypothesis classes
	- Networks
	- Probabalistic Graphical Models
	- Finite Automata
	- Causal Models
	- (posterior distributions: unbounded agents)

Costs:
	- per unit computation
	- for samples
	- [directed sampling cost alternatives]
		- to get a sample with a given cost
		- to change the sampling distribution
		- going back may be free OR have $k$ different slots for samples.

Utilities:
	- Accuracy
	- Lack of Cost


Reward Models:
	MODELING ALONE, NO OPTIMIZATION:
		- with probability Ïµ, the game ends at this round, and recieve reward for accuracy
		- More generally: testing happens at round t with probability drawn from a known prior distribution.
	BOTH OPTIMIZATION AND MODELING:
		- At some point, have to make a decision
		- Have to make decisions every round.
		- [Multi-armed bandit]? Have to


But the problem is that the maximization problem itself has costs. SO the question is: in a meta-rational sort of way, how do you prevent yourself from wasting too much time on decisions, and also from spending too little time on them? What's the ideal way of allocating shared resources to observing, to thinking, and to exploring? How does this differ based on what your actual preferences look like?
	For example, do pragmatic and theoretical reasoners have different behaviors under this model? Things we should see: if you don't care about something as much, spend less time making good decisions aboMulti-armed banditut it. If you don't know very much, spend more time exploring than thinking. If you care about truth alone, spend

Framed as a reinforcement learning problem:
Actions: observe sample, change distribution, step of computation.
	Each action has a cost.
	There's also a second set of utilities and actions (E, A)



Additional things we've talked about:
	A modal logic for thinking about what ways in which two variables could differ
	A causal model in which you have to distinguish variables. Nailing down levels of abstraction.


GIVEN: reinforcment learning problem, with an environment, agent, rewards, actions.
Augment to a bounded reinforcement learning problem with additional costs for computation, observation, and directed sampling.

Doing nothing should give you

Bounded Scientist:
	Slightly less time sensitive, focused only on predictive power and alignment of optima.
	Decisions are slower in nature, and have lots of data (observation is cheap)
	BUT experimental design is expensive (and necessary).
	How much time should be spent observing / analyzing current data, vs using the model to discern the difference between two other settings?

	If perfect reasoner, still a question: when do you ask for samples? This is related to active learning. Differences:
		Active Learning: We choose what samples to ask for labels on, in each iteration
		This case: in the limit, what's the amount of samples we would like to request a stream from a distribution?

	Alternative scoring for function: not how good expected value is, but rather how bad the worst possible error is.

	If imperfect reasoner: split time between fitting models, and experimental design / counterfactual reasoning.



Problem with the cost per computation model: the result is obviously going to be that there's an optimal time to stop computation, and there's no clear result showing that the time at which you stop is stable with regard to any of these othter factors. Reserach like this makes an assumption which is obvious, and gets a result that obviously falls out mathematically, and the interprets it in a new way to provide support for a proposition which we already sort of believed but wanted new credence for.

Instead, what we should do is to look for how simply by the very nature of being bounded, such a cost arises naturally --- for instance, if the environment is structured somehow similar to ours, and we constantly get both an action and a computation, how do we weave them together to get maximum reward? We have a lot of actions we could take without thinking, which could get huge reward or risk.

Related: what if we're risk averse?




OPTIONS FOR SETUP
	1. THOUGHT AND ACTIONS ARE PARALLEL
		At each step, choose one computation and one action. Actions have predefined rewards, and computations

		Questions: how often do we choose no-op before acting (or alternatively, how many computations per move before we )
