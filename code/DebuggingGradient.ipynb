{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pdg import PDG\n",
    "from dist import RawJointDist as RJD\n",
    "from lib import A,B,C,D\n",
    "from lib.square import with_indeps, consist_with_P, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ_opt = with_indeps.optimize_score(gamma=0.001, tol=1E-30)\n",
    "φ = consist_with_P.factor_product(repr='atomic')\n",
    "μCP_opt1, μCP_iter1 = consist_with_P.optimize_score(gamma=1, store_iters=True)\n",
    "μCP_opt0, μCP_iter0 = consist_with_P.optimize_score(gamma=0, store_iters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "μCP_GS_ord, μCP_iterGSo = consist_with_P.iter_GS_ordered(store_iters=True)\n",
    "μCP_GS_β, μCP_iterGSβ = consist_with_P.iter_GS_beta(store_iters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be the case that any unweighted PDG $M$ will have the same distribution $q_{\\gamma=0}$ that minimizes its semantics $ [[ M ]]$ at $\\gamma=1$, which is equal to its factor product $\\Pr_{\\varphi_{M}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What GIbbs sampling do to $\\mathit{Inc}$ and $\\mathit{IDef}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... let's set up a PCA to visualize what's happening. with both optimizations, $P$, and $\\varphi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b57c47dc21f41059842ca68d9ae81d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SHAPE = φ.data.reshape(-1).shape\n",
    "\n",
    "Ppt = np.array([P.data.reshape(*SHAPE)])\n",
    "φpt = np.array([φ.data.reshape(*SHAPE)])\n",
    "idx0pt = np.linspace(0, 1, len(μCP_iter0))\n",
    "idx1pt = np.linspace(0, 1, len(μCP_iter1))\n",
    "\n",
    "iter0pt = np.array(μCP_iter0)\n",
    "iter1pt = np.array(μCP_iter1)\n",
    "bigmatrix = np.vstack([Ppt, φpt, iter0pt, iter1pt])\n",
    "\n",
    "def iitransform(M, μflatmat):\n",
    "    return [(M.Inc(μ).real,M.IDef(μ).real) for μ in map(lambda d : RJD(d.reshape(*M.dshape), M.varlist), μflatmat)]\n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib widget\n",
    "\n",
    "# Do for γ=0 (P)\n",
    "plt.plot(*zip(*iitransform(consist_with_P, iter0pt[0:])), 'b', alpha=0.1)\n",
    "plt.scatter(*zip(*iitransform(consist_with_P, iter0pt)), cmap='Purples', c=idx0pt, alpha=0.3)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [Ppt])); plt.plot(Xs,Ys, '*w', markersize=15,alpha=0.8)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [μCP_opt0.data])); plt.plot(Xs,Ys, 'xb', markersize=15,alpha=0.8)\n",
    "plt.annotate\n",
    "\n",
    "# Do for γ=1 (φ)\n",
    "plt.plot(*zip(*iitransform(consist_with_P, iter1pt[0:])), 'r', alpha=0.1)\n",
    "plt.scatter(*zip(*iitransform(consist_with_P, iter1pt)), cmap='Oranges', c=idx0pt, alpha=0.3)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [φpt])); plt.plot(Xs,Ys, 'or', markersize=15,alpha=0.8)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [μCP_opt1.data])); plt.plot(Xs,Ys, 'xr', markersize=15,alpha=0.8)\n",
    "\n",
    "#now for GS\n",
    "plt.plot(*zip(*iitransform(consist_with_P, μCP_iterGSo)), 'g', alpha=0.1)\n",
    "plt.scatter(*zip(*iitransform(consist_with_P, μCP_iterGSo)), cmap='Greens', c=np.linspace(0,1,len(μCP_iterGSo)), alpha=0.3)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [μCP_GS_ord.data])); plt.plot(Xs,Ys, '+g', markersize=15,alpha=0.8)\n",
    "\n",
    "plt.plot(*zip(*iitransform(consist_with_P, μCP_iterGSβ)), 'k', alpha=0.1)\n",
    "plt.scatter(*zip(*iitransform(consist_with_P, μCP_iterGSβ)), cmap='Greys', c=np.linspace(0,1,len(μCP_iterGSβ)), alpha=0.3)\n",
    "Xs,Ys = zip(*iitransform(consist_with_P, [μCP_GS_β.data])); plt.plot(Xs,Ys, 'xk', markersize=15,alpha=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirld, that the optimziation does not quite get either the oange dot (the distribution $P$ which is supposed to be the # of factors), and also that the blue one does not minimize Inc. I need to make sure the gradient is correct.... and then implement accelerated gradient descent properly?\n",
    "\n",
    "Questions:\n",
    " 1. Why is the high density of orange points not where the final red X lies?\n",
    " 3. Why is the factor product (orange circle) so far from the red X?\n",
    " 2. Why is the blue X so high in Inc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.9658301999899563)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = consist_with_P\n",
    "mscore = M._build_fast_scorer(gamma=0)\n",
    "\n",
    "mscore(μCP_GS_β.data)[0], mscore(μCP_opt0.data)[0] # uh-oh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the optimization isn't working properly. But this should be a convex function... let's look at the supposed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.11677672154621e-24, 7983.54056253065)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mscore(μCP_GS_β.data)[1]**2).sum(), (mscore(μCP_opt0.data)[1]**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the gradient of the optimal is much larger... does it point towards the optimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score @ x: 1.966;\t score @ x + ϵ·grad: 9.204;\t  score @x + ϵ·v: 1.954\n"
     ]
    }
   ],
   "source": [
    "xopt = μCP_GS_β.data.reshape(-1)\n",
    "x = μCP_opt0.data.reshape(-1)\n",
    "gradx = mscore(x)[1]\n",
    "\n",
    "vec = (xopt - x)\n",
    "vec /= np.sqrt((vec*vec).sum())\n",
    "ϵ = 0.001\n",
    "\n",
    "# np.allclose(vec.reshape(-1).dot(vec.reshape(-1)), (vec*vec).sum().reshape(-1))  #True\n",
    "print(\"score @ x: %.3f;\\t score @ x + ϵ·grad: %.3f;\\t  score @x + ϵ·v: %.3f\"%\n",
    "      (mscore(x)[0], mscore(x + gradx*ϵ)[0], mscore(x + vec * ϵ)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the gradient is wrong. Some possible reasons this could be the case:\n",
    " - I computed the gradient wrong in the math (unlikely because I was careful and I keep revisiting this)\n",
    " - The penalty is messing things up.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
