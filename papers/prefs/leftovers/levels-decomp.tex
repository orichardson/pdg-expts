\documentclass{article}

%\usepackage{fullpage}
%\usepackage{amsmath,amssymb}

\input{prefs-commands.tex}

%\addbibresource{../refs.bib}
%\addbibresource{../maths.bib}

\title{levels}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}


\begin{document}
%	\chapter{Motivation}
%	\chapter{Formalism}
%	This representation is your mental model of the world: the sets represent the possibilities you are aware of; the categorical structure on the sets represent 

	\section{Motivating a Cognitive Hierarchy for Preference Change}
	Recall our goals:
	\begin{enumerate}[nosep]
		\item A better description of human preference change
		\item A prescription for how to specify computation in synthetic agents.
		\item Unification of agentive concepts.
	\end{enumerate}
	
	We achieve these with a model of an agent's awareness and world representation, given by these conditional margin graphs. We have seen that many common calculations simply reduce to calculating ``best guess'' marginals from the rest of the graph; sometimes these inferences are guaranteed to be exact. These graphs also have a number of other nice properties we've explored, such as: \todo{}
	
	Now, we've already shown that this representation is useful for standard 1-off tasks, providing a model for why many ``irrational'' behaviors occur, but we now examine how it embeds into the world temporally. Because we have now allowed possible inconsistencies into beliefs and preferences, we now need to  In particular, suppose we have an agent $\alpha$, who has a model $(\cal A, L)$ at some time $t_0$.
	
	
	If this is the static system, and we would like to know how it evolves in time, we can consider modeling it as a system of autonomous differential equations. This would entail giving coordinates $q$ to $A$ and a transformer $F$ such that $\dot q = F q$. $F$ then captures all of the information about $\alpha$ that it does not itself know, i.e., is not part of the model.
	
	
	Here are some possible issues:
	\begin{enumerate}
		\item Epxressivity: we would like to be able to represent a much wider class of agents, than the vNM-rational ones --- in particular, we would like to express. 
		
		\item An agent might abandon its
		
		\item Maybe when given an opportunity, it will abandon its preferences immediately
		\item 
		
	\end{enumerate}
	
	On one hand, we're allowing for more general agent representations, and hence made the theory's predictions much weaker if we don't know anything about the internal state of the agent. On the other hand, we've made much stronger statements about how an agents preferences are likely to evolve, given some weak assumptions.  
	
	
	
	\section{The Cognitive Hierarchy}
%	When faced with the possibility of self reference, there are generally two tricks you can use to untangle and formalize things: hierarchies and fixed points. For instance, the presentation of knowledge as a modality in logic results in the Common knowledge as a least fixed point.
	
	
	Cognitive hierarchies have been used to characterize finite players in games, etc. 
	
	
	\textbf{Level zero.} You don't think (about minds) at all. To the extent that a 0-process thinks at all, it is because
	
	
	
	Why do we want a hierarchy? 

\end{document}