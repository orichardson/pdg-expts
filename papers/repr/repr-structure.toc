\contentsline {section}{\numberline {1}Motivating Examples}{3}{section.1}
\contentsline {subsection}{\numberline {1.1}For Interpreting Arrows By Themselves, not guaranteeing independence}{3}{subsection.1.1}
\contentsline {subsubsection}{\numberline {1.1.1}Framing Problems?}{3}{subsubsection.1.1.1}
\contentsline {subsubsection}{\numberline {1.1.2}Separate Sources of Information which merge}{4}{subsubsection.1.1.2}
\contentsline {subsection}{\numberline {1.2}For adding new variables}{4}{subsection.1.2}
\contentsline {subsubsection}{\numberline {1.2.1}Keeping around old model while knowing the specific case}{4}{subsubsection.1.2.1}
\contentsline {subsubsection}{\numberline {1.2.2}Discovering new correlations}{4}{subsubsection.1.2.2}
\contentsline {subsubsection}{\numberline {1.2.3}For forgetting least important things}{4}{subsubsection.1.2.3}
\contentsline {section}{\numberline {2}Why EDGs are good}{4}{section.2}
\contentsline {subsection}{\numberline {2.1}Problem: sometimes you don't have a distribution. Sometimes overconstrained; other times under-constrained}{4}{subsection.2.1}
\contentsline {subsection}{\numberline {2.2}Other graphical models represent single distributions, arguably not the best form for an epistemic state.}{4}{subsection.2.2}
\contentsline {subsection}{\numberline {2.3}We make it easy to add and remove nodes from the graph.}{4}{subsection.2.3}
\contentsline {subsubsection}{\numberline {2.3.1}{\bfseries \sffamily TODO} (future: Connection to Single Static Assignment?)}{5}{subsubsection.2.3.1}
\contentsline {subsection}{\numberline {2.4}Can simulate BNs}{5}{subsection.2.4}
\contentsline {section}{\numberline {3}Definitions and Semantics}{5}{section.3}
\contentsline {subsection}{\numberline {3.1}Define an EDG:}{5}{subsection.3.1}
\contentsline {subsection}{\numberline {3.2}Interpretation of EDGs}{5}{subsection.3.2}
\contentsline {subsubsection}{\numberline {3.2.1}Example}{5}{subsubsection.3.2.1}
\contentsline {subsubsection}{\numberline {3.2.2}Interpretation with sub-probability measures}{5}{subsubsection.3.2.2}
\contentsline {subsubsection}{\numberline {3.2.3}Example Dinky Interpretation:}{5}{subsubsection.3.2.3}
\contentsline {subsubsection}{\numberline {3.2.4}Interpreting nodes as more than sets}{6}{subsubsection.3.2.4}
\contentsline {subsubsection}{\numberline {3.2.5}Externalization: most of these can be moved into their own nodes, with their own edges.}{6}{subsubsection.3.2.5}
\contentsline {subsubsection}{\numberline {3.2.6}Maybe for later: figure out what exactly needs to be true of the the target domain of interpretation for useful results to follow}{6}{subsubsection.3.2.6}
\contentsline {subsection}{\numberline {3.3}Partially Interpreted Models}{6}{subsection.3.3}
\contentsline {subsection}{\numberline {3.4}Consistency Semantics:}{7}{subsection.3.4}
\contentsline {subsubsection}{\numberline {3.4.1}Binary version of consistency results in set of distributions}{7}{subsubsection.3.4.1}
\contentsline {subsubsection}{\numberline {3.4.2}Continuous one is a weighted set of distributions}{7}{subsubsection.3.4.2}
\contentsline {subsubsection}{\numberline {3.4.3}Taking the one with the highest consistency-entropy score interprets it as a single distribution.}{7}{subsubsection.3.4.3}
\contentsline {section}{\numberline {4}Reducing Inconsistency}{7}{section.4}
\contentsline {subsection}{\numberline {4.1}Discussion of Inconsistency}{7}{subsection.4.1}
\contentsline {subsubsection}{\numberline {4.1.1}Examples for all of these are not too hard to produce, maybe not the best use of space.}{7}{subsubsection.4.1.1}
\contentsline {subsection}{\numberline {4.2}Consistency of node structure}{7}{subsection.4.2}
\contentsline {subsubsection}{\numberline {4.2.1}Pairwise Consistency (local)}{8}{subsubsection.4.2.1}
\contentsline {subsection}{\numberline {4.3}Consistency for Edges}{8}{subsection.4.3}
\contentsline {section}{\numberline {5}Simulating Other Things}{8}{section.5}
\contentsline {subsection}{\numberline {5.1}Other Descriptions of Uncertainty}{8}{subsection.5.1}
\contentsline {subsubsection}{\numberline {5.1.1}Probability (obviously)}{8}{subsubsection.5.1.1}
\contentsline {subsubsection}{\numberline {5.1.2}Belief Functions}{8}{subsubsection.5.1.2}
\contentsline {subsubsection}{\numberline {5.1.3}More generally, lower probability distributions}{9}{subsubsection.5.1.3}
\contentsline {subsection}{\numberline {5.2}Expected Utility}{9}{subsection.5.2}
\contentsline {subsection}{\numberline {5.3}Simulation of BNs with max entropy}{9}{subsection.5.3}
\contentsline {subsubsection}{\numberline {5.3.1}Conversion from BN to EDG in constant space}{9}{subsubsection.5.3.1}
\contentsline {subsubsection}{\numberline {5.3.2}Theorem: Center of this EDG is the distribution encoded}{9}{subsubsection.5.3.2}
\contentsline {subsection}{\numberline {5.4}Belief Updating}{9}{subsection.5.4}
\contentsline {subsubsection}{\numberline {5.4.1}Regular Conditioning}{9}{subsubsection.5.4.1}
\contentsline {subsubsection}{\numberline {5.4.2}Jeffrey's Rule by minimizing inconsistency}{9}{subsubsection.5.4.2}
\contentsline {subsubsection}{\numberline {5.4.3}Pearl's Rule by adding nodes and then minimizing inconsistency}{9}{subsubsection.5.4.3}
\contentsline {subsection}{\numberline {5.5}Constraint Satisfaction Problems}{9}{subsection.5.5}
\contentsline {subsubsection}{\numberline {5.5.1}Problem with Factor Graphs: Normalization done globally so you can't control things that happen.}{9}{subsubsection.5.5.1}
\contentsline {subsubsection}{\numberline {5.5.2}Richer picture of inconsistency than factor graphs because I can actually see how far off each constraint is off and assign blame properly.}{10}{subsubsection.5.5.2}
\contentsline {section}{\numberline {6}Learning Problems}{10}{section.6}
\contentsline {subsection}{\numberline {6.1}Learning your BN online from observations}{10}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}Supervised Learning in this framework, with losses}{10}{subsection.6.2}
\contentsline {subsubsection}{\numberline {6.2.1}Illustration that the addition of additional losses outside can be done exactly once,}{10}{subsubsection.6.2.1}
\contentsline {subsection}{\numberline {6.3}View DNNs as instances of this model}{10}{subsection.6.3}
\contentsline {subsubsection}{\numberline {6.3.1}{\bfseries \sffamily TODO} ? Does recognizing this inconsistency rather than normalizing it away protect from going off of}{11}{subsubsection.6.3.1}
\contentsline {subsection}{\numberline {6.4}Abstraction}{11}{subsection.6.4}
\contentsline {subsubsection}{\numberline {6.4.1}Talk about information compression}{11}{subsubsection.6.4.1}
\contentsline {subsubsection}{\numberline {6.4.2}{\bfseries \sffamily TODO} Relate to Fixing a Broken Elbo paper}{11}{subsubsection.6.4.2}
\contentsline {section}{\numberline {7}Properties}{11}{section.7}
\contentsline {subsection}{\numberline {7.1}Theorem: NP-hard to minimize consistency in general.}{11}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Belief Propagation}{11}{subsection.7.2}
\contentsline {subsubsection}{\numberline {7.2.1}can be done just like in a BN.}{11}{subsubsection.7.2.1}
\contentsline {subsubsection}{\numberline {7.2.2}The sum-product or max-product algorithms can be implemented without additional space if we enrich our sets to be weighted.}{11}{subsubsection.7.2.2}
\contentsline {subsubsection}{\numberline {7.2.3}Conjecture: has a guaranteed convergence rate for sub-distributions.}{11}{subsubsection.7.2.3}
\contentsline {subsection}{\numberline {7.3}Information Theoretical View}{11}{subsection.7.3}
\contentsline {subsection}{\numberline {7.4}Thermodynamics Analogy}{12}{subsection.7.4}
\contentsline {subsubsection}{\numberline {7.4.1}Minimizing Lexicographical Free Energy gives a distribution.}{12}{subsubsection.7.4.1}
\contentsline {subsubsection}{\numberline {7.4.2}Setting a positive temperature allows for a trade-off between inconsistency and entropy.}{12}{subsubsection.7.4.2}
\contentsline {section}{\numberline {8}Background (write at end)}{12}{section.8}
\contentsline {subsection}{\numberline {8.1}Bayesian Networks}{12}{subsection.8.1}
\contentsline {subsubsection}{\numberline {8.1.1}Belief Propogation}{12}{subsubsection.8.1.1}
\contentsline {subsection}{\numberline {8.2}Markov Networks (MRFs)}{12}{subsection.8.2}
\contentsline {subsubsection}{\numberline {8.2.1}Relation to Gibbs Random Fields, Hammersly Clifford Theorem}{12}{subsubsection.8.2.1}
\contentsline {subsubsection}{\numberline {8.2.2}Normalization NP-hard}{12}{subsubsection.8.2.2}
\contentsline {subsection}{\numberline {8.3}Factor Graphs (alternative characterization of most MRFs)}{12}{subsection.8.3}
