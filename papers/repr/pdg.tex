\documentclass{article}


\input{../model-commands.tex}

\usepackage{comment}
\includecomment{vfull}
\includecomment{vcat}


%%%%%% Commands to highlihght changes in the document.
\definecolor{note-fg}{rgb}{0.1,0.4,0.0}
%\newcommand\changed[1]{\colorbox{light gray}{\parbox{\linewidth}{#1}}}
\newcommand\changed[1]{{\color{note-fg} #1}}
\newcommand\changeon{\color{note-fg} }
\newcommand\changeoff{\color{black} }

\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/]  % activate!

\AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
\AtEndEnvironment{tikzcd}{\tikzexternalenable}



\usepackage[nointegrals]{ wasysym }

\addbibresource{../refs.bib}
\addbibresource{../maths.bib}


% symbols for genders, hour glass.
\newcommand{\mfem}{\mathclap\female\male}
\newsavebox{\hourglassbox}
\savebox{\hourglassbox}{\includegraphics[height=.8em]{hourglass.png}}
\newcommand\hourglass{\usebox{\hourglassbox}}

%%% What do I even call them anyway?
\newcommand{\modelname}{Probabilistic Dependency Graph}
\newcommand{\modelnames}{Probabilistic Dependency Graphs}
\newcommand{\MN}{PDG}
\newcommand{\MNs}{PDGs}

%%%%%%%%%%%%%%%% SHORTCUTS FOR COMMONLY USED THINGS %%%%%%%%%%

\newcommand\Set{\textbf{Set}}
\newcommand\FinSet{\textbf{FinSet}}
\newcommand\MeasSet{\textbf{MeasSet}}
\newcommand\MaxEnt{{\substack{\mathbf{Max}\\\mathbf{Ent}}}}
%\def\bmu[#1|#2]{\boldsymbol\mu\boldsymbol{[} #1 \boldsymbol{|} #2 \boldsymbol{]}}
\newcommand\bmu{\boldsymbol{\mu}}

\title{Dependency Graphs}
\author{Oliver Richardson,  \texttt{oli@cs.cornell.edu}}

\begin{document}
	\maketitle

	\begin{abstract}
		We introduce \modelnames, which can be regarded both as a probabilistic graphical model, and as a collection of soft, local, constraints. The additional representational flexibility allows us to represent both under and over-constrained belief states, as well as a modularity that makes the models easier to modify. They also have a clean theoretical backing, and reduce appropriately to existing, commonly used representations such as Bayesian Networks and constraint graphs. Some algorithms, notably including belief propagation, lift to the more general setting.
	\end{abstract}

	\tableofcontents

	\section{Introduction}
%	Because writing down distributions explicitly is intractable, and so it is necessary to have.
	{\color{blue}
	%JOE-1
		%Although inconsistency was our major motivation, I’m not sure that that’s how I would write  the introduction.  Here’s a possible alternative: There are a number of quite successful graphical representations of uncertainty.  We’re going to introduce one more.  What makes our novel and interesting?  Two things:  (1) it can represent local information and (2) it can represent inconsistency.  Here are some examples to show that both features are quite useful 
		%in practice.  The examples will also give some intuition for our representation.  [[GIVE EXAMPLES.]]  Point out that our approach seems cognitively
		%simpler.  Discuss (at a high level) the two possible semantics for the language.  Say that the “weighted probabilities” approach has two significant advantages over the more obvious “set of probability measures”  approach:  (1) It allows us to consider inconsistent scenarios, since the distributions do not have to be consistent with the graph.  (2) It gives us a natural way of going from a local picture to a global one; there are various choices, and we can consider the “best” one(s).  (3) It allows us to connect our approach to Bayesian networks (and factor graphs?) in a straightforward way.    
	
	
%	\textbf{Outline, from your suggested one.}
%	There are many representations of uncertainty. Probability is one of them. Graphical models are a compact representations of spaces and distributions on them. 
%	The most important feature is not the fact that the model takes the form of a graph, nor that the variables are intelligible, but rather
%		that both (1) the space of possible worlds, and (2) the distribution over them are both determined implicitly and compactly.
%	However, these representations are ultimately committed to representing probability distributions; I argue that the graphical model bit is a
	}

	
	There are already many graphical models
	
	Why use a graphical model at all?
	\begin{enumerate}
		\item Compressed, implicit representation of a giant joint distribution, making use of independence
		\item An intuitive way of communicating complex models.
		\item A local picture of knowledge, where , 
	\end{enumerate}
	
	I argue that the 
	
	%%%%%%%%%%%%%%%%%%%

	Inconsistency is bad. Believing a logically inconsistent formula can lead you to arbitrarily bad conclusions, having an infeasible set of constraints makes all answers you could give wrong, and having inconsistent preferences can lose you infinite money. We don't want to build inconsistent systems or agents with incoherent views of the world, and so, where possible, we design them so they cannot possibly be broken in this way. Suppose, for example, that we are trying to represent some quantity that must be a point on the unit circle. We could do it with an $x$ and $y$ coordinate, but this could be problematic because $x^2+ y^2$ might not be 1 --- it would be safer and harder to go awry if we parameterize it by an angle $\theta \in [0, 2\pi)$ instead. In the absence of performance benefits (like needing to regularly use the $y$-coordinate and not wanting to compute a sine), why would we take the first approach, introducing a potentially complex data-invariant, when we could avoid it?

	This line of thought, though common and defensible, is flawed if we are not perfectly confident in the design of both our system and the ways it can interact with the outside world. Using similar logic, we might ask ourselves: Why ask programmers for type annotations when all instructions are operationally well-defined at run-time?  Why use extra training data if there's already enough there to specify a function? Why estimate a quantity in two ways when they will yield different answers? Why repeat and rephrase your ideas when this could make you contradict yourself? Why write test cases when they could fail and make the project inconsistent? Why conduct an experiment if it could just end up contradicting your current knowledge?

	These questions may seem silly, but there is a satisfying information theoretic answer to all of them: redundancy, though costly, is the primary tool that we use to combat the possibility of being wrong. Maintaining data invariants can be expensive but provides diagnostic information; in the example above, settings of $x$ and $y$ that don't lie on the unit circle provide diagnostic information that something has gone wrong.
	In many cases, it is also possible to paper over problems by forcibly re-instating local data invariants: for instance, we could re-normalize any values of $x$ and $y$ (so long as $xy \neq 0$; we can chose an arbitrary point otherwise) at every step. While this would reduce inconsistency, it also hides red flags.

	Using a Bayesian Network to represent a probability distribution is like representing a circle with $\theta \in [0, 2\pi)$. 
	By construction, the result must be a distribution, and nothing can possibly go wrong so long as we can always decide on exactly one distribution which is sufficient for our purposes.
%	By construction, the result must be a point on the circle, and nothing can possibly go wrong so long as we're sure that we will always have exactly enough information to determine such a point (for instance, we could never be totally clueless about the point, or just know its $x$ coordinate).
	
	
	The process of mechanistically forcing invariants is homologous to the standard practice for factor graphs: practitioners will often just assume that the density it defines is normalizable, and either forcibly re-normalize or cleverly avoid computing the normalization constant while still assuming that one exists; behavior is usually left unspecified in the unlikely event that it is not defined or zero.

	It is clear that, while inconsistency is bad, being able to identify it is extremely useful. To that end, we introduce a more general representation of knowledge and uncertainty which can be both under and over-constrained (from the perspective of producing a probability distribution), can be used to emulate a large class of both probabilistic models and constraint sets, and enjoys many additional properties which make them more useful than the specific variants. % All of this comes at the cost of being slightly more difficult to analyze in general case: most decision problems on \modelnames\ are NP hard. % and some are incomputable.


	\begin{example} \label{ex:planet}
		Suppose we have a belief about how size and composition affect the habitability of a planet: say we're astrobiologists, and we have some sense of how likely we are to find life on a given planet, supposing we knew how big it is and whether it's mostly made of rocks or gas. That is to say that we have a conditional probability table $\Pr(\text{Life} ~|~ \text{Size}, \text{Composition})$, which we are used to graphically depicting like this:
		\begin{ctikzpicture}
			\node[dpadded] (SS) at (-0.4, 2) {Size};
			\node[dpadded] (CC) at (3, 2) {Composition};
			\node[dpadded] (LL) at (1.3,0) {Life};
			\draw[arr] (CC) -- (LL);
			\draw[arr] (SS) -- (LL);
		\end{ctikzpicture}
		This picture looks like a Bayesian Network, which is somewhat misleading. In order to interpret this graph as a quantitative BN, we also would need probability distributions over Size and Composition --- things we may not be willing to give.\footnote{Situations like these are incredibly common and important. In statistical learning theory, this is the difference between a generative and a discriminative (or conditional) model}
		From a probabilistic perspective, our beliefs are under-specified. This is a problem (though not a novel one) with Bayesian networks.

		A bigger problem occurs when our biologist friend reminds us that life requires water, and gives us a probability estimate for the existence of life on a planet, with and without water. We trust this friend completely, and totally believe these probabilities. Unfortunately, but there's no way to incorporate it into our picture, because we don't know what the correlations are between water, size, and composition; neither are we prepared to give a probability of live given a full description of the three, and may not even have the space to keep such a thing.\footnote{If \textit{Size} and \textit{Composition} each have $\approx\sqrt{N}$ elements, and \textit{Water} had $\approx N$ elements, it would be $O(N^2)$ to store a full joint table, compared to $O(N)$ for the two individual ones.} %At this point the Bayesian Network is not at all a convenient way of storing the information at hand.
		Let $S, C, W, L$ be shortenings of Size, Composition, Water, and Life, respectively.
		Intuitively, we want to draw instead a picture that looks more like this:

		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (S) at (-0.4, 2) {$S$};
			\node[dpadded] (C) at (3, 2) {$C$};
			\node[dpadded] (L) at (1.3,0) {$L$};
			\node[dpadded] (W) at (-2,0) {$W$};
			\cmergearr{S}{C}{L}{1.3,1.1}
			\draw[arr] (W) -- (L);
		\end{tikzpicture}
		\end{center}
	
		which represents having two conditional probability tables on $L$: one from $S \times C$ and the other from $W$. This would allow us to combine the two facts that we know, without also providing more information than we're prepared to equivocate on. It is worth noting that there is now a possibility of being inconsistent, in the sense that is possible to specify the conditional distributions in the links in such a way that no joint distribution on all variables will marginalize out to them --- for instance, if all estimates of $L$ from the $W$ are strictly smaller than any probability estimate of $L$ from $S \times C$.
		
		The probabilists in us might not be willing to so easily give up the notion that these data define a probability distribution, at least implicitly. 
		% We presumably already have a prior over everything, and in the event that we d
		Perhaps the reason Bayesian networks are insufficient to represent this epistemic state is because the state is not a distribution and hence invalid; maybe there's something simple we can do to turn it into a one? It turns out that we can (almost always) get a distribution and simultaneously commit to preserving the relative ratios of the specified probabilities within the links, and even more clearly exposing the independence structure that we think of Bayesian Networks as giving. 
		
		%specified to be probability distribution. If we had had a prior, perhaps we could have ``conditioned'' it on the new probability information that our colleague gave us.%
		%\footnote{
		%	``Conditioned'' gets quotation marks because the statement is not an event --- it is not information about the actual world $(S,C,L,W)$, but a meta-statement about probabilities. A probabilist can now avoid this by incorporating the things that a colleague can say into their mental model, and conditioning on the things that are said, which are now truly events. There are two issues with this. First, thsi runs into the problem that you already needed to have all worlds modeled in your head. Even if you allow yourself to form new concepts and worlds as the come to attention,  
		%	}
		 

%		Together with the perspective that a Bayesian Network really encodes independence, this line of thought might cause one to think that the problem was a restrictive class of graphical model. Perhaps really we wanted a factor graph:
		This can be done by treating the conditional distributions $p(L =l \mid S=s, C=c)$ and $p(L=l \mid W=w)$ as \emph{factors}, which multiplied together give the relative probability density of any setting of variables $S \times C \times W \times L$
		\[ \Pr(s, c, w, l) \propto \phi_1(s,c,l) \phi_2(w,l) \]
		where $\phi_1(s,c,l) = p(L=l \mid S=s, C=c)$, $\phi_2(w,l) = p(L=l\mid W=w)$. This can also be represented graphically, with a \emph{factor graph}---a commonly used graphical model hailed as a strict generalization of Bayesian Networks and Markov networks. 
		
		\begin{center}
		\begin{tikzpicture}
			\node[dpadded,inner sep=0.6em, circle] (S) at (-0.4, 2) {$S$};
			\node[dpadded,inner sep=0.6em, circle] (C) at (3, 2) {$C$};
			\node[dpadded,inner sep=0.6em, circle] (L) at (1.3,0) {$L$};
			\node[dpadded,inner sep=0.6em, circle] (W) at (-2,0) {$W$};
			
			\node[light pad] (f1) at (1.3, 1.3){$\phi_1$};
			\node[light pad] (f2) at (-0.3, 0){$\phi_2$};
						
			\draw[thick] (S) -- (f1) -- (C) (f1) -- (L);
			\draw[thick] (W) -- (f2) -- (L);
		\end{tikzpicture}
		\end{center}
		We now have a distribution that combines our beliefs, but this is not exactly what we had in mind earlier. Beyond simply the inevitable effects \changed{of representing our knowledge as a distribution, such as forcing us to implicitly adopt marginal distributions over the variables $S,C$, and $W$, a product of factors has additional undesirable properties:}

		\begin{enumerate}
			\item We can't weight the pieces of information differently. Though the scale of each factor $\phi_i$ gives us a degree of freedom in which to encode this information, it cannot be used, as $(a\phi_1) (b\phi_2) = (ab) (\phi_1\phi_2)$, and the aggregate coefficient $ab$ too is normalized out to form the distribution.
			
			\item The resulting picture does not encode conditional probabilities in quite the way that we had wanted: now updating on $S$ does not preserve $L\mid C,S$, bringing $L$ along as required, but rather does something unclear and very global: we've lost the dependency structure we had in the first few pictures. Relatedly, we have lost the directedness of the edges, and with it, hope that the edges represent anything causal. Furthermore, the addition of new factors can dramatically change the meanings of existing ones. For all of these reasons, it is incredibly difficult to interpret part of the graph by itself.For instance, knowing the joint distribution does not determine the values of the factors.
%			\item The addition of a new factor 
			
			\item If at least one factor is zero for every setting of $S,C,W,L$, no distribution is defined --- in the face of inconsistency, the entire formalism ceases to work at all.
		\end{enumerate}		
	\end{example}
	
	While offer a solution of great generality, they sacrifice a great deal of interpretability and destroy a lot of the important internal features of our original belief representation, so that they can represent distributions.
	
	This is not good, but much worse is the way that they sweep under the rug issues wherever possible. In example \ref{ex:planet},
	
	Akin to a  in some ways destroy inconsistency that may have been a big deal had it come to the surface.
	
	
	\begin{example}
		Consider the classic example used to introduce Bayesian nets, in which the four variables are interest are booleans indicating whether a person ($C$) develops cancer, ($S$) smokes, ($SH$) is exposed to 2nd hand smoke, and ($PS$) has parents who smoke, presented graphically as follows:
		\[ \begin{tikzcd}[center base, column sep=2em, dpad={fill opacity=0, draw=gray}]
					& S \ar[dr] \\
					PS \ar[ur]\ar[dr] && C \\
					& SH \ar[ur]
				\end{tikzcd} \]
				
		This is a compact representation of a joint distribution over all four variables, which achieves compactness by taking advantage of independence between variables. It encodes an assumption that every node is independent of its non-descendants given its parents.
		
		Most of the time, we do not make the independence assumption because we know for certain that the variables are independent; rather, we just suspect that the identified links are by much more important than the others. Determining for sure that smoking  and second hand smoke are independent, controlling for parents' smoking habits, would extremely difficult, and to do properly would require much more empiricism to validate. Why even bother jumping through this hoop? Because we wanted a BN to be shorthand for a probability distribution. But we have freed ourselves from these shackles, and make no such assumption.

		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {true};
			\node[dpadded] (PS) at (2,0) {$PS$};
			\node[dpadded, fill opacity=0.16] (S) at (4, 1.4) {$S$};
			\node[dpadded, fill opacity=0.16] (SH) at (4, -1.4) {$SH$};
			\node[dpadded, fill opacity=0.16] (C) at (6.5,0) {$C$};
	
			\draw[arr] (1) -- (PS);
			\draw[arr] (PS) -- (S);
			\draw[arr] (PS) -- (SH);
			\cmergearr{SH}{S}{C}{5.1,0}
		\end{tikzpicture}
		\end{center}		
		The node on the far left is a special node which only takes one value, and allows us to represent unconditional distributions as arrows, visually making clear the difference between a lack of distribution, and an unconditional one. 
		
		Now, suppose you read a very thorough empirical study which demonstrates that people who use tanning beds have a 10\% incidence of cancer, compared with 1\% in the control. Just as in the previous example, this cannot be encoded directly into the Bayesian Network. 
		The \MN\, on the other hand, has no trouble, and is simply the union of the two pieces of information:
				
		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {true};
			\node[dpadded] (PS) at (2,0) {$PS$};
			\node[dpadded, fill opacity=0.16] (S) at (4, 1.4) {$S$};
			\node[dpadded, fill opacity=0.16] (SH) at (4, -1.4) {$SH$};
			\node[dpadded, fill opacity=0.16] (C) at (6.5,0) {$C$};
			\node[dpadded, fill opacity=0.16] (T) at (8.5,0) {$T$};
	
			\draw[arr] (1) -- (PS);
			\draw[arr] (PS) -- (S);
			\draw[arr] (PS) -- (SH);
			\cmergearr{SH}{S}{C}{5.1,0}
			\draw[arr] (T) -- (C);
		\end{tikzpicture}
		\end{center}
		Note that the right half of the diagram (shaded slightly darker) has the same topology as in example \ref{ex:planet}.
	\end{example}
	
	Benefits of this representation:
	\begin{enumerate}[nosep]
		\item We can represent both over-constrained and under-constrained mental states, both of which we argue are an important component of an agent's state.
		\item Over-constrained models may be inconsistent; such inconsistencies provide a natural way of prescribing changes in mental state. Moreover, many standard algorithms, such as belief updating via Jeffrey's rule, as well as marginalization algorithms such as belief propagation, can be regarded as special cases of consistency reduction.
		\item We can emulate the functionality of not only other graphical models (such as Bayesian Networks, and to a large extent, factor graphs), but also other non-probabilistic notions of uncertainty.
		\item The local interpretation of arrows makes it much less invasive to add, remove, and partially interpret parts of the model, compared to other graphical models.
		\item This modularity makes it possible to add explicit rules to embed logic within the model
		\item By allowing agents to merge, split, and compress variables, we also make it possible for agents to design their own representations. With these tools, in conjunction with consistency
		\item In contrast with a simple collection of constraints, inconsistencies are local, and individual mistakes have limited impact on expectations.
	\end{enumerate} % trade-off: harder to analyze.


%	\begin{enumerate}[nosep]
%		\item This representation more naturally matches what humans are aware of, encoding small locally consistent models rather than one giant probability distribution
%		\item It is a strictly more general representation--- we can easily convert BNs to these diagrams (section \ref{sec:convert2bn})
%		\item This allows composition of arrows to be defined, and gives meanings to paths (section \ref{sec:composition}).
%		\item Allowing variables to be added and removed makes
%		\item Changing and partially determining arrows is more reasonable.
%		\item We can now represent inconsistency, which will allow us to capture mental states which, and . While we agree with the classical picture in that inconsistency is bad, now we can talk about it
%	\end{enumerate}


	% Redundency is important: types in programming languages, more data in ML systems.
	% Puts gurads
	% Makes it possible to combine knowledge without destroying old knowledge.
	% preference updating
	
	\section{Related Work}

	\section{Worlds}
	Other than allowing for inconsistency, the biggest difference between \modelnames\ and other graphical models is their interaction with the underlying space: a \MN\ .
	
	The standard approach to probabilistic modeling is to start by selecting a measurable space of possible outcomes $\Omega$, and then put a normalized measure on it and compute desired quantities with it. Before you can begin to think about random variables, which are defined as set $X_i : \Omega \to V_i$ from outcomes to the set of values $V_i$ that $X_i$ can take on, you have to specify $\Omega$. This construction works well, so long as $\Omega$ is large enough to express everything you ever cared to conceptualize. Because agents are expected to have probability distributions over $\Omega$, the set of worlds that they consider possible must effectively stay constant over time, to use mechanisms such as conditioning as a sole way of changing a mental state.

	Still, a we might wiggle our way around this using only classical probability: we could say that $\Omega$ is some very large set of outcomes that is guaranteed to be expressive enough to capture anything we care about, and then

	To be clear, we've already given up on the possibility of being a Bayesian, because we clearly don't have priors on arbitrary concepts we haven't considered yet if we don't even know the extent of the space, but we might be able to do this with some under-constrained mechanism.

	This strategy, works so long as you are an omniscient modeler. If you are modeling a system in which you know the set of all possible outcomes, either implicitly or explicitly, you can just collect them and use this to be $\Omega$, marginalize out appropriately, and let agents figure out their own distributions on subspaces of $\Omega$. Still, this is not entirely satisfying, for several reasons.


	\todo{most of these are unfinished thoughts, some need to be deleted}
	\begin{enumerate}
		\item While it is true that the there will be subspaces of $\Omega$ which are isomorphic to the sets of worlds $W_i$ that the agents are modeling in their heads, the embedding is not at all clear \todo{}

		\item There may not be an omniscient modeler, and even if there were one, it seems very strange for an agent to have any access to it. Suppose you are using probabilities to describe real uncertainties in your life. To do this the standard way, you need to chose the subspace of the one true $\Omega$ \todo{why is this problematic? for reasons other than inability to change?}

		\item Agents can never gain access via any standard mechanism to new worlds. There's no principled way to add worlds from $\Omega$ to $W_i$. Effectively, they can never learn new concepts.

		\item Any updates must be done on the entire space of things you consider possible \todo{response: of course, this is all handled implicitly, so it's taken care of}.

		\item While agents are free to merge multiple states of $\Omega$ into a single state in $W_i$, they cannot do the reverse: an agent cannot have a finer granularity than $\Omega$ for discerning events. This would . This also implies that agents are logically omniscient.
	\end{enumerate}

	For all of these reasons, we take the view that probability should be thought of subjectively,

%	\footnote{in addition to being notoriously counter-intuitive.}

	At the same time, starting with a set of random variables $\{X_i\}$, and setting $\Omega = \prod_{i} V_i$ to be every possible assignment of variables is also an abuse of the word ``possible''.


	\section{Equivalent Definitions and Variants}


	\begin{defn}\label{def:model}
		A \emph{strict \modelname} is a tuple $(N, L, \bmu, \mathcal V)$ where
		\begin{itemize}[nosep]
			\item $\mathcal N : \Set$~~is a finite collection of nodes
			\item $\mathcal L \subseteq N \times N$~~is a set of directed links between nodes
			\item $\mathcal V : \mathcal N \to \MeasSet $~~is an $N$-indexed family of measurable sets, representing the values that a node can take
			\item $\bmu: ((A,B): \mathcal L) \to \mathcal V(A) \to \Delta(\mathcal{V}(B))$~~is a family of conditional probability distributions on $\mathcal V(B)$ indexed by the values of $A$ for every link $(A,B) \in \mathcal L$ %between the values $\mathcal V(A) \to \mathcal{B}$, for each link $(A,B) \in \mathcal L$, or more technically, a family of distributions on the values of target of the link, given each of the 

		\end{itemize}
	\end{defn}

	The definition of $\bmu$ is probably more familiar than it looks. If every $\mathcal V(N)$ is finite with all subsets measurable, then $\bmu_{A,B}$ is just a conditional probability table, just as in a Bayesian Network. For those more familiar with stochastic processes, this is a stochastic matrix. 

	The definition $\bmu$ is slightly over-simplified if not everything is measurable. More generally if $\mathcal V(A) = (X, \mathcal A)$ and $\mathcal V(B) = (Y, \mathcal B)$, then for any link $L \in \mathcal L$, we're really referring to a function $\bmu_L : X \times \mathcal B \to [0,1]$ such that $\bmu_L(x,-): \mathcal B \to [0,1]$ is a probability distribution and $\bmu_L(-, S)^{-1}(R) \in \mathcal A$ for any $S \in \mathcal B$, and measurable subset $R \subseteq [0,1]$, technically making $\bmu_{A,B}$ a \textit{Markov Kernel} from $A$ to $B$.

	\begin{example}
		In example \ref{ex:planet}, we displayed the arrow from $S$ and $C$ to $L$ as a directed hyper-edge. While we would like to maintain this intuition, it turns out that we can simplify our formalism by de-sugaring this picture into the following:
		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (S) at (-0.5, 2) {$S$};
			\node[dpadded] (C) at (3.1, 2) {$C$};
			\node[dpadded] (L) at (1.3,0) {$L$};
			\node[dpadded] (W) at (-2,0) {$W$};
%			
			\node[dpadded,light pad] (SC) at (1.3, 1.4){\footnotesize $S \times C$};
%			
			\draw[arr, ->>] (SC) -- (S);
			\draw[arr, ->>] (SC) -- (C);
			\draw[arr] (SC) -- (L);
			\draw[arr] (W) -- (L);
		\end{tikzpicture}
		\end{center}
		The double headed arrows are for degenerate conditional distributions, which are fully deterministic, but for now this is not terribly relevant. We can now present this \MN\ formally with the elements specified in definition \ref{def:model}; below we assume everything is measurable and omit this part of the formalism.
		
		\begin{align*}
			\mathcal N &= \{S,~ C, ~L, ~W, ~S\times C \} \\
			\mathcal L &= \{ (S \times C, L), (W, L), (S\times C, S), (S\times C, C)\} \\
			\mathcal V &= \begin{cases}[r]
			\mathcal V(S) &= \{\mathit{big}, \mathit{small} \}\\
			\mathcal V(C) &= \{ \mathit{rocky}, \mathit{gasseous} \} \\
			\mathcal V(L) &=  \{ l, \lnot l \} \\
			\mathcal V(W) &= \{ \textit{none}, \textit{some}, \textit{mostly}\}\\
			\mathcal V(S \times C) &= \mathcal V(S) \times \mathcal V(C) 
				% = \small\text{$\{(big, rocky), (small,rocky), (big, gasseous), (small,gasseous)\}$}
			\end{cases}
		\end{align*}
		\begin{align*}
			\boldsymbol\mu=\begin{cases}
			\boldsymbol\mu[S\times C, L] = \begin{idxmat}{{big, rocky}, {small,rocky}, {big, gasseous}, {small,gasseous}}{$l$,$\lnot l$}
					.1 & .9 \\
					.2 & .8 \\
					.05 & 0.95 \\
					0.00001 & 0.99999
				\end{idxmat} \qquad  
					\boldsymbol\mu[W, L] = \begin{idxmat}{{none}, {some}, {mostly}}{$l$,$\lnot l$}
					0 & 1 \\
					.005 & .995 \\
					.05 & 0.95 \\
				\end{idxmat}\\
			\boldsymbol\mu[S\times C, C] = \begin{idxmat}{{big, rocky}, {small,rocky}, {big, gasseous}, {small,gasseous}}{rocky,gasseous}
				1 & 0 \\
				1 & 0\\
				0 & 1 \\
				0 & 1 
			\end{idxmat} \qquad  
			\boldsymbol\mu[S\times C, S] = \begin{idxmat}{{big, rocky}, {small,rocky}, {big, gasseous}, {small,gasseous}}{small,big}
				0 & 1 \\
				1 & 0 \\
				0 & 1 \\
				1 & 0
			\end{idxmat}
			\end{cases}
		\end{align*}
	\end{example}

	This works pretty well for the two edges that we described before, but the structural overhead of the additional de-sugaring: the $\boldsymbol\mu[S\times C\to S]$ and $\boldsymbol\mu[S\times C\to C]$ tables, as well as the set $\mathcal V(S \times C)$ seem like they didn't need to be specified, and one might even feel that it would be a mistake to allow any other table. Some reasons for this design decision include:
	\begin{itemize}[nosep]
		\item It is easier to prove things about graphs than directed hyper-graphs. Similarly, defining directed paths is a lot simpler.
		\item We can eliminate the clunkiness by fusing the model with an algebra, as in \cref{sec:algebra} --- which will give us a lot more than modeling the hyper-edges directly.
		\item We will eventually also want to allow for the possibility of keeping only a relaxed, approximate representation of $\mathcal V$ and $\bmu$, and in particular, of the ones constructed logically in this way. By specifying them explicitly for now, we will have to do less work to regain manual control in \cref{sec:abstraction}
	\end{itemize}
%	In some ways, this these product nodes are kind of like anonymous functions: if you need them

	\subsection{Alternate Presentations}
	\subsubsection{Random Variables}
	If $\mathcal W = (W, \mathcal F, \mu)$ is a measure space, and $\mathcal X = \{ X_i: W \to \mathcal V(X_i) \}_{i \in I} $ is a collection of measurable random variables on $W$,\footnote{that is: $\mathcal V(X_i)$ is a measurable space, taking the form $(D, \mathcal D)$, and $X_i : W \to D$ is a set function such that for every $B \in \mathcal D$, the set $X_i^{-1}(B) \in \mathcal F$} and 
	{\color{gray}$\mathcal L \subseteq I \times I$ is a collection of pairs of variables such that the agent } 
	\todo{what is a way of phrasing this that doesn't sound like it's shoehorned in? $\mathcal L$ really can represent anything an agent knows. Any subjective conditional probability distribution $\mu'$ such that the only measurable subsets are ``axis aligned'', in that they involve queries on only one variable, can be represented by $\mathcal L$, and for other queries we can simply change variables.}, we call $(\cal W, X)$ an \emph{ensemble}.
	%and $(W, \mathcal F', p)$ is a subjective probability representing an agent's belief 
	
	
	\begin{prop}
		There is a natural correspondence between strict \MNs\ as defined in \cref{def:model}, and ensembles such that \todo{spell this out explicitly to avoid vague categorical intuition} \ldots $\mu$'s are defined on same set and produce same values.
	\end{prop}
	\begin{proof}
		\textit{/outline:}
		On the one hand, $(\prod_{N \in \cal N} \mathcal V(N).\text{set}, \bigotimes_{N \in \cal N} \mathcal V(N).\text{algebra}, \bmu)$ is a measure space, with $\{X_N = \pi_N : \left(\prod\mathcal V(N')\right) \to  \mathcal V(N) \}_{N \in \cal N}$ a set of random variables
		
		and  on the other, $(I, \mathcal L, \mathcal X', \mu|_{\cal L})$ is a strict \MN.
	\end{proof}
	
	This is the technical underpinning of our flippant, noncommittal treatment of possible worlds: any time we are thinking in terms of random variables or probability distributions on a fixed set $W$, we can instead reduce
	
	
	The complexity of the representation is $O(XV + L V^2)$, compared to $O(XW)$
	
	

	\subsection{Sub-stochastic Transitions}
	
	In this section we will see why we called the object in \cref{def:model} a \textit{strict} \MN.	
	Sometimes an otherwise very useful variable might not apply in a small percentage of cases; in this case, we want a way of putting all of the extra probability mass in a ``something else happened'' bucket, giving us effectively a sub-stochastic matrix, or a a lower probability on singletons. For instance, the variable describing whether or not your answer is correct doesn't make sense if you weren't solving problems; the amount of money in your wallet doesn't make sense if you don't own one, and so forth. So now, when you're trying to predict the probability of certain amounts of money in your wallet, some of the probability mass needs to go into the ``not applicable / something else'' bucket. 
	
	There are several closely concepts that we will be able to employ with our framework after integrating them
	\begin{enumerate}[nosep]
		\item Allowing random variables to be partial, rather than total functions of $W$. 
		\item Relaxing the requirement $\int_W \mu \mathrm{d}w = 1$ to $\int_W \mu \mathrm{d}w \leq 1$
		\item Requiring that matrices be sub-stochastic, rather than stochastic
		\item Replacing probabilities, with the more general class of lower probability measures.
%		\item Errors
	\end{enumerate}
 
	This generalization is useful, but our primary motivation for this generalization is so that we can represent implication, and thus a weakening of knowledge as it travels through our graph, in a way that is not just entropy (which might not be distinguishable from certain knowledge of a high entropy distribution otherwise). 

	At first glance, though, it might not be clear why this particular weakening buys us anything at all, because we can always just add the ``something else'' bucket $\bullet$, to $\mathcal V(X)$ for each $X$, and come up with a new strict \MN. A variable which might not make sense can always take a \texttt{null} value, and so now the set of possible is once again exhaustive. From the perspective of providing conditional distributions, however, this resolution poses a problem: our marginals now require us to estimate distributions from a null value--- this is problematic, as a big part of the reason we've been using links to avoid assigning probabilities to everything. Suppose you are trying to represent the belief that you're happier when you get the right answer as a marginal link $L[\mathrm{RightAns}\to \smiley]$. We now need a distribution on happiness when you get the right answer, when you get the wrong answer, and also for when $\bullet$. Why might it not be applicable? Are you not solving problems because you're skiing? Because you've been injured? Maybe you are solving problems but there are multiple right answers? You can't just answer with a prior over happiness if you want to have consistent beliefs, because solving problems and happiness might be correlated. One \emph{could} have such a thing but it seems unreasonable not to be able to express a belief about ``does the right answer make you happy?'' without also answering the much more difficult question, ``how happy are you when `the right answer' is not applicable to your current situation?''

	To see how this increases our expressive power, suppose $A, B$ are binary variables (taking values $a, \bar a$ and $b, \bar b$ respectively). While we can easily easily represent $A = B$, $A = \lnot B$ as stochastic matrices,

	\[ p(B \mid A) = \begin{idxmatphant}{$a$,$\bar a$}{$b$, $\bar b$}{} 1 & 0 \\ 0 & 1 \end{idxmatphant}
	\qquad\text{and}\qquad p(B \mid A) = \begin{idxmatphant}{$a$,$\bar a$}{$b$, $\bar b$}{} 0 & 1 \\ 1 & 0 \end{idxmatphant}
	\]

	we cannot (via stochastic matrices) represent an assertion that $A \Rightarrow B$ without also giving a distribution over $B$ given $\bar a$. One strategy is a uniform prior (used in \cite{logicalinduction}), but this can easily lead to avoidable inconsistencies --- perhaps for totally different reasons you have very good reason to believe that the true distribution of $B$ is true in 90\% of cases; you don't want an arbitrary assumption of a prior competing with actual knowledge.

	For this reason, we drop the requirement that our null element, $\bullet$, indexes a distribution in marginals. Below is an example of transition matrix $A \to B$ including the extra element. As mentioned, the last row is not something we are keeping track of.
	
	\[ \begin{idxmat}{$a_0$, $a_1$, $a_2$, $\bullet$}{$b_0$, $b_1$,  $\bullet$}
	.2 & .1 & 0.7 \\
	0 & 0 & 1 \\
	1 & 0 & 0\\\hline
	{\color{gray}.2} & {\color{gray}.6} & {\color{gray}0}
	\end{idxmat} \]

	Furthermore, because the final column is just whatever is necessary to make the rows sum to 1, we don't need to keep that either; as a result, it is sufficient to keep a smaller matrix without any $\bullet$-indices; the only price that we pay is that this matrix is \emph{sub}-stochastic rather than stochastic: its row entries sum to at most 1, rather than exactly 1. Composition works just as before; the product of sub-stochastic matrices is sub-stochastic. A probability distribution alone, and by extension a standard Bayesian network cannot do this --- because we require the look-up tables to exactly match all possible values, we can't drop any without totally giving up on any world which looks like that.	

	\subsubsection{Relation to Partial Functions of $W$}
	\subsubsection{Reduction to Lower Probability Measures}
	
	\begin{vcat}
		\subsection{Categorical Definition}
		 \note{I will not put any time into this, as it's not going in the paper, but it's here as a placeholder, and I'll list some reasons why this is worth thinking about.}
		One reason this works out so nicely is every construction is universal. We can in fact give a simpler categorical presentation of \MNs\ for those who already know category theory. The highlights are as follows:
		\begin{enumerate}
			\item A \MN\ is an attention-shaped diagram in the Markov category. That is, functor from the free category generated by the graph $(\mathcal N, \mathcal L)$ representing attention, to the Markov category. Indeed $\mathcal V$ is the action on objects, assigning each $\mathcal N$ to a measurable set, $\bmu$ is the action on morphisms, sending edges in $\mathcal L$ to Markov kernels between their associated objects. 
			\begin{enumerate}
				\item Composition works out in general as we place no restrictions on anything, but
				\item If every edge in $\mathcal L$ represents the causal structure of their relationship, then the image of the resulting diagram will be flat, and so effectively there will only be at most one, belief, and no possibility of conflict.
				\item Interpreting with a different model of uncertainty (such as the powerset, giving us non-deterministic possibility) is simply an exchange of interpretation. However, for nice interaction with deterministic functions and logic, this notion of uncertainty must be a monad.
			\end{enumerate}
			
			\item This highlights the role of the ``qualitative'' and ``quantitative'' versions of this framework (which work out much more cleanly than for BNs in a categorical sense)
			
			\item A limit of this diagram is a space of worlds and all of the random variables as functions. A colimit is a the strongest thing that must be true according to the model (suspicion: this is somehow related to common knowledge). There is some strangeness about how samples work that I have not yet figured out.
		\end{enumerate}
	\end{vcat}

	\section{Semantics}\label{sec:semantics}
	These graphs admit multiple semantics. As discussed in section \ref{sec:worlds}, we think of \modelnames\ as being a representation of knowledge in and of themselves, rather than a compression of something more fundamental such as a probability distribution. Still, we will find it useful to interpret them in various ways: doing so will make it possible to compare them more directly with existing graphical models, which one thinks of as really just being compressed distributions. In this section, we would like to highlight three important semantics.
	
	\subsection{As Sets of Distributions}\label{sec:set-of-distribution-semantics}
	If the focus is on under-constrained models, then just as a BN represents a distribution on joint space, a \MN\ might might thought of as representing the set of all distributions that marginalize out to it exactly. 
	
	\begin{defn}
		If $M = (\mathcal N, \mathcal L, \mathcal V, \boldsymbol\mu)$ is a \MN, let $\llbracket M \rrbracket_\Set$ be the set of distributions over the variables in $M$ consistent with $\boldsymbol\mu$ on every marginal. Formally,
		
		\[ \Big\llbracket M \Big\rrbracket_\Set := \left\{\mu \in \Delta\left[~\prod_{N \in \mathcal N}\mathcal V(N)\right] ~\middle|~
		\begin{array}{l}
		\mu(B = b \mid A = a) = \boldsymbol\mu[A,B](b \mid a) \\[0.2em]
		\qquad\text{for all $A, B \in \mathcal L$, $a \in\mathcal V(A)$, and $b \in \mathcal V(B)$} \end{array} \right\}\]
	\end{defn}	
	
%	This is much stronger than the 

	\subsection{As Weighted Distributions}
	
		
	\begin{defn}
		If $M = (\mathcal N, \mathcal L, \mathcal V, \boldsymbol\mu)$ is a \MN\ with joint worlds $W$, and $\ell: \Delta W \to \mathbb D$ is a scoring function, let $\llbracket M \rrbracket^{\ell}$ be the set of distributions that dominate. Formally,
		
		\[ \Big\llbracket M \Big\rrbracket^{\ell}(\mu) := \left\{\mu \in \Delta\left[~\prod_{N \in \mathcal N}\mathcal V(N)\right] ~\middle|~
		\begin{array}{l}
		 \ell(\mu) \succeq \ell(\mu') \\[0.2em]
		\qquad\text{for all $\mu' \in \Delta \prod \mathcal V$} \end{array} \right\}\]
	\end{defn}

	
	\subsection{As Distributions}
	
	To satisfy any lingering desire to compress all of the information to a single distribution, we also offer a way of interpreting a \MN\ as a single distribution.
	
	\begin{defn}
		If  $M$ is a \MN, $\bbr{-}_{\mathbf S}$ is a semantics, $(D, \preceq)$ is an ordered set, and $\ell : \Delta W_M \to \mathbb (D, \leq)$ is a scoring function for probabilities, let \emph{the upper $\leq$-frontier under $\ell$}, denoted $\bbr{M}_{\mathbf S}^\ell$, be the set of distributions that are not strictly dominated by any others. More formally,
%% ALTERNATE, BIG
%		\[ \Big\llbracket M \Big\rrbracket^{\ell}(\mu) := \left\{\mu \in \Delta\left[~\prod_{N \in \mathcal N}\mathcal V(N)\right] ~\middle|~
%			\begin{array}{l}
%			\ell(\mu) \succeq \ell(\mu') \\[0.2em]
%			\qquad\text{for all $\mu' \in \Delta \prod \mathcal V$} \end{array} \right\}\]
		\begin{align*}
			\llbracket M \rrbracket_{\mathbf S}^\ell &=  \left\{\mu \in \llbracket M \rrbracket_{\mathbf S} ~\Big|~ \forall \mu' \in \llbracket M \rrbracket_{\mathbf S}.~ \ell(\mu') \preceq \ell(\mu)  \right\} \\
%			\llbracket M \rrbracket^{\bf S}_\ell &= \left\{\mu \in \llbracket M \rrbracket_\Set ~\Big|~ \forall \mu' \in \llbracket M \rrbracket_\Set.~ \ell(\mu) \geq \ell(\mu')  \right\} 
		\end{align*}
	\end{defn}

	\begin{fact}
		\[ \bbr{M}_{\Set}^\ell = \left\{\mu \in \Delta\prod \mathcal V :\bbr{M}^\ell(\mu) \right\} \]
	\end{fact}
	
	One particularly useful one for emulating Bayesians is the following one, maximizing entropy:
	\[ \Big\llbracket M \Big\rrbracket_\MaxEnt := \Big\llbracket M \Big\rrbracket_\Set^{-H(\cdot)} \]
	
	This corresponds to a lexicographical ordering on distributions
	
	Similarly, we can define 
	\[ \Big\llbracket M \Big\rrbracket_\MaxEnt := \Big\llbracket M \Big\rrbracket_\Set^{-H(\cdot)} \]
		
	

%	\section{Rebuilding Standard Pictures}
%	In an extreme case, we can give up all of the compression
	
	\section{Relations to Other Graphical Models}
	\subsection{Bayesian Networks}
	\subsection{Factor Graphs}

	\section{Relations to Other Representations of Uncertainty}
	\modelnames\ are far from the first formalism to provide a weaker notion of uncertainty than probability. Belief functions, inner measures, sets of probabilities, lower probabilities, weighted sets of probabilities, and plausibility measures have all been studied extensively in the past. One feature that each of these has in common is that they are under-specified, from the perspective of wanting probabilities for everything.

	The natural question now becomes: to what do these under-constrained representations of belief correspond to under-constrained bits of a \modelname?

	\subsection{Sets of Probability Measures}
	As we discuss in section \ref{sec:set-of-distribution-semantics}



	\section{Using Inconsistency}

	Given a distribution $\mu$, and a
	
	\begin{defn}[consistency]
		A \modelname\ $M = ({\cal N, L, V}, \bmu)$ is \emph{consistent} if there exists some joint probability $\Pr$ on all of the variables, or equivalently, if $\bbr{M}_\Set \neq \emptyset$
	\end{defn}

	\begin{equation}
		\zeta({\cal N, L, V}, \bmu)) := \inf_{p \in \Delta(W^{\mathcal V})}~\sum_{L[A, B]\in \cal L}~\mathop{\mathbb E}_{a \sim p(A)} \left[\kldiv[\Big]{L(a) }{ p(B \mid a) } \right]
	\end{equation}
	

	\section{Algorithms}
	\subsection{Belief Propagation}


	\begin{vcat}
		\section{Algebra}\label{sec:algebra}
		\begin{defn}
			If $\sigma$ is a signature, a $\sigma$-\MN\ $M'$ on a \MN\ $M=(\mathcal N, \mathcal L, \mathcal V, \mu)$ is a \modelname\ $(\mathcal N', \mathcal L', \mathcal V', \mu')$ such that
			\begin{itemize}
				\item $\mathcal N':= T_\sigma(\mathcal N)$ is the term algebra for the signature $\sigma$ over the alphabet $\Sigma = \mathcal N$.
				\item $\mathcal L' = \mathcal L \cup \mathcal L^\sigma$ is $\mathcal L$ extended with extra links for operations that are 
			\end{itemize}
		\end{defn}
		
		\begin{example}
			content
		\end{example}
		
	\end{vcat}
	\section{Conclusions}
\end{document}
