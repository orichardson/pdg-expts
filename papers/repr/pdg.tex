%BEGIN_FOLD
\documentclass{article}

%%%%%%%%%%%% FORMATTING INCOMPATIBLE WITH NEURIPS TEMPLATE %%%%%%%%%
%\usepackage[margin=1in]{geometry}
%\usepackage{parskip}

%\setlength{\skip\footins}{1cm}
%\setlength{\footnotesep}{0.4cm}

\usepackage[nonatbib, preprint]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{microtype}      % microtypography

\usepackage[backend=biber,
	style=alphabetic, %authoryear
%	citestyle=authoryear,
%	natbib=true,
	maxcitenames=1,
	url=true, 
	doi=true]{biblatex}
	

%joe3: all this should be unnecessary with a decent bibstyle
%oli3: possibly, but I wanted to make the year a link.
\DeclareCiteCommand{\parencite}[\mkbibparens]
	{\usebibmacro{prenote}}
	{\usebibmacro{citeindex}%
		\printtext[bibhyperref]{\usebibmacro{cite}}}
	{\multicitedelim}
	{\usebibmacro{postnote}}

\input{../model-commands.tex}
% \input{model-commands.tex}
\usepackage{nicefrac}

\usepackage[noabbrev,nameinlink]{cleveref}
%\crefname{lemma}{lemma}{lemmas}
\crefname{examplex}{example}{examples}
\crefname{defn}{definition}{definitions}
\hypersetup{colorlinks=true, linkcolor=blue!50!black, urlcolor=magenta}

\usepackage{float}
\usepackage{subcaption}

\usepackage{comment}

% Wrap definitions...
\definecolor{fulldefncolor}{rgb}{0.7,0.7,.7}
\specialcomment{fulldefn}{\begingroup\color{fulldefncolor}\begin{defn}}{\end{defn}\endgroup}
\definecolor{quickdefncolor}{rgb}{0.7,0.7,.7}
\specialcomment{quickdefn}{\begingroup\color{quickdefncolor}\begin{defn}}{\end{defn}\endgroup}


%END_FOLD

%%%% Version knobs %%%%%. 

\excludecomment{notfocus}

\definecolor{vfullcolor}{gray}{0.85}
\specialcomment{vfull}{\begingroup\color{vfullcolor}}{\endgroup}
% \excludecomment{vfull} %\includecomment{vfull}
\excludecomment{vcat} %\includecomment{vcat}
\excludecomment{vnotation} %\includecomment{vnotation}

\excludecomment{quickdefn}
\excludecomment{fulldefn}

\specialcomment{revising}{\begingroup\color{red}}{\endgroup}
% \pagestyle{plain}

%BEGIN_FOLD
\definecolor{notationcolor}{rgb}{0.9,0.9,.9} % purple
\newcommand{\notation}[2][]{#1}
\begin{vnotation}
	\renewcommand{\notation}[2][]{{\color{notationcolor} #2}}
\end{vnotation}

\newcommand{\commentout}[1]{\ignorespaces}
\newcommand{\vfullfootnote}[1]{}
\begin{vfull}
	\renewcommand{\vfullfootnote}[1]{\footnote{#1}}
\end{vfull}
%\excludecomment{nonintro}


%%%%%%% Commands to highlihght changes in the document.%%%%%%%%%%%%%%%%%%%%%
%\definecolor{note-fg}{rgb}{0.0,.4,0.2} % green
\definecolor{note-fg}{rgb}{.2,0,.4} % purple

%\newcommand\changed[1]{\colorbox{light gray}{\parbox{\linewidth}{#1}}}
\newcommand\changed[1]{{\color{note-fg} #1}}
\newcommand\changeon{\color{note-fg} }
\newcommand\changeoff{\color{black} }

\usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/]  % activate!

\AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

%\crefname{section}{\S\!}{\S\!}

%oli3: these are required for sure, for biblatex.
% If you don't want to manage it yourself, I'll put it on overleaf.
\addbibresource{../refs.bib}
\addbibresource{../uncertainty.bib}
\addbibresource{../maths.bib}
\addbibresource{graphical-models.bib}

%%%%%%%%%%%%%%%% SHORTCUTS FOR COMMONLY USED THINGS %%%%%%%%%%%%%%

\newcommand\Set{\textbf{Set}}
\newcommand\FinSet{\textbf{FinSet}}
\newcommand\MeasSet{\textbf{MeasSet}}

% Semantics
%\newcommand\SD{_{\text{SD}}}
\newcommand\SD{_{\text{sd}}}
%\newcommand\Weighted{_{\text{WD}}
		
%\newcommand\MaxEnt{{\substack{\mathbf{Max}\\\mathbf{Ent}}}}
\newcommand\MaxEnt{_{\mathbf H}}


\newcommand{\none}{\varobslash}
\def\sheq{\!=\!}
\newcommand{\lowrightarrow}[1]{\mathrel{\raisebox{-2pt}{$\xrightarrow{#1}$}}}
\DeclareMathOperator\dcap{\mathop{\dot\cap}}
\newcommand{\doubleheadrightarrow}{%
	\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}


%\def\bmu[#1|#2]{\boldsymbol\mu\boldsymbol{[} #1 \boldsymbol{|} #2 \boldsymbol{]}}

\newcommand\bb[1]{\mathbbm{#1}}

\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bp}[1][L]{\mat{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal E}
\newcommand{\sfM}{\mathsf M}

\newcommand{\alle}[1][L]{_{ X \xrightarrow{\!\!#1} Y }}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%% What do I even call them anyway?
\newcommand{\ModelName}{Probabilistic Dependency Graph}
\newcommand{\modelname}{probabilistic dependency graph}
\newcommand{\modelnamehyper}{probabilistic dependency hypergraph}
\newcommand{\modelnames}{\modelname s}
\newcommand{\MN}{PDG}
\newcommand{\MNH}{PDH}
\newcommand{\MNs}{\MN s}

\numberwithin{equation}{section}

\title{\ModelName s}

\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.

%% UAI  reviewing is double-blind.
%
%% The author names and affiliations should appear only in the accepted paper.
%%
%\author{
%	{\bf Oliver E. Richardson%\thanks{Footnote for author to give an alternate address.}
%		}\\
%%	Computer Science Dept. \\
%	Cornell University\\
%	Address\\
%\And
%	{\bf Joseph Y. Halpern}  \\
%	Cornell University      \\
%	Address \\
%}
%

%END_FOLD
\begin{document}
	\maketitle
	
	\begin{abstract}
		We introduce a new graphical model, a Probabilistic Dependency Graph (PDG). PDGs can capture inconsistent beliefs in a natural way and are, in a precise sense, more modular than standard approaches such as Bayesian networks (BNs), in that they make it easier for a modeler to incorporate new information and restructure the representation, without having to commit to a particular distribution. As we show, a BN, when viewed as a PDG, may represent many distributions, but we can recover the distribution given by the semantics of BNs as the one that minimizes the uncertainty in the distribution beyond that actualized in the tables, among all the distributions consistent with the conditional probability tables in the BN. We relate PDGs to other graphical models, and show by example how PDGs are an especially natural modeling tool.
	\end{abstract}

%	\tableofcontents

	\section{INTRODUCTION}

	\commentout{We introduce the \ModelName\ (\MN), a directed graphical model for specifying local beliefs. They are strictly more expressive and modular than existing directed graphical models, and in particular this allows them represent inconsistent belief states.
	
	
	To be clear, inconsistencies are bad. Still, constantly examining every possible interaction between beliefs is taxing and restrictive, making them difficult to avoid: reasonable people are often simultaneously unaware of any inconsistencies in their beliefs, and yet still think it probable that they are not entirely consistent.%
%oli1: what do you think of the point in this footnote? Do you think it's worth bringing up somewhere? 
%joe2: I don't think it adds much, and I think there are many other
%reasons that arguments exist.
		\footnote{In some sense, this is the reason arguments exist: it is possible to get a person to agree to premises and reject a conclusion, revealing inconsistency---inconsistency which can then be used to change someone else's mental state. }		
	Most graphical models eliminate the possibility of inconsistency by fiat, 
	making them poorly suited to representing inconsistent belief states.
	While we do not endorse inconsistency, we nonetheless think it important to represent: in addition to modeling an overwhelmingly common feature of humans, the possibility of inconsistency 
	allows our model (called a \modelname\ or \MN)
	to portray intermediate stages of belief updating (\Cref{sec:belief-update}), 
	provides a rationale for providing multiple justifications 
	(corroborating evidence means more in the presence of possible conflict; see \Cref{ex:corrob}),
	and	recasts standard algorithms such as belief propagation and conditioning on evidence as resolutions of inconsistency (\Cref{sec:algorithms}). }

In this paper we introduce yet another graphical for modeling beliefs, \emph{Probabilistic Dependency Graphs} (PDGs). There are already many such models in the literature, notably including Bayesian networks (BNs), factor graphs, but also many others \parencite[For an overview, see][]{koller2009probabilistic}. Why does the world need one more?

Our original motivation for introducing PDGs was to be able capture inconsistency. While we do not endorse inconsistency, we wanted to be able to model the process of resolving it; to do so, we had to model the inconsistency itself. But our approach to modeling inconsistency has many other advantages.
Most importantly, \MN s are significantly more modular than other directed graphical models: they can be readily restricted and combined in ways that cause problems for other representations. 
We start with some examples to motivate PDGs and display some of these properties.
%joe3*: I don't know ho	w we modify the graph without changing it's
%informational content.  That doesn't make sense to me.  I also don't
%now what it means to add information ``efficiently''
%oli3: Since our meeting, I've remembered what I meant: you can change
% representations around, e.g., with products. I recognize how confusing
% this is now, and have said something else instead.
%oli3: As for "efficiently", it costs something to add things to a BN
% because you have to recompute new tables. Just collecting the
% information is "free". Computationally. 
%We can efficiently both add new information to a graph without changing the existing structure, and also modify the graph without changing its informational content.
% Though we may incur inconsistency by doing so, relaxing this
% coupling makes it easier to talk about representational learning.
%joe2: Cut; I don't understand what it means to ``consolidate
%computational costs'', and we don't discuss this issue. 
%oli2: Rather than paying the update cost for every observation and
%thought, you can just incorporate observations and think about them
%later. This seems very congruous with the story, and in particular
%with the ``you may not want to resolve conflicts right away'' bit,
%partly because it's expensive. I don't know the bit I wrote
%specifically about this is still in the document or not. 
%joe2: I also don't know what it would mean for a representation to be
%more dramatic. 
%This is a case of ``show, don't tell''.  Let the examples do the talking.
%oli2: I agree and I should probably get some more examples. Still I
%wanted this paragraph to go somewhere: it seems to me the reason it's
%beneficial to the things you're keeping track of, and the processing
%you need to make sense of it, is because you can do weirder graph
%transformations and resolve all of the inconsistency at once, rather
%than ensuring things are up-to-date at all times. 
%oli2*: Analogy: constantly making sure the document has the right
%formatting. Ultimately the formatting may help but making sure it's
%corretly formatted after every character you type is `taxing and
%restrictive'. Fixing your habits allows you to make bigger
%modifications to the document, and consolidate the cost of fixing the
%formatting.
%joe3: maybe so, but if you're going to claim this, you have to give
%examples.   You can't just make the claims
%	As a result, we can make more dramatic transformations of
%        representation, and consolidate the computational costs of
%        resolving them. 
%
%
%joe2: Oliver, instead of spraying the reader with a lot of
%unexplained bullets, you need to go over these points carefully
%later, and explain them.  I assume that by ``theoretical backing''
%you mean semantics.
%oli2: I am really referring to how nicely the energy and composition
%bits work out, compared to some other graphical models which I view
%as hackier: by thinking of the tables as being in the edges, lots
%things seem to magically fall into place. Most of these things I know
%I won't be able to explain in this space, but I want to get to at
%least the energy and composition. I know I need to support this later
%if I want it in the intro. If I can't get to convincing examples by
%the end I agree we need to cut it.
%joe3: not only do you have to support it.  You shouldn't bring it up
%until you've illustrated it.
%joe2: The semantics is discussed later.  If you mean something else, you
%have to make it clear.  I don't know what it means to ``promise
%deeper parallels''.  Let the examples show how PDGS are a natural
%etension, rather than saying it. 
%oli2: While examples are definitely be better than hand waving,
%though this would not be the place for them. We are advertising the
%story at a high level here; I'm trying to mark this as something I
%want in the story, in the same way that we're trying to advertise
%modulairty without examples at this point.
%joe3: You don't ``advertise'' modularity just by saying you have it.
%Sometimes you can get away with saying ``As we shall show, ...'', but
%you want to minimize that
%joe2: We really don't want to talk about
%type thoery here, nor do we want to make mysterious claims about
%locality.  That's not what an introduction is for.  I cut all this.
%	Furthermore, the theoretical backing is clean, promising
%        deeper parallels to other fields.  
%	PDGs themselves are natural extensions of BNs, and the nature
%of this extension complements both the information theoretic
%foundations of graphical models (\Cref{sec:thermo}) and the type
%theory we gesture towards. 
%	The modifications we make also bring graphical models in line
%        with other notions of locality in mathematics,  
%joe1: If you're going to bring these up, you would have to explain
%the connection.  We do not want to do that here (maybe somewhere much
%later in the full paper), so we shouldn't mention it here.  If you
%mention it, you need to add references.
%oli1: why should it here? We haven't fully explained anything yet. I mention it again when I the union precisely.
%oli1: I've now added one possible reference but I'm not really leaning on it. There are a bunch of 
% related constructions and I call out manifolds because everyone has heard of them, but if you can read the reference
% you probably also know this happens all over the place. You might not have realized that this was similar to the union
% construction if I don't mention it though.
%joe2: NO! we don't want to bring this up in the second paragraph of
%the introduction, before you've given the reader some
%intuitions. Maybe you can hint at some of this stuff in the
%discussion section at the end.  But it definitely shouldn't go here
%oli2: Ok. I'm now on the same page about cutting this, but I'm still
%confused about how this is different from the sentence advertising
%and then explaining modularity.
%joe3: in that case, you explained it right away.  Here you don't
%	such as manifolds \parencite[e.g.,][p.12]{atlases}, in which
%        multiple restricted pictures can be glued together to form
%        something with global structure. 
%
%	We start by giving some examples that motivate PDGs and display some of their advantages over Bayesian Networks in particular.
% We start with some examples to motivate PDGs and display some
%joe3: it's OK to recap some advantages at the ed, but many of the
%ones on your list are ones we won't get to in the paper.
%oli3: Don't you think it's useful to list the things we're able to do that couldn't go in the paper? In any case, this section will be modified accordingly; what's there will be what we want to give them.
%        of these advantages; those looking for a more comprehensive
        %        list can skip to \Cref{sec:list-of-benefits}.
% of these advantages.
%	We use the rest of the section to give some of examples focusing on the benefits of this improved modularity, motivate the design of agents that might , and highlight the failure of Bayesian Networks to capture what we have in mind (we compare to other graphical models in \Cref{sec:other-graphical-models,sec:many-relations-graphical-models}). PDGs have other desirable features as well; for a complete list, see \Cref{sec:list-of-benefits}.
	
	\begin{example}[the simplest inconsistency]
		\label{ex:guns-and-floomps}
		You arrive in a foreign country known for having very clear laws. From prior reading, you ascribe probability 0.95 to owning guns being against the law. Upon arrival, you end up talking to some teenagers who use the local slang, after which you believe that with probility .1, the law prohibits floomps. 
		
		The obvious way to represent this as a BN involves two binary random variables, $F$ (taking values $\{f, \overline f\}$), indicating the legality of floomps, and $G$ (taking values $g, \overline g$) indicating the legality of guns. The semantics of a Bayes Net offer us two choices: we either assume that $F$ and $G$ are independent and give (unconditional) probabilities of $F$ and $G$, or we choose a direction of dependency,
%joe3: incorporate one of the two what?  what does ``incorporate''
%mean here?
%oli3: I'm 
   and give one of the two unconditional probabilities and a conditional probability table. As there is no reason to believe that either variable depends on the other, the natural choice is to assume independence, giving us the following BN:
		
		
		\begin{center}
			\scalebox{0.9}{
			\begin{tikzpicture}[scale=0.8,ampersand replacement=\&]
	
				\node[dpadded, circle] (floomp) at (-1.2,0) {$F$};
				\node[dpadded, circle] (gun) at (1.2,0) {$G$};
	
				\matrix [table with head, column 1/.style={leftrule},
					 column 2/.style={rightrule}, row 2/.style={bottomrule}] at (-3.5,0) {
					\vphantom{$\overline f$} $f$ \& $\overline f$\\
					.9 \& .1\\
				};
				\matrix [table with head, column 1/.style={leftrule},
					 column 2/.style={rightrule}, row 2/.style={bottomrule}] at (3.5,0) {
					 \vphantom{$\overline g$}$g$ \& $\overline g$\\
					 .05 \& .95\\
				};
			\end{tikzpicture}
			}
		\end{center}
		
		Now suppose that you later discover that ``floomp'' is likely to be another word for gun, and come to believe that if floomps are legal (resp., illegal), then there's a 92\% chance guns are as well. Let $\mat r$ be the conditional probability table (cpt) that describes this belief. A first reaction might be to simply incorporate this conditional information by adding $F$ as a parent of $G$, and then associating the cpt $\mat r$ with $G$. But then what should we do with the original probability we had for $G$?  Should we just discard it?
			% which amounts to throwing out our old prior distribution on $G$; 
			% another instinct might be to perform a Bayesian update, but this new information is conditional and probabilistic, not an event.
		It is easy to check that there is no probability distribution that is consistent with the two original priors on $F$ and $G$ and also the cpt $\mat r$, so if we are to represent our information with a BN, which always represents a consistent distribution, we must resolve the inconsistency.  
%		In fact, $E$ conflicts with the other two tables, in the sense that there is no joint distribution on $\{F, G\}$ such that all three tables are accurate. 
%		Therefore, there is no Bayes Net that contains all three pieces of information exactly: one has to resolve the inconsistency first.
			
		%There are three different dimensions along which this could be resolved: rejecting either prior belief, or $E$; in general any mixture would do. 
                %joe3: removed paragraph break, and discussion of
                %``your interests'', which are largel irrelevant
%		
%However, it may not be in your best interest to sort this out right away.
		However, it may be better not to sort this out right away. How to resolve it may be clearer if you can get confirmation that guns are indeed floomps, or read the laws more carefully.

		By way of contrast, consider the corresponding \MN.
		In a \MN, the cpts are attached to edges, rather than nodes of the graph. The cpt associated with an edge $e$ from $X$ to $Y$ is a matrix $\mat e$, where the element $\mat e_{x,y}$ at row $x$ and column $y$ is the conditional probability $\Pr(Y \!\!=\!\!y \mid X \!\!=\!\! x)$. In order to represent unconditional probabilities, we introduce a \emph{unit variable} $\mathsf 1$ which takes on only one possible value,	which we denote $\star$. Thus, we have the PDG depicted in \Cref{fig:gun-floomp-diagram}, where the edges from $\mathsf 1$ to $F$ and $G$ are associated with the unconditional probabilities of $F$ and $G$, and the edge from $F$ to $G$ is associated with the cpt $\mat r$.
		\begin{figure}[h]
			\centering
			\scalebox{0.9}{
			\begin{tikzpicture}
				\node[dpadded] (true)  at (0,1.7) {$\mathsf 1$};
				\node[dpadded] (floomp) at (-1.5,0) {$F$};
				\node[dpadded] (gun) at (1.5,0) {$G$};			
				
				\draw[arr] (true) -- coordinate(A) (floomp);
				\draw[arr] (true) -- coordinate(B) (gun);
	
				\node[above left=0.5em of A] {
					\begin{idxmat}{$\star$}{$f$, $\overline f$}
						.90 & .10 \\
					\end{idxmat}
				};
				\node[above right=0.5em of B] {
					\begin{idxmat}{$\star$}{$g$, $\overline g$}
						.05 & .95 \\
					\end{idxmat}
				};
	
				\definecolor{heldout}{rgb}{0.6, 0.6, .8}	
				\draw[heldout, dashed, arr] (floomp) -- node[fill=white] (C) {$\mat r$} (gun);
				\node[below=1em of C] {
					\color{heldout}
					$\mat r =\!\!\!$\begin{idxmat}[\color{heldout}\smalltext]{$f$,$\overline f$}{$g$, $\overline g$}
						0.92 & 0.08 \\ 0.08 & 0.92 \\
					\end{idxmat}
				};
			\end{tikzpicture}
			}
			\caption{An inconsistent \MN, requiring resolution}
			\label{fig:gun-floomp-diagram}
		\end{figure}


		The original state of knowledge consists of all three nodes and the two black edges from $\mathsf 1$. This is like Bayes Net that we considered above, except we no longer assume $F$ and $G$ to be independent; we merely record the constraints imposed by the given probabilities.
	
		The key point is that we can incorporate the new information into
		our original representation (the graph in \Cref{fig:gun-floomp-diagram}
		without the edge from $F$ to $G$) simply  by adding the edge from $F$
		to $G$ and the associated cpt $\mat r$.  Doing so does not change the meaning
		of the original edges.  This presentation lets us simply include
		information, and resolve inconsistencies later. Unlike a Bayesian
		update, the operation is even reversible: all we need to do recover
		our original belief state is delete the new edge, effectively making
		it possible to mull over and then reject an observation.
	\end{example}

	%oli3:
	\Cref{ex:guns-and-floomps} suggests that there is something to be gained by avoiding the independence assumptions enforced by BN (that every node is independent of its non-descendants given its parents). After all, independences are notoriously difficult to demonstrate emperically and unobserved confounders cannot be eliminated.
	Still, conditional independences are useful, and as Pearl \parencite{pearl1989conditional} has argued forcefully, omnipresent.
	%: in $\chi^2$ test for instance, independence is the null hypothesis.	 
	It seems that PDGs have lost this key benefit of BNs.  As we shall see, this is not the case.  
	

	%	Most of the time, we do not make the independence
	%assumption in a BN because we know for certain that the
	%variables are independent; rather, we just suspect that the
	%identified edges are by much more important than the
	%others. Determining for sure that smoking  and second hand
	%smoke are independent, controlling for parents' smoking
	%habits, would extremely difficult, and would require
	%empiricism to validate. 
	

	\begin{example}[emulating a BN]\label{ex:smoking}
		We now consider the classic (quantitative) Bayesian network $\cal B$, which has four binary variables indicating whether a person ($C$) develops cancer, ($S$) smokes, ($\mathit{SH}$) is exposed to second hand smoke, and ($\mathit{PS}$) has parents who smoke, presented graphically in \Cref{subfig:smoking-bn}. We now walk through what is requried to represent $\cal B$ as a \MN, which we call $\Gamma(\mathcal B)$, shown as the solid nodes and edges in \Cref{subfig:smoking-pdg}.
% 
		\begin{figure*}[ht!]
			\centering
			
			\begin{subfigure}[b]{0.3\textwidth}
				\scalebox{0.9}{
				\begin{tikzcd}[center base, column sep=1.8em, row sep=1em, dpad={fill opacity=0, draw=gray}, 
					ampersand replacement=\&]
				\& S \ar[dr] \\
				PS \ar[ur]\ar[dr] \&\& C \\
				\& SH \ar[ur]
				\end{tikzcd}}
				\caption{Bayesian Network, $\cal B$}
				\label{subfig:smoking-bn}
			\end{subfigure}%			
			\hspace{2em}\vline\hspace{2em}
			\begin{subfigure}[b]{0.5\textwidth}
				\scalebox{0.9}{
				\begin{tikzpicture}[center base]
				\fill[opacity=0.2, fill=orange!80!black] (2.7,1.35) rectangle (7.1, -1.35);
				
				\node[dpadded] (1) at (0,0) {$\mathsf 1$};
				\node[dpadded] (PS) at (1.65,0) {$\mathit{PS}$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (S) at (3.2, 0.8) {$S$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (SH) at (3.35, -0.8) {$\mathit{SH}$};
				\node[dpadded, fill=black!.16, fill opacity=0.9] (C) at (4.8,0) {$C$};
				
				\draw[arr] (1) -- (PS);
				\draw[arr] (PS) -- (S);
				\draw[arr] (PS) -- (SH);
				\mergearr{SH}{S}{C}
				
				\node[dpadded, fill=black!.16, fill opacity=0.35, dashed] (T) at (6.5,0) {$T$};
				\draw[arr,dashed] (T) -- (C);	

				\draw[very thick, |-|, color=orange!50!black] (2.7, 1.35) --node[above=0.5em]{Restricted PDG in Examples~\ref{ex:grok-ablate}~and~\ref{ex:grok-union}} (7.1,1.35);
				
				\end{tikzpicture}}
				\caption{Corresponding \MN, $\Gamma(\mathcal B)$, and restriction}
				\label{subfig:smoking-pdg}
			\end{subfigure}
		
			\caption{Graphical models representing conditional relationships in \Cref{ex:smoking,ex:grok-ablate,ex:grok-union}}
			\label{fig:smoking-bn+pdg}
		\end{figure*}
		
%		The BN is a compact representation of a joint distribution over all four variables, which achieves compactness by taking advantage of independence between variables. 
%joe3: a BN is just a graph; it's not ecnoding anything
%                distribution by encoding the an assumption that every
%oli3: 
% The BN achieves a compact representation of a join distribution by assuming that every node is independent of its non-descendants given its parents. 
%joe3: removed paragraph break. Rewrote sentences; we're not using
%different lenses, but giving differentsemantics
%oli3: that's the same thing.
%
%A PDG does not make these assumptions. However, by viewing the
%PDG through a different lens (we offer several in
%\Cref{sec:semantics}), we can further interpret the constraints. In
%particular, the maximum entropy distribution consistent with the
%oli3: this is false. Maximum entropy is exactly these assumptions.
% We do not make these assumptions when giving semantics to a PDG.

		We start with just nodes corresponding to the variables in $\cal B$, and also a special node $\sf 1$ from \Cref{ex:guns-and-floomps}. and an edge ${\sf 1}$ to $\mathit{PS}$, to which we associate the unconditional probability that one's parents smoke, given by the table attached to $\mathit{PS}$ in $\cal B$.
		We can also directly re-use the cpts on $S$ and $\mathit{SH}$, assigning them, respectively, to the edges $PS \to S$ and $PS \to SH$ in $\Gamma(\mathcal B)$. There are two remaining problems: (1) modeling the remaining table in $\cal B$, which corresponds to the conditional probability of $C$ given $S,SH$, and (2) recovering the additional independece assumptions in the BN.
		
		We start with (1). We should not simply add the edges $S \to C$ and $SH \to C$ that are present in $\cal B$, because, as we saw in \Cref{ex:guns-and-floomps}, this would mean supplying two \emph{separate} tables, one indicating the probability of $C$ given $S$, and the other indicating the probability of $C$ given $\mathit{SH}$. Though we could get both tables by marginalizing, doing so would lose an important correlation that is present in $\cal B$.  
		To distinguish the joint dependence on $S$ and $\mathit{SH}$, for now, we draw an edge with two tails, sometimes called a \emph{hyperedge}, which completes the diagram in \Cref{subfig:smoking-pdg}.
	\end{example}

	We now return to the issue of independence (2). There are many distributions consistent with the conditional marginal probabilities in the cpts, and the independences presumed by $\cal B$ need not hold for them. Rather than encoding the extra probabilistic information as cpts, we develop a maximum-entropy semantics for PDGs, which allows us to (temporarily) view  $\Gamma(\mathcal B)$ as representing the distribution which provides the \emph{least additional information} beyond the information already contained in the cpts, among all those consistent with them. In \Cref{thm:bns-are-pdgs}, we will show that this distribution is the same as the one specified by $\cal B$.
%oli1: I added the following sentence, but I'm not
                %sure how worried people will be and the current plan
%is not to talk about sampling in the abstract.
%joe3: definitely not for here
%		\begin{vfull}
%			On might worry that we've merely constructed
%		some object we have no computational access to, but note that in this
%		case, sampling is no harder in the \MN, We discuss this problem in
%		more depth in \Cref{sec:sampling}. 
%		\end{vfull}
	Doing this, however, does not limit us to the features of BNs. 
	
	\begin{example}[continues=ex:smoking]
		Suppose we read a thorough empirical study which demonstrates that people who use tanning beds have a 10\% incidence of cancer, compared with 1\% in the control (call the cpt for this $\mat p$), and would like to add this information to $\cal B$. 
		The first step is clearly to add a new node labeled $T$, for ``tanning bed use''.  But simply making $T$ a parent of $C$ (as clearly seems appropriate, given the incidence of cancer depends on tanning bed use), requires a substantial expansion of the cpt, and, in particular, requires us to guess at the interactions between tanning beds and smoking. 
		
		%TODO
		% \moveme{
		% 	In some sense, the only reasonable guess is that . So why put it here?	 }
		
		The corresponding PDG, $\Gamma(\mathcal B)$, on the other hand, has no trouble:
		We can simply add the node $T$ with an edge to $C$ that is associated with $\mat p$. 
		
		It is now technically possible for our knowledge to be inconsistent. For instance, if the distribution on $C$ given $S,H$ by its original cpt were always deterministically ``has cancer'' for every possible value of $S$ and $H$, but the distribution according to the new cpt from $T$ were deterministically ``no cancer''. 
		% In this particular case, such a selection of cpts seems like an implausible edge case; we conclude that the additonal modularity we gain from a PDG is useful even when there is no inconsistency.
		

%		\begin{center}
%			\scalebox{0.9}{
%			\begin{tikzpicture}
%			\node[dpadded] (1) at (0,0) {$\mathsf 1$};
%			\node[dpadded] (PS) at (1.65,0) {$\mathit{PS}$};
%			\node[dpadded, fill opaci semi-colon representty=0.16] (S) at (3.3, 0.8) {$S$};
%			\node[dpadded, fill opacity=0.16] (SH) at (3.3, -0.8) {$\mathit{SH}$};
%			\node[dpadded, fill opacity=0.16] (C) at (4.8,0) {$C$};
%			\node[dpadded, fill opacity=0.05,dashed] (T) at (6.5,0) {$T$};
%			
%			\draw[arr] (1) -- (PS);
%			\draw[arr] (PS) -- (S);
%			\draw[arr] (PS) -- (SH);
%			\mergearr{SH}{S}{C}
%			\draw[arr,dashed] (T) -- (C);
%			\end{tikzpicture}}
%		\end{center}		
	       
		% This is again an illustration of the modularity and the possibility for inconsistency; compare the right half of the diagram (shaded slightly darker) with the topological equivalent in example \ref{ex:planet}.
	\end{example}	


% Goal here: introduce a graph with more stuff:
%  - Graph union.
	We have seen that we can easily add information to \MNs; we now show that they can also also do the reverse.  

	\begin{example}[restriction]\label{ex:grok-ablate}
		After the communist uprising, children were raised communally, and so parents' smoking habits no longer had any impact on them. Grok is reading her favorite book on graphical models, and she realizes that while the node $\mathit{PS}$ in \Cref{subfig:smoking-bn} has lost its usefulness, and nodes $S$ and $\mathit{SH}$ no longer ought to have $\mathit{PS}$ as a parent, the other half of the diagram---that is, the node $C$ and its dependence on $S$ and $\mathit{SH}$---should apply as before.
		%oli4: this next sentence is less useful, and can be removed; its purpose is to pre-emptively push against a desire to margnialize and get a new BN. 
		{\color{gray} The rise of the communist party also came with changes in smoking habits, so a new unconditional distribution on $S$ could not be obtained by eliminating the variable $PS$. }
		
		Grok knows that this restricted model is no longer a BN, but rather a \emph{conditional} BN \parencite{koller2009probabilistic}. While a BN would require an association prior beliefs with $S$ and $\mathit{SH}$, a conditional BN (CBN) allows for these nodes to be marked as observations; observation nodes do not have associated beliefs. 
		Although restrictions of CBNs to subgraphs are still CBNs, they are stil not always well-behaved when nodes are removed. For instance, by deleting the node $B$ in a chain 
		\scalebox{0.6}{
		\begin{tikzcd}[dpad={light pad}, column sep = 1.3em, AmpRep]
			A \ar[r] \& B \ar[r] \& C
		\end{tikzcd}},
		one concludes that $A$ and $C$ are independent, an conclusion incompatible with the original diagram containing all three nodes.  
		% note: it's incompatible with the structure, but not the rest.
		
		PDGs are more modular in both respects. Consider the PDG fragment analogous to the CBN we first examined, in which $\mathit{PS}$ and the arrows out of it have been deleted (see the box in \Cref{subfig:smoking-pdg}). In this case, one can see that there are no edges leading to $S$ or $\mathit{SH}$, and hence no distributions specified on them; no special modeling distinction between observation nodes and other nodes are required. 
		Because they make no independence assumptions, the information in this fragment is truly a subset of the information in the whole \MN. 		
	\end{example}
	
	Being able to restrict your knowledge and look at a local picture may be a nice property, but a much more compelling reason to PDGs is their ability to aggregate information.
	
	\begin{example}\label{ex:grok-union}
		Grok dreams of becoming Supreme Leader ($\it SL$), and has come up with a plan. She has noticed that people who use tanning beds have significantly more power and than those who don't. Unfortuantely, her mom has always told her that tanning beds causes cancer: in particular, that 15\% of people who use tanning beds get it, compared to the baseline 2\%. Let $\mat q$ be the cpt associated to this beleif. 
		Grok believes people will make fun of her if she uses a tanning bed and gets cancer, making becoming $\it SL$ impossible. This mental state is depicted as a PDG on the left of \Cref{fig:grok-combine}.
		
		\begin{figure}
			\colorlet{colororiginal}{blue!50!black}
			\colorlet{colorsmoking}{orange!80!black}
			\tikzset{hybrid/.style={postaction={draw,colorsmoking,dash pattern= on 5pt off 8pt,dash phase=6.5pt,thick},
				draw=colororiginal,dash pattern= on 5pt off 8pt,thick}}
			\centering
			\scalebox{0.8}{
			\begin{tikzpicture}[thick, draw=colororiginal, text=black]
				\node[dpadded] (C) at (0,0) {$C$};
				\node[dpadded] (T) at (2,0){$T$};
				\node[dpadded] (SL) at (1,-1.5){$\it SL$};
				
				\draw[arr] (T) to[bend right] node[above]{$\mat q$} (C);
				\mergearr{C}{T}{SL}
			\end{tikzpicture}
			\hspace{2em}\vline\hspace{2em}
			\begin{tikzpicture}

				
				\begin{scope}[postaction={draw,colorsmoking,dash pattern= on 3pt off 5pt,dash phase=4pt,thick}]
					
					\node[dpadded,hybrid] (C) at (0,0) {$C$};
					\node[dpadded,hybrid] (T) at (2,0){$T$};
				\end{scope}
				
				\begin{scope}[thick, draw=colororiginal, text=black]
					\node[dpadded] (SL) at (1,-1.5){$\it SL$};
					\draw[arr] (T) to[bend right] node[above]{$\mat q$} (C);
					\mergearr{C}{T}{SL}
				\end{scope}


				\begin{scope}[thick, draw=colorsmoking, text=black]
					\node[dpadded] (S) at (-1.4, 0.8) {$S$};
					\node[dpadded] (SH) at (-1.45, -0.8) {$\mathit{SH}$};
					\draw[arr] (T) to node[fill=white, fill opacity=0.5,text opacity=1]{$\mat p$} (C);
					\mergearr{S}{SH}{C}
				\end{scope}
				
				
				
			\end{tikzpicture}
			}
			
			\caption{\small Grok's prior (left) and combined (right) knowledge}
			\label{fig:grok-combine}
		\end{figure}
		
		Grok is reading about graphical models because she vaguely remembers that the variables in \Cref{ex:smoking}, match the ones she already knows about. When she finishes reading the statistics on smoking and the original study on tanning beds (associated to a cpt $\mat p$ in \Cref{ex:smoking}), but before she has time to reflect, we can represent her (conflicted) knowledge state as the union of the two graphs, depicted graphically on the right of \Cref{fig:grok-combine}. 
	 
		The union of the two \MNs, even with overlapping nodes and is still a \MN. This would not have been possible in general with a BN. In order to represent the two different sources of information (the mother's wisdom and study) regarding the distribution on $T$ given $C$,  we needed to give $\sf M$ two parallel edges. Had we not done this, we would have needed to chose between the two or decide on a middle ground immediately. As we are already allowing inconsistency, merely recording both is much more in keeping with the way we have handled other types of uncertainty.
		%
		%TODO: I should not say this yet. This is a related story that I haven't told yet. 
		%Moreover, if Grok were to later discover that her mother had been faithfully transmitting the results of an unrelated study, she would be justified in increasing her certainty that a cpt roughly like $\mat p$ and $\mat q$ were correct.
		% This suggests a result that is perhaps obvious in retrospect: the mere \emph{possibility} of inconisistency increases the value of consistency. For an agent that is guaranteed to be consistent by design, corroborating evidence has no value. 
	\end{example}
	Modeling inconsistency does not mean giving up on coherence: even though the cpts $\mat p$ and $\mat q$ are different, they are numerically close---not all inconsistencies are equally bad. Quantifying this will be the focus of \Cref{sec:weighted-semantics}. 
	
	
	While a PDG is in some sense merely a set of constraints (the cpts), these constraints themselves have a useful computational meaning. Regarding cpts as stochastic matrices, we can get cpts corresponding to paths by multiplying them; equivalently, thought of as probabilistic functions, we can compose them.
	For instance, in \Cref{ex:grok-union}, if we were to give Grok unconditional probabilities in the form of vectors $\smash{(\vec s, \vec h, \vec t)}$ over the possible values of $\mathit{S, SH}$ and $\mathit T$ respectively, she could compute three distinct estimates for $\mathit{SL}$. This is perhaps clearest visually, but for cliarity, if $\mat S$ is the cpt for the orange hyperedge that computes $C$ from $\mathit{S, SH}$, and $\mat L$ is the cpt for the blue hyper edge, which computes $\mathit{SL}$ from $\mathit{C, T}$, and $[\vec a; \vec b]$ is a vertical stacking of the vectors $\vec a$ and $\vec b$, then
	\[ \mat L \Big[\mat p \vec t; \vec t\ \Big],
		\qquad \mat L \Big[\mat q \vec t; \vec t\ \Big], \quad\text{and}
		\quad \mat L \Big[\mat S \big[\vec s; \vec h\big], \vec t\ \Big]  \]
	will all be probabilistic estimates of $\mathit{SL}$, which can be used in different circumstances: the first two are applicable even if given only $\vec t$, and the last requires all three values.
	This property gives PDGs more useful structure than most collections of constraints. 
	
	%
	% This will important intuition for understanding why PDGs are different from other sets of constraints, though we leave its formal treatment and implications to a forthcoming paper.

	These examples give a taste of the power of \MNs.  In the coming sections, we formalize PDGs and relate them to other approaches.		
% \begin{notfocus}
%	\begin{enumerate}[nosep]
%		\item This representation more naturally matches what humans are aware of, encoding small locally consistent models rather than one giant probability distribution
%		\item It is a strictly more general representation--- we can easily convert BNs to these diagrams (section \ref{sec:convert2bn})
%		\item This allows composition of arrows to be defined, and gives meanings to paths (section \ref{sec:composition}).
%		\item Allowing variables to be added and removed makes
%		\item Changing and partially determining arrows is more reasonable.
%		\item We can now represent inconsistency, which will allow us to capture mental states which, and . While we agree with the classical picture in that inconsistency is bad, now we can talk about it
%	\end{enumerate}


	% Redundency is important: types in programming languages, more data in ML systems.
	% Puts gurads
	% Makes it possible to combine knowledge without destroying old knowledge.
	% preference updating
	
	
	\section{FORMALISM AND SYNTAX}\label{sec:formal+syntax}
	
	Having seen some examples of \MNs, we now provide the formal definitions.\footnotemark
	Although it is possible to formalize the hyperedges directly, we opt for a different approach here, in which they are actually a shorthand for a small widget with an extra node. 
	The following definition therefore only contains ordinary edges with a single head and tail.%
		\footnotetext{Although we call the objects in \cref{def:model} \MNs, they are a specific case of a very similar, but more general representation for which we want to reseve the name; to reduce confusion for future readers, these are technically \emph{strict} \MNs.}
		%this second one is only in the vfull comment.
		\vfullfootnote{In the factor graph literature, especially with regard to loopy belief propagation \parencite{wainwright2007graphical}, it is common to call a collection of marginals that are not necessarily all compatible with a distribution \emph{pseudomarginals}, making a PDG in some sense a collection of `conditional' pseudomarginals. This gives an alternate, more technically precise expansion of PDG as ``Pseudomarginal Dependency Graph''.}

%	\moveme{Rather than representing a probability distribution, PDGs can be thought of as \emph{constraints} on distributions.}
% joe2: I cut this.  What is the point of the example?  Why is it here? What does it tell me that other examples haven't already told me.
% oli2: It is not particularly helpful. It is here because I was told
% many times not to go straight into the definitions; to slow down and
% motivate things, and then once I put this in you stopped mentioning
        % it.
%joe3: you certainly shouldn't go straight into the definitions, and
%you should certainly slow down and motivate things.  I stand by that!
%But you need to have in mind the big picture story.  Every example
%has to carry it's weight.            
	\commentout{Compared to a Bayesian Network, a PDG still consists of a directed graph, and the edges still inform conditional probability tables, but now each edge is interpreted individually. Consider the graph
	\[ A \!\rightarrow\! C \!\leftarrow\! B,\]
	which would be interpreted as three tables $\Pr(C\mid A, B), \Pr(A), \Pr(B)$ in a BN. Interpreting it as a \MNs, there are no distributions on $A$ or $B$, and the arrows into $C$ would be split into two separate tables $\Pr(C \mid A)$ and $\Pr(B \mid A)$, rather than a joint one. }

	\def\mnvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1)}
	\begin{defn}[\MN]\label{def:model}
		A \emph{\modelname} is a tuple $\mnvars[]$ where
		\begin{description}[nosep]
			\item[$\N$] $\notation{:\Set}$~is a finite collection of nodes, corresponding to variables
			
			\item[$\Ed$] $\notation{\subseteq \N \times \N \times \mathit{Label}}$~~is a collection of directed edges, 	each with a source, target, and a (possibly empty) label\notation{$\ell \in \mathit{Label}$}.
%  \footnote{Though likely to be
%  inconsistent, a PDG might admit parallel edges, in which we need to
%  tag edges with a label to identify them, but in most cases, we will
%  use empty labels.} 
%joe2: CUT! graphs don't have ``parallel edges''  Multigraphs (which we are not considering) do except that they're not called parallel edges.
%oli2: I think the footnote was already cut. I know they're called
%multi-graphs. It is commmon to say "graph" when they mean
%"multi-graph" (from a topological perspective, multi-graph is the
%standard object, and called a graph) and to not deal with the edge
%definitions carefully; after being told my formal symbolic
%definitions were too much I was trying here to follow the more
%cavalier style (I would have totally said multi-graph, but
%hyper-graph was so taboo so I just said graph). Unimportant, but I am
%certain that `parallel edges' is a fairly standard term for two edges
%with the same terminals in a multi-graph; Bobby used it in lecture
%yesterday, and a google result reveals tons of these use cases; I
%don't care what we call them though. 
% In any case, I still feel strongly that a multigraph is much more natural here in particular. I know this is not the best use of my time to respond here now, but since we keep clashing on this, and a bunch of my results later on hinge on this definition, I'll try to explain more carefully. Some reasons:
% (1) When combining two graphs that share a link, you want to be able to combine them even if you don't know for sure that the eges are disjoint. Doing otherwise would be structurally enforcing consistency in the way that we are trying to argue against.
% (2) There's a nice special case of resolving parallel edges (of the same temperature): interpreting two parallel edges from 1 in the max entropy semantics is exactly Dempster's rule of combination. I think this is very cool. We decided to leave it out of the abstract b/c it's no longer something people care about as much, but it's true, and not possible to even frame this without parallel edges. There are are other similar cases.
% (3) Composition of directed edges, which I want to talk about all over the place, almost invariably will result in parallel edges. In particular:
% (3.1) When we use inconsistency to modify beliefs later on. In figure 8, $p'$ and $p$ are parallel. 
% (3.2) THe whole story of being able to use "cached beliefs" which we have kind of dropped for this paper but will be important for the discussion of preferences, and is still applicable here, relies on 
% (3.3) We can define ``strong consistency'' (Dexter's suggested term, not mine; this is the alternate notion of inconsistency arising from having different paths?) as the special case where the path multi-graph is just a normal graph
% (4) Many other ideas I haven't presented to you yet require multi-edges to do properly. Doing this leaves open the possibility of keeping belief histories, for example. 
% Who knows if I can motivate the multi-graph to your satisfaction, but I don't want to write a bunch of hacky case work to define things later on, when this _slightly_ more general version gives me so much more. 
%joe3: I don't think that any of the above is going to make it into
%the paper.  If and when we find a need for multigraphs, I'm happy to
%have them, with the appropriate motivation.
			\item[$\V$] $\notation{\N \to \mathbf{Set}}$ associates each node $N \in \N$ with a set $\V(N)$,
			 representing the values that node $N$ can take. 
			\item[$\mat p$] $\notation{\colon \big(\!({A,B,\ell})\colon \! \Ed \big) \to \V(A) \to \Delta\V(B)}$
			% HYPERGRAPH \mat p TYPE: $\colon\!\big(\!({\bf A,B})\colon \! \Ed \big) \to \prod\limits_{A\in \bf A} \!\! \V(A) \to \underline\Delta\left[\prod\limits_{B \in \bf B}\!\!\V(B)\right]$
			  associates, for each edge $L = (X,Y, \ell) \in
                          \Ed$ and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$. 
		\end{description}
	\end{defn}
	If $\sfM$ is a \MN, we reserve the names $\mnvars[^\sfM]$ for its components, so that we may reference one (e.g., $\Ed^\sfM$) without naming them all explicitly; when the choice of $\sfM$ is clear we may omit the subscript.
	We write $\V(\sfM)$ for set of all possible joint settings of the variables in $\sfM$. 

	While the definition above is sufficient to represent the class of all legal \MNs,
	we often use two additional bits of syntax to represent common constraints:  
	\begin{itemize}
		\item A special variable $\sf 1$ that always takes its unique value $\star$. It is used to represent unconditional distributions, as in \Cref{ex:guns-and-floomps,ex:smoking}. 
% For strict \MNs, this is no different from any other variable that can take only a single value. 
%joe3*: I'll take any additidddon compelling examples. I didn't find them
%compelling before
	  \begin{vfull}
		\begin{examplex}\label{ex:worldsonly}
			A probability distribution $p$ over a measurable set $W$ of possible worlds is represented as 
			\begin{center}
				\scalebox{0.8}{
				\begin{tikzpicture}
					\node[dpadded] (1) at (0,0) {$\sf 1$};
					\node[dpadded] (W) at (3,0) {$W$};
					
					\draw[arr] (1) to node[fill=white]{$p$} (W);
				\end{tikzpicture}}
			\end{center}
		\end{examplex}
		\end{vfull}
		\item Double-headed arrows, $A \doubleheadrightarrow B$, which visually indicate the degenerate special case of a cpt that assigns all of its mass to $f(a)$ for each $a \in A$ (corresponding to a deterministic function $f : A \to B$)
	\end{itemize}


We can now explain how we capture the multi-tailed edges that were used in \Crefrange{ex:smoking}{ex:grok-union}.
That notation can be viewed as shorthand for the graph that results by adding a new node at the junction representing the joint value of the nodes at the tails, with projections going back.  For instance,
% the diagram of the PDG in the shaded box of \Cref{subfig:smoking-pdg}
the diagram displaying Grok's prior knowledge in \Cref{ex:grok-union}, on the left of \Cref{fig:grok-combine}
is really shorthand for the following \MN:
	\begin{center}
		\scalebox{0.8}{
		\begin{tikzpicture}
			\node[dpadded] (SL) at (-1.0,0) {$\mathit{SL}$};
			
			\node[dpadded,light pad] (CT) at (-2.9, 0){$\scriptstyle C \times T$};
			\node[dpadded] (C) at (-4.8, -0.6) {$C$};
			\node[dpadded] (T) at (-4.8, 0.6) {$T$};
			
	%				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
	%				\draw[arr, color=violet] (X) -- (S);
	%				\draw[arr, color=violet] (X) -- (C);
	%				\draw[arr, dashed, color=violet] (X) -- (SC);
			
			\draw[arr, ->>] (CT) -- (C);
			\draw[arr, ->>] (CT) -- (T);
			\draw[arr] (CT) -- (SL);
			\draw[arr] (T) to [bend right=90, looseness=2] (C);
	\end{tikzpicture}}
	%%%%%%%%%%%%%%%%%  smoking fragment: %%%%%%%%%%%%%%%%%%%%%%
% 		\scalebox{0.8}{
% 			\begin{tikzpicture}
% 				\node[dpadded] (C) at (-1.0,0) {$C$};
% 				\node[dpadded] (T) at (0.5,0) {$T$};
% 
% 				\node[dpadded,light pad] (SSH) at (-2.9, 0){$\scriptsize \mathit{SH} \times S$};
% 				\node[dpadded] (S) at (-4.8, 0.6) {$S$};
% 				\node[dpadded] (SH) at (-5.0, -0.6) {$\mathit{SH}$};
% 
% %				\node[dpadded, dashed,color=violet] (X) at (6.5,0) {$X$};
% %				\draw[arr, color=violet] (X) -- (S);
% %				\draw[arr, color=violet] (X) -- (C);
% %				\draw[arr, dashed, color=violet] (X) -- (SC);
% 
% 				\draw[arr, ->>] (SSH) -- (S);
% 				\draw[arr, ->>] (SSH) -- (SH);
% 				\draw[arr] (SSH) -- (C);
% 				\draw[arr] (T) -- (C);
% 		\end{tikzpicture}}
	\end{center}
% That is, we inserted a node labeled $SH \times S$ at the junction.  As
% the notation suggests, $\V( \mathit{SH} \times S) = \V(\mathit{SH}) \times \V(S)$.
% The cpt for $(h,s) \in \V(\mathit{SH} \times S)$  associated with 
% the edge from $\mathit{SH} \times S$ to $\mathit{SH}$ gives probability 1 to $h$;
% similarly, the cpt for $(s,c)$  associated with 
% the edge from $ C \times C$ to $S$ gives probability 1 to $s$.
That is, we inserted a node labeled $C \times T$ at the junction.  As
the notation suggests, $\V( C \times T) = \V(C) \times \V(T)$.
%joe2: this is not the time to start talking about matri\mathit{SL}es
%Thus, $\V(S \times \mathit{SL}) = \V(S) \times \V(\mathit{SL})$; the matrix asso\mathit{SL}iated with
For any joint setting $(c,t) \in \V(C \times T)$ of both variables, the cpt for
the edge from $C \times T$ to $C$ gives probability 1 to $c$;
similarly, the cpt for the edge from $ C \times T$ to $T$ gives probability 1 to $t$.

This trick will not work for a BN. We need cpts to be associated with edges, so that the projections do not get in the way of the other information we would like to encode. In this case, we would like to give some probabilistic information on the edge from $T$ to $C$, but if we think of this picture as representing a BN, then $C$ would require a table for joint settings of $(C \times T, T)$. The only reasonable way to provide this cpt is to ignore the second component, and use the value of $c$ determined by $C \times T$---which clearly has no more information than the projection itself.    

\subsection{Weighted PDGs}

We may also consider the edges as having different certainties: if we associate a different coefficient $\beta_L$ to each edge $L$, we can define

\begin{defn}
	A weighted PDG $(\sfM, \beta)$ is a PDG $\sfM$ together with a \emph{certainty} $\beta_L \in \mathbb R^{\geq 0} \cup \{\infty\}$ for each edge $L \in \Ed^\sfM$.
\end{defn}

We an always take a PDG $\sfM$ and uniformly assign every edge the same inverse temperature $\beta_0$; we'll call this $\sfM{[\beta:=\beta_0]}$.

%TODO: say this elsewhere.
% As a sanity check, we can verify that with $\beta = \infty$, corresponding to very low ambient uncertainty, our weighted distribution exactly gives us a point mass on the global free energy minimum, which is the maximum entropy distribution.


\subsection{Operations on PDGs}
As illustrated in \Cref{ex:grok-ablate,ex:grok-union}, it is possible to syntactically combine and restrict PDGs in constant time without additional assumptions. We leave the formal details to \Cref{sec:pdg-operations}.





	\section{SEMANTICS}\label{sec:semantics}
%joe1: Much too wordy, and hides the key points
	% We view of \modelnames\ as being a representation of beliefs in and of themselves, rather than a compression of something more fundamental such as a probability distribution. That said, it is still incredibly important to interpret them in various ways, as this will give us ways of comparing them to other graphical models, prove things about \MNs, and give a lot more intuition about how they work.
%oli2: Your changes are better written, but it does not mention the
%point that I was trying to make: that the constraints themselves,
%rather than the distsributions, should be manipulated. The semantics
        %are just views of the distribuiton.
	There is more than one way of giving semantics to a \MN.  We discuss three approaches below.
	The first is perhaps the most simplest: associating with a PDG a set of distributions, intuitively, the ones that are consistent with it.	This set will be empty if the PDG is inconsistent.
	%
	The second approach associates a PDG with a scoring function, indicating the fit of an arbitray distribution $p$, and can be thought of as a \emph{weighted} set of distributions \parencite[cf.][]{halpern2015weighted}.  For an inconsistent \MN, some distributions may still be better than others.
  %joe3*: Added some sentences here, which incorporates some of your
  %material below.  We need to make that we're using the same notation
  %for a distribution throughout.  We'll probably want to move the
  %discussion of free energies in factor graphs to the factor graph
  %section (since that's where the paper will probably end).  
	Roughly speaking, the larger the changes required to the cpts in a PDG before a distribution $p$ becomes consistent with the PDG, the lower the weight associated with $p$.  There are a number of ways of associating a weight with a distribution that implement this intuition.  We take one approach that turns out to be closely related to the theory of free energies in factor graphs (see \Cref{sec:thermo} for more discussion).
	%oli2: I like this sentence, but I've moved it into the section on weighted distributions.
	%Intuitively, distributions that are consistent with small perturbations of a PDG are better than ones that are consistently only with a large perturbation. 
	%oli2: we've litterally already said this twice already in the introduction. Also this section is also going to be like 1 sentence long because I'm going to define it without symbols, 
	% in any case, this is not the place for comparing to BNs. We've done so much of this already, in the introduction, etc.
	% there's a section later on comparing to existing graphical models. Why are we focusing on BNs so much? 
%joe3: we are focusing on BNs because they're still by far the most
%popular approach in practice.    It's still what's taught in
%introductory AI courses, with good reason.%oli3: what do you mean by friendlier?

  %  \commentout{
	Our third approach chooses a particular distribution from the weighted set of distributions defined by the second 	approach, thus associating with a PDG a unique distribution. As we shall show, it is this third approach that allows us to recover BNs from PDGs. 

      %	Even if it is now intuitive how PDGs can be used to represent beliefs, by providing them with semantics---that is, by viewing them in terms of standard, well-established notions of uncertainty---we will be able to better articulate the relationships between PDGs and other graphical models.
%joe3*: I strongly object to this.  It's like saying that we give
%semantics to Python so that we can compare it to C++..  That's not
%the primary motivation!  We give semantics so that the objects we use
%(PDGs) have a clear and unambiguous meaning (or more than one such).
	%oli2: I originally had adopted your phrase "There is more than one way to do this", but decided that this is misleading: for a given target (distributions, sets of distributions, etc.), we only provide one, and indeed think that our constructions are natural. 
%joe3: I strongly disagee with the next sentence.  Does that mean that
%if we had decided that we wanted PDGs to represent a single
%probability that our semantics that associates with a PDG a set of
%PDGs would be in appropriate?   There's nothing intrinsic about a BN
%or a PDG that says its semantics has to be a unique probablity (or  a
%set ofprobabilities)
%	Because a BN or Factor Graph is designed to specify a joint
%        distribution, its unique semantics as a distribution is
%        appropriate. The same is not true of \MNs.  
%joe3: this is not the least bit analogous to the approach taken by
%BNs  It is a way of getting a unique distribution that's very
%different in spirit from the way the BNs lead to a unique
%distribution.  It's quite surprising (at least to me) that they're
%equivalent.  
        %	We provide an analogous interpretation
%        (\Cref{sec:maxent-semantics}), allowing us to view PDGs as
%        distributions (recovering BN semantics as promised), but this
%        approach does not adequately capture the inconsistency or
%        modularity we wanted to model.  
	%
	%oli2: I want to replace the name "weighted (set of)
	%distribution(s)"; the current term emphasizes the SET of%oli3: what do you mean by friendlier?

	%distributions invovled, each with a weight (though in fact,
	%we have a weight for ANY distribution, and we're suggesting
	%somethign closer to computing a goodness of fit than looking
	%up a weight). While equivalent and we should still make the
	%connection, we might be better off using something like
	%"distribution scoring", though that is not perfect
	%either. Adopting the shorter verison of the original
	%"Weighted distribution" might be ok as well. Do you have
	%thoughts?
%joe3: Well, for better or for worse, you can't change history.  We've
%already given it a name.  I'm perfecty happy with the notion of a
%weighed distibution, but the semantics doesn't consider a single
%weighted distribution, but a set of them.  
%joe3: your comment below captures my problem with this exactly.
%	Ultimately, we find (\Cref{sec:weighted-semantics}) that a
%        \emph{weighted distribution}
%        \parencite[cf.][]{halpern2015weighted} much better captures
%        these features; furthermore, as we will see, the weighted
%        distribution semantics is closely related to a configuration
%        energy, which in \Cref{sec:thermo} we compare with the already
%        well-developed theory of free energies in factor graphs.  

\subsection{\MN S AS SETS OF DISTRIBUTIONS}\label{sec:set-of-distribution-semantics} 
	%oli2: removed waffling.
%joe3: I really don't like this.  That's not how I think of a PDG. I
%don't thinking that bringing in BNs is helpful.  A PDG hs no
%independence assumptions.  I think that thi muddies the waters.
%oli3: I don't see why what you wrote above would be problematic for the  bit that I have... indeed, PDGs have no assumptions.  
	% If we think of a PDG as BN except without independence assumptions, then just as a BN represents a single distribution on joint settings of its variables, a PDG might be thought of as representing the  \emph{set of all distributions} that are compatible with the given tables, with or without the independence assumptions.
	% If we had a (quantitative) BN is of a DAG $G$ labeled by variables, and a assingment $f$ of cpts to variables, we could obtain a qualitative one by simply looking at the graph structure, which corresponds to a collection of independence assumptions. There is then a set of all distributions that have those indepies. Alternatively, we could just take the assignments $f$ at the nodes. 
	
	We have been thinking of a PDG as a collection of constraints on distributions, specified by matching cpts. From this perspective, the first natural thing to consider is the set of all distributions that match.
	
		% As the data of PDG consists of a collection of cpts, the natural thing to do . 

	\begin{defn} \label{def:set-semantics} %oli2: made words better. 
		If $\sfM\sheq\mnvars[]$ is a \MN, let $\bbr\sfM\SD$ be
%joe3
%                the set of distributions over the variables in $\sfM$
                the \emph{s}et of \emph{d}istributions over the
                variables in $\sfM$ 
%joe3
%                that whose conditional marginals on every edge are
                for which the conditional probabilities are exactly 
                those given by $\boldsymbol\mu$.
%joe3: added a bit of formality
%oli4: there are labels on edges again 
          That is, $p \in \bbr\sfM\SD$ iff, for all edges $L = (X,Y, \ell) \in
          \Ed$,  $x \in \V(X)$,  and $y \in \V(Y)$, we have that
          $p(Y\sheq y \mid X\sheq x)$ is the $x,y$ entry in the cpt given by
          $\bp$.
%oli4: I don't understand how this easier to read than my definition below, and and it's way less visually distinct and intentionally typeset. But I'll keep it.    
		\notation{Formally,		
		\[ \Big\llbracket \sfM \Big\rrbracket\SD \hspace{-2ex} := \!\left\{\mu \!\in\! \Delta \V_\none (\sfM) \middle|\!
		\begin{array}{l}
		\mu(B\!\! =\!\!b \mid A\!\!=\!\!a) \geq \boldsymbol\mu_L(b \mid a) \\[0.1em]
		~\text{$\forall (A, B,\ell) \!\in\! \Ed$, $a \!\in\!\mathcal V_A$, $b \!\in\! \mathcal V_B$} \end{array}\!\!\! \right\}\]
		}
	\end{defn}

	As we said earlier, we say that the PDG $\sfM$ is \emph{consistent} if
	$\bbr\sfM\SD \ne \emptyset$, and consistent otherwise.
	
	
	\begin{vfull}
		Note that being inconsistent is not the same things as \emph{over-constrained}: 	
		\begin{defn}

			$\sfM = \mnvars[]$ is over-constrained if there exists
			  \emph{some $\mat p'$} assigning cpts to the same edges as
			  $\mat p$, such that $(\N, \Ed, \V, \mat p')$ is inconsistent
			  \notation{(i.e., $\bbr{\N^\sfM, \Ed^\sfM, \V^\sfM, \mat p}\SD
				= \emptyset$)}, and under-constrained if there are
			  multiple distributions in $(\N, \Ed, \V, \mat p')$ for
			  \emph{every such $\mat p'$}, making this a property of the
			  qualitative PDG $(\N, \Ed, \V)$.  
		\end{defn}

		We know that an under-constrained PDG is consistent without even looking at the tables. However if a we know that an \emph{over-constrained} PDG is actually consistent (when it could have easily contradicted itself), the information provides corroborating evidence, and one can take this as support in favor of the beliefs. 
	\end{vfull}

%joe3        
        %	\subsection{\MN S AS WEIGHTED
       	\subsection{PDGS AS DISTRIBUTION SCORINGS} \label{sec:weighted-semantics} 
%oli3: It's still not a set of distributions with a weight. 
%	If a probability distribution is a soft constraint on the set
%of possible worlds, then a PDG is a soft constraint on the set of
%probability distributions. 

	\subsubsection{Continuous Inconsistency}
        
    The semantics of \Cref{def:set-semantics} treats all inconsistent PDGs as equivalent, which is unsatisfying. For instance, in \Cref{ex:grok-union}, the two cpts $\bf p$ and $\bf q$ were close but did not quite agree. To address this, we generalize the semantics presented in \Cref{sec:set-of-distribution-semantics}. For a candidate distribution $\mu$, rather than assigning it an all-or-nothing score of whether or not it is consistent, we give it a continuous one, encoding how far we have to nudge our tables to make our PDG consistent. Intuitively, distributions that would be consistent with only small perturbations of the cpts $\mat p$ are better those that would require larger ones.  
 	%joe1*: I don't understand this.  I don't know what it means
 	%to ``express'' and edge. and ``similarity'' comes out of the
 	%blue.  You need to *slow down* here, perhaps giving an
 	%example.  You can't suddenly talk about bits to encode
 	%things, when it's not clear what needs to be encoded.  The
 	%rest of this section needs to be completely rewritten.  We
 	%don't need the tecnical results; we need intuition.  There
 	%should be no talk about temperature.  
 	%oli1: expressing an edge is just having the associated
        %table. I changed it, but this seems clear to me... I feel
        %like unless I do it almost entirely sybmolically this will be
        %a valid complaint.
%joe3*: It would be good to have an example of degrees of
%inconsistency here, before we give them a score.  One of the
%xamles from the intro would work, with variants of the cpt making it
%more or less consistent.

	We measure the magnitude of this perturbation with relative entropy, which results in interesting connections to other measures of uncertinty. In particular, for each link $L = (X,Y, \ell)$, and each $x \in \V(X)$, we measure the divergence from $\bp$ to $\mu_Y \mid X=x$ (the marginal $\mu_Y$ of $\mu$, conditioned on $X=x$),
	 % which is intuitively the expected number of extra bits required to transmit a code optimized for $\mu$ if using the cpt $\bp$ instead. 
	We weight these costs by the probabilities $\mu(X=x)$, and sum across all links to get following definition:
        
%joe3 	
	% 	We are missing two important details we need to make this work. 
	% 	First, we want to score distributions based on exactly the
	%        marginals for the edges $\Ed$ that an agent claims to know
	%        something about.  
	% 	Second, we measure distance by relative entropy.
%oli4: this is not correct, we don't compare this way.
	% There are many ways to capture this intuition.  We consider a
	% particular approach that has interesting connections to other
	% representations  of uncertainty.  The idea is to measure the distance
	% between two distributions using relative entropy
	% Specifically, we
	% compare two PDGs $\sfM_1 = (\N, \Ed, \V, \mat p)$ and $\sfM_1 = (\N,
	% \Ed, \V, \mat p')$ 
	% that have the same set of nodes, ranges of the nodes,
	% and edges, but may differ in the function associating cpts with
	% edges.  For each edge $L \in \Ed$ and each value $x \in \V(X)$, we
	% compute the entropy of $\bp(x)$ relative to
	% $\bp'(x)$; then we sum over all $x \in \V(X)$ and all edges
	% $L$. 

	\begin{defn}\label{def:zeta-score}
%joe3: can we use a friendlier letter than \zeta
%oli3: what do you mean by friendlier?
          %	  The inconsistency, $\zeta$, of a PDG $\sfM =
       	  The \emph{inconsistency} of a PDG $\sfM = \mnvars[]$ with respect to a distribution $\mu \in \Delta[\V(\sfM)]$ is 
		\[
			\zeta(M ; \mu) := %\inf_{p \in \Delta(W^{\mathcal V})}~
			\!\!\!\!\!\sum_{X \xrightarrow{\!\!L} Y}\!\! \mathop{\mathbb E}_{x \sim \mu_{_X}} \left[\kldiv[\Big]{ \mu_Y(\cdot | X \sheq x) }{\bp(x) } \right]
		\]

%oli2: I suppose I also need to define relative entropy?
%joe3: yes
		where $\thickD$ is the relative entropy.
%oli2: I cut this description, as I did a lot of explaining above.
%		taken between the two distributions over $B$: one given by the edge from $\bmu_{A,B}(a)$ and the other given by the marginal distribution of $p$ conditioned on $A = a$, over the variable $B$. 
                %joe3: moved back from below; this is where it belongs:
%joe3: you're inconsistent between using M and \sfM for a PDG.
%Fortunately, it's hard to distnguish them.  But you should still be
%consistent.  
        The \emph{inconsistency of PDG $\sfM = \mnvars[]$} is the minimum over all distributions $p$ of the inconsistency of $\sfM$ with respect to $p$:
%joe3: you're switching between \mu and p again
%		\[ \zeta(M) = \inf_{ \mu \in \Delta [W_{\cal V}]}
%\zeta (M; \mu) . 
		\[ \zeta(M) = \inf_{\mu \in \Delta [\V(\sfM)]} \zeta (M; \mu) . \]
        \end{defn}

%joe3: Sorry; I can't follow this intuition at all.  More comments below
 	We want to minimize the total number of bits required to
        encode all of the relevant conditional marginals of some
        distribution $\mu$.  
 	More precisely, fix a distribution $\mu$. For each edge $L = (X, Y,
%joe3: Where is the optimal code that we are supposedly given?
        \ell) \in \Ed$ and $x \in \V(X)$ we are given an optimal code
        for $Y$ drawn the distribution $\bmu_{X,Y,\ell}(x)$, and asked to transmit
        data from $p_Y( \cdot \mid x)$.
%joe3: I don't understand the following sentence at all
%oli4: modified.
       To obtain the total cost for $L$, we take a weighted average of these costs, where the weight for the value $x$ is the probability $\mu_X(x)$.
 		
	For even more intuition, imagine two agents with identical beliefs $\sfM$, about a set of variables that are in fact distributed according to $\mu$. For each edge $L = (X,Y, \ell) \in \Ed^\sfM$, values $x,y \in \V(X)$ is chosen according to $\mu_{_{XY}}$ and $x$ given to both agents. $\zeta(\sfM;\mu)$ is the total number of expected bits required to communicate $y$ with a code optimized for $\bp$ instead of the true conditional distribution $\mu_Y|(X=x)$.
	If $\sfM$ is inconsistent, then there will be a cost no matter which distribution $\mu$ is used to generate the data.
	Conversely, if $\sfM$ is consistent, then any distribution $\mu \in \bbr{\sfM}\SD$ will have $\zeta(\sfM; \mu) = 0$. 

	
	\subsubsection{Extra Information}
	While $\bbr{\sfM}\SD$ and $\zeta(\sfM;-)$ distinguish only between distributions based on their compatibility with $\sfM$, but even among distributions consistent with a set of constraints, some seem more natural than others. In particular, there is already a rich literature of chosing distributions with maximum entropy \parencite{jaynes1957information, wainwright2008graphical} consistent with constraints, in part because they intuitively they intuitively ``have the least additional information'' beyond those given in the constraints. 
	
	One might hope that this distribution would be the one described by a Bayseian Net. Unfortunately, the maximum entropy distribution associated with the cpts of a BN does not have the right conditional independences, which has resulted in a great deal of work \cite{williamson2001foundations}\cite{holmes2001independence} in modeling BNs as maximum entropy solutions to an expanded set of constraints, which include additional causal information.
	
	We have discovered that there is an alternate way to recover the distribution corresponding to a BN, which is more in the spirit of minimizing the amount of additional information, and does not requrire any additional assumptions about independences. 
	The key insight is that the information in a cpt itself might not be fixed, which entropy maximization alone will exploit, which we have to control for.
	%
	In more detail, cpts contain counterfactual information: a cpt of $Y$ given $X$ not only tells you the distribution of $Y$ for the distribution on $X$ that is actualized, but also for arbitrary distributions on $X$. As a result, one can maximize entropy not only by selecting a distribution which makes things as independent as possible, but also by simply putting extra mass on rows of the cpt which already prescribe more uncertainty. We therefore define the extra information below.
	
	\begin{defn}
		The \emph{extra information}, $\H^{\sfM}$, given by a distribution $p$ that satisfies the constraints of a PDG $\sfM = \mnvars[]$ is given by
		\[ \H^{\sfM}(p) := \sum\alle \Big[ \E_{x \sim p_X}  \H (\bp (x))  \Big] - \H(p) \] 
		where $p_X$ is the marginal of $p$ on the variable $X$, and $\H(\bp(x))$ is the entropy of the distribution on $Y$ specified by the cpt $\bp$ when $X = x$. 
	\end{defn}
	% The cpts may not (in the entropy sense) 
	We can think of the extra information as the sum of the entropies that \emph{actually} result from each table, in the context of distribution $p$, minus the total entropy of the distribution. Alternatively, we can think of the negation of the extra information, $-\H^\sfM(p)$ as the information in $p$, which has not already been specified by the cpts in $\sf M$. We will show in \Cref{sec:bn-convert} the distribution with minimal extra information with respect to the cpts of a BN $\cal B$ is the unique one specified by $\cal B$.
	\begin{vfull}
		One might also recognize the extra information as having the form of a free energy; we explore this connection in \Cref{sec:thermo}. 
	\end{vfull}

	
	
	



%We might want to go even further, and distinguish among distributions
%that are consistent with $\sfM$.
%We do this using the entropy function:
%%joe3: note that here you're using \mu for a probability, whereas
%%before you used only p.  Also writing entropy in the more standard way
%$$H(\mu) =-\sum_{x \in \mathrm{Supp}(\mu)} \mu(x) \log(\mu(x)).$$ 
%%joe3*: Can you give some intuition for why this is reasonable?
%%oli3: yeah. Here are several arguments:
%% (1) if the constraints are all you know, and you choose another distribution, you're somehow claiming to know more than you do. This is the maximum entropy distribution associated with some other constraints. 
%% (2) Specifying the wrong distribution is costly --- but much cheaper if you specified a uniform distribution. For this reason, the relative entropy from uniform to any other distribution is cheaper than the relative entropy to go back: the maximum entropy distribution is the most adaptable, paying the smallest price to specialize, where price is the expected surprise, = log (1 / p), related to energy of the associated Boltzmann distribution.
%%oli3*: More generally, I do not think it's worth the space to do any motivation like this at all. I could do it, and it would make me a better person, but it takes a lot of time, will require a lot of energy from readers, and it's a very standard thing. My guess is at least half are already on board with maximizing entropy, and providing a remedial introduction to such a deep topic in a 9-page abstract is not a good use of space. I also imagine that it will come off as patronizing to those who know what they're doing in the context of an original research paper. 
%We think of distributions with higher entropy as being ``better''.
%%joe3: reorganizing what you wrote.  
%Since we want to minimize inconsistency and maximize entropy, we
%subtract one from the other (with relative weighting $\alpha$), to get
%a score $\mathcal U_\alpha(\sfM; p)$:
%	\begin{equation}
%		\mathcal U_\alpha(\sfM; p) := \zeta(\sfM;p) - \alpha
%                H(p). \label{eq:freeenergy-weighted} 
%	\end{equation} 
%joe3: This may be true, but why say it?        
%	Thought of this way, in specifying a \MN, a modeler has not
%        only specified a a distribution, but also a higher-order
%        object, that scores all distributions.  
%oli3: because mixing up the levels is problematic. Maybe it doesn't need to be said.
%joe3: why does this mean someting is wrong?
%oli3: rewording and uncommenting.
% $\zeta$ provides us with a meaningful continuous score, but as
% a semantics for PDGs, there's still something missing: $\zeta$ can
% only distinguish between those distributions $p$ that are
%         \emph{inconsistent} with $\sfM$.
% 

    \subsection{Putting The Scores Together}
	Both the extra information $\H^\sfM$ and 
	
	We therefore combine the
        two. Since we want to minimize inconsistency and maximize
        entropy, we subtract one from the other.  We also multiply the
        entropy by a weight $\alpha \ge 0$, to indicate how important
        maximizing entropy is relative to minimizing inconsistency.
        This gives us a score for a $\mathcal U(\sfM; p)$ 
        follows: 
	\begin{equation}
		\mathcal U_\alpha(\sfM; p) := \zeta(\sfM;p) - \alpha H(p) \label{eq:freeenergy-weighted}
	\end{equation}
	Note that with this scoring function, a smaller score for a distribution means that is better.

%joe3: I don't understand what ``ambient background uncertainty''
%means, or why more background uncertainty should mean that we weight
%the entropy factor more.  This is presumably bound up with
%someintuitions that I haven't got  (and you haven't shared with the reader).
%oli3: Imagine: probability \alpha that you got really unlucky with what you saw and are just totally wrong about how the world works. Your subjective assesment of the extent of your unknown unknowns. Alternatively, analogous to the probability \delta of a total disaster in a Chernoff or Hoeffding bound.
%This modification can be thought of as modeling ambient background
%uncertainty at level $\alpha$.  
%joe3*: why on earth should this be true?  Why should a more uniform
%distribution be better than a less uniform one just because I think
%the world may have changed?
%oli3: because if you know nothing, maximum entropy is cheapest. I'm certain this has been motivated a lot in information theory, but I don't know the best place to find a convincing reference. I was told to look up Nima Anari's white paper on maximum entropy programs and why we bother, but I haven't done this yet. Again, I don't think it's our job to motivate this, and none of the other UAI papers do. It has been motivated before. 
%If you cannot trust your own beliefs or think the world may have
%changed a lot since you formed them, higher-entropy distributions are
%better fits.	 
%joe3
%For $\alpha = 0$, we score purely on consistency.
%joe3: this statement is technically false.  I cut it.  It would be
%true if \alpha where infitesimal, but we don't want to go there.  I
%suspect ou mean ``increasing consistency'', not ``reducing
%consistency''.  It's ust that more consistency makes \zeta smaller
%oli3: I want this case to be the default. I meant "reducing inconsistency / \zeta"
%A very small $\alpha \in O(\epsilon)$ corresponds to a lexicographical
%ordering, in which 
%reducing consistency is most important, and ties are broken by
%maximizing entropy.  
As $\alpha$ gets larger, we start to be willing to trade some
%joe3: I simply don't understand why you would hedge with a more
%uncertain guess
%inconsistency for the safety of hedging with a more uncertain guess,
inconsistency for an increase in entropy.
%joe3: I see no reason to identify more uncrtain with greater
%entropy.  Why is an agent who is certain that a coin is far more
%uncertain than one who thinks that the probability of heads is 1/3.
%You have a certain intuition (which I realize is shared by some
%others, but definitely not me) for entropy.  You need to share it
%with the reader if you're going to say things like this.
%and distributions with lots of uncertainty become increasingly
%appealing; as $\alpha \to \infty$, the best distribution approaches
%the uniform one.
%joe3*: I would strongly prefer not to think of negative values of
%alpha.  You don't use them anywhere, as near as I can tell.  Why
%distract the reader with them?  I also don't understand this
%intuition at all.
%oli3: I haven't used them yet, so I'll leave them uncommented.
% I'll try to share my intuition for now, so you have a head start when I explain it wrong later: they correspond to cases where there's the exact opposite incentive: when the loss is about the expectation of the micro-state, rather than getting the distribution right and minimizing surprise / coding errors.
% For instance, the optimal Bayes classifier, which guesses a binary label that has the highest probability, is an instance of a setting where $\alpha$ is negative. I suspect this will come up a lot in the paper I want to write next on decision theory.
% However, this setting is very non-convex, and so we won't be able to prove much about it with optimization.
%Negative values of alpha, intuitively corresponding
%to a belief that the world is extremely ordered, are also
%psychologically plausible.  
	
%	if $\zeta(\sfM; p)$ is the additional information of communicating $p$ with knowledge $\sfM$, and $H(p)$ is the amount of information in $p$, then $\mathcal U_\alpha(\sfM; p)$ is the information required to communicate $p$ given a code optimized for the marginals of $\sfM$, 
	

	
	\begin{example}[continues=ex:worldsonly]
		Recall our simplest example, which directly encodes an entire distribution $\mu$ over the set $W$. 
		In this case, there is only one link, the expectation
                is over a single element, and the marginal on $W$ is
%joe3: now you're using both \mu and p to represent distribution.
%This is really bad notation.  Pick one and stick with it (and its variants)
                the entire distribution. Therefore, $\zeta(\sfM; p) =
%joe3: sorry; I don't understand the intuition of the information
%required to turn on distribution into another.  I don't understand
%how information can affect distributions.
%oli3: I'm again trying to provide intuition about the coding theory! I'll try again.
%                \kldiv{\mu}{p}$, and so the inconsistency is just the
%               information required to turn $\mu$ into $p$, minimized at
%                $p^* = \mu$. Furthermore, for $\alpha > 0$,
                \kldiv{\mu}{p}$, so the inconsistency is just the information $\mu$ and $p$, so is minimized when $\mu$ is $p$.
%joe3: I couldn't parse this; to the extent that I could parse it, I
%couldn't understand it.  I cut it.  If you want to reinstate it, you
%have to explain it better.
% Furthermore, if $\alpha > 0$,
%                $\mathcal U_\alpha (\sfM; p)$ may be minimized
%                somewhere more entropic than $\mu$, intuitively
%                corresponding to the idea that if your environment is
%                chaotic, a more uncertain distribution may fit reality
%                better than the beliefs you actually hold. 
	\end{example}

%joe3*: You said that there was a different motivation for this
%transformation.  You should briefly explain it here.  Even if the
%details are in Section 5, you should give a rough intuition here.  If
%you're going to normalize, you should do that here too, and motivate
%it.  If you need to do some normalization, this should be done and discussed
Note that $U_\alpha(\sfM; p)$ can be anywhere between $-\infty$ and
$\infty$.  It is useful to transform it so that it is always
non-negative.
Let $V_{\alpha,\beta}(\sfM;p) = \exp(-\beta U_\alpha(\sfM;p) ) $.  Here
  $\beta$ is a decay parameter,
%joe3*: I don't understand this intuition at all
  % This class of functions has a single decay rate parameter $\beta$
  which intuitively corresponds to a strength of
        commitment to a particular distribution. 
With the score given by $V_{\alpha,\beta}$, a bigger score denotes a
``better'' distribution.  Since relative entropy is always
non-negative, if $\alpha=0$, we must have that $V_{\alpha,\beta} \in
  [0,1]$.  Thus, $V_{0,\beta}$ acts like a weight in the sense of
  Halpern and Leung \citeyear{halpern2015weighted}.

%oli3: I don't think this is an important distinction. The semantics
% can be thought of a single object which is a function 
% (\alpha, \beta) \to \Delta W \to \mathbb R
%We can now define the second semantics, which is really a family of semantics:
We can now define the second semantics, parameterized by $\alpha,\beta$:
$$\bbr{\sfM}_{\alpha,\beta} = \{(p, V_{\alpha,\beta}(M;p)): p \in
\Delta(\times_{X \in \N} \V(X))\}.$$
%oli3: This is a counter-productive way of phrasing it, because we're scoring
% ALL distributions, and technically just gives the definition of a function...
%Thus is, the elements of $\bbr{\sfM}_{\alpha,\beta}$ are pairs $(p,
%w(p))$ consisting of a probability distribution and its weight.
% I also don't think we should replace it with "thus it is a function... " because
% this is clear from the notation and definition.

%joe3: said it above
\commentout{
	For reasons that we will explain carefully in
        \Cref{sec:thermo}, we do this with an exponential decay.  
	This class of functions has a single decay rate parameter $\beta$ which intuitively corresponds to a strength of commitment to a particular distribution.
	%	(To avoid confusion, one could fix $\beta = 1$ for now.)
	%
	\begin{equation}
		\bbr{\sfM}_{\alpha,\beta} := \mu~\mapsto~\exp \big\{ -\beta \cdot \mathcal U_\alpha(\sfM; \mu) \big\} \label{eqn:weighted-semantics}
	\end{equation}
	
	For $\alpha = 0$ the maximum value of $\bbr{\sfM}$ is 1. It is always possible to re-normalize so that the maximum is 1, but this discards some consistency information, as factor graphs do.
}
%joe3: \end{commentout}
	
%	Inconsistency is something we try to minimize, making it analogous to a physical system trying to minimize its potential energy. If we think of the inconsistency as a potential energy directly, we can use something like a Boltzmann law (\Cref{eq:boltzmann}) to get likelihood over distributions. We therefore define a weighted distribution by

%	One nice consequence of this definition is that the maximum value of $\bbr{M}_\zeta(\mu)$ is $1$, in keeping with the standard on weighted distributions \parencite{halpern2015weighted}; indeed all consistent distributions have weight 1. Still, there's something missing: we've now captured inconsistency

        %joe3*: This is not a story!  You can't just write ``Here are
        %3 results that you might be interested in''.  
        %oli3*: Why not, in general? We'll move on to something else. Honestly this seems kind of optimal from a reader's perspective:
        % We don't want to explain things that are out of the way of the story. We'll need to use them, so they need to be in this paper, but they're not the most exciting. Some people will get something out of knowing these results, and I don't want to just hide them away. I'm trying to explicitly avoid talking too much about these because they're dry, but I know some people will benefit. Because of the organization, they're VERY easy to skip over visually. I highly doubt this will get in anyone's way of the story; it's like a list of references at the end of a chapter of the book.
        %The first
        %``result'' is a definition, and it should appear earlier.  (I
        %moved it back.)  The second is not part of the current story
        %(and belongs in a separate section in any case); the third is
        %misplaced and should be part of the result that needs it

%\commentout
{        
	We now offer generalizations of some results found in in
        \Cref{sec:set-of-distribution-semantics}:
        \Cref{def:cont-inconsist} is the continuous version of
        inconsistency we were searching
        for. \Cref{prop:union-weight-semantics} shows that the
        weighted semantics $\bbr{-}$ also has the modularity
        properties we wanted, and \Cref{thm:zetaconvex} will let us
        define concrete distributions just like its more specific
        counterpart. 
	
	\begin{defn}\label{def:cont-inconsist}
		The inconsistency of a PDG $M = (\cal N, L, V, \bmu)$ is the minimum value that could be achieved with any test distribution $p$; that is, 
		\[ \zeta(M) = \inf_{ \mu \in \Delta \V(\sfM)} \zeta (M; \mu) 
		%			= -\log \sup_{p \in \Delta[W_{\cal V}]} \bbr{M}_\zeta (p)
		\]		
	\end{defn}
	%	\Cref{prop:union-weight-semantics} shows that the weighted distributions also have the modularity properties we're interested in.
	\begin{prop}[name=\Cref{prop:union-set-semantics} analog]\label{prop:union-weight-semantics}
		$\bbr{\sfM \cup \sfM'} = \bbr{\sfM} + \bbr{\sfM'}$
	\end{prop}
	\begin{lemma}[name=\Cref{prop:convex} analog, restate=thmzetaconvex]\label{thm:zetaconvex}
		$\zeta$ is a convex function. % in $p$.
	\end{lemma}
	\begin{coro}\label{cor:u-convex}
		$\mathcal{U}_\alpha(\sfM; p)$ is convex in $p$ for $\alpha \geq 0$, and strictly so for $\alpha> 0$. Furthermore, $\bbr{-}$ is quasiconvex---that is, all of its level sets are convex sets.
	\end{coro}

	As a result of \Cref{cor:u-convex}, $\bbr{M}$ is a quasiconvex function 
	%, and hence every local minimum is a global minimum. 
	In particular, \Cref{prop:convex} is the special case for the level set $\mathcal U \leq 0$ for $\alpha = 0$. 
	
%        \subsection{\MN S AS MAX ENTROPY
%DISTRIBUTIONS} \label{sec:maxent-semantics} 
}
%joe3: \end{commentout}
	
%	\moveme{
%	Instead, we leave the picture alone, and tackle the independence by using our maximum entropy semantics: the distribution encoded by the BN is the maximum entropy one encoded by the \MN.}

	The semantics given in \Cref{sec:set-of-distribution-semantics} does not allow us to express independencies very effectively.
	For instance, in \Cref{ex:smoking}, to fully get the joint representation given by the BN, we also need to somehow assume that $SC \CI S \mid PS$. This is possible to do explicitly with an extra arrow meaningless arrow, but this solution doe not scale well, looks like it dependence (the opposite of our intention), and clutters the diagram. 
	We now show that by applying the principal of maximum entropy, we can recover all of a BN's independence assumptions at once. This should make intuitive sense: maximizing entropy tend to `make things as independent as possible'. 
	Those familiar with the view of graphical models as separable exponential families may have even seen a similar result in an undirected setting e.g., \parencite[cf.][pp. 37-39]{wainwright2008graphical}. 
	We proceed with the theorem, and discuss some consequences below.

\begin{vfull}
		%	\[ \Big\llbracket M \Big\rrbracket^{\ell}(\mu) := \left\{\mu \in \Delta\left[~\prod_{N \in \mathcal N}\mathcal V(N)\right] ~\middle|~
		%	\begin{array}{l}
		%	\ell(\mu) \succeq \ell(\mu') \\[0.2em]
		%	\qquad\text{for all $\mu' \in \Delta \prod \mathcal V$} \end{array} \right\}\]
		
		
		
		%	When ultimately pressed on a decision, an agent has incentives to temporarily act as though their beliefs were simply a distribution. Via maximum-entropy semantics,
		
		\begin{defn}
			If  $\sfM$ is a \MN, $\bbr{\sfM}_{\mathbf S} \subseteq \Delta \V(\sfM)$ is a set of distributions, $(D, \preceq)$ is an ordered set, and $\ell : \Delta W_{\sfM} \to \mathbb (D, \leq)$ is a scoring function for probabilities, let \emph{the upper $\leq$-frontier under $\ell$}, denoted $\bbr{\sfM}_{\mathbf S}^\ell$, be the set of distributions that are not strictly dominated by any others. Explicitly,
			
			\begin{equation*}
				\bbr{\sfM}_{\mathbf S}^\ell =  \left\{\mu \in \llbracket M \rrbracket_{\mathbf S} ~\Big|~ \forall \mu' \in \bbr{\sfM}_{\mathbf S}.~ \ell(\mu') \preceq \ell(\mu)  \right\} \\
			\end{equation*}
		\end{defn}
\end{vfull}

	\begin{theorem}\label{thm:uniqmaxemnt}
		If $\sfM$ is a consistent \MN, then there is a unique $\mu \in \bbr\sfM\SD$ that maximizes the Shannon entropy, 
		$$H(\mu) = \!\!\!\sum_{x \in \mathrm{Supp}(\mu)}\!\!\! \mu(x) \log \frac{1}{\mu(x)},$$
		where $\mathrm{Supp}(\mu\notation[)]{ : \Delta W) = W}$ is the support of $\mu$.
	\end{theorem}
	\begin{proof}
		\vspace{-1em}
		This is a direct consequence of \Cref{prop:convex} and the fact that entropy is strictly concave.
	\end{proof}	

	\begin{defn}
		If $\sfM$ is a consistent \MN, let $\bbr{\sfM}\MaxEnt$ be the unique distribution $\mu \in \bbr{\sfM}\SD$ of maximum entropy.
	\end{defn}


	% we can motivate composition here!
	\begin{example}[composition]
		Consider a slight alteration of \Cref{ex:sd-compose-unconditional} in which $\sf A$, which had an unconditional distribution on $X$, is replaced with $\mathsf A' := Z \xrightarrow{p'} X$, representing a distribution conditioned on $Z$. 
		As before,
		\[ \bbr[\Big]{{\sf A' \cup B}}\SD = \bbr[\Big]{{\mathsf Z} \xrightarrow{p} X \xrightarrow{q} Y}\SD \]
		Suppose we are interested in the conditional marginal of $Y$ given $Z$. In this case, $\bbr{{\mathsf Z} \xrightarrow{p} X \xrightarrow{q} Y}\SD $ contains distributions $\mu$ with varying of $\mu(y \mid z)$, and so we can no longer conclude anything uniformly about this conditional marginal for all distributions in $\bbr{{\sf A' \cup B}}\SD$. 
		
		Still, we can get an estimate of this quantity using the maximum entropy semantics, which conveniently turns out to be the composition of $p$ and $q$ as probabilistic functions.
		$$ \bbr{{\sf A \cup B}}\MaxEnt(y \mid z) = \sum_{x \in \V(X)}\!\! p (x \mid z)\ q(y \mid x) = q \circ p $$
		We claim more is true: if $\sfM$ is \emph{any} PDG that is not over-constrained, and with $\sfM \supseteq \sf A \cup B$, i.e., containing $\sf A \cup B$ as a sub-graph, then
		$ \bbr{\sfM}\MaxEnt(y \mid z) = q \circ p$,
		suggesting that this composition is in some sense the best guess we have for the conditional marginal. 
		This can be verified directly, but we will instead prove the more general result in our next result (\Cref{thm:maxent-hull}.).
		%
	\end{example}

	The semantics $\bbr{-}\MaxEnt$ presented here and \Cref{thm:uniqmaxemnt} are all the tools we need to show that PDGs subsume BNs, but this semantics is still lacking something: it only works when the distribution is consistent.
	We return to the idea of using a PDG to represent a unique distribution in \Cref{sec:uniq-dist-semantics-2}, after having developed our final semantics. 



%joe3
%        \subsection{UNIQUE DISTRIBUTIONS,
        %          AGAIN}\label{sec:uniq-dist-semantics-2}
    \subsection{PDGS AS UNIQUE DISTRIBUTIONS}\label{sec:uniq-dist-semantics-2} 

%joe3: cut; I don't think that this adds anything
%	A weighted distribution is closely related to a second order
%        distribution: a distribution over distributions, which can be
%        collapsed to a single distribution by taking an expectation. 
%	The reason we say `closely related' is that $\bbr{M}$ will
%        likely not have finite support, and won't always be
%        integrable. Fortunately, if $\alpha > 0$, then by
%        \Cref{cor:u-convex}, $\mathcal U_\alpha$ has a unique minimum,
%        $\mu^*$. Therefore, taking a limit as $\beta \to \infty$, we
%        know $\bbr{M}_{\alpha,\beta}$ converges to the weighted
%        distribution that puts weight 1 on $\mu^*$ and weight 0 on all
                %        other distributions, as shown below:
%joe3*: I have no idea why this should put weight 1 on a single
%distribution.  I'm lost!  I think we should just give a third
%semantics that amounts to picking the distribution of higest weight, 
%after stating a theorem that says that there always is one.  (Your
%results on convexity will be used in the proof of the theorem, but
%don't need to be stated separately.
%oli3*: I'm just using the standard trick to achieve the maximum without taking 
% a supremum. A supremum is not continuous, but we can get it for these strongly
% convex spaces by taking the associated temperature of the Boltzmann distribution to
% zero.  We're just doing this one level up, for the weighted distributions...
	\[ \Big(\lim_{\beta \to \infty} \bbr{\sfM}_{\alpha,\beta}(\mu)
        \Big)  = \Big(\lim_{\beta \to \infty} \exp(-\beta\ \mathcal
        U_\alpha) \Big)(\mu) = \delta_{\mu, \mu^*} \] 
	
%	Rather than writing out the limits, we will now substitute $\epsilon$ and $\infty$ for arbitrarily small and large positive numbers, respectively.
%	Therefore there is always some $\beta$ for which this is
	It turns out that every specific distribution we have given is a special case of this, with some tuning of our $\alpha$ and $\beta$ parameters. 
	%oli2: Ok this last part is maybe a bit much, but I hadn't slept and kept going instead of finishing the rest.
	% I still think I should state something like the proposition below but 
	\begin{align*}
		\Big(\bbr{\sfM}_{\alpha_0}^*\Big)(w) &:= \lim_{\substack{\alpha \to \alpha_0 \\ \beta \to \infty}} 
		\E_{\mu \sim \bbr{\sfM}_{\alpha,\beta}} [\mu(w)] \numberthis\label{eqn:higher-expectation} \\
		&= \lim_{\alpha \to \alpha_0 } \lim_{\beta \to \infty}  \frac{1}{Z} \sum_{\mu \in \Delta\V(\sfM)} \bbr{\sfM}_{\alpha,\beta}(\mu) \mu(w) 
	\end{align*}
	
	defined where the sum is finite; $Z$ is a normalization constant across all worlds $w \in \V(\sfM)$. 
	Using this construction, we can show that each of our other semantics can be expressed in terms of the weighted distributions.
	
	\begin{prop}
%		Every semantics can be recovered from $\bbr{\sfM}$. In particular,
		$\bbr{\sfM}\SD = \{ \mu \in \Delta \V(\sfM) : \bbr{\sfM}_0 (\mu) > 0 \}$, 
		and if $\sfM$ is consistent, then $\bbr{\sfM}\MaxEnt = \bbr{\sfM}_{0^+}^*$
	\end{prop}
	
	The important feature of this construction is that it gives us a uniform way of getting distributions based on our confidence in our beliefs.

	
%	This corresponds to a lexicographical ordering on \emph{all} distributions (as opposed to simply the ones in $\bbr{M}\SD$), where we order first by satisfaction of constraints, and then by entropy. In general, we might be willing to relax the constraints a little, and sacrifice some fit for aa more general distribution. This allows us to define a free energy; see \Cref{sec:thermo-background} for more.
	
%joe1*: I cut this.  You haven't explained in what sense a conditional
%distribuiton is a program, you havne't motivated it, it takes us too
%far afield ...	
	\begin{vfull}
	\subsection{As Probabilisitic Programs}\label{sec:prog-semantics}
	
	One final way of viewing PDGs is as a set of probabilistic programs, corresponding to the edges. 
	Conditional distributions can be thought of as probabilistic programs. As a result, we can compose and run them: paths from $A$ to $B$ correspond to noisy estimates of $B$ from $A$.
	
	Specifically, if $f(b \mid a) : \mathcal V_A \to \Delta \mathcal V_B$ and $g(c \mid b) : \mathcal V_B \to \Delta \mathcal V_C$ are conditional distributions, then the probabilistic composition $g\circ f : \mathcal V_A \to \Delta\mathcal V_C$ is
	\begin{align*}
		(g\circ  f) (c \mid a) :=  \sum_{b \in \mathcal V B}\!\! f (b \mid a)\ g(c \mid b)
	\end{align*}
	
	This can be recognized as a matrix multiplication $f$ and $g$ regarded as sub-stochastic matrices.
	Thinking about graphical models this way makes thinking about chains of reasoning simpler, gives us a way out of storing probability tables, and suggests additional applications.
	
	More formally, we can define
	\[ \bbr{M}_\lambda = \left\{
			\begin{aligned}
				 \text{paths } p = N_0 \xrightarrow{p^1} N_1 \xrightarrow{p^2}\cdots\xrightarrow{p^n}N_n \\
				 \text{such that } (N_{i-1}, N_i, p^i) \in \Ed^\sfM
			\end{aligned}
		\right\} \]
	
	\begin{example}
		Composition of arrows in a tables in chain is simply an easy case of varaible elimination. 
		
		\[
			\scalebox{0.8}{
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& C
			\end{tikzcd}\hspace{3em}
			\begin{tikzcd}[dpad, ampersand replacement=\&]
				A \ar[r]\& B \ar[r] \& C
			\end{tikzcd}}
		\]	

		Conversely, factorization of a table $A \to C$ into tables $A \to B$ and $B \to C$ (i.e., a stochastic matrix factorization) corresponds to splitting a program into two steps, and the data necessary to describe it will be smaller if $|B|$ is small.
	\end{example}	
	
	% It also has not escaped us that PDGs have a particularly nice description in categorical terms, which we do not pursue further here.
	
	Furthermore, thinking about the mental state of an agent as a collection of programs you could run from any concept gives our first natural interpretation of a sub-distribution (more in \Cref{sec:full-model}): probability mass assigned to $\none$ by a edge $p$ has not terminated yet (if at all). 
	Even given infinite time, some paths in $\bbr{\sfM}_\lambda$ may be infinite.
	
	\begin{defn}
		A PDG $\sfM$ is \emph{strongly consistent} if every collection of paths $P \subseteq \bbr{\sfM}_\lambda$ is compatible, in that 
		$$\bigcap_{p \in P}\ \bbr*{\vphantom{\Big|}p^0\circ \cdots\circ p^k}\SD \neq \varnothing$$
	\end{defn}

	\begin{example}
		Bayesian Networks and conditional Bayesian Networks are strongly consistent.
	\end{example}

	\begin{prop}
%		$ \text{strongly consistent}  \subsetneq \quad 
%		\text{strictly consistent}  \subsetneq  \text{consistent} $
		Any PDG $M$ that is strongly consistent is also consistent, but some strongly consistent PDGs are not strictly consistent.
	\end{prop}

%	\begin{example}
%		The trace graph of a 
%	\end{example}


% This means we can sample them
% Also, compose them
% and a

	\end{vfull}
	
	


%\begin{notfocus}
	\section{RELATIONS TO OTHER GRAPHICAL MODELS}\label{sec:other-graphical-models}
	%joe2*: Oliver, for the abstract, cut the figure, and talk about BNs,
	%factor graphs, and perhaps one other model.  Less is more.  A huge
	%cluttered picture does not help anyone.  You can perhaps say that, in
	%the full paper, we relate what we're doing to other approaches, such
	%as x, y, and z.   But let's discuss this before you do it.
	%oli2: I will talk about those three, and reduce the figure. But I think it is helpful to have anything a non-trivial number of people use all in one place; it helps people get a much better map of . Unlike text which you have to read in order, I think this map will be helpful for anyone, as long as we're careful to place the models we talk about on it. 
	
	PDGs are surprisingly general. Many graphical models, and their abilities to represent one another are depicted in \Cref{fig:model-transformations}; we discuss the highlighted nodes. We now discuss some of the best known classes; see \Cref{sec:many-relations-graphical-models} for a full description of the diagram.

	\usetikzlibrary{decorations.markings}	
	\begin{figure*}[t]
		\centering
		\tikzset{attn/.style={draw, fill=magenta, fill opacity=0.3, font=\Large\bfseries, inner sep=4pt}}
		\begin{tikzpicture}
			\begin{scope}[every node/.style={ellipse, fill, fill opacity=0.05,text opacity=1,
						outer sep=3pt,font=\bfseries}, xscale=2.5,yscale=1.2]
				\node (KB) at (-3, 0.5) {KB};
				\node (CG) at (-3, 1.5) {CG};

				\node (CRF) at (-1.2, 0.4) {CRF}; % CFG
%				\node (CRF) at (-2, 0) {CRF};
			
				\node (MRF) at (-2, 1) {MRF};
				\node[draw, attn] (FG) at (-1,1.35) {FG};
				\node (SDFG) at (0,1.5) {FG$^\rightharpoonup$};
				\node[attn] (DFG) at (1,1.35) {FG$^\rightarrow$};
				\node[attn] (BN) at (2,1) {BN};

				\node (CBN) at (2,0) {CBN};
				\node (DN) at (1.3, 0.6) {DN}; 

				\node (sPDGH) at (0,0.5) {sPDG$_{\text{hyper}}\!\!$};
				\node (PDGH) at (-0.8,-.5) {PDG$_{\text{hyper}}\!\!\!$};
				\node (PDG) at (0,-.85) {PDG};
				\node[attn, fill=black, fill opacity=0.9, text=white] (sPDG) at (0.8,-.5) {sPDG};

				\node (prog) at (3, -0.2) {PProgSet};
				
				\node (CPS) at (1, -1.4) {CPS};	
				\node (PlateBN) at (-2.5, -1.2) {PlateBN};
				\node (LPS) at (-1,-1.4) {$\underline {\mathcal P}$};
			\end{scope}
	
			% lossless		
			\begin{scope}[every edge/.append style={->}]%right hook->
				\draw (BN) edge (DFG) (DFG) edge (SDFG);
				\draw (MRF) edge (FG) (FG) edge (SDFG);
%				\draw[->] (DFG) -- (FG);
				\draw (CBN) edge[bend left = 5, shorten >=7pt] (sPDGH);
				\draw (CG) edge[bend left=10] (FG);
				\draw (KB) edge (CG);
				
				\draw (sPDGH) edge (PDGH) (sPDG) edge (PDG);
				
				\draw (BN) edge (DN) (DN) edge (sPDGH);
				\draw (DFG) edge (sPDGH);
				
				\draw (MRF) edge (CRF);% (CRF) edge (CFG);
				\draw (BN) edge (CBN);
				\draw (FG) edge (CRF); %crf
				
				\draw (CG) edge[out=-55, in=195, looseness=1.5, shorten >=7pt] (sPDGH);
				\draw (prog) edge[bend left=5] (sPDG);
				\draw (CPS) edge[out=180,in=-30] (PDG);
				\draw (PlateBN) edge[bend right=5] (PDGH);
				\draw (LPS) edge[out=0, in=-150] (PDG);
			\end{scope}
	
			% PDG Equivalences
%			\draw[->, transform canvas={yshift=2pt}] (PDGH) -- (PDG);
%			\draw[->, transform canvas={yshift=-2pt}] (PDG) -- (PDGH);
%			
%			\draw[->, transform canvas={yshift=2pt}] (sPDGH) -- (sPDG);
%			\draw[->, transform canvas={yshift=-2pt}] (sPDG) -- (sPDGH);
			
			\draw[double equal sign distance, shorten <=0pt, shorten >=0pt] (PDGH) -- (PDG);
			\draw[double equal sign distance] (sPDGH) -- (sPDG);

			
			% Projections. Lose information but preserve something.
			\begin{scope}[every edge/.append style={densely dashed, orange, ->}]
				\draw (sPDGH) edge[bend left=10, out=10] (FG) (sPDGH) edge[bend right=10, out=-5] (FG);
				\draw (SDFG) edge[bend right=20] (FG);
			\end{scope}
			% Inefficient conversions.
			\begin{scope}[every edge/.append style={ultra thick, dotted, line cap=round, shorten >=2pt,
					decoration={markings,mark=at position 1 with {\arrow[xshift=0pt,scale=.8]{>}}},
					postaction={decorate}}]
				\draw (CRF) edge (sPDGH);
				\draw (SDFG) edge (sPDGH);
			\end{scope}
		
%			\draw[->, transform canvas={xshift=-3pt}] (DDW) -- node[left]{$E_\beta$} (EDW);
%			\draw[->, dashed, transform canvas={xshift=3pt}] (EDW) -- node[right]{$P_\beta$} (DDW);
%			
%			\draw[->] (DW) to[bend left=10] node[sloped,fill=white]{$D({-\Vert})$} (EDW);
%			
		\end{tikzpicture}
		\caption{Transformations Between Graphical and Epistemic Models. Solid arrows indicate a model being a special case of another. Orange dashed transformations lose information, and the thick arrows are inefficient translations. For a full description, check \Cref{sec:many-relations-graphical-models}. }
		\label{fig:model-transformations}
	\end{figure*}

	
	\subsection{BAYESIAN NETWORKS} \label{sec:bn-convert}
		

	
%	\begin{enumerate}
%		\item (1) In contrast with a Bayesian Network, in which each node has a set of parents, each node of a PDG has possibly many sets of parents, where each set of parents corresponds to a different constraint, associated to a different table, and (2) We no longer require conditional independence of non-descendants given children
%		
%		\item A BN is just a PDG where every cycle commutes		
%		% \item A tree. 
%	\end{enumerate}
	
%	\begin{center}
%		\scalebox{0.8}{
%		\begin{tikzcd}[center base, column sep=1.5em, ampersand replacement=\&]
%			\& A \ar[dl]\ar[dr] \\
%			B \ar[dr] \&\& C \ar[dl]\\
%			\& D \&
%		\end{tikzcd}
%		\begin{tikzcd}[center base, column sep=1.4em, dpad, row sep=1em, ampersand replacement=\&]
%			\& \mathsf 1 \ar[d] \& \\
%			\& A \ar[dl]\ar[dr] \& % \ar[dd,dashed, gray] 
%			\\
%			B \& \& C \\
%			\& B \times C \ar[ul, gray!70] \ar[ur, gray!70]\ar[d] \& \\
%			\& D \&
%		\end{tikzcd}
%		}
%	\end{center}
%	\vspace{0.5em}

	Intuitively, we would like 


	\begin{defn}
		The \emph{extra information}, $H^{\sfM}$, given by a distribution $p$ that satisfies the constraints of a PDG $\sfM = \mnvars[]$ is given by
		\[ \H^{\sfM}(p) := \left(\sum_{L = (X,Y, \ell)\in \cal L} ~~\E_{x \sim p_X}  \H (\bp (x)) \right) - \H(p) \] 
		where $p_X$ is the marginal of $p$ on the variable $X$, and $\H(\bp(x))$ is the entropy of the distribution on $Y$ specified by the cpt $\bp$ when $X = x$. 
	\end{defn}
	% The cpts may not (in the entropy sense) 
	We can think of the extra information as the sum of the entropies that \emph{actually} result from each table, in the context of distribution $p$, minus the total entropy of the distribution.
	\begin{theorem}[restate=thmbnsRpdgs]\label{thm:bns-are-pdgs}
		If $\cal B$ is a Bayesian network, then there is a unique probability distribution $\mu^*$ consistent with the PDG $\Gamma(\cal B)$ with minimal extra information $\H^{\Gamma(\mathcal B)}(p)$. Furthermore, $\mu^* = \Pr_{\cal B}$. That is to say, $\mu^*$ is the same distribution as the unique one that satisfies all of the conditional independences of the $\cal B$.	
	\end{theorem}

	
	% d-separation? I don't have a lot to say but it the specialness of the ``colider'' or head-to-head nodes in determining connectedness  is related to the difference in interpretations I think.

	
%oli2: I was thinking of putting a list of comparisons here but I can't think of good ones right now; we've covered most everything earlier.
%	\begin{fact}
%		\begin{enumerate}
%			\item Subgraphs of BNs are not always BNs, but subgraphs of PDGs are.
%			\item 
%		\end{enumerate}
%	\end{fact}

	
	\subsection{FACTOR GRAPHS} \label{sec:factor-graphs}

	\tikzset{fgnode/.style={dpadded,inner sep=0.6em, circle},
	factor/.style={light pad}}	

	% What I want to see is a serious discussion of the advantages and disadvantages of factor graphs vs. PDGs, illustrated by examples. This is critical.
	
	
	We now compare PDGs with factor graphs, a general class of \emph{undirected} graphical models, often thought of as a generalization of BNs and Markov Networks.
	%todo: hint at MN relation in beginning. 
	%PDGs can simulate them (\cref{def:fg-convert}), but not without large cpts and sneaky use of inconsistency. 
	
	
	%% informal, unclear.
\begin{quickdefn}
	A \emph{factor graph} is a collection of random variables $\mathcal X = \{X_i\}$ and a collection of \emph{factors} $\{\phi_\alpha\colon X_\alpha \to \mathbb R_{\geq0}\}_{\alpha \in \mathcal I }$ over subsets $\alpha$ of $\mathcal X$.
\end{quickdefn}
	\begin{defn}
		A \emph{factor graph} on variables $\{X_i\}$ is a set of \emph{factors} $\{\phi_\alpha\colon X_\alpha \to \mathbb R_{\geq0}\}_{\alpha \in \mathcal I }$ over subsets $\alpha$ of the variables.
		
		More precisely, a factor graph $ (\{\phi_\alpha\}_{\alpha \in \cal I})$ on an indexed set of random variables $\mathbf X = \{ X_j \}_{j \in J}$, 
		is a pair $(\mathcal I, \boldsymbol\phi)$, where $\cal I$ is a set,
		each element $\alpha\notation{\in \mathcal I}$ of which determines a selection $\iota(\alpha) \subseteq J$ of the variable indices, and
		$\boldsymbol\phi$ is an indexed collection of \emph{factors} $\{\phi_\alpha\}_{\alpha \in \mathcal I }$, 
		where each factor $\phi_\alpha \colon \mathcal X_\alpha \to \mathbb R_{\geq 0}$ assigns a non-negative score to joint settings $\vec x_\alpha \notation{\in \mathcal X_\alpha}$ of every variable in $\iota(\alpha)$, all values of which we denote by $\mathcal X_\alpha\notation{ := \prod_{j \in \iota(\alpha)} \mathcal V(X_j)}$. 
	\end{defn}
\begin{fulldefn}
	A \emph{factor graph} $(\mathcal I, \phi)$ on an indexed set of random variables $X : \Sigma_{}$, where each $X_i$ can take values $\V(X_i) =: \mathcal X_i$, consists of  
	% technically, a dependent sum \mathbb X : \Sigma_{j : J} X_j
	%		a set $\cal I$, where each $\alpha \in \cal I$ is a
	% technically a multi-subset of 2^J...
	a set of \emph{factors} $\{\phi_\alpha\}_{\alpha \in \mathcal I }$, where each $\alpha$ determines a selection $\iota(\alpha) \subseteq 2^J$ of the variable indices, and the associated factor $\phi_\alpha \colon \mathcal X_\alpha \to \mathbb R^{\geq 0}$ assigns a non-negative score to a setting
	$\vec x_\alpha \in \mathcal X_\alpha := \prod_{j \in \iota(\alpha)} \mathcal X_j$ of the variables corresponding to $\iota(\alpha)$.
	
\end{fulldefn}

	% $(J, \mathcal I)$
	While the qualitative structure $(\mathbf X, \mathbf{Pa})$ of a BN on variables $\mathbf X$ is a directed graph, the qualitative structure $(\mathbf X, \mathcal I)$ of a factor graph on $\mathbf X$ is
%		technically an undirected \emph{multi-hyper-graph}
%	 		\footnote{That is, a set of ``nodes'' $\N$ and a collection (possibly containing multiple copies) of ``hyper edges'' $\Ed$, each of which corresponds to a subset of $\N$}
%	 	on $\bf X$, which is equivalent to 
	a bipartite graph $({\mathbf X, \cal I}, \iota)$ with extra vertices (drawn as squares) corresponding to the factors. 
	%Examples of these can be seen in \Cref{fig:fg-convert,}
	
%	\note{Though easier to define in terms of MRFs, and this obscures the relationship to BNs and MRFs; this  paper in particular is an attempt to claim that adding and removing nodes is not something to sweep under the rug.}
	
	
	
%	The important thing about 
	A factor graph $\Phi = (\{\phi_\alpha\}_{\alpha \in \cal I})$ on $\mathbf X$ defines a probability distribution over $\V(\mathbf X)$ by 
	\begin{align*}
		\Pr\nolimits_\Phi(\vec x) &:\propto \prod_{\alpha \in \cal I} \phi_\alpha(\vec x_{\alpha}) 
		&= \frac{1}{Z_\Phi} \prod_{\alpha \in \cal I} \phi_\alpha(\vec x_{\alpha})
	\end{align*}
	where $\vec{x}$ is a joint setting on all of the variables, $\vec{x}_\alpha$ is the restriction of $\vec{x}$ to only the variables selected by $\alpha$, and $Z_\Phi$ is the constant required to normalize the distribution. 
	There are several ways of parameterizing factor graphs; we start with the simplest one.
		
	\subsubsection{Specifying Factors Directly}

	How does one design a distribution with the factors? \Cref{ex:fg-exam} illustrates some features of how they work.
	% has to be placed early or else the page is wrong. 
	\begin{figure*}[t!]
		\centering
		\begin{subfigure}[b]{0.22\textwidth}
		\scalebox{0.9}{
		\begin{tikzpicture}[center base]
			\node[fgnode] (F) at (-1.5,0) {$F$};
			\node[fgnode] (G) at (1.5,0) {$G$};
			\node[factor, above=0.5 of F] (f) {$\phi_F$};
			\node[factor, above=0.5 of G] (g) {$\phi_G$};
			
			\draw[thick] (F) -- (f) (G) -- (g);
			\draw[thick, dashed] (F) -- node[factor, fill=white]{$T$} (G);
		\end{tikzpicture} }
			\caption{}\label{subfig:fg-gf}
		\end{subfigure}%
		\hspace{2em}\vline\hspace{2em}%
		\begin{subfigure}[b]{0.3\textwidth}
		\scalebox{0.9}{
			\begin{tikzpicture}[center base, scale=0.9]
				\node[fgnode] (S) at (-0.4, 2) {$S$};
				\node[fgnode] (C) at (3, 2) {$C$};
				\node[fgnode] (L) at (1.3,0) {$L$};
				\node[fgnode, dashed] (W) at (-2,0) {$W$};
				
				\node[factor] (f1) at (1.3, 1.3){$\phi_1$};
				\node[factor, dashed] (f2) at (-0.3, 0){$\phi_2$};
				
				\draw[thick] (S) -- (f1) -- (C) (f1) -- (L);
				\draw[thick, dashed] (W) -- (f2) -- (L);
		\end{tikzpicture} }
			\caption{}\label{subfig:fg-planet}
	
		\end{subfigure}%
		\hspace{2em}\vline\hspace{2em}%
		\begin{subfigure}[b]{0.3\textwidth}%
%			\vspace{-1em}
			\scalebox{0.9}{
				\begin{tikzpicture}[center base, xscale=1.4,
					fgnode/.append style={minimum width=3em}]
					\node[factor] (1) at (1.65,1) {};
					\node[factor] (center) at (3.95, 0){};
					
					\node[fgnode] (PS) at (1.65,-0.5) {$\mathit{PS}$};
					\node[fgnode] (S) at (3.3, 0.8) {$S$};
					\node[fgnode] (SH) at (3.3, -0.8) {$\mathit{SH}$};
					\node[fgnode] (C) at (4.8,0) {$C$};
					
					\draw[thick] (1) -- (PS);
					\draw[thick] (PS) --node[factor, fill=white]{} (S);
					\draw[thick] (PS) --node[factor, fill=white]{} (SH);
					\draw[thick] (S) -- (center) (center) -- (SH) (C) -- (center);
					
					
					\node[fgnode, fill opacity=0.02,dashed] (T) at (4.8, -2) {$T$};
					\draw[thick,dashed] (T) -- node[factor, fill=white]{}  (C);	
			\end{tikzpicture}}
			\caption{}\label{subfig:fg-smoking}
		\end{subfigure}%
		\caption{Candidate factor graphs for \Cref{ex:guns-and-floomps,ex:planet,ex:smoking}}
		\label{fig:fg-intro-examples}
	\end{figure*}
	
%	A factor graph is really just an exponential family \cite{wainwright2008graphical}, 


	\begin{example}\label{ex:fg-exam}
		To contrast with our other examples, which will all be local and correspond to directed models, we first present a more general factor graph designed to display some stranger features.
		
		Suppose Alice, Bob, Clara, and David each had a take-home exam; let $\mathbf X = \{A, B, C, D\}$ be binary random variables taking $\{1,0\}$, corresponding to whether or not each person passed the exam. 
		
		We want a joint distribution over possible outcomes; our knowledge, depicted graphically in \Cref{fig:fg-exam}, is as follows:	
		
		\begin{figure}[H]
			\centering
			\scalebox{0.8}{
				\begin{tikzpicture}[scale=0.75]
					%			\node[bpt={a1|$a_1$}] at (0,0.2){};
					%			\node[tpt={a2|$a_2$}, below=.4 of a1] {};
					%			\node[bDom={$A$ (A) around \lab{a1}\lab{a2}},fgnode] {};
					%
					%			\node[bpt={b1|$b_1$}] at (4,0.2){};
					%			\node[tpt={b2|$b_2$}, below=.4 of b1] {};
					%			\node[bDom={$B$ (B) around \lab{b1}\lab{b2}}] {};
					
					\node[fgnode] (A) at (0, 0) {$A$};
					\node[fgnode] (B) at (3, -1) {$B$};
					\node[fgnode] (C) at (3.5, 1.4) {$C$};
					\node[fgnode] (D) at (6, -1) {$D$};
					
					
					
					\node[factor] (f1) at (-2, 0){$\phi_1$};
					\node[factor] (f2) at (1.8,.4){$\phi_2$};
					\node[factor] (f3) at (1.3, -1.3){$\phi_3$};
					\node[factor] (f4) at (6, 1){$\phi_4$};
					
					
					\draw[thick] (f1) -- (A) -- (f2) -- (B) -- (f3) -- (A);
					\draw[thick] (C) -- (f2);
			\end{tikzpicture} }	
			\caption{Factor Graph: exam scores}
			\label{fig:fg-exam}
		\end{figure}
		
		
		\begin{enumerate}[nosep]
			\item[$\phi_1$.] A. priori., Alice is 4 times as likely to pass as not, and so $\phi_1(a) = 4$ if $a = 1$, and 1 otherwise.
			\item[$\phi_2$.] Alice, Bob, and Clara collaborated. Clara is very persuasive, and Alice trusts her, so an outcome in which everyone gets the same score is 8 times more likely; one in which only Alice and Clara share a score is 4 times as likely, and one in which only Bob and Clara share a score is twice as likely.
			\[ \phi_2(a,b,c) = \left\{\begin{aligned}
				8 &~~ \text{if~} a = b = c; &
				4 &~~ \text{if~}c = a \neq b; \\
				2 &~~ \text{if~}c = b \neq a; &
				1 &~~ \text{otherwise}
			\end{aligned}\right. \]
			\item[$\phi_3$.] Alice thinks very poorly of Bob, and ultimately reverses the answers to all his questions; she's guarantee to fail if he passes, and vice versa. $\phi_3(a,b) = 1$ if $a \neq b$ and 0 if $a=b$.
			\item[$\phi_4$.] The test is on factor graphs, which was unlikely, so $\phi_4() = 0.25$. This is true independent of anyone's scores, and doesn't bear on the distribution, so it will get normalized out.
		\end{enumerate}
		We don't know anything about David. \todo{numpy. Compute table.}
		Note some features of this example:
		\begin{itemize}[nosep]
			\item $\phi_3$ totally overrides the first case of $\phi_2$: The directions of an individual factor are just suggestions that are resolved globally.
			%The intuition of relative likelihoods, only works locally.
			\item Although $\phi_3$ was symmetric, our story is not: Alice doesn't trust Bob, and not the other way around. There is an important distinction in the story (this changes Alice's score, and not Bob's), but this cannot be captured.
%			To capture a conditional probability distributions, you need to impose \emph{local} normalization constraints \parencite{frey2012extending}. In this case, this means insisting that  $\sum_{a} \phi_3(a,b) = 1$
			\item To get any marginal distribution such as $\Pr(B)$, you have to take into account every factor, including those such as $\phi_1$ that are not connected to $B$.
			\item To emphasize that a factor is more important, we cannot simply scale it, as the scaling will be normalized out; the only control available is to changing the variance of its items. When viewed as a exponential family, \todo{links}
		\end{itemize}
	\end{example}
	Generally, factor graphs are learned from data or translated from another model, rather than specified by hand. \Cref{ex:fg-exam} should make it clear why: there is a lot of freedom in specifying the factors, even though the particular settings of which do matter, the interactions are global and it's hard to see how they will play out. Still, they are excellent descriptions of independencies.
%	 David's score is independent of everything else in the picture, and though the other three are a clique, we can see different interactions

	Any BN $\mathcal B = (\N, \mathbf{Par}, \V, p)$ can be seen naturally as a factor graph, which we denote $\Phi(\mathcal B)$. By the their global semantics, we have
	\begin{align*}
		\Pr_{\cal B}(\vec x) = \prod_{N \in \N} p_N( \vec x_N \mid \mathbf{Par}_N(\vec x)).
	\end{align*} 
	Factors can be read off directly: set $\cal I = \N$, connect every variable $X$, and all of its parents $\mathbf{Par}_X$, to the factor corresponding to $X$, by $\iota(X) := \{X\} \cup \mathbf{Par}_X$. Finally, define the function $\phi_X(x, \vec{y}) := p_X( X \!\!=\!\! x \mid \mathbf{Par}_X \!\!=\!\! \vec y)$ to simply be the cpt at $X$.
	Examples of this can be seen in the solid components of \Cref{subfig:fg-gf,subfig:fg-smoking}, which correspond to the initial to BNs in \Cref{ex:guns-and-floomps,ex:smoking}, respectively.
	
	This suggests an obvious way to view a PDG $\sfM$ as a factor graph $\Phi(\sfM)$: just like for a BN, ignore the directions of the edges and use the cpts as factors. 
	%If $\sfM$ is a PDG, let $\Phi(\sfM)$ be the factor graph obtained in this way. There may not be a way to recover the PDG from the resulting factor graph.
	
	 %Moreover, this process does not 
	\todo{discuss conditions under which $\bbr{\sfM}\MaxEnt = \Pr_{\Phi(\sfM)}$.}
	\todo{Discuss representation of factor graph in terms of \MN.}


	
	\begin{example}\label{ex:planet-fg}
		In our planet example, we treat each edge as a factor, the product of which gives the correct relative likelihoods for each of $S \times C \times W \times L$. Our initial knowledge, consisting only of the cpt, we have 
		\[ \Pr(s, c, w, l) \propto \phi_1(s,c,l)  \]
		where $\phi_1(s,c,l) = p(l \mid s,c)$, and no normalization is required.
		
		
		In contrast with BNs, there is no structural barrier to adding a new node, and factor $\phi_2(w,l) \!=\! \Pr(L\!=\!l\mid W\!=\!w)$ --- though to make sense of this as a probability we have to re-compute the normalization constant. The combination of the two factors is represented graphically in \Cref{subfig:fg-planet}, in which circles represent variables, and the boxes represent factors that depend on variables they connect to. 		
	\end{example}

	\subsubsection{Canonical Parameters}
	There is also a more standard presentation in terms of potentials, which makes the global resolution less mysterious, but also makes
	If we restrict to factors that are strictly positive, %by the Hammersley-Clifford theorem,
	then we can set $ \varphi_\alpha := \log \phi_\alpha$, giving us:
	\[ \Pr{\Phi} (\vec x)  = \exp \left\{ \sum_\alpha \varphi_\alpha(\vec x_\alpha)  - \ln Z_\Phi \right\} \] 
	
	
%	\[ \Pr{} (\vec x)  = \frac{1}{Z(\vec\theta)} \exp \left\{ \sum_\alpha \theta_\alpha \varphi_\alpha(\vec x) \right\} \] 

	\subsubsection{Shortcomings}

	While factor graphs are powerful statistical models, we argue that they are not well suited to modeling for epistemic state, for several reasons. 

	\begin{enumerate}
		\item They are undirected, making causal modeling, sampling, and compositions (e.g., from \cref{ex:sd-compose-unconditional,ex:randomvars}) difficult. This is resolved in \parencite{frey2012extending} by directed factor graphs. 
		\label{fgproblem:undirected}
		\item The global normalization process is over-eager in sweeping all inconsistencies, as we saw in \Cref{ex:planet-fg}. As a result, a local view of a few factors may not provide any information about the distribution. For instance, in  \Cref{ex:fg-exam}, $\phi_2$ suggests a qualitatively different joint distribution on $A,B,C$ than the one obtained after incorporating $\phi_3$. \label{fgproblem:global}
		\item Factors cannot be re-weighted by importance while still preserving the ratios of likelihoods between alternatives. recall the point . \label{fgproblem:reweight}
		\item There is no possibility of corroborating evidence \label{fgproblem:corrob}
		\item They are volatile: the addition of a new node can arbitrarily change the distribution \label{fgproblem:volatile} (\Cref{ex:fg-volatile,ex:fg-volatile-2})
	\end{enumerate}


	In \Cref{ex:fg-volatile}, the designer is lucky in a sense: it is obvious that the model is broken, and the fix is to delete a single suspicious-looking factor. Of course, without trying the NP hard normalization, there's no way to tell that anything is wrong. In general, things could be much worse: the failure to normalize can be spread out over many nodes, and even if the factor graph normalizes, a single additional factor connected to all variables can unilaterally force the factor graph to represent an arbitrary distribution at any world that has not been marked as impossible by another factor. This global normalization process in some sense is a catch-all fix that ensures that the factor graph is well-defined, but does not preserve any local meanings whatsoever.
	
	
	By contrast, PDGs are unaffected by any data that is not connected  to the rest of the graph, raise red flags when things go wrong, and no individual link can override another: instead it gets stuck in an inconsistent state; we assert once again that this is much more desirable.
	
	%	\todo{This section, too, requires a lot of editing. I have a list of features I want to object to, that I still need to put here in place of this}
	%This is a lot more modular (we can add and remove factors as we like). 
	%We now have a distribution that represents both beliefs, but this is not really what we were thinking of earlier.
	%Beyond simply the inevitable effects of representing our knowledge as a distribution, such as forcing us to implicitly adopt marginal distributions over the variables $S,C$, and $W$, a product of factors has additional undesirable properties that are not shared by \MNs:
	%	While general, factor graphs sacrifice interpretability and important internal features of our original belief representation, so that they can represent distributions.
	
	

	
	We now verify a few facts about the relations between the semantics of these conversions.
	
	%	\begin{align*}
	%		\Phi( \mathcal N, \Ed, \mathcal V, \bmu ) = (\alpha, \{ \phi_{} ) \}
	%	\end{align*}
	
	\begin{prop} \label{prop:}
		If $\mathcal B$ is a BN, then $\Phi(\mathcal B) = \Phi(\Gamma(\mathcal B))$
	\end{prop}
	
	
	
	\begin{conj}\label{thm:noninj}
		$\bbr{M}\MaxEnt = \Pr_{\Phi(M)}$
	\end{conj}
	\begin{coro}\label{coro:same-dist;different-weight}
		For every distribution $\mu \in W_{\cal V}$, there exist PDGs $M_1$ and $M_2$ that represent the same distributions, $\bbr{M_1}\MaxEnt=\bbr{M_2}\MaxEnt=\mu$, but $\bbr{M_1} \neq \bbr{M_2}$. 
	\end{coro}
	\begin{proof}
		For any $\mu$ we can always add marginals which are already assumed, altering probability but different free weighted distributions; by \Cref{thm:noninj}, this depends only on the probability.
	\end{proof}
	
	A PDG clearly encodes more information than just the distribution: this is true for both Bayesian Networks and Factor Graphs as well. In both cases, this is often cast as a flaw, as this makes them poor choices as canonical descriptions of distributions, which is why so much attention is given to I-maps in \parencite{koller2009probabilistic}. However, separation from a probability distribution has not been empirically damaging. Despite being less expressive and obscuring independence relations, BNs continue to be a more popular modeling tool. The causal picture they can provide, beyond anything in the distribution, is evidently worth a lot.




%\begin{notfocus}
	\subsection{DIRECTED FACTOR GRAPHS}
	
	One solution, by \parencite{frey2012extending} is to also enforce some local constraints, in the form of a local normalization.  While this indeed solves issues \cref{fgproblem:undirected,fgproblem:global}, directed factor graphs still leave some bits of issues \cref{fgproblem:corrob,fgproblem:reweight,fgproblem:volatile} unaddressed.
	
	Directed factor graphs are much more explicit with their factorizations than BNs, are as expected, even more closely related to PDGs. However, they too cannot capture scenarios such as \cref{ex:randomvars}. Consider example \ref{ex:directedfg}
	
	\begin{example}\label{ex:directedfg}
		\todo{Choose a different directed factor graph example that doesn't rely on sub-stochasticity}
	\end{example}

	
	\section{Operations on PDGs}\label{sec:pdg-operations}
	\subsection{GRAPH OPERATIONS}
	To model the process of adding information to a \MN, we use a graph union. While clearest in \Cref{ex:grok-union}, we can also view adding the individual cpt $\mat r$, as a union of the original PDG with the single-cpt PDG $[F \smash{\xrightarrow{\mat r}} G]$ as we did in \Cref{ex:guns-and-floomps}; \Cref{ex:smoking} similar, but with an extra endpoint.
	
	Though it seems to be a natural construction, there is a subtlety that makes this definition non-standard: we take the \emph{ordinary} union of the nodes, but the \emph{disjoint} union of the edges. We need an ordinary union of the vertices so that we can glue the two models together in the right places, but we need the disjoint union of the edges, because if two PDGs share an edge, the tables may not match and the only clear thing to do is to keep both, as we do in \Cref{ex:grok-union}. 
	We now define the graph union formally. 

	\begin{defn}[union] \label{def:model-union}
		If $\sfM, \sfM'$ are \MN s such that $\V^\sfM(N) = \V^{\sfM'}(N)$ for every $N \in  \N^{\sfM} \cap \N^{\sfM'}$, then $\sfM \cup \sfM'$ is a PDG with the ordinary union of their nodes (necessary to align and glue PDGs together), and \emph{disjoint union} of their edges. \notation{Explicitly,
		\begin{align*}
			\N^{\sfM \cup \sfM'} &= \N^\sfM \cup \N^{\sfM'},  \\
			\Ed^{\sfM \cup \sfM'} \!=& \Ed^\sfM \sqcup \Ed^{\sfM'}\!
				=  \{ (A, B, \text{inl}(\ell)P) : (A,B,\ell)\in \Ed^\sfM \}  \\
					&\qquad\qquad \cup \{ (A, B, \text{inr}(\ell)) : (A,B,\ell)\in \Ed^{\sfM'} \} \\ 
			\V^{\sfM \cup \sfM'} (N) &= \begin{cases}
					\V^{\sfM}(N) & \text{if }N \in \N^\sfM \\
					\V^{\sfM'}(N) &\text{if }N \in \N^{\sfM'} 
				\end{cases}\\
			\bmu^{\sfM \cup \sfM'}_L &= \begin{cases}
				\bmu^{\sfM}_{A, B, \ell} &\text{if } L = (A, B, \text{inl} (\ell)) \\
				\bmu^{\sfM'}_{A, B, \ell} &\text{if } L = (A, B, \text{inr} (\ell)) 
			\end{cases}
		\end{align*}}
	\end{defn}
	The condition that $\V^\sfM$ and $\V^{\sfM'}$ agree on the shared variables is necessary for $\V^{\sfM\cup \sfM'}$ or $\bmu^{\sfM \cup \sfM'}$ to be well-defined.%
		%oli4: this is commented out, don't worry.
		\vfullfootnote{For those familiar with manifolds, it is analogous to a gluing condition for an atlas of charts}
	The restriction from \Cref{ex:grok-ablate} is more straightforward.%
	%oli4: how much story, vs terseness?
	%, but we provide it for completeness.
	
	\begin{defn}[restriction]\label{def:restriction}
		The \emph{restriction} of $\sfM = \mnvars[]$ to a subgraph $(\N' \subseteq \N, \Ed' \subseteq \Ed)$ of $(\N, \Ed)$, is the PDG, $\sfM|_{\N', \Ed'} = (\N', \Ed', \V |_{\N'}, \bmu|_\Ed')$, where 
		$\V|_{\N'}$ and $\bmu|_\Ed'$ are the same functions on the their respectively smaller domains $\N$ and $\Ed$. 
	\end{defn}
	


	%oli2: this first sentence I believe to be overkill, but I'm including it because I'm now trying really hard to claim that I've motivated the graph union.
%joe3: ``enjoying modularity'' seems like strange wording to me.  What
%we've said, in any case, is that PDGs are more modular than other
%approaches. 
%	We have said repeatedly that PDGs enjoy modularity, and seen
%oli3:
	We have seen
        in \cref{ex:guns-and-floomps,ex:planet,ex:smoking} cases in
        which capturing the relevant information involves taking a
%joe3*: as I said, you've never talked about these examples in terms of
%union.  I think that there may be a useful discussion to be had about
%how modularity corresponds to union, and I understand that once you
%have union, youll want multigraphs.  This isn't going to make it to
%the abstract, and there's no question that this is the wrong place
%for it.  I could imagine a section where you talk about modularity
%and union, say that PDGs make sense even if they are multigraphs adn
%prove the theorem.
%oli3: that's the plan now. BUt it's not that it even makes sense, so
% much as that it _only_ makes sense with multi-graphs.
        union of two graphs, some of which may include new
        concepts. We wish to verify that our semantics are
        well-behaved with respect to this composition.	  
	We therefore ask: what happens if we combine two PDGs $\sfM$
        and $\sfM'$ together? Intuitively, the set of distributions
        $\bbr{\sfM \cup \sfM'}\SD$ consistent with the combined
        constraints $\sfM\cup \sfM'$ should be the intersection of the
        distributions $\bbr{\sfM}\SD \cap \bbr{\sfM'}\SD$ consistent
        with each PDG separately. This is almost correct, but $\sf M$
        and $\sfM'$ may be over different set of variables, in which
        case the sets of distributions are automatically disjoint, as
        they are over different sets of possible worlds. To address
        this, we define a more sophisticated intersection of
        distributions that must agree on all overlapping
        marginals. %(\cref{def:marginal-dist-intersection}) 
	
	\begin{defn}[$\dcap$]\label{def:marginal-dist-intersection}
		If $R$ and $S$ are sets of distributions, $R \subseteq \Delta X$ over the set $X$ and $S\subseteq \Delta Y$ over the set $Y$, then
%oli: remove the coment below to hide the notation.
%		\notation[$R \dcap S$~]
			{$$R \dcap S := \Big\{ \mu \in  \Delta [X \!\times\! Y] ~\Big|~ (\mu_{X}, \mu_{Y}) \in R \times S \Big\}  $$}%
		is the set of distributions over joint settings of $X$ and $Y$, whose marginals $\mu_X$ and $\mu_Y$ are each compatible with some distribution in $R$ and $S$ respectively. 
		
		This it the natural extension of an intersection to distributions on different, possibly overlapping sets --- in particular, if $X = Y$, then $R \dcap S$ = $R \cap S$ and if \notation[$X$ and $Y$ are disjoint]{$X \cap Y = \varnothing$}, then $R \dcap S = R \times S$. 
	\end{defn}
	
	
%	It is now natural to ask: how does this semantics interact with the PDG union (\Cref{def:model-union})? 	
	Now that we have the correct definition, we immediately get our desired property:
	
	\begin{prop}\label{prop:union-set-semantics}
		$\bbr{M \cup M'}\SD = \bbr{M}\SD \dcap \bbr{M'}\SD$.
	\end{prop}

	\Cref{prop:union-set-semantics} can be interpreted as a statement of modularity: we can straightforwardly get the semantics for a combined diagram based only on its counterparts. 
	From the two special cases of $\dcap$ discussed above, one can see that adding new edges, (which we will see correspond to observations in \Cref{sec:belief-update}), cuts down the set of possible distributions, just like conditioning, and adding new variables to a consistent model freely increases the number of valid distributions like one would expect. We would like to emphasize that all of this is done through a by combining \MNs.
	
	\begin{example}\label{ex:sd-compose-unconditional}
		Suppose we now have two PDGs with only one edge apiece, $\mathsf A = {\mathsf 1} \xrightarrow{p} X$ and $\mathsf B = X \xrightarrow{q} Y$. We would hope that the semantics treat this like composition: that the unconditional distribution on $X$ provided by $p$ would be `plugged in' to the conditional distribution $q(y \mid x)$; indeed, this is what happens:
		%
		\begin{align*}
			&\bbr[\Big]{{\sf A \cup B}}\SD = \bbr[\Big]{{\mathsf 1} \xrightarrow{p} X \xrightarrow{q} Y}\SD \\
				&= \Big\{  \mu \in \Delta(\V(X) \times \V(Y)) : \mu_X = p,~\mu_{Y|X} = q \Big\} 
		\end{align*}
		where $\mu_X$ is the marginal of $\mu$ on $X$, and $\mu_{Y|X}$ is the cpt of conditional marginals on $Y$ for each setting of $X$.
		For any choice of $p$ and $q$ there is exactly one such distribution, given by $\mu(x,y) = p(x) q(y \mid x)$.
	\end{example}

	It turns out that this semantics only results in convex sets. This observation will be useful in the next section.
	\begin{lemma}[restate=thmsetconvex] 
		\label{prop:convex}
		$\bbr{\sfM}\SD$ is convex, for any PDG $\sfM$.
	\end{lemma}%

	%This will be interesting to explore in the full paper, but it's definitely not high priority here.
	\begin{vfull}
	If the intersection of two sets is convex, then 
	\begin{conj}\label{prop:intersect-set-semantics}
		$$\bbr{M \cap M'}\SD = \text{ConvHull}(\bbr{M}\SD \mathop{\dot\cup} \bbr{M'}\SD).$$
	\end{conj}
	\end{vfull}
	
	\section{THERMODYNAMICS OF PDGS}\label{sec:thermo}
	\begin{figure}[t]
		\centering
		\scalebox{0.9}{
		\begin{tikzpicture}
			\node[ellipse,draw, outer sep=4pt] (DW) at (0,0) {$\Delta W$};
			\node[ellipse,draw, outer sep=4pt] (EW) at (0,2.4) {$\text{Energy}^W$};
			\node[ellipse,draw, outer sep=4pt] (DDW) at (4,0) {$\Delta (\Delta W)$};
			\node[ellipse,draw, outer sep=4pt] (EDW) at (4,2.4) {$\text{Energy}^{\Delta W}$};
			
			\node[right=0.5em of EDW, blue] {$\mathcal U_\alpha(\sfM; \cdot)$};
			\node[right=0.8em of DDW, blue] {$\bbr{\sfM}_{\alpha,\beta}$};
			\node[left=0.8em of DW, blue] {$\mu$};
			\node[left=0.5em of EW, blue] {$\log\frac{1}{\mu}$};
			
			\draw[->, transform canvas={xshift=3pt}] (DW) -- node[right]{$E_\beta$} (EW);
			\draw[->, transform canvas={xshift=-3pt}] (EW) -- node[left]{$P_\beta$} (DW);
			
			\draw[->, transform canvas={xshift=-3pt}] (DDW) -- node[left]{$E_\beta$} (EDW);
			\draw[->, dashed, transform canvas={xshift=3pt}] (EDW) -- node[right]{$P_\beta$} (DDW);
			
			\draw[->] (DW) to[bend left=10] node[sloped,fill=white]{$D({-\Vert~})$} (EDW);
			
			\draw[->] (EW) to[bend left=15] node[above] {$\E^*$} (EDW);
			\draw[->] (EDW) to node[fill=white] {$\E$} (EW);

			\draw[->] (DDW) to node[below] {$\E$} (DW);
		\end{tikzpicture}}
		\caption{Energy / Distribution Transformations. 
			%The nodes are thermodynamic objects, the arrows are ways of constructing one from another
		}
		\label{fig:energies-and-dists}
	\end{figure}
	We now look at the weighted distribution semantics of PDGs from a thermodynamic perspective: this will provide better rationale for the parameter choices in \Cref{sec:weighted-semantics}, and draw some more explicit contrasts between PDGs and factor graphs.	Let $W$ be finite set of states, called ``micro-states'' on which the distribution is supported.
	
	Our technical starting point will be the Boltzmann distribution \eqref{eq:boltzmann}, which asserts that the probability $P$ of being in a state exponentially decreases as its energy $U$ increases; the rate of exponential decay is related to the ``inverse temperature'', $\beta$; here $Z_U(\beta)$ is a normalization constant. Fixing $\beta$, we can of course, invert the Boltzmann distribution \eqref{eq:invbolz}, obtaining an energy from a probability. A probability distribution over $W$ is called a configuration, or macro-state.
 	\begin{align}
	 P_{\beta}(U) &:= w \mapsto  \frac{1}{Z_U(\beta)}\exp\Big(-\beta U(w)\Big) \label{eq:boltzmann} \\
 		E_{\beta}(\mu) &:= w \mapsto \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right) \label{eq:invbolz}
 	\end{align}
 	Conversions between the two correspond to going up and down on the left of \Cref{fig:energies-and-dists}. 
 	Now $\mathcal U$, as defined in \eqref{eq:freeenergy-weighted} is an un-normalized badness score, making it like an energy; \eqref{eqn:weighted-semantics}, is the un-normalized Boltzmann distribution for this energy. The parameter $\beta$, which we described earlier as a certainty, plays the physical role of an inverse temperature: lower is more chaotic. 
 	
 	$\mathcal U$ is not just an arbitrary construction either: it is analogous to a free energy. Why is the most favorable configuration not just a point mass as the minimum energy? Because in a world where an ambient temperature makes things more diffuse, doing things would require a lot more energy. Rather than just minimizing the average energy of a configuration $\nu$, you're better off minimizing the Gibbs free energy \eqref{eqn:gibbs-free-energy}. 
 	\begin{equation}
 		G_E(\nu) = {\E}_\nu( E )  - T S(\nu) \label{eqn:gibbs-free-energy}
 	\end{equation}
 	Analogously, why not put all of your weight on the one distribution you think is most likely? Because in a slightly chaotic world, doing so could actually incur a lot more inconsistency. Instead, we're better off minimizing $\cal U$. $\alpha$ is more transparently a temperature here, with higher values indicating higher preparedness for background chaos. The higher order expectation we take in \eqref{eqn:higher-expectation} corresponds to the bottom edge of \Cref{fig:energies-and-dists}, and the diagonal, which is the natural way to construct free energies from a distribution, is a KL divergence. This can be seen directly, as well, in \Cref{ex:energy-from-distrib}.
 	%
	See \ref{sec:thermo-background}, and \parencite{bethe,friston} for more comprehensive background. %and \cite{} for weighted probability distributions.
	
	
	It follows from \Cref{cor:}
	A very weak version of this can already be seen in un-normalized factor graphs: by multiplying a factor $\phi$ by a constant $\alpha$, one obtains a free energy $G' = - \ln \alpha + G$, i.e., with a mere additive shift. However, this shift doesn't really distinguish belief states, which is part of why we're so eager to normalize the distribution.
	There is also an opportunity to modify $\beta$, but in standard graphical model literature, people set $\beta = 1$ and forget about it.%
		\footnote{A similar complaint, is lodged in \parencite{fixing-broken-elbo}, in which many information theoretic trade-offs are hidden by assuming $\beta = 1$}


	\begin{examplex}[continues=ex:worldsonly]\label{ex:energy-from-distrib}
		For the PDG $\sf M$ that encodes just a probability distribution $\mu$ over $W$,  $\zeta(\sfM; \nu) = \kldiv{\nu}{\mu}$. This quantity is also equal to $\mathcal G_{E(\mu)}$, the Gibbs free energy for the potential landscape associated to $\mu$ at temperature $\beta = 1$.
	\end{examplex}


	A priori, \Cref{thm:free-energy-strictly-more-expressive} might be thought of as merely a novel function we came up with, but in fact this is not the case--- when the PDG is a Bayesian network, this is just the normal Gibbs free energy.

	\begin{prop}\label{prop:bn-free-energy}
		For any Bayesian Network $B$, 
		\[ \bbr{\Gamma(B)} = D(- || \Pr\nolimits_B) = \mathcal G_{E(\Pr_B)} \]
	\end{prop}
	
	For factor graphs, the connection to thermodynamic parameters has previously been made explicit \parencite{bethe}, which allows us to formulate the following, stronger result:
	
	\begin{conj}\label{thm:fg-free-energy}
		For a factor graph $\Phi$, $\bbr{\Psi(\Phi)^{(\beta)}} = \mathcal G_{E_\beta(\Phi)}$.
	\end{conj}

	As a result, the weighted distribution semantics co-incide exactly with the notions of free energy on standard graphical models; we therefore can view PDGs as implicitly providing a more expressive class of free energies, corresponding to weighted distributions, which in turn can be naturally adapted to be distributions themselves.
	
%	\begin{prop}
%		The Bethe free energy is equivalent to the Gibbs free energy of $M$ iff $M$ is strongly consistent.
%	\end{prop}


%	\begin{conj}\label{thm:free-energy-strictly-more-expressive}
%		The weighted distributions generated by PDGs are strictly more expressive than those generated by BNs, Factor Graphs, or directed factor graphs.
%	\end{conj}
%	\begin{proof}
%		The first two parts come from \Cref{thm:fg-free-energy,prop:bn-free-energy}. Since 
%	\end{proof}
%	\begin{coro}
%		Local minima of the Bethe free energy are fixed points of loopy belief propagation in \MNs		
%	\end{coro}

	

		
	\section{WEIGHTED PDGs}
	
	We may also consider the edges as having different strengths: if we associate a different coefficient $\beta_L$ to each edge $L$, we can define
	
	\begin{defn}
		A weighted PDG $(\sfM, \beta)$ is a PDG $\sfM$ together with a \emph{certainty} $\beta_L \in \mathbb R^{\geq 0} \cup \{\infty\}$ for each edge $L \in \Ed^\sfM$.
	\end{defn}
	
	with corresponding inconsistency $\zeta((\sfM, \beta) ; p) :=$
	\[
	%\inf_{p \in \Delta(W^{\mathcal V})}~
	\sum_{\substack{L \in \cal L\\L = (A,B, \ell)}}\!\! \mathop{\mathbb E}_{a \sim p(A)} \left[\frac{1}{\beta_L}\kldiv[\Big]{\bmu_{L}(a) }{ p(B | A \sheq a) } \right]
	\]
	
	We an always take a PDG $M$ and uniformly assign every edge the same inverse temperature $\beta_0$; we'll call this $\sfM^{(\beta\colon\!=\beta_0)}$.
	
	As a sanity check, we can verify that with $\beta = \infty$, corresponding to very low ambient uncertainty, our weighted distribution exactly gives us a point mass on the global free energy minimum, which is the maximum entropy distribution.
	
	%	\begin{prop}
	%		If $M$ is consistent, $\bbr{M^{\beta\!:\!=\infty}}^* = \bbr{M}\MaxEnt$.
	%	\end{prop}
	
	
	
	
	\begin{example}[continues=ex:guns-and-floomps]
		Recall the PDG from our first example, in figure \ref{fig:gun-floomp-diagram}. Suppose that both the of the initial edges $1 \to F$ and $1\to G$ have certainty $\beta_{1 \shortto G} = \beta_{1\shortto F} = 1$, and we give our new observation a very low certainty of $\beta_T = 10^{-3}$.
		%
		In this case $\zeta(\sfM; \beta)$ is very close to zero, since the inconsistency can be resolved by a revision to the table $T$, which is cheap since $\beta_T$ is small. The $\zeta$-smallest perturbation effectively deletes the new observation. 
		
		%		In this example, the temperature parameters are like Barycentric coordinates over the triangle of edges, indicating how much each will would change in response to the inconsistency.
	\end{example}	
	
% 	\begin{example}[Dempster's rule]
% 		Suppose $p$ and $q$ are distributions over a set $W$, corresponding to the PDG $\sfM$ in \Cref{fig:parallel}. 		
% 
% 		\begin{figure}
% 			\centering
% 			\scalebox{0.8}{
% 			\begin{tikzpicture}
% 				\node[dpadded] (1) at (0,0) {$\sf 1$};
% 				\node[dpadded] (W) at (3,0) {$W$};
% 				\draw[arr] (1) to[bend left] node[fill=white]{$p$} (W);
% 				\draw[arr] (1) to[bend right] node[fill=white]{$q$} (W);
% 			\end{tikzpicture}
% 			}
% 			\caption{parallel}
% 			\label{fig:parallel}
% 		\end{figure}
% 		Then $\sfM$
% 
% %		For those familiar with factor graphs, this example might 
% 	\end{example}

	
	
	\begin{vfull}
		\section{Relations to Other Representations of Uncertainty}
		\expandafter\expandafter\expandafter\MakeUppercase\modelnames\ are far from the first formalism to provide a weaker notion of uncertainty than probability. Belief functions, inner measures, sets of probabilities, lower probabilities, weighted sets of probabilities, and plausibility measures have all been studied extensively in the past. One feature that each of these has in common is that they are under-specified, from the perspective of wanting probabilities for everything.
		
		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (outcomes) at (0,0) {$\Omega$};
			\node[dpadded] (1) at (-2,0) {$\mathsf 1$};
			
			\draw[arr] (1) -- (outcomes);		
		\end{tikzpicture}
		\end{center}
		
	
		The natural question now becomes: to what do these under-constrained representations of belief correspond to under-constrained bits of a \modelname?
		
		\subsection{Conditional Probability Spaces}
		
		\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (outcomes) at (0,0) {$\Omega$};
			\node[dpadded] (1) at (-2,0) {$\mathsf 1$};
			\node[dpadded] (U) at (2,1) {$U$};
			\node[dpadded] (V) at (2,-1) {$V$};
			
			
			\draw[arr] (1) -- (outcomes);		
			\draw[arr, ->>] (outcomes) -- (U);
			\draw[arr, ->>] (outcomes) -- (V);
			
			
		\end{tikzpicture}
		\end{center}
		
	
		
	
		\subsection{Sets of Probability Measures}
	%	As we discuss in section \ref{sec:set-of-distribution-semantics}
	
	
		
		\subsection{Lower Probabilities}
	\end{vfull}


	\section{USING INCONSISTENCY}	
	\subsection{BELIEF UPDATING} \label{sec:belief-update}
	Belief revision, both through Bayes' and Jeffrey's rules, can be thought of as the addition of a new marginal to a distribution, and then a resolution of inconsistency. In Dietrich, List, Bradly \cite{dietrich2016belief}, a belief revision is an update $p \mapsto p_I$ of a belief state $p$ to a new one consistent with the input $I$. 
	
	For us, belief revision consists simply of the addition of a new edge to the picture, followed by a resolution of the resulting inconsistency. 
	With reference to \Cref{fig:belief-update}, an extension of \Cref{ex:randomvars}, consider the following update. 
	Upon noisily observing the variable $B$ to with probabilities indexed by $\pi = \{\pi_b\}_{b \in \V(B)}$, Jeffrey's rule prescribes a posterior probability $p'$ of any event $E$ by:
	\[ p'(E) = \sum_{b \in B} p(E \mid B\sheq b) \pi_b \]
	Bayes Rule corresponds the particular case of Jeffrey's rule, in which the variable is binary and the outcome is certain.

	
	\begin{figure}[h]
		\centering
%		\scalebox{0.8}{
%			\begin{tikzpicture}[center base]
%				\node[dpadded] (1) at (0,3) {$\sf 1$};
%				\node[dpadded] (W) at (0,0) {$\cal W$};
%				\node[dpadded] (B) at (-2,1) {$B$};
%				
%				\draw[arr] (1) to node[fill=white]{$p$} (W);
%				\draw[arr] (1) to node[fill=white]{$\pi$} (B);
%				
%				\draw[arr, gray] (W) to[bend left=10] (B);
%				\draw[arr, dashed] (B) to[bend right=30] (W);	
%		\end{tikzpicture}}
		\scalebox{0.8}{
		\begin{tikzpicture}[center base]
			\useasboundingbox (-3,-1) rectangle (3.5,4);
			\node[dpadded] (1) at (0,3) {$\sf 1$};
			\node[dpadded] (W) at (0,0) {$W$};
			\node[dpadded] (B) at (-2,1) {$B$};
			\node[dpadded] (E) at (2.5, 0){$E$};
			\coordinate (Q) at (6,0); % to even out controls
			
			\draw[arr] (1) to node[fill=white]{$p$} (W);
			\draw[arr] (1) to node[fill=white]{$\pi$} (B);
			
			\draw[arr, gray, ->>] (W) to[bend left=10] (B);
			\draw[arr, dashed] (B) to[bend right=30] (W);	
			
			\draw[arr, ->>] (W) to (E);
			
			\draw[arr,blue!50] (1) .. controls (-5.5,1.5) and (-2,-2) .. node[fill=white]{$p'(E)$} (E);
			\draw[arr,orange!70] (1) .. controls (0.5,1) and (1,0.5) .. node[fill=white]{$p(E)$} (E);
		\end{tikzpicture}}
		\caption{PDG Belief Updating via Inconsistency}
		\label{fig:belief-update}
	\end{figure}
	
	To understand the update visually in \Cref{fig:belief-update}, imagine the original distribution $p$ from $\sf 1$ to $W$ being replaced by the path $p' := p(W \mid B) \circ \pi$  on the left. The gray arrow on the bottom left is the definition of the random variable, as in \Cref{ex:randomvars}, and the dashed one is its inversion, which can be computed by Bayes' rule.  %that factors through $B$ via the new observation $\pi$.
	To query the resulting distribution on, an arbitrary event $E$, with an indicator variable of the same name. Initially, we got a marginal on $E$ by going through $p$; we now use $p'$. Effectively, the orange path to $E$ has been replaced by the blue one.

	
	To observe $\pi$, we simply view it as a cpt conditioned on $\star$ and add it to our collection. 
	Although it is likely to be inconsistent, resolving this inconsistency in a way that retains $\pi$ is a belief update. 
	Even failure retain $\pi$ entirely may not be a concern: so long as an they continue to observe or remember, an agent endures discomfort until $\pi$ is incorporated. This setting is arguably more natural than a standard one: without spending energy, it is easy to forget or partially reject implications of the observation.	
	Once again, with a \MN, the resolution need not happen immediately. This makes the approach more convincing for cognitively bounded agents, who might have more pressing matters than sorting through beliefs, and who might do them out of order.

	\section{ALGORITHMS}\label{sec:algorithms}
	\subsection{BELIEF PROPAGATION}
	\todo{figure out what the minimal things I can say here and still convince people  it's inconsistency reduction}
	
	
	\subsection{SAMPLING}
	
	One of the nice about directed graphical models is that the model itself is roughly a sampling algorithm. For instance, taking a Bayes Net $\cal B$ and generating samples according to the tables is an efficient way to sample $\Pr_{\mathcal B}$.

	This works because there is only one path, but more generally, for any conditional marginal $Y|X$, we can think of all of the different paths in the PDG different ways an agent with knowledge $\sfM$ can get probabilistic estimates of the conditional distribution $\bbr{\sfM}\MaxEnt(Y | X)$. The next result states that, in a precise sense, these various estimates bound the location of the marginal for this maximum entropy distribution, which suggests an efficient sampling algorithm for $\bbr{\sfM}\MaxEnt(Y | X)$, after learning some weights.
	
	\begin{conj}\label{thm:maxent-hull}
		For any PDG $\sfM = \mnvars[]$ containing variables $X, Y$, the maximum entropy conditional marginal $\bbr{\sfM}\MaxEnt(y \mid x)$ is a convex mixture of the conditional marginals generated by the paths from $X$ to $Y$.  That is, there exist weights $\{\alpha_i \geq 0\}$ on the paths in $\sf M$ and a bias weight $\alpha_0$ with $\sum_i {\alpha_i} = 1$ and
		\[ \bbr{\sfM}\MaxEnt(Y \mid X) = \alpha_0 \  p^{\text{unif}}_Y \sum_{p \in \bbr{\sfM}_\lambda(X, Y)} \alpha_i (p_1 \circ \ldots \circ p_k) \]
		where $p^{\text{unif}}_Y$ is the uniform distribution on $Y$, and $\bbr{\sfM}_\lambda(X,Y)$ is the set of paths from $X$ to $Y$ generated by composition and Bayes Rule in $\sfM$. 
	\end{conj}

	One natural choice of these $\alpha$'s is the certainty scores for each edge, given by a weighted \MN, but we do not have any further formal results in this direction.
	Note that it is common for humans to make decisions in this way: to estimate whether something is realistic by following multiple chains of reasoning weighting them by strength of argument.
	
%	\begin{conj}
%		The conditional marginal of the maximum entropy distribution $\bbr{M}\MaxEnt(b \mid a)$ is in the convex hull of the compositions of paths $A \to B$. 
%	\end{conj}
	





	\section{DISCUSSION}
	
	\subsection{Constraints and composition}
	
	\todo{There are many things I could write. Have to decide what I actually want to say.}
	
	
	A probability distribution is in many ways the appropriate idealized object to carry around with you.
	
	Question: if we had more mental power, would we be more or less consistent? Unclear. Do dogs have more or less consistent beliefs than we do? They may just not have as many concepts. We clearly use a lot of our computation for storing things and making up concepts.
	
	

	
	\begin{vfull}
	\subsection{Useful Avenues of Empirical study}
	
	\begin{enumerate}
		\item Figure out how to empirically measure some kind of inconsistency, and lots of imagined correlates, such as amount of indecision, other people taking advantage of you, environments that would encourage "double-think". If there's a robust, multi-feature correlation between IQ and inconsistency, one concludes that additional mental power does not lead to coherence, and therefore some logical limits may not be as relevant as previously thought.
		\item 
	\end{enumerate}
	\end{vfull}
	
	

	\subsection{Probabilities Still Encode Well}	
	In some sense, while we have yet to find a mental state that is not encoded in some probability distribution, the choice of underlying space is extremely important, and we argue that it changes rapidly. Moreover, sometimes one has to make up new internal mental variables, which also changes the underlying space. PDGs offer a way to describe distributions, together with a number of internal parameters one might not be actively aware of. The relevant parameters, can always be internalized \todo{define internalization} until we reach a distribution. 
	
	However, storing knowledge in the form of another graphical model is extremely cumbersome if the set of worlds changes quickly.	

	\begin{example}
		\todo{recall coin example, internalize biases, sets of dists, etc.}
	\end{example}
	\begin{example}
		\todo{Point to appendix where we discuss factor graph conversions: these internalize the energy}
	\end{example}
	
	
	\begin{vfull}
	\subsection{HOW TO THINK OF PDGS}
	\begin{itemize}
		\item A bayesian network with explicit higher order edges
		\item A vectorized / bundled version of conditional probability spaces that includes torsion
		\item An attention-shaped diagram into the Markov Category
		\item A second-order constraint on worlds that allows you to modify free energies.
	\end{itemize}
	\end{vfull}

	\subsection{FUTURE WORK}
	\todo{Real Sentences.} Categorical representation, details of sum-product algorithm on graphs with sub-distributions, the orginal goal: modeling dynamic preferences.

	\begin{vfull}
		\subsection{Inconsistency} \label{sec:consistency-ethos}
	
		Believing a logically inconsistent formula can lead you to arbitrarily bad conclusions, having an infeasible set of constraints makes all answers you could give wrong, and having inconsistent preferences can lose you infinite money. We don't want to build inconsistent systems or agents with incoherent views of the world, and so, where possible, we design them so they cannot possibly be broken in this way. Suppose, for example, that we are trying to represent some quantity that must be a point on the unit circle. We could do it with an $x$ and $y$ coordinate, but this could be problematic because $x^2+ y^2$ might not be 1 --- it would be safer and harder to go awry if we parameterize it by an angle $\theta \in [0, 2\pi)$ instead. In the absence of performance benefits (like needing to regularly use the $y$-coordinate and not wanting to compute a sine), why would we take the first approach, introducing a potentially complex data-invariant, when we could avoid it?
		
		This line of thought, though common and defensible, is flawed if we are not perfectly confident in the design of both our system and the ways it can interact with the outside world. Using similar logic, we might ask ourselves: Why ask programmers for type annotations when all instructions are operationally well-defined at run-time?  Why use extra training data if there's already enough there to specify a function? Why estimate a quantity in two ways when they will yield different answers? Why repeat and rephrase your ideas when this could make you contradict yourself? Why write test cases when they could fail and make the project inconsistent? Why conduct an experiment if it could just end up contradicting your current knowledge?
		
		These questions may seem silly, but there is a satisfying information theoretic answer to all of them: redundancy, though costly, is the primary tool that we use to combat the possibility of being wrong. Maintaining data invariants can be expensive but provides diagnostic information; in the example above, settings of $x$ and $y$ that don't lie on the unit circle provide diagnostic information that something has gone wrong.
		In many cases, it is also possible to paper over problems by forcibly re-instating local data invariants: for instance, we could re-normalize any values of $x$ and $y$ (so long as $xy \neq 0$; we can chose an arbitrary point otherwise) at every step. While this would reduce inconsistency, it also hides red flags.
		
		Using a Bayesian Network to represent a probability distribution is like representing a circle with $\theta \in [0, 2\pi)$.
		By construction, the result must be a distribution, and nothing can possibly go wrong so long as we can always decide on exactly one distribution which is sufficient for our purposes.
		%	By construction, the result must be a point on the circle, and nothing can possibly go wrong so long as we're sure that we will always have exactly enough information to determine such a point (for instance, we could never be totally clueless about the point, or just know its $x$ coordinate).
		
		
		The process of mechanistically forcing invariants is homologous to the standard practice for factor graphs: practitioners will often just assume that the density it defines is normalizable, and either forcibly re-normalize or cleverly avoid computing the normalization constant while still assuming that one exists; behavior is usually left unspecified in the unlikely event that it is not defined or zero.
	\end{vfull}
	
	\section{CONCLUSION}
	\subsection{A LIST OF PDG BENEFITS}\label{sec:list-of-benefits}
	\begin{enumerate}[nosep]
		\item PDGs can represent both over-constrained and under-constrained mental states. 
		\item In particular, they may be inconsistent, which gives agents using PDGs the qualitatively new kind of `epistemic modesty': the possibility of realizing that something is wrong with their beliefs.
		\item Many standard algorithms, including as belief propogation, conditioning, and belief revision, can be regarded as resolution of inconsistency.
		\item PDGs can emulate the functionality of other graphical graphical models.
		\item PDGs are more modular, making it much less invasive to combine, reduce, or partially interpret parts of the model, compared to alternatives.
		\item The modularity enables type-forming rules which can be used to implement deductive inference.
		\item The many standard ways of adding and eliminating variables provides an answer to the question, ``why these possible worlds?''
		\item Compared with a standard constraint satisfaction problem, individual components have of have limited impact on the semantics.
		\item The class of free energies defined by PDGs is strictly more expressive than those given by alternative graphical models.
	\end{enumerate} % trade-off: harder to analyze.
	
	\subsubsection*{References}
	\printbibliography[heading=none]
	
	\onecolumn
	\appendix
	
	\section{PROOFS}
	\thmsetconvex*
	\begin{proof}
		Choose any two distributions $p, q \in \bbr{M}\SD$ consistent with $M$, any mixture coefficient $\alpha \in [0,1]$, and any edge $(A,B) \in \Ed$.
		
		By the definition of $\bbr{M}\SD$, we have $p(B = b \mid A = a) = q(B = b \mid A = a) = \bmu_{A,B}(a,b)$.  
		For brevity, we will use little letters ($a$) in place of events ($A = a$).
		Therefore, $p(a\land b) = \bmu_{A,B}(a,b) p(a)$ and $q(ab) = \bmu_{A,B}(a,b) q(a)$. Some algebra reveals:
		\begin{align*}
			\Big( \alpha p + (1-\alpha) q \Big) (B = b \mid A = a) &= 
			\frac{\Big( \alpha p + (1-\alpha) q \Big) (b \land a)}{\Big( \alpha p + (1-\alpha) q \Big) (a)} \\
			&= \frac{ \alpha p(b \land a) + (1-\alpha) q(b \land a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
			&= \frac{ \alpha \bmu_{A,B}(a,b) p(a) + (1-\alpha) \bmu_{A,B}(a,b) q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)} \\
			&=\bmu_{A,B}(a,b) \left(\frac{ \alpha  p(a) + (1-\alpha) q(a) }{\Big( \alpha p(a) + (1-\alpha) q (a)}\right)\\
			&= \bmu_{A,B}(a,b)
		\end{align*}
		and so the mixture $\Big(\alpha p + (1-\alpha) q \Big)$ is also contained in $\bbr{M}\SD$.
	\end{proof}
	
	
	\thmzetaconvex*
	\begin{proof}
		It is well-known that $D_{KL}$ is convex, in the sense that 
		\[ D_{KL}(\lambda q_1 + (1-\lambda) q_2 \ ||\ \lambda p_1 + (1-\lambda) p_2) \leq \lambda D_{KL} (q_1\ ||\ p_1) + (1-\lambda) D_{KL}(q_2\ ||\ p_2) \]
		Choose any edge $L \in \Ed$ from $A$ to $B$, and also any $a \in \mathcal V(A)$. 
		Setting $q_1 = q_2 = \bp(a)$, we get
		\[ D_{KL}(\bp(a) \ ||\ \lambda p_1 + (1-\lambda) p_2) \leq \lambda D_{KL} (\bp(a) \ ||\ p_1) + (1-\lambda) D_{KL}(\bp(a)\ ||\ p_2) \]
		Since this is true for every $a$ and edge, we can take a weighted sum of these inequalities for each $a$ weighted by $p(A=a)$, and therefore
		\begin{align*}
			\E_{a\sim p(A=a)} D_{KL}(\bp(a) \ ||\ \lambda p_1 + (1-\lambda) p_2) &\leq \E_{a\sim p(A=a)}\lambda D_{KL} (\bp(a) \ ||\ p_1) + (1-\lambda) D_{KL}(\bp(a)\ ||\ p_2) \\
			\sum_{(A, B) \in \cal L}~\E_{a\sim p(a)} D_{KL}(\bp(a) \ ||\ \lambda p_1 + (1-\lambda) p_2) &\leq \sum_{(A, B) \in \cal L}~\E_{a\sim p(A)}\lambda D_{KL} (\bp(a) \ ||\ p_1) + (1-\lambda) D_{KL}(\bp(a)\ ||\ p_2) \\
			\zeta(M ; \lambda p_1 + (1-\lambda)p_2) &\leq \lambda \zeta(M;p_1) + (1-\lambda) \zeta(M;p_2)
		\end{align*}
	\end{proof}
	
	\thmbnsRpdgs*
	\begin{proof}
		\def\Pa{\mathbf{Pa}}
		\def\H{\mathrm{H}}
		Let $\mathcal B = (G, f)$ be a quantitative Bayesian Network, where $G = (\N, \Pa)$ is a DAG on the variables $\mathbf X = \{ X_1, \ldots X_k\}$, with an acyclic parent relation $\Pa$, so that $\Pa(X)$ is the set of all parents of node $X \in \mathbf X$. As $\Pa$ is acyclic, there exists a (``topologically sorted'') total ordering $N_1, N_2, \ldots, N_k$ of all of the nodes in $\N$, such that every node's parents have smaller indices than the node itself---that is, if $N_i \in \N$, and $N_j \in \Pa(N_i)$, then $j < n$.
		
		If $\vec x$ is an assignment to every variable, the BN's global semantics give probability 
		\[ \Pr\nolimits_{\mathcal B}(\vec x) = \prod_{N \in \N} p(x_{N} \mid x_{\Pa(N)})  
			 = \prod_{i=1}^k p(x_{i} \mid x_{\Pa(N_i)}) 
		\]
		Where $x_N$ is the value of $\vec x$ for variable $N$, $x_{\Pa(N)}$ is the vector of values at every node that is a parent of $N$, and the second equation uses our order. 
		
		
		For any distribution, we have
		\[  \]
		\begin{align*}
			\H(X_1, X_2, \ldots, X_k)  &= \sum_{i =1}^k \H(X_i \mid X_1, \ldots X_{i-1}) \\
				&\leq \sum_{i = 1}^k \H(X_i \mid \Pa(X_i)) \\
				&= 
		\end{align*}
	
		
		Now, we can directly lower bound the entropy of this distribution:
		
		\begin{align*}
			H(\Pr\nolimits_{\mathcal B}) &= \sum_{\vec x \in \V(\mathcal B)} \Pr\nolimits_{\mathcal B} (\vec x) \log \Pr\nolimits_{\mathcal B} \\
				&= \sum_{\vec x} \left(\prod_{i=1}^k p(x_{i} \mid x_{\Pa(N_i)})\right)  \log \left(\prod_{i=1}^k p(x_{i} \mid x_{\Pa(N_i)}) \right) \\
				&= \sum_{\vec x} \prod_{i=1}^k p(x_{i} \mid x_{\Pa(N_i)})  \log \left(\prod_{i=1}^k p(x_{i} \mid x_{\Pa(N_i)}) \right) \\
				&= \\
				&\geq
		\end{align*}
	\end{proof}


	


	\section{Alternate Presentations}
	\subsection{Random Variables}
	If $\mathcal W = (W, \mathcal F, \mu)$ is a measure space, and $\mathcal X = \{ X_i: W \to \mathcal V(X_i) \}_{i \in I} $ is a collection of measurable random variables on $W$,\footnote{that is: $\mathcal V(X_i)$ is a measurable space, taking the form $(D, \mathcal D)$, and $X_i : W \to D$ is a set function such that for every $B \in \mathcal D$, the set $X_i^{-1}(B) \in \mathcal F$} and 
	{\color{gray}$\Ed \subseteq I \times I$ is a collection of pairs of variables such that the agent } 
	\todo{what is a way of phrasing this that doesn't sound like it's shoehorned in? $\Ed$ really can represent anything an agent knows. Any subjective conditional probability distribution $\mu'$ such that the only measurable subsets are ``axis aligned'', in that they involve queries on only one variable, can be represented by $\Ed$, and for other queries we can simply change variables.}, we call $(\cal W, X)$ an \emph{ensemble}.
	%and $(W, \mathcal F', p)$ is a subjective probability representing an agent's belief 
	
	
	\begin{prop}
		There is a natural correspondence between strict PDGs as defined in \Cref{def:model}, and ensembles such that \todo{spell this out explicitly to avoid vague categorical intuition} \ldots $\mu$'s are defined on same set and produce same values.
	\end{prop}
	\begin{proof}
		\textit{/outline:}
		On the one hand, $(\prod_{N \in \cal N} \mathcal V(N).\text{set}, \bigotimes_{N \in \cal N} \mathcal V(N).\text{algebra}, \bmu)$ is a measure space, with $\{X_N = \pi_N : \left(\prod\mathcal V(N')\right) \to  \mathcal V(N) \}_{N \in \cal N}$ a set of random variables
		
		and  on the other, $(I, \Ed, \mathcal X', \mu|_{\cal L})$ is a strict \MN.
	\end{proof}
	
	This is the technical underpinning of our flippant, noncommittal treatment of possible worlds: any time we are thinking in terms of random variables or probability distributions on a fixed set $W$, we can instead reduce
	
	
	The complexity of the representation is $O(XV + L V^2)$, compared to $O(XW)$
	
	\subsection{Hyper Graph Conversion}\label{sec:hyper-convert}
	We have mentioned that the direct definition in terms of hyper-edges is possible; we give it below.
	
	\begin{defn}[\MNH]\label{def:hypermodel}
		A \emph{\modelnamehyper} is a tuple $\mnvars[]$ where
		\begin{itemize}[nosep]
			\item $\N$~~is a finite collection of nodes
			\item $\Ed \subseteq 2^{\N} \times 2^{\N} \times \mathrm{Label}$~~is a set of directed edges, each of which has a source and target subset of $\N$.
			\item $\V$ associates each node $N \in \mathcal N$ with a set $\V(N)$ or $\V_N$, representing the values that node $N$ can take.
			\item $\bmu$
			 % $\colon\!\big(\!({\bf A,B})\colon \! \Ed \big) \to \prod\limits_{A\in \bf A} \!\! \V(A) \to \underline\Delta\left[\prod\limits_{B \in \bf B}\!\!\V(B)\right]$
			%%% Above is the type of $\bmu$. I think it's important to have it there.
			associates conditional probability (sub)-distributions on the joint settings of $\bf B$ indexed by the joint settings of variables in $\bf A$ for every edge $({\bf A,B}) \in \Ed$ %
			\note{The type of $\bmu$ is $\big(\!({\bf A,B})\colon \! \Ed \big) \to \V(A) \to \underline\Delta\V(B)$. It doesn't take up much space and answers lots of questions about the words above.}
		\end{itemize}
	\end{defn}
	
	Next, we can formaly show how one might convert between the two representations.
		
	\begin{example}[continues=ex:planet]
		In early examples, we displayed a directed hyperedge. While we would like to maintain this intuition, the way to do this is using only graphs with edges, as in \Cref{def:model}, is to use intermediate nodes:
		
		\begin{center}
			\begin{tikzpicture}
				\node[dpadded] (S) at (-0.5, 2) {$S$};
				\node[dpadded] (C) at (3.1, 2) {$C$};
				\node[dpadded] (L) at (1.3,0) {$L$};
				\node[dpadded] (W) at (-2,0) {$W$};
				
				\node[dpadded,light pad] (SC) at (1.3, 1.4){$S \times C$};
				
				\draw[arr, ->>] (SC) -- (S);
				\draw[arr, ->>] (SC) -- (C);
				\draw[arr] (SC) -- (L);
				\draw[arr] (W) -- (L);
			\end{tikzpicture}
		\end{center}
		We will sometimes use double headed arrows like this to emphasize degenerate conditional distributions, which are deterministic.
		We can now present this PDG formally with the elements specified in definition \ref{def:model}.
		
		\hfill\begin{minipage}{0.4\textwidth}	
			\begin{align*}
				&\mathcal N = \{S,~ C, ~L, ~W, ~S\times C \} \\
				&\Ed = \{ (S \times C, L), (W, L), (S\times C, S), (S\times C, C)\} \\
				\mathcal V &\left\{\begin{aligned}
					\mathcal V(S) &= \{\mathit{b}, \mathit{s} \}\\
					\mathcal V(C) &= \{ \mathit{r}, \mathit{g} \} \\
					\mathcal V(L) &=  \{ l, \lnot l \} \\
					\mathcal V(W) &= \{ \textit{none}, \textit{some}, \textit{mostly}\}\\
					\mathcal V(S \times C) &= \mathcal V(S) \times \mathcal V(C) 
					% = \small\text{$\{(big, rocky), (small,rocky), (big, gasseous), (small,gasseous)\}$}
				\end{aligned}\right.\\
			\end{align*}
		\end{minipage}%
		\begin{minipage}{0.5\textwidth}
			\begin{align*}
				\bmu & \left\{~\begin{aligned}
					\boldsymbol\mu[S\times&C, L] = &\boldsymbol\mu[S\times&C, S] = \\[-0.6em]
					&\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{$l$,$\lnot l$}
						.1 & .9 \\
						.2 & .8 \\
						.05 & 0.95 \\
						0.001 & 0.999
					\end{idxmat} 
					&&
					\quad\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{s,b}
						0 & 1 \\
						1 & 0 \\
						0 & 1 \\
						1 & 0
					\end{idxmat}
					\\[0.5em]
					\boldsymbol\mu[W, &L] =  &\boldsymbol\mu[S\times&C, C] =\\[-0.6em]
					&\begin{idxmat}{{none}, {some}, {mostly}}{$l$,$\bar l$}
						0 & 1 \\
						.005 & .995 \\
						.05 & 0.95 \\
					\end{idxmat}
					&&
					\quad\begin{idxmat}{{b,s}, {s,r}, {b, g}, {s,g}}{r,g}
						1 & 0 \\
						1 & 0\\
						0 & 1 \\
						0 & 1 
					\end{idxmat}
				\end{aligned}\right.\\[-1em]
			\end{align*}
		\end{minipage}
		\vspace{0.5em}
		
		This works, but the structural overhead of the additional de-sugaring: the $\boldsymbol\mu[S\times C\to S]$ and $\boldsymbol\mu[S\times C\to C]$ tables, as well as the set $\mathcal V(S \times C)$ seem like they didn't need to be specified, and one might even feel that it would be a mistake to allow any other table. Some reasons for this design decision include:
		\begin{itemize}[nosep]
			\item It is easier to prove things about graphs than directed hyper-graphs. Similarly, defining composition and paths becomes a lot simpler.s
			\item We can eliminate the clunkiness by fusing the model with an algebra, as in \Cref{sec:algebra} --- which will give us a lot more than modeling the hyperedges directly.
			\item We will eventually also want to allow for the possibility of keeping only a relaxed, approximate representation of $\mathcal V$ and $\bmu$, and in particular, of the ones constructed logically in this way. By specifying them explicitly for now, we will have to do less work to regain manual control in \Cref{sec:abstraction}
		\end{itemize}
	\end{example}
	
	
	The choice to formalize PDGs this way is a design consideration that makes some things cleaner, but we can just as well formalize multi-tailed edges directly, as follows:
	
	\begin{defn}[\MNH]\label{def:modelhyper}
		A \textit{\modelnamehyper} (\MNH) is tuple $(\N, \boldsymbol\Ed, \V, \bmu)$ where $\N$ and $\V$ are as before, $\boldsymbol\Ed \subseteq 2^\N \times 2^\N \times \mathrm{Label}$ is a set of `hyperedges', i.e., edges whose source and target are sets of nodes, and for each edge $L = ({\bf A, B}, \ell) \in \boldsymbol\Ed$, we have a table of distributions $\bp$ on \emph{joint settings} of the variables in the set $\bf B$ for each joint setting of the variables in $\bf A$.
	\end{defn}
	
	\Cref{thm:hyperequiv} shows PDGs and \MNH s to be equivalent, though in different cases one may seem more natural than the other, as illustrated in the following theorem.
	
	\begin{theorem}[restate=thmhyperequiv]\label{thm:hyperequiv}
		Every \MNH\ $H$ is equivalent to a PDG $\sfM$ with additional variables. That is, for each semantics $\bbr{-}$ we define, $\bbr{H} = \bbr{\sfM}$.
	\end{theorem}
	\begin{proof}
		\todo{}
	\end{proof}
	
	This theorem justifies taking the PDG as primary, an ordinary collection of nodes and edges, which makes it cleaner to define and compose paths. 

	
	\section{Formalism for other Graphical Models}
	\begin{defn}
		A Baysian network (BN) is a tuple
		\[
		\mathcal B = \left(\mathcal N : \mathbf{FinSet}, ~~\mathrm{Par}: \mathcal N \to 2^{\mathcal N},~~ \mathcal S: \mathcal N \to \mathbf{FinSet},~~\Pr: \prod_{N : \mathcal N}  \left[ \mathcal S_N \times \left(\prod_{P : \mathrm{Par}(N)} \mathcal S_P\right)  \to [0,1] \right] \right)
		\]
		such that
		\begin{itemize}[nosep]
			\item the graph $\bigcup_{N, P \in \mathrm{Par}(N)}(N, P)$ is acyclic, i.e., there exists no cycle of nodes $N_0, N_1, \cdots, N_k = N_0$ in $\mathcal N^k$ such that $N_{i+1} \in \mathrm{Par}(N_i)$ for each $i \in \{0, 1, \cdots, k\}$.
			\item For all $N \in \mathcal N$, $\Pr(N)$ is a probability distribution on $\mathcal S_N$, i.e., 
			\[ \forall N\in \mathcal N.~\forall \vec{p} \in {\prod_{P : \mathrm{Par}(N)} \mathcal S_P}.~~ \sum_{n \in \mathcal S_{N}} \Pr_N(\vec{p}, n) = 1\]
		\end{itemize}
	\end{defn}
	
	
	\begin{defn} \label{def:bnconvert-formal}
		If $B = (\mathcal N, \mathrm{Par}, \mathcal S, \Pr)$ is a Bayesian Network, then let $\Gamma (B)$ denote the corresponding PDG given by the procedure in \Cref{sec:bn-convert}. Explicitly, 
		\[ \Gamma\mathcal B :=  (\mathcal N', \Ed, \mathcal V, \bmu) \]
		where % $\mathcal N'$ is the original nodes, plus
		\begin{align*}
		\mathcal N' &=  \left\{ \Big.\{N\} \mid N \in \mathcal N\right\} \cup \left\{ \mathrm{Par}(N) ~\middle|~ N \in \cal N \right\} \\%
		\Ed &= \left\{ \vphantom{\Big|}(\mathrm{Par}(N), \{N\}) \mid N \in \mathcal N \right\} \cup 
		\left\{\vphantom{\Big|} (P, \{X\}) \mid X \in P, P = \mathrm{Par}(N) \text{ for some }N \in \mathcal N \right\} \\
		\mathcal V_N &= \prod_{X \in N} \mathcal S_X \\
		%					{\color{gray}\Sigma_N = \bigotimes_{X \in N} 2^{\mathcal S_X}, \text{the product algebra of discrete $\sigma$-algebras}} \\
		\mathbf p &= \begin{cases}
		(\mathrm{Par}(N), \{N\}) &\mapsto \lambda(p, B).~ \displaystyle\sum_{b \in  B} \Pr(b \mid p) \\
		(P, X) &\mapsto, \lambda (p, B).~ \displaystyle \mathbbm 1_{\displaystyle\pi_X(p) \in B}
		\end{cases}
		\end{align*}
		%\cpm p(\frac{a}{z}|b)
	\end{defn}
	All we've done is explicitly add parent nodes and projection edges to our graph, and also subtly (by adding curly braces in the right places and taking unions rather than disjoint unions) eliminated the duplicate nodes arising from edges in the original BN which only have a single parent.
	
	\section{Thermodynamics}\label{sec:thermo-background}
	Let $W$ be a finite set of states.
	
	\textbf{From Potentials to Distributions.}
	Suppose $U: W \to \mathbb R$ is a potential function, assigning an energy to each state. Imagine there's a particle that could be in any number of states, that the only consideration in transitioning from one state $w$ to another $w'$ is the energy of each state,%
		\footnote{The thermodynamics, of course, ignore the kinetics of the system. Thought of an Ising model, the edges form a complete graph, and the edge weights are uniform. Thought of as a stochastic matrix, it is rank one, whose latent variable is just the energy of a state.}
	and that low-energy states are more exponentially more likely,\footnote{this can also be replaced by weaker assumptions; see the thermodynamics literature for more motivation}
	the unique stationary state is the Boltzmann distribution:
	\begin{equation}
		 \mu(w) \propto \exp( - U(w) / kT ) \label{eq:boltzmann-appendix}
	\end{equation}

	where $k$ is the Boltzmann constant and $T$ is the thermodynamic temperature. Note that at unboundedly high temperatures, the differences between potentials don't matter (all states are equally likely), whereas at as the temperature approaches zero, the Boltzmann distribution puts zero mass on anything that's not a global minimum, and otherwise splits the mass equally. Therefore, if $U$ achieves a unique global minimum $w^*$, the corresponding $\mu(w) = \delta_{w,w^*}$ is a point mass on the minimum energy world $w^*$.
	
	It is standard and notationally useful to re-parameterize with the inverse temperature $\beta := 1/kT$ -- and we will refer to the Boltzmann distribution associated to a given potential $U$ (and inverse temperature $\beta$) as 
	\[ P_{\beta}(U) := w \mapsto  \frac{1}{Z_U(\beta)}\exp\Big(-\beta U(w)\Big) \]
	Where $Z_U(\beta) = \sum_{w \in W} \exp(-\beta U(w))$ is a normalizing factor, sometimes called the ``partition function''.	
	
	\textbf{From Distributions to Potentials}.	
	On the other hand, under similar assumptions, if given a probability distribution $\mu$ over $W$, there is a natural potential energy that resulted in it, 
	\[ E_{\beta}(\mu) := w \mapsto \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right)  \]
	which might be recognizable as negative log liklihood or the ``surprise'' of an event happening. By construction, $P_\beta \circ E_\beta$ is the identity on probability distributions:
	\begin{align*}
		 \Big(P_\beta \circ E_\beta(\mu)\Big) (w) &= \frac{1}{Z_{ E_\beta (\mu) }} \exp \left( - \ln \left(\frac{1}{\mu(w)}\right) \right) \\
		 &= \left(\frac{1}{\sum\limits_{w' \in W} \mu(w')}\right)\mu(w) \\
		 &= \mu(w)
	\end{align*}
	and $E_\beta \circ P_\beta$ is the identity on potential functions (up to a constant factor):
	\begin{align*}
		\Big(E_{\beta}\circ P_\beta(U)\Big)(w) &= \frac{1}{\beta} \ln \left(\frac{1}{\frac{1}{Z_U(\beta)}\exp(-\beta U(w))}\right) \\
		&=  \frac{1}{\beta} \Big[\ln Z_U(\beta) - (-\beta U(w)) \Big]\\
		&= U(w) + \frac{1}{\beta} \ln Z_U (\beta)
	\end{align*}
	The constant factor $-\frac{1}{\beta} \ln Z_U(\beta)$ coincides with the Heimholtz free energy of the system. Note that at constant temperature, this quantity is a durable feature of either a distribution or its associated energy landscape. 
%	
%	\begin{align*}
%		 0 = -\frac{1}{\beta} \ln Z_U(\beta) &= - \frac{1}{\beta} \ln \sum_{w \in W} \exp(-\beta U(w)) \\
%		 \iff 1 = \sum_{w \in W} \exp(-\beta U(w))
%%		 	&= -\frac{1}{\beta} \mathop{\mathrm{LSE}}_{w \in W}(-\beta U(\beta))
%	\end{align*}

	\textbf{Free Energy and Favorability.} Given a potential $U$, corresponding to a distribution $\mu$ as above, we now turn the question of how thermodynamically favorable a new distribution $\nu$ would be.%
		\footnote{From a statistical mechanics perspective, $W$ are the micro-states of the system, and a distribution over them is a configuration, or a macro-state.}
	For which we use the Gibbs free energy, $G_U(\nu) := {\E}_\nu( U ) - \frac{1}{\beta} H(\nu)$, which we think of a system as minimizing. The intuition here is that our new distribution $\nu$ is favorable if it has low average energy. However, at higher temperatures it also costs energy to compress the distribution: while a point mass at the minimum value of $U$ may be the lowest energy distribution, tightly controlling it to that degree also costs energy, when there's some ambient temperature causing randomness. From an epistemic perspective, even if a belief distribution $p$  is the one that best fits constraints, one might want to temper this by other possible configurations, and more so when there's higher ambient macroscopic uncertainty (temperature). Note also that the Gibbs Free Energy is a weighted probability distribution: it assigns a `favorability' score to distributions.
	
	If $U$ was generated by a probability distribution $\mu$, we then have
	
	\begin{align*}
		G_\mu(\nu) &= {\E}_\nu( E_\beta(\mu) )  - T S(\nu) \\
		&= \sum_{w \in W}\nu(w) \frac{1}{\beta} \ln \left(\frac{1}{\mu(w)}\right) - T \left[k \sum_{w \in W} \nu(w) \ln \left(\frac{1}{\nu(w)}\right)\right]\\
		&=  \frac{1}{\beta}\left[\sum_{w \in W}\nu(w) \ln \left(\frac{1}{\mu(w)}\right) - \sum_{w \in W} \nu(w) \ln \left(\frac{1}{\nu(w)}\right)\right]\\
		&=  \frac{1}{\beta}\left[\sum_{w \in W}\nu(w) \left(\ln \frac{1}{\mu(w)} - \ln \frac{1}{\nu(w)}\right)\right]\\
		&= \frac{1}{\beta} D \left(\nu || \mu \right)
	\end{align*}

	Where $D(\nu || \mu)$ is the relative entropy from $\nu$ to $\mu$. 
	
	Note that \todo{}
	\begin{enumerate}
		\item By Gibbs inequality, the $D(\nu || \mu) \geq 0$, and equal to zero precisely when $\nu = \mu$, and so the free energy of a configuration $\nu$ in a potential that was designed for $\mu$ is minimized by $\mu$ itself.
		
		\item 
	\end{enumerate}

	
	


	
	\textbf{Free Energy as a Design Tool.}
	
	This connection between thermodynamics and probability theory is already well utilized:
	\begin{enumerate}
		\item A Markov Random Field is specified with potentials $U_e$ for each edge; a factor graph is specified with potentials for a subset of cliques.
		\item The belief propagation algorithm computes local minima of the Bethe free energy, an approximation to the true Gibbs free energy.
	\end{enumerate}


	The dominant representation tool for mental states is the probability distribution, rather sets or weighted sets of them. % This is partly because they are easier to compute with, and because when faced with decisions at gun point, they are the most
	One issue with this is that there are distinct mental states that collapse to the same probability distribution (e.g., the coin flip: being uncertain about a process vs its outcome). The second one is that one might not have the right space for the distribution
	
	The insight here is that these are related: one can simply internalize the structure of the uncertainty. This some precedent for this: Pearl's rule, for instance, prescribes a new random variable to describe the uncertainty.	
	%%%
	
	Consider a factor graph on a set of variables $\{ X_i \}$, with only a single factor $\phi$ which connects to every variable. The free energy is $G_\phi(U)$
	
	\[ \frac{1}{\sum_{\vec x} \phi(\vec x)} \phi(u) \]
	
%	The normalization constant $Z = \sum_{\vec x} \phi(\vec x)$
	
	Any factor graph defines a free energy by \todo{finish}
	
	The Bethe approximation to the free energy is an estimate based only on the marginals on single pairs of nodes.
		
	With a PDG, the free energy becomes
	\[ \sum \]
	\todo{Write out $\zeta$, proofs of theorems}
	
	\section{Overview And Conversions Between Graphical Models}
	\label{sec:many-relations-graphical-models}
	
	\todo{There is a ton to do here.}
	
	
	\section{Structure-editing PDG Operations}
	
	While both PDGs and \MNH s are equivalent, and despite the fact that dealing with sets of variables is standard, we chose PDGs over \MNH s as the face of the paper. One of the primary reasons to do this is that it puts products on equal footing with other equally valid structural modifications we could have done instead, rather than specializing the definitions for products.
	
	\begin{enumerate}
		\item Latent variable nodes, e.g., through VAEs. Useful for representation learning and modeling bounded agents that just remember the gists of things.
		
		\item Sums nodes. For when one is being forced to chose between two options which might otherwise be unrelated, and the basic constructor for variables from points.
		
		\item Exponential nodes. Any positive temperature arrow can be reasoned about through expansion into its parameters.
		
		\item Compression nodes: e.g., truncation nodes for propositions. It may not matter exactly what proof you have so long as you've proved one exists. That a variable takes a value may be just as important as it.
	\end{enumerate}
	
	
	\section{More Examples}\label{sec:more-examples}
	
	\begin{example}
		\label{ex:corrob}
	\end{example}
	
	\begin{example}[Maximum Entropy with cpts is not the BN distribution]\label{ex:counterexample}
		Consider the Bayesian network 
		\begin{tikzcd}[cramped, sep=small]
			A \ar[r] & C & B \ar[l]
	 	\end{tikzcd}
		where $A$ and $B$ are binary, and $C$ can take $2^k$ values, including $c_0$. We now give the associated tables: both $A$ and $B$ get prior unconditional probabilities of $\nicefrac12$ apiece, and set $C$'s cpt to be
		\[
			\begin{idxmat}{{$a$,$b$},{$\bar a$, $b$},{$a$, $\bar b$},{$\bar a$, $\bar b$}}{$\Delta C$}
				\mathit{Uniform} \\ \delta_{c,c_0 }\\ \delta_{c,c_0} \\ \mathit{Uniform} \\
			\end{idxmat}
		\]
		where $\delta_{c,c_0}$ is the degenerate distribution that puts all mass on $c_0$. Looking at entropy, the uniform distribution on $C$ gets $k$ bits, and each of $A$ and $B$ we know each give one bit. 
		The semantics of a BN require that $A$ and $B$ are independent, since neither is a descendent of the other and neither has parents.  However, doing so results in a distribution of entropy $H(p) = 2 + k/2$ (one for each of the independent bits, and an expected k/2 bits from getting the uniform distribution on $C$ half the time), whereas if we correlate $A$ and $B$ so that they are always equal, we get $1 + k$ bits, one total bit from $A$ and $B$, and $k$ from $C | A,B$. For any finite $k$, this is still not the maximum entropy distribution, but it is much higher entropy than the one the BN suggests.
		
		Therefore the maximum entropy distribution consistent with the tables does not encode the independece assumption that a BN does. 
	\end{example}
	
	\begin{example}\label{ex:randomvars}
		Consider random variables $X_1$, $X_2$  on a set
                $\Omega$ of outcomes (distributed according to $p$),
                taking values in the set $\mathcal X$. This can be
                represented as the PDG below. 
		\begin{center}
			\scalebox{0.8}{
				\begin{tikzpicture}
				\node[dpadded] (1) at (0,0) {$\sf 1$};
				\node[dpadded] (W) at (2.5,0) {$\Omega$};
				\node[dpadded] (X1) at (5,1) {$X_1$};
				\node[dpadded] (X2) at (5,-1) {$X_2$};
				
				\draw[arr] (1) to node[fill=white]{$p$} (W);
				\draw[arr, ->>] (W) to node[fill=white]{$X_1$} (X1);
				\draw[arr, ->>] (W) to node[fill=white]{$X_2$} (X2);
				\draw[arr, gray] (X1) to node[right] {$p$} (X2);
				\end{tikzpicture}}
		\end{center}
		The setup so far, in black above, can be captured with a BN, but it is impossible to also articulate conditional probabilistic relations amongst the variables in the same time: in a BN, once we add a variable $\Omega$ which caracterizes all possible worlds as a parent of a variable (e.g., $X_2$), any other dependences will be irrelevant. Given a world $\omega$ and values of other variables, the cpt associated to $X_2$ would simply deterministically return the value of $X_2$ in $\omega$. 
		
		As a result, a BN has to choose between encoding conditional probabilistic information, and the knowledge of the complete information from $\Omega$. This is not true with a PDG, which makes it possible to simultaneously model the structure of the random variables around an agent's beliefs, in addition to the beliefs themselves.
	\end{example}

	
	\begin{vcat}
		\section{Categorical Presentation}
		\note{I will not put any time into this, as it's not going in the paper, but it's here as a placeholder, and I'll list some reasons why this is worth thinking about.}
		One reason this works out so nicely is every construction is universal. We can in fact give a simpler categorical presentation of PDGs for those who already know category theory. The highlights are as follows:
		\begin{enumerate}
			\item A PDG is an attention-shaped diagram in the Markov category. That is, functor from the free category generated by the graph $(\mathcal N, \Ed)$ representing attention, to the Markov category. Indeed $\mathcal V$ is the action on objects, assigning each $\mathcal N$ to a measurable set, $\bmu$ is the action on morphisms, sending edges in $\Ed$ to Markov kernels between their associated objects. 
			\begin{enumerate}
				\item Composition works out in general as we place no restrictions on anything, but
				\item If every edge in $\Ed$ represents the causal structure of their relationship, then the image of the resulting diagram will be flat, and so effectively there will only be at most one, belief, and no possibility of conflict.
				\item Interpreting with a different model of uncertainty (such as the powerset, giving us non-deterministic possibility) is simply an exchange of interpretation. However, for nice interaction with deterministic functions and logic, this notion of uncertainty must be a monad.
			\end{enumerate}
			
			\item This highlights the role of the ``qualitative'' and ``quantitative'' versions of this framework (which work out much more cleanly than for BNs in a categorical sense)
			
			\item A limit of this diagram is a space of worlds and all of the random variables as functions. A colimit is a the strongest thing that must be true according to the model (suspicion: this is somehow related to common knowledge). There is some strangeness about how samples work that I have not yet figured out.
		\end{enumerate}
		
		
		\section{Algebra}\label{sec:algebra}
		\begin{defn}
			If $\sigma$ is a signature, a $\sigma$-PDG $M'$ on a PDG $M=(\mathcal N, \Ed, \mathcal V, \mu)$ is a \modelname\ $(\mathcal N', \Ed', \mathcal V', \mu')$ such that
			\begin{itemize}
				\item $\mathcal N':= T_\sigma(\mathcal N)$ is the term algebra for the signature $\sigma$ over the alphabet $\Sigma = \mathcal N$.
				\item $\Ed' = \Ed \cup \Ed^\sigma$ is $\Ed$ extended with extra edges for operations that are 
			\end{itemize}
		\end{defn}
		
		\begin{example}
			content
		\end{example}		
	\end{vcat}
	

% \end{notfocus}
\end{document}
