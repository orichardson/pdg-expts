\subsection{As Probabilisitic Programs}\label{sec:prog-semantics}

One final way of viewing PDGs is as a set of probabilistic programs, corresponding to the edges.
Conditional distributions can be thought of as probabilistic programs. As a result, we can compose and run them: paths from $A$ to $B$ correspond to noisy estimates of $B$ from $A$.

Specifically, if $f(b \mid a) : \mathcal V_A \to \Delta \mathcal V_B$ and $g(c \mid b) : \mathcal V_B \to \Delta \mathcal V_C$ are conditional distributions, then the probabilistic composition $g\circ f : \mathcal V_A \to \Delta\mathcal V_C$ is
\begin{align*}
(g\circ  f) (c \mid a) :=  \sum_{b \in \mathcal V B}\!\! f (b \mid a)\ g(c \mid b)
\end{align*}

This can be recognized as a matrix multiplication $f$ and $g$ regarded as sub-stochastic matrices.
Thinking about graphical models this way makes thinking about chains of reasoning simpler, gives us a way out of storing probability tables, and suggests additional applications.

More formally, we can define
\[ \bbr{M}_\lambda = \left\{
\begin{aligned}
 \text{paths } p = N_0 \xrightarrow{p^1} N_1 \xrightarrow{p^2}\cdots\xrightarrow{p^n}N_n \\
 \text{such that } (N_{i-1}, N_i, p^i) \in \Ed^\sfM
\end{aligned}
\right\} \]

\begin{example}
Composition of arrows in a tables in chain is simply an easy case of varaible elimination.

\[
\scalebox{0.8}{
\begin{tikzcd}[dpad, ampersand replacement=\&]
A \ar[r]\& C
\end{tikzcd}\hspace{3em}
\begin{tikzcd}[dpad, ampersand replacement=\&]
A \ar[r]\& B \ar[r] \& C
\end{tikzcd}}
\] 

Conversely, factorization of a table $A \to C$ into tables $A \to B$ and $B \to C$ (i.e., a stochastic matrix factorization) corresponds to splitting a program into two steps, and the data necessary to describe it will be smaller if $|B|$ is small.
\end{example} 

% It also has not escaped us that PDGs have a particularly nice description in categorical terms, which we do not pursue further here.

Furthermore, thinking about the mental state of an agent as a collection of programs you could run from any concept gives our first natural interpretation of a sub-distribution (more in \Cref{sec:full-model}): probability mass assigned to $\none$ by a edge $p$ has not terminated yet (if at all).
Even given infinite time, some paths in $\bbr{\sfM}_\lambda$ may be infinite.

\begin{defn}
A PDG $\sfM$ is \emph{strongly consistent} if every collection of paths $P \subseteq \bbr{\sfM}_\lambda$ is compatible, in that
$$\bigcap_{p \in P}\ \bbr*{\vphantom{\Big|}p^0\circ \cdots\circ p^k}\SD \neq \varnothing$$
\end{defn}

\begin{example}
Bayesian Networks and conditional Bayesian Networks are strongly consistent.
\end{example}

\begin{prop}
% $ \text{strongly consistent}  \subsetneq \quad
% \text{strictly consistent}  \subsetneq  \text{consistent} $
Any PDG $M$ that is strongly consistent is also consistent, but some strongly consistent PDGs are not strictly consistent.
\end{prop}

% \begin{example}
% The trace graph of a
% \end{example}


% This means we can sample them
% Also, compose them
% and a

