

\documentclass{article}


\input{../model-commands.tex}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}

\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/]  % activate!
\usepackage{etoolbox}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\cleartheorem{defn}
\declaretheorem[name=Definition,style=definition,qed=$\square$]{defn}


%\let\Horig\H
%\renewcommand{\H}{\mathop{\mathrm H}}
%\newcommand{\E}{\mathop{\mathbb E}}
\newcommand{\bp}[1][L]{\mathbf{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal A}

\DeclareMathAlphabet{\mathdcal}{U}{dutchcal}{m}{n}
\DeclareMathAlphabet{\mathbdcal}{U}{dutchcal}{b}{n}

\newcommand{\ed}[3]{#2
	\overset{\smash{\mskip-5mu\raisebox{-1pt}{$\scriptscriptstyle
				#1$}}}{\rightarrow} #3} 
\newcommand{\alle}[1][L]{_{ \ed {#1}XY}}

\newcommand{\dg}[1]{\mathbdcal #1}
\newcommand\Pa{\mathbf{Pa}}
\newcommand{\PDGof}[1]{{\mathbdcal{p\kern-0.05em d\kern-0.125em g}} (#1)}
%\newcommand{\PDGof}[1]{{\dg M}_{#1}}

%Crazier future/past operations
\DeclareMathOperator\dcap{\mathop{\dot\cap}}
\newcommand{\tto}{\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}

%\DeclarePairedDelimiter{\bbr}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\SD}{\llbracket}{\rrbracket_{\text{sd}}}
%\DeclarePairedDelimiterXPP{\SD}[1]{}{\llbracket}{\rrbracket}{_{\text{sd}}}{#1}
\newcommand{\IDef}[1]{\mathit{IDef}_{#1}}
\newcommand\Inc{\mathit{Inc}}

\newcommand{\none}{\varobslash}
\newcommand{\begthm}[2]{\begin{#1}[restate=#2,label=#2]}
%\twocolumn
\title{The PDG Manual}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}
\begin{document}
	\maketitle
	\tableofcontents
	\listoffigures
	\listoftheorems
	\clearpage

%	\section{About PDGs}
	\def\pdgvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1, \alpha#1,\beta#1)}
	\begin{defn}[sPDG]\label{def:sPDG}
		A strict PDG is a tuple $\pdgvars[]$ where
		\begin{description}[nosep]
			\item[$\N$]~is a finite collection of nodes, which are identified with variables,
			\item[$\Ed$]~is a collection of directed edges (arrows), each with a source, target, and a (possibly empty) label.
			\item[$\V$]~associates each node $N \in \N$ with a set $\V(N)$,
			representing the values that the variable $N$ can take. 
			\item[$\mathbf p$] associates, for each edge $L = (X,Y, \ell) \in \Ed$ and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$, whenever $\beta_L > 0$.
			\item[$\beta$]~associates to each edge $L$, a number in $[0,\infty]$, indicating certainty in the conditional distribution $\bp(Y \mid X)$ 
			\item[$\alpha$]~associates to each edge $L$, a number in $[0,1]$, indicating degree of belief that $L$ holds causally.
		\end{description}
		\vspace{-1.4em}
	\end{defn}

	If $\dg M$ is a PDG, we reserve the names $\pdgvars[^\dg M]$
	for its components, so that we may reference one (e.g.,
	$\Ed^\dg M$) without naming them all explicitly. We may further omit the superscript in contexts where only one PDG is present. 
	We write $\V(S)$ for the set of possible joint settings of a set $S$
	of variables; in particular, 
	we write $\V(\dg M)
	= \prod_{N \in \N^\dg M} \V^\dg M(N)$
	for all settings of the variables $(\N^\dg M, \V^\dg M)$.
	
%	\subsection{Semantics}
	\begin{defn}[set of distribution semantics] \label{def:set-semantics} 
		If $\dg M\!=\!\pdgvars[]$ is a PDG, let $\SD{\dg M}$ be the \emph{s}et of \emph{d}istributions over the variables in $\dg M$ whose conditional marginals are exactly those given by $\mat p$.
		That is, $\mu \in \SD{\dg M}$ iff, for all edges $L = (X,Y) \in \Ed$,  $x \in \V(X)$,  and $y \in \V(Y)$, we have that $\mu(Y = \cdot \mid X\!=\! x) = \bp(x)$.
		{
			\[ \SD[\Big]{\dg M} = \!\left\{\mu \!\in\! \Delta \V_\none (\dg M) \middle|\!
			\begin{array}{l}
				\mu(B\!\! =\!\!b \mid A\!\!=\!\!a) \geq \boldsymbol\mu_L(b \mid a) \\[0.1em]
				~\text{$\forall (A, B,\ell) \!\in\! \Ed$, $a \!\in\!\mathcal V_A$, $b \!\in\! \mathcal V_B$} \end{array}\!\!\! \right\}\]
		}
		$\dg M$ is \emph{consistent} if $\SD{\dg M}$ is inhabited (non-empty), and \emph{inconsistent} otherwise.
	\end{defn}

\begin{defn}[incompatibility and inconsistency]\label{def:inc}
	The \emph{incompatibility} of a PDG $\dg M = \pdgvars[]$ with
	a joint distribution $\mu$, denoted $\Inc_{\dg M}(\mu)$, is  
	\[
	\Inc_{\dg M }( \mu) := 
	\!\!\!\sum \alle \beta_L \E_{x \sim \mu_{_X}}
	\left[\kldiv[\Big]{ \mu(Y\!= \cdot\mid X \!=\! x) }{\bp(x) } \right] ,
	\]
	where $\kldiv{\mu}{\nu} = \sum_{w} \mu(w) \log\frac{\mu(w)}{\nu(w)}$ is the 
	relative entropy from $\nu$ to $\mu$.
%	
			The \emph{inconsistency} of $\dg M$, 
		denoted $\Inc(\dg M)$, is the
		minimum possible incompatibility of $\dg M$ with any
		distribution $\mu$,  
		\[ \Inc(\dg M) = \inf_{ \mu \in \Delta [W_{\cal V}]} \Inc_{\dg M}(\mu) . \]
\end{defn}
$\SD{\dg M}$ and $\Inc_{\dg M}$ distinguish only
between distributions based on their compatibility with
$\dg M$, but even among distributions that match the
marginals, some more closely match the qualitative structure
of the graph than others.  
Qualitatively, an think of an edge $\ed LXY$ of a PDG $\dg M$ as a claim that the value of $Y$ can be (noisily) computed from
$X$ alone.  
Therefore, to best match the qualitative structure of $G$, statistical asymmetries and dependencies between variables should
 be efficiently described by giving the conditional probabilities corresponding to the edges of $G$. After all, if we believe the structure of $G$, then these conditional probabilities describe every possible interaction between variables. 

To formalize this, we require only the underlying multigraph $G^{\dg M} :=
(\N^{\dg M}, \Ed^{\dg M})$ of $\dg M$. 
Given $G$ and $\mu$, contrast the amount of
information required to 
\begin{enumerate}[label=(\alph*)]
	\item directly describe a joint outcome  $\mat w ~ \sim \mu$
	drawn from $\mu$, and 
	\item separately specify, for each edge $\ed LXY$, the value
	%joe10
	%          $\mat w_Y$  given the value $\mat w_X$, in expectation. 
	$\mat w_Y$ (the projection of $\mat w$ onto the variable
	$Y$) given the value $\mat w_X$, in expectation. 
\end{enumerate}
% (a)  and (b)
When these two quantities are equal,
the total length (in expectation) of specifying the outcome on each link, is precisely the same length as
an optimal description of the joint outcome. That is, $\mu|_G$ carries enough information to precisely resolve all of the randomness of $\mu$.
%joe10: misplaced  
%In particular, this is true if
%$\mu$ is a degenerate distribution, in which case the agent, who knows
%$\mu$, requires no descriptions at all.  
%joe10*: Your definition of (b) > (a) seems to me to be identical to
%your eplanation of (a) > (b).  I thin the former is wrong.
%oli12*: Below I wrote
% "there are extra correlations in \mu not described by G" which is the
% exact opposite of "extra dependences in G that are not described by \mu"
%joe11*: I don't understand your sentence above.  If there are
%correlations, then there must be dependencies.  
%oli13: I... think we might be on the same page? The implication is reversed
% in the two quoted phrases.
%oli12: The former is correct, because Entropy measures uncertainty, not dependence.
% also "dependence" of a distribution is undefined, which is why I write
% "correlations". Reverting, because the focus is wrong. It's about \mu, not G
%
%
%If (b) $>$ (a), then there are extra correlations in $\mu$ that are
% If (b) $>$ (a), then there are dependencies suggested by $G$ that are
% not present in $\mu$
%oli12: This special case is not qualitatively different from the one where
% two different edges X -> Y  and  Y <- Z
% , or there may be parallel edges in $G$ (since it is a multigraph) 
% that represent dependencies redundantly.  
%oli12: More importantly, as you've stated yourself, we view G as fixed and correct, for the purposes of evaluating \mu. So saying "if there are parallel edges" is not helpful for talking about \mu. 
If (b) $>$ (a), then a specification of an outcome along each $L \in \Ed$ is redundant; all of the randomness in the system could have been resolved with this amount of information; we conclude that there are extra correlations in $\mu$ that are not suggested by $G$. 
The more (b) exceeds (a), the larger the degree of redundancy, and thus the less favorable an agent who believes $G$ to qualitatively correct, will judge $\mu$ to be. Because $\mu$ falls short of the expected level of randomness from this structure given these conditional distributions, we say that $\mu$ has a $G$-entropy deficit.

On the other hand, if (a) $>$ (b), then
a sample of
$\mu$ requires additional information to specify, beyond
what could be used to encode outcomes of the marginals selected by $G$. This happens, when the structure in $G$ does not fully constrain the distribution, and often happens when there are fewer links than nodes, or cycles present. Whereas before $\mu$ had a defecit of entropy with respect to $G$, it now has a $G$-information \emph{surplus}, as now $\mu$ has more randomness than we could possibly resolve by specifying link outcomes, which reflects a hedging, due to the qualitative lack of information in $G$. For a defense of this preference for higher randomness in the absence of information to the contrary, see \cite{maxent}\cite{adversarial_protection}. The more (a) exceeds (b), the more evenly $\mu$ allocates mass in contexts that $G$ says nothing about, and so the better the qualitative fit of $\mu$ to $G$.
%joe11*: I don't like this, and don't really understand it.  Why are
%we suddenly making things  unspecified by G constants.  I cut it.
%oli13: We're not imposing this; I'm just looking at a class of instances, and 
% I think particular class could really help clarify why we get negative
% numbers. Rewriting, and old version beneath it.
%joe13* :I don't understand why this should be true, and don't
%understand why yu want ot make things a contsnt.  Again, I feel
%strongly that this hurts far more than it helps.
%oli15*: You wanted to know what zero means. I'm motivating negative
%numbers. In any case we still need the punchline. partially
%reinstated. 
%joe14*: I still don't understand.  What does it mean to fix things
%that are unkonwn, and why should we want to do this?  If you can't
%explain this better, then I feel strongly we should cut the next line.
%oli16: A distribution fixes something that is unknown, if there's a
%feature that has no presence in $G$, and according to $\mu$ this
%feature is a constant.
%joe15: I simply did not understand the sentence above at all.  I
%wrote a whole book on uncertainty.  I'm considered an expert in the
%area, yet I find the language in your preceding sentence completely
%mysterious.  I feel like we're speaking different languages.  
%We don't want to fix things that are unknown. You've got it backwards: I'm
%saying that such a distribution DOES in fact match all of the independencies of
%$G$, so it would not be fair to ding it. But another distribution that did all
%of this and also made these unknown features in $G$ also difficult to describe
%--- this distribution deserves extra credit.
%Such a $\mu$ is arguably a \emph{better} fit to $G$, than a
% Such a $\mu$ is arguably a \emph{better} fit to $G$ than a
% distribution that that merely obeys the dependencies (and fixes things
% that are unknown) [[REWRITE]]. 
%oli16:rewrite.
%joe15*: I give up.  I don't understand this at all, so it clearly is
%not helping my intuition.  I have no clue why we're comparing to a
%distribution that just exhibits the dependence structure of G (which
%I assume means that \mu has all the conditional independencies
%expressed by the edges of G.  Where did that come from?  I have no
%idea what it means for G to to articular features of an outcome.  My
%best guess is that you're trying to say something about non-edges,
%but that's a guess.
%oli17: IDef has two effects:
% (Here "wherever" means for every region of the info diagram, i.e., subset
% of variables that excludes some other subset.)
% (1) to give a max-entropy result wherever G does not specify something
% (think: no edges, or a cycle that allows multiple solutions), and 
% (2) to give a min-entropy result whenever it is over-specified. For regions
% with exactly 2 overlapping variables, this amounts to a statement of
% independence.
%joe16: This may be true; I don't understand IDef well enough.  I
%suspect it's not true, but just describes extreme cases.  In any case, it
%is definitely *not* what we should be writing here.  It's far too
%dependent on the details of this scoring function, and doesn't get at
%the essence of what we hope the scoring function is trying to do.
%You keep saying ``the scoring function does the right thing''.  What
%we should be saying here is what the ``right thing'' is, not
%describing what the scoring function does.
%oli17 Examples. In [X -> Y], X is qualitatively under-specified, and in 
% [ X <-> Y ], the mutual data between X and Y is qualitatively under-specified.
% On the other hand, in [X <- 1 -> Y], the area shared in both X and Y is
% over-specified (as it is specified by both edges), and so the resulting IDef
% term penalizes mutual information, giving a min-entropy result.
%
%oli17: now, with regard to the above, I'm trying to hint at (1). I'm trying to 
% that if G has nothing to say about a variable Z (maybe no edges to Z), or about the mutual information between X and Y (maybe there's a cycle, with many soultions), then $\mu$ gets bonus points for not being certain about it. 
%
%Such a $\mu$ is arguably a \emph{better} fit to $G$ than a
%distribution that that merely exhibits the dependence structure of
%$G$, because it expresses uncertainty about features  
%%oli16 next line optional
%(of an outcome) 
%that $G$ does not articulate.
%is silent on.
\begin{defn}\label{def:info-deficiency}
	For a multi-graph $G = (\N, \Ed)$ over a set $\N$ of variables,
	define the \emph{$G$-information deficiency}
	of distribution $\mu$, denoted $\IDef{G}(\mu)$,
	by considering the difference between (a) and (b), 
	where we measure the amount of information needed for a description
	using (conditional) entropy: 
	\begin{equation}
		%oli12:
		% \IDef{\dg M}(\mu) := \sum_{\ed{L}XY \in \Ed^{\dg M}} \H_\mu(Y\mid X)
		% - \H(\mu).  
		\IDef{G}(\mu) := \sum_{(X,Y) \in \Ed} \H_\mu(Y\mid X) - \H(\mu). 
		\label{eqn:alt-extra}
	\end{equation}
	%\footnote{Recall that $H_\mu(Y\mid X)$, the
	Recall that $H_\mu(Y\mid X) = - \sum_{x,y \in \V(\{X,Y\})} \mu(x,y) \log \mu(y\mid x)$ is the
	\emph{conditional entropy of $Y$ given $X$} with respect to $\mu$.
	For a PDG ${\dg M}$, we take $\IDef{\dg M} = \IDef{G^{\dg M}}$.         
\end{defn}

$\IDef{}$ can be thought of as a qualitatively customize-able maximum-entropy approach \cite{Jaynes57}, in which one tries to maximize entropy everywhere, while simultaneously minimizing the conditional entropy of each $\H(Y \mid X)$ for each qualitative dependence of $Y$ on $X$.

We illustrate $\IDef{\dg M}$ with some simple examples.  
\begin{example}
Suppose that $\dg M$ has two nodes, $X$ and $Y$.  
If $\dg M$ has no edges, the $\IDef{\dg M}(\mu) = - H(\mu)$.
%joe11
There is no information required to specify, for each edge in ${\dg
	M}$ from $X$ to
$Y$, the value ${\mat w}_Y$ given ${\mat w}_X$, since there are no
edges.
Since we view smaller numbers as representing a better fit,
$\IDef{\dg M}$ in this case will prefer the distribution that
maximizes entropy.
If $\dg M$ has one edge from $X$ to $Y$, then since
$H(\mu) = H_{\mu}(Y \mid X) + H_\mu(X)$ by the well known 
\emph{entropy chain rule} \cite{mackay2003information},
$\IDef{\dg   M}(\mu) = -H_{\mu}(X)$.
Intuitively, while knowing the conditional probability $\mu(Y \mid X)$
is helpful, to completely specify $\mu$ we also need 
$\mu(X)$.     Thus, in this case, $\IDef{\dg
	M}$ prefers distributions that maximize the entropy of 
the marginal on $X$.
If $\dg M$ has  
sufficiently many parallel edges
%an edge $1 \to X$, and also
from $X$ to $Y$
and $H_{\mu}(Y \mid X) > 0$ 
(so that $Y$ is not totally determined by $X$)
then we have $\IDef{\dg M}(\mu) > 0$, because the redundant edges add no
information, but there is still a cost to specifying them.
In this case, $\IDef{\dg M}$ prefers distributions that make $Y$ a
deterministic function of $X$ will maximizing the entropy of the
marginal on $X$.
Finally, if ${\dg M}$ has an edge from $X$ to $Y$ and another from $Y$
to $X$, then 
%oli12
% we minimize
a distribution $\mu$ minimizes
$\IDef{\dg M}$ when 
% $X$ and $Y$ depend on each other 
%joe11: I don't like ``have randomness'', and it's unneessary.  I
%don't see why ``vary together'' is better than ``depend on each
%other'', but I'll leave it
%$X$ and $Y$ both individually have randomness, but vary together
$X$ and $Y$  vary together
(so that $H_\mu(Y \mid X) = H_\mu(X \mid Y) = 0$) while
maximizing $H(\mu)$, for example, by taking $\mu(0,0) = \mu(1,1) = 1/2$.
\end{example}

	
	The extra information is the sum of the entropies that
	\emph{actually} result from each table, in the context of
	distribution $\mu$, minus the total entropy of the
	distribution. 
	%joe4L*: I don't understand the next sentence, but it seems like a
	%useful intuition.  Could hou explain it better.
	%oli8: deleted "Alternatively"
	%	Alternatively, we can think of
	We can think of
	%oli8: shortened, deleted unnecessary symbol 
	% the negation of the extra  information, $-\H^\dg M(\mu)$ 
	its negation
	%joe7*: I have no idea what this means, nor why it represents a
	%quantity of interest
	as the uncertainty in $\mu$,
	which has not already been specified by the cpds in $\dg M$.
	%joe4*: I now think that this is premature        
	%	We will show in \Cref{sec:bn-convert} the distribution with
	%        minimal extra information with respect to the cpds of a BN
	%        $\cal B$ is the unique one specified by $\cal B$. 
	%joe4*: premature!
	%	\begin{vfull}
	%		One might also recognize the extra information as
	%having the form of a free energy; we explore this connection in
	%\Cref{sec:thermo}.  
	%	\end{vfull}
	
	%joe4*: I don't undrestand this, since I don't understand the causal
	%interpretaiton.  I would strongly prefer to cut it from here, and
	%then have a separate section on the causa interpretaino.
	%	Note that when every $\alpha_L = 0$, minimizing the extra
	%        information corresponds to maximizing entropy subject to
	%        constraints, which is arguably the right thing to do if we
	%        take the constraints at face value, rather than causally (see
	%        \Cref{ex:counterexample}), justifying their naming. 
	
	%%joe3*: Can you give some intuition for why this is reasonable?
	%%oli3: yeah. Here are several arguments:
	%% (1) if the constraints are all you know, and you choose another distribution, you're somehow claiming to know more than you do. This is the maximum entropy distribution associated with some other constraints. 
	%% (2) Specifying the wrong distribution is costly --- but much cheaper if you specified a uniform distribution. For this reason, the relative entropy from uniform to any other distribution is cheaper than the relative entropy to go back: the maximum entropy distribution is the most adaptable, paying the smallest price to specialize, where price is the expected surprise, = log (1 / p), related to energy of the associated Boltzmann distribution.
	%%oli3*: More generally, I do not think it's worth the space to do any motivation like this at all. I could do it, and it would make me a better person, but it takes a lot of time, will require a lot of energy from readers, and it's a very standard thing. My guess is at least half are already on board with maximizing entropy, and providing a remedial introduction to such a deep topic in a 9-page abstract is not a good use of space. I also imagine that it will come off as patronizing to those who know what they're doing in the context of an original research paper. 
	%We think of distributions with higher entropy as being ``better''.
	%%joe3: reorganizing what you wrote.  
	%Since we want to minimize inconsistency and maximize entropy, we
	%subtract one from the other (with relative weighting $\alpha$), to get
	%a score $\mathcal U_\alpha(\dg M; p)$:
	%	\begin{equation}
	%		\mathcal U_\alpha(\dg M; p) := \Inc(\dg M;p) - \alpha
	%                H(p). \label{eqn:full-score} 
	%	\end{equation} 
	%joe3: This may be true, but why say it?        
	%	Thought of this way, in specifying a PDG, a modeler has not
	%        only specified a a distribution, but also a higher-order
	%        object, that scores all distributions.  
	%oli3: because mixing up the levels is problematic. Maybe it doesn't need to be said.
	%joe3: why does this mean someting is wrong?
	%oli3: rewording and uncommenting.
	% $\Inc$ provides us with a meaningful continuous score, but as
	% a semantics for PDGs, there's still something missing: $\Inc$ can
	% only distinguish between those distributions $p$ that are
	%         \emph{inconsistent} with $\dg M$.
	% 
	%oli8:added remark
	\begin{remark}
	 	one may also consider the variant, where we use $\E_{x \sim \mu_X} \H (\bp (x))$ in place of $\H_\mu(Y \mid X)$ to define the $G$-information deficit, as in 
		\begin{equation}
			\IDef{\dg M}^{\mat p}(\mu) := \sum\alle \H_\mu(Y\mid X) - \H_\mu
			\label{eqn:alt-extra}
		\end{equation}
		This has the benefit of having a linear first term and enjoying strong convexity for all values of $\gamma$. However, it is a more complex and less qualitatively separated. For distributions $\mu \in \Inc_{\dg M}$, these quantities are the same; therefore, the difference lies exclusively in the way it scores distributions that are already inconsistent with the edges.
		We can use this fact to give us a stronger maximum-entropy theorem for Bayesian Networks than has previously be given in \cite{williamson2000}.
	\end{remark}
	
	
	
	$\Inc({\dg M}, \mu)$ and $\IDef{\dg M},\mu)$ give us two measures
	of compatibility between ${\dg M}$ and a distribution $\mu$.
	We take the score of interest to be their sum, with the trade-off
	controlled by a parameter $\gamma \ge 0$:
	\begin{equation}
		\bbr{\dg M}_\gamma(\mu).
		:= \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)
		\label{eqn:full-score}
	\end{equation}
	%joe9*: I think the proposition and the following sentence are
	%worth adding 
	
	%joe10
	%        The following just make precise that the scoring semantics
	The following just makes precise that the scoring semantics
	generalizes the first semantics.
	% \begin{prop}[restate=prop:sd-is-zeroset]\label{prop:sd-is-zeroset}
	\begthm{prop}{prop:sd-is-zeroset}
	%joe9:try to avoid ``any''
	%joe10
	%For all PDGs, $\dg M$, $\SD{\dg M} = \{ \mu : \bbr{\dg
	For all PDGs $\dg M$, we have that $\SD{\dg M} = \{ \mu : \bbr{\dg
		M}_0(\mu) = 0\}$. 
\end{prop}

While we focus on this particular scoring function
in the paper, 
%oli12: I object to the word "largely". You also need a "because".
% largely
%joe11: added the ``because''
%in part
in part because
it leads to interesting results and has deep
connections to the free energy of a factor graph \cite{KF09},
other scoring functions may well end up being of interest. 
%%BEGIN_FOLD
%   \subsection{PDGs As Unique Distributions}\label{sec:uniq-dist-semantics}
%		
%		Finally, we provide an interpretation of a PDG as a probability distribution.  
%		%joe7: I'm not sure that we've said it before, so it's not a reiteration
%		%    Before we provide this semantics, we would like to reiterate that
%		Before we provide this semantics, we stress that
%		this distribution does \emph{not} capture all of the important
%		%joe7
%		%    information in the PDG---in particular, as we have stressed, a PDG
%		information in the PDG---for example, a PDG
%		can represent inconsistent knowledge states. 
%		%oli8
%		%	That said, the
%		Still, by giving a distribution, we enable comparisons with other graphical models. 
%		It also 
%		%oli12
%		%joe11: undid; I find ``reveals to be'' awkward English
%		%oli14: fair. "show to be" is also good, but you gave me the first one
%		shows that PDGS are 
%		%reveals PDGs to be
%		a surprisingly flexible tool for specifying distributions. 
%		The idea is to select the distributions with the best score.
%		%joe8*:
%		We thus define 
%		\begin{equation}
%			\bbr{\dg M}_\gamma^* = \argmin_{\mu \in
%				\Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu).
%		\end{equation}   
%		
%		In general, $\bbr{\dg M}_\gamma^*$ does not give a unique
%		distribution.  But if $\gamma$ is sufficiently small, then it does:
%		%joe8*: cleaner statement, which gets at what you want.  Are there any
%		%other useful sufficient conditions that we can give (like the ones
%		%that arise in factor graphs)?
%		%oli10: In its current state, this encompasses factor graphs.
%		% The only other sufficient condition I can see:
%		%   - It's true for any \gamma > 0 if the PDG has edges that are a subset
%		%		of those in a BN.
%		%  - There are some more involving \alphas (e.g.,  we can take convex
%		%  combinations of BNs this way).
%		%joe9: Well, alpha is gone, we certainly can't mention that.  I
%		%thought we had some conditions with determinism, but I'm OK with
%		%leaving it.
%		%oli11*: oh right! it's definitely also true that if it was convex
%		%before adding a set of deterministic edges, it continues to be convex
%		%afterwards. Is this actionable?
%		%joe10: probably not worth it
%		\begthm{prop}{prop:sem3}%\begin{prop}
%		If $\dg M$ is a PDG and $0 < \gamma \leq \min_L \beta_L^{\dg M}$, then $\bbr{\dg
%			M}_\gamma^*$ is a singleton. 
%		\end{prop}
%		
%		%joe8: added some motivation and discussion
%		In this paper, we are interested in the case where $\gamma$ is small;
%		this amounts to emphasizing the accuracy of the probability
%		distribution as a description of probabilistic information, rather than
%		the independence structure of the PDG.  This is what was going on in
%		all the examples in the introduction.  This motivates us to consider
%		what happens as $\gamma$ goes to 0.  If $S_\gamma$ is a set of
%		probability distributions for all $\gamma \in [0,1]$, we define
%		$\lim_{\gamma \rightarrow 0} 
%		S_\gamma$ to consist of all distributions $\mu$ such that 
%		there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with
%		$\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in
%		S_{\gamma_i}$ for all $i$. 
%		%oli10*: We still need to prove that this limit is unique. This is
%		%non-trivial, so I think we should reference the appendix.. I think
%		%figured out a way to do this that will be rather hellish, but haven't
%		%slogged through the algebra. I'm still holding out hope. 
%		%oli11: added
%		It can be further shown that 
%		\begthm{prop}{prop:limit-uniq}
%		%oli12:added
%		%joe11: as a general rule, avoid using ``any''
%		%  For any $\dg M$,
%		For all $\dg M$,
%		$\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$ is a singleton.
%		\end{prop}
%		Let
%		$\bbr{{\dg M}}^*$ be the unique element of $\smash{\lim\limits_{\gamma
%		\rightarrow 0}} \bbr{{\dg M}}_\gamma^*$. 
%		%joe8: this should go in the appendix, where we  can discuss
%		%convexity.
%		%oli11: You've now also removed strong convexity from the appendix...
%		\commentout{
%		There is a unique such distribution because, as we now
%		show, the score is strongly convex
%		which can be found efficiently \cite{strongconvexopt}.
%		}
%		%oli9: this prop is false for now
%		\commentout{
%		\begin{prop}\label{prop:u-convex}
%		$\bbr{\dg M}_\gamma(\mu)$ is $\gamma$-strongly convex.% in $\mu$.
%		\end{prop}
%		%oli7: I can do a better job here; I was feeling rushed when I wrote it.
%		%joe7*: Is this result still true, now that we're going with the other
%		%definition of extra information?
%		%oli9: Only if \alpha\gamma < \beta for all links.
%		\commentout{
%		\begin{proof}
%			$\Inc_{\dg M}( \mu)$ is convex in $\mu$
%			(\Cref{thm:inc-convex}), and $\gamma\sum\alle \E_{x\sim \mu_X}
%			\H(\bp(x))$ is linear in $\mu$.  
%			Negative entropy is $1$-strongly convex
%			(\Cref{prop:neg-ent-convex}), so $- \gamma \H(\mu)$ is $\gamma$-strongly convex.
%			The sum of a $\gamma$-strongly convex, linear, and
%			convex functions must be $\gamma$-strongly convex. 
%			%		, and strongly so when the coefficient on $-\H$ ($\gamma$) is positive. 
%			%(see \cite{Rockafellar1970ConvexA})
%		\end{proof}
%		}
%		}%oli9: end commentout
%		%joe7: \end{commentout}   
%		%oli8: change text
%		% We can now define the unique distribution semantics:
%		%joe7
%		%    We use this to get our desired semantics
%		%oli9 added alternate proposition for Extra.
%		%joe8*: I simplified the wording.  In any case, this needs a proof and
%		%can go to the appendix.  We'll need the space.
%		\commentout{
%		\begin{prop}\label{prop:convex-if-gamma-small}
%		If $\dg M$ is a PDG and
%		%joe8*
%		%  $\beta_0$ is a constant less than any
%		%        $\beta_L \in \beta^{\dg M}$, then for any $\gamma < \beta_0$,
%		$\gamma < \min_L \beta_L^{\dg M}$, then
%		$\bbr{\dg M}_\gamma$ is a strictly convex function of $\mu$.%
%		%  		\footnote{All proofs can be found in \Cref{sec:proofs}.}
%		\end{prop}
%		
%		
%		
%		%oli9: expanded this, added footnote.
%		% Proposition~\ref{prop:u-convex} allows us to define our desired
%		\Cref{prop:convex-if-gamma-small} allows us to define our desired
%		semantics by ensuring the limit%
%		\footnote{$\mu$ is in this limit iff there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in \bbr{\dg M}_{\gamma_i}$ for all $i$.}
%		in \eqref{eq:uniqdist} is well-defined.
%		
%		%oli8: reformat with equation, added the limit.
%		\begin{equation}
%		\bbr{\dg M}_* := \lim_{\gamma\to 0^+}\argmin_{\mu \in
%			%joe7
%			%                   \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu) 
%			%oli9: I missed this instance of \U when I eliminated it.
%			% \Delta\V(\dg M)} \mathcal U_\gamma(\dg M,\mu). 
%			\Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu). 
%		\label{eq:uniqdist}
%		\end{equation}
%		}
%		%joe8: \end{commentout}
%		%oli11: removed paragraph break
%		The semantics has an important property: 
%		\begthm{prop}{prop:consist}
%		%joe15: it's not ``in particular''.  
%		%$\bbr{\dg M}^* \in \bbr{\dg M}_0^*$; in particular, if $\dg M$ is consistent,
%		$\bbr{\dg M}^* \in \bbr{\dg M}_0^*$, so if $\dg M$ is consistent,
%		then $\bbr{\dg M}^* \in \SD{\dg  M}$.
%		\end{prop}
%		
%		% \begin{defn}
%		% 	For $\gamma > 0$,
%		% 	$\bbr{\dg M}^*_\gamma := \arg\min_{\mu \in \Delta\V(\dg M)} \mathcal U_\gamma(\dg M;\mu)$
%		% \end{defn}
%		
%		%oli8:
%		%joe7
%		\commentout{        
%		\begin{remark}
%		If $\dg M$ is a consistent PDG, then 
%		$\bbr{\dg M}'_* = \bbr{\dg M}_*$
%		where $\bbr{\dg M}'_*$ is the variant of \eqref{eq:uniqdist} which uses the alternate formulation $\IBal'$ of the extra information in place of $\IDef$.
%		\end{remark}
%		}
%		
%		%joe4*: this comes out of the blue, since you haven't discussed union
%		%for the other two semantics %(which is as it should be; it's a distraction)   
%		
%		\begin{vleftovers}
%		In contrast with the other two semantics, $\UD{\dg M_1 \cup
%		\dg M_2}$ cannot be calculated from $\UD{\dg M_1}$ and
%		$\UD{\dg M_2}$. However, it is effectively the only semantics
%		offered by alternative graphical models, which contributes to
%		their relative lack of modularity. We return to this after a
%		more careful treatment of unions in
%		\Cref{sec:pdg-operations}.
%		\end{vleftovers}
%END_FOLD

	\subsection{Lax PDGs}
	\begin{defn}[PDG]\label{def:PDG}
		A (lax) PDG is a tuple $\pdgvars[]$ where
		\begin{description}[nosep]
			\item[$\N$]~is a finite collection of nodes, which are identified with variables
			\item[$\Ed$]~is a collection of directed edges (arrows), each with a source, target, and a (possibly empty) label.
			\item[$\V$]~associates each node $N \in \N$ with a set $\V(N)$,
			representing the values that the variable $N$ can take. 
			\item[$\mathbf p$] associates, for each edge $L = (X,Y, \ell) \in \Ed$ and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$, whenever $\beta_L > 0$.
			\item[$\beta$]~associates to each edge $L$, a number in $[0,\infty]$, indicating certainty in the conditional distribution $\bp(Y \mid X)$ 
			\item[$\alpha$]~associates to each edge $L$, a number in $[0,1]$, indicating degree of belief that $L$ holds causally.
		\end{description}
		\vspace{-1.4em}
	\end{defn}
	

	
	\section{Generalized Semantics}
	Let $d$ be a distribution over $\Ed^{\dg M}$. 
	Choose a link $\ed LXY \in \Ed$, and let $\mathbf Z := \N^\dg M \setminus \{X, Y\}$ be the set of all other variables, so that a joint setting is characterized by a value $(x,y,\mat z)$.
	
	The link $L$ and its associated cpt $\bp$ can be extended to a transformer $\tau_L: \Delta \V(\dg M) \to \Delta \V(\dg M)$ on joint distributions in a couple of ways, such as:
\[
	\begin{aligned}
		\tau(\mu)(x,y,\mat z) :&= \mu(x,y,\mat z)\frac{\bp(y \mid x)}{\mu(y\mid x)} \\
		&= \mu(x)\; \bp(y \mid x)\; \mu(\mat z \mid x,y) 
	\end{aligned}\qquad\text{and}\qquad
	\begin{aligned}
		\tau(\mu)(x,y,\mat z) :&= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x, \mathbf z)} \\
		&= \mu(x,\mat z)\; \bp(y \mid x)
	\end{aligned}\\
\]
	I have yet to discover which of these things is correct. 
%	\begin{align*}
%		 \tau(\mu)(x,y,\mat z) &:= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x)} 
%			 &  \tau(\mu)(x,y,\mat z) &:= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x, \mathbf z)} \\
% 			&= \mu(x)\; \bp(y \mid x)\; \mu(\mat z \mid x,y) 
%			 &   &= \mu(x,\mat z)\; \bp(y \mid x)
%	\end{align*}
%	

		
	
	
\end{document}