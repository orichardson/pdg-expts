% !TeX TXS-program:bibliography = txs:///bibtex
\documentclass{article}


\input{../model-commands.tex}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}

% \usetikzlibrary{shapes.geometric}
\usetikzlibrary{backgrounds}
\tikzset{dpad0/.style={outer sep=0.05em, inner sep=0.3em, draw=gray!75, rounded corners=4, fill=black!08, fill opacity=1}}
\tikzset{arr0/.style={draw, ->, thick, shorten <=0pt, shorten >=0pt}}
\tikzset{arr1/.style={draw, ->, thick, shorten <=1pt, shorten >=1pt}}
\tikzset{arr2/.style={draw, ->, thick, shorten <=2pt, shorten >=2pt}}
\tikzset{is bn/.style={background rectangle/.style={fill=blue!35,opacity=0.3, rounded corners=5},show background rectangle}}

\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/]  % activate!
\usepackage{etoolbox}
\AtBeginEnvironment{tikzcd}{\tikzexternaldisable} %... except careful of tikzcd...
\AtEndEnvironment{tikzcd}{\tikzexternalenable}

\cleartheorem{defn}
\declaretheorem[name=Definition,style=definition,qed=$\square$]{defn}
\usepackage[noabbrev,nameinlink]{cleveref}


%\usepackage{subcaption}
\usepackage{subfig}
\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
\renewcommand\thesubfigure{(\alph{subfigure})}

%\let\Horig\H
%\renewcommand{\H}{\mathop{\mathrm H}}
%\newcommand{\E}{\mathop{\mathbb E}}
\newcommand{\bp}[1][L]{\mathbf{p}_{\!_#1\!}}
\newcommand{\V}{\mathcal V}
\newcommand{\N}{\mathcal N}
\newcommand{\Ed}{\mathcal A}



\DeclareMathAlphabet{\mathdcal}{U}{dutchcal}{m}{n}
\DeclareMathAlphabet{\mathbdcal}{U}{dutchcal}{b}{n}

\newcommand{\ed}[3]{#2
	\overset{\smash{\mskip-5mu\raisebox{-1pt}{$\scriptscriptstyle
				#1$}}}{\rightarrow} #3} 
\newcommand{\alle}[1][L]{_{ \ed {#1}XY}}

\newcommand{\dg}[1]{\mathbdcal{#1}}
\newcommand{\var}[1]{\mathsf{#1}}
\newcommand\Pa{\mathbf{Pa}}

\newcommand{\PDGof}[1]{{\mathbdcal{p\kern-0.05em d\kern-0.125em g}} (#1)}
%\newcommand{\PDGof}[1]{{\dg M}_{#1}}

%Crazier future/past operations
\DeclareMathOperator\dcap{\mathop{\dot\cap}}
\newcommand{\tto}{\rightarrow\mathrel{\mspace{-15mu}}\rightarrow}

%\DeclarePairedDelimiter{\bbr}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\SD}{\llbracket}{\rrbracket_{\text{sd}}}
%\DeclarePairedDelimiterXPP{\SD}[1]{}{\llbracket}{\rrbracket}{_{\text{sd}}}{#1}
\newcommand{\IDef}[1]{\mathit{IDef}_{#1}}
\newcommand\Inc{\mathit{Inc}}

\newcommand{\none}{\varobslash}
\newcommand{\begthm}[2]{\begin{#1}[restate=#2,label=#2]}


%\twocolumn
\title{The PDG Manual}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}

\begin{document}

	\maketitle
	\tableofcontents
	\listoffigures
	\listoftheorems
	\clearpage
	%some day...
	% \twocolumn 
	
	\section{PDGs}
	\def\pdgvars[#1]{(\N#1, \Ed#1, \V#1, \mat p#1, \alpha#1,\beta#1)}
	\begin{defn}[sPDG]\label{def:sPDG}
		A strict PDG is a tuple $\pdgvars[]$ where
		\begin{description}[nosep]
			\item[$\N$]~is a finite collection of nodes, which are identified with variables,
			\item[$\Ed$]~is a collection of directed edges (arrows), each with a source, target, and a (possibly empty) label.
			\item[$\V$]~associates each node $N \in \N$ with a set $\V(N)$,
			representing the values that the variable $N$ can take. 
			\item[$\mathbf p$] associates, for each edge $L = (X,Y, \ell) \in \Ed$ and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$, whenever $\beta_L > 0$.
			\item[$\beta$]~associates to each edge $L$, a number in $[0,\infty]$, indicating certainty in the conditional distribution $\bp(Y \mid X)$ 
			\item[$\alpha$]~associates to each edge $L$, a number in $[0,1]$, indicating degree of belief that $L$ holds causally.
		\end{description}
		\vspace{-1.4em}
	\end{defn}

	If $\dg M$ is a PDG, we reserve the names $\pdgvars[^\dg M]$
	for its components, so that we may reference one (e.g.,
	$\Ed^\dg M$) without naming them all explicitly. We may further omit the superscript in contexts where only one PDG is present. 
	We write $\V(S)$ for the set of possible joint settings of a set $S$
	of variables; in particular, 
	we write $\V(\dg M)
	= \prod_{N \in \N^\dg M} \V^\dg M(N)$
	for all settings of the variables $(\N^\dg M, \V^\dg M)$.
	
	\subsection{Semantics}
	\begin{defn}[set of distribution semantics] \label{def:set-semantics} 
		If $\dg M\!=\!\pdgvars[]$ is a PDG, let $\SD{\dg M}$ be the \emph{s}et of \emph{d}istributions over the variables in $\dg M$ whose conditional marginals are exactly those given by $\mat p$.
		That is, $\mu \in \SD{\dg M}$ iff, for all edges $L = (X,Y) \in \Ed$,  $x \in \V(X)$,  and $y \in \V(Y)$, we have that $\mu(Y = \cdot \mid X\!=\! x) = \bp(x)$.
		{
			\[ \SD[\Big]{\dg M} = \!\left\{\mu \!\in\! \Delta \V_\none (\dg M) \middle|\!
			\begin{array}{l}
				\mu(B\!\! =\!\!b \mid A\!\!=\!\!a) \geq \boldsymbol\mu_L(b \mid a) \\[0.1em]
				~\text{$\forall (A, B,\ell) \!\in\! \Ed$, $a \!\in\!\mathcal V_A$, $b \!\in\! \mathcal V_B$} \end{array}\!\!\! \right\}\]
		}
		$\dg M$ is \emph{consistent} if $\SD{\dg M}$ is inhabited (non-empty), and \emph{inconsistent} otherwise.
	\end{defn}

\begin{defn}[incompatibility and inconsistency]\label{def:inc}
	The \emph{incompatibility} of a PDG $\dg M = \pdgvars[]$ with
	a joint distribution $\mu$, denoted $\Inc_{\dg M}(\mu)$, is  
	\[
	\Inc_{\dg M }( \mu) := 
	\!\!\!\sum \alle \beta_L \E_{x \sim \mu_{_X}}
	\left[\kldiv[\Big]{ \mu(Y\!= \cdot\mid X \!=\! x) }{\bp(x) } \right] ,
	\]
	where $\kldiv{\mu}{\nu} = \sum_{w} \mu(w) \log\frac{\mu(w)}{\nu(w)}$ is the 
	relative entropy from $\nu$ to $\mu$.
%	
			The \emph{inconsistency} of $\dg M$, 
		denoted $\Inc(\dg M)$, is the
		minimum possible incompatibility of $\dg M$ with any
		distribution $\mu$,  
		\[ \Inc(\dg M) = \inf_{ \mu \in \Delta [W_{\cal V}]} \Inc_{\dg M}(\mu) . \]
\end{defn}
$\SD{\dg M}$ and $\Inc_{\dg M}$ distinguish only
between distributions based on their compatibility with
$\dg M$, but even among distributions that match the
marginals, some more closely match the qualitative structure
of the graph than others.  
Qualitatively, an think of an edge $\ed LXY$ of a PDG $\dg M$ as a claim that the value of $Y$ can be (noisily) computed from
$X$ alone.  
Therefore, to best match the qualitative structure of $G$, statistical asymmetries and dependencies between variables should
 be efficiently described by giving the conditional probabilities corresponding to the edges of $G$. After all, if we believe the structure of $G$, then these conditional probabilities describe every possible interaction between variables. 

To formalize this, we require only the underlying multigraph $G^{\dg M} :=
(\N^{\dg M}, \Ed^{\dg M})$ of $\dg M$. 
Given $G$ and $\mu$, contrast the amount of
information required to 
\begin{enumerate}[label=(\alph*)]
	\item directly describe a joint outcome  $\mat w ~ \sim \mu$
	drawn from $\mu$, and 
	\item separately specify, for each edge $\ed LXY$, the value
	$\mat w_Y$ (the projection of $\mat w$ onto the variable
	$Y$) given the value $\mat w_X$, in expectation. 
\end{enumerate}
% (a)  and (b)
When these two quantities are equal, the total length (in expectation) of specifying the outcome on each link, is precisely the same length as an optimal description of the joint outcome. That is, $\mu|_G$ carries enough information to precisely resolve all of the randomness of $\mu$. If (b) $>$ (a), then a specification of an outcome along each $L \in \Ed$ is redundant; all of the randomness in the system could have been resolved with this amount of information; we conclude that there are extra correlations in $\mu$ that are not suggested by $G$. The more (b) exceeds (a), the larger the degree of redundancy, and thus the less favorable an agent who believes $G$ to qualitatively correct, will judge $\mu$ to be. Because $\mu$ falls short of the expected level of randomness from this structure given these conditional distributions, we say that $\mu$ has a $G$-entropy deficit.

On the other hand, if (a) $>$ (b), then a sample of $\mu$ requires additional information to specify, beyond what could be used to encode outcomes of the marginals selected by $G$. This happens, when the structure in $G$ does not fully constrain the distribution, and often happens when there are fewer links than nodes, or cycles present. Whereas before $\mu$ had a defecit of entropy with respect to $G$, it now has a $G$-information \emph{surplus}, as now $\mu$ has more randomness than we could possibly resolve by specifying link outcomes, which reflects a hedging, due to the qualitative lack of information in $G$. For a defense of this preference for higher randomness in the absence of information to the contrary, see \cite{maxent}\cite{adversarial_protection}. The more (a) exceeds (b), the more evenly $\mu$ allocates mass in contexts that $G$ says nothing about, and so the better the qualitative fit of $\mu$ to $G$.
%oli17: IDef has two effects:
% (Here "wherever" means for every region of the info diagram, i.e., subset
% of variables that excludes some other subset.)
% (1) to give a max-entropy result wherever G does not specify something
% (think: no edges, or a cycle that allows multiple solutions), and 
% (2) to give a min-entropy result whenever it is over-specified. For regions
% with exactly 2 overlapping variables, this amounts to a statement of
% independence.
\begin{defn}[$G$-information deficit]\label{def:info-deficiency}
	For a multi-graph $G = (\N, \Ed)$ over a set $\N$ of variables,
	define the \emph{$G$-information deficiency}
	of distribution $\mu$, denoted $\IDef{G}(\mu)$,
	by considering the difference between (a) and (b), 
	where we measure the amount of information needed for a description
	using (conditional) entropy: 
	\begin{equation}
		\IDef{G}(\mu) := \sum_{(X,Y) \in \Ed} \H_\mu(Y\mid X) - \H(\mu). 
		\label{eqn:idef}
	\end{equation}
	%\footnote{Recall that $H_\mu(Y\mid X)$, the
	Recall that $H_\mu(Y\mid X) = - \sum_{x,y \in \V(\{X,Y\})} \mu(x,y) \log \mu(y\mid x)$ is the
	\emph{conditional entropy of $Y$ given $X$} with respect to $\mu$.
	For a PDG ${\dg M}$, is we take $\IDef{\dg M} = \IDef{(\N^{\dg M}, \Ed^{\dg M})}$, the information defecit with respect to its underlying hyper-graph.
\end{defn}

The $G$-information deficit is the total uncertainty \emph{actually} result from each table, in the context of distribution $\mu$, minus the total entropy of the distribution. We can think of its negation as the uncertainty in $\mu$, which has not already been specified by the cpds in $\dg M$. 	

$\IDef{.}$ can be thought of as a qualitatively customize-able maximum-entropy \cite{Jaynes57} approach, in which one tries to maximize or minimize the variation with in a subset of the variables, depending on how qualitatively constrained this subset is, given the values outside of it, a notion which naturally leads to the information profile \Cref{info-profile}. We illustrate $\IDef{\dg M}$ with some simple examples.  


\begin{example}[some example descriptions]
Suppose that $\dg M$ has two nodes, $X$ and $Y$. If $\dg M$ has no edges, the $\IDef{\dg M}(\mu) = - H(\mu)$.
There is no information required to specify, for each edge in ${\dg M}$ from $X$ to $Y$, the value ${\mat w}_Y$ given ${\mat w}_X$, since there are no edges. Since we view smaller numbers as representing a better fit, $\IDef{\dg M}$ in this case will prefer the distribution that maximizes entropy.

If $\dg M$ has one edge from $X$ to $Y$, then since $H(\mu) = H_{\mu}(Y \mid X) + H_\mu(X)$ by the chain rule, $\IDef{\dg M}(\mu) = -H_{\mu}(X)$. Intuitively, while knowing the conditional probability $\mu(Y \mid X)$ is helpful, to completely specify $\mu$ we also need $\mu(X)$. Thus, in this case, $\IDef{\dg M}$ prefers distributions that maximize the entropy of the marginal on $X$. If $\dg M$ has sufficiently many parallel edges
%an edge $1 \to X$, and also
from $X$ to $Y$ and $H_{\mu}(Y \mid X) > 0$ (so that $Y$ is not totally determined by $X$) then we have $\IDef{\dg M}(\mu) > 0$, because the redundant edges add no information, but there is still a cost to specifying them. In this case, $\IDef{\dg M}$ prefers distributions that make $Y$ a deterministic function of $X$ will maximizing the entropy of the marginal on $X$. Finally, if ${\dg M}$ has an edge from $X$ to $Y$ and another from $Y$
to $X$, then a distribution $\mu$ minimizes $\IDef{\dg M}$ when 
$X$ and $Y$ are correlated (so that $H_\mu(Y \mid X) = H_\mu(X \mid Y) = 0$) while
maximizing $H(\mu)$, for example, by taking $\mu(0,0) = \mu(1,1) = 1/2$.
\end{example}
\begin{example}[visualizing the information profile]
\definecolor{subfiglabelcolor}{RGB}{0,0,0}
\begin{figure}
	\centering
	\def\vsize{0.4}
	\def\spacerlength{0.5em}
\scalebox{0.85}{
%apparently  I have to manually step the figure number to make subfigures number properly.
\stepcounter{figure}
\makebox[\textwidth][c]{
	\refstepcounter{subfigure}
	\begin{tikzpicture}[center base]\label{subfig:justX-0}
		\node[dpad0] (X) at (0,1){$X$};
		\draw[fill=green!50!black]  (0,0) circle (\vsize)  ++(-90:.22) node[label=below:\tiny$X$]{};
%		\useasboundingbox (current bounding box);
		\node at (-0.5, 0.6){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}\!
% \hspace{\spacerlength}
% \adjustbox{valign=b}{
% \renewcommand{\arraystretch}{1.2}
\begin{tabular}{c}	
	\refstepcounter{subfigure}\label{subfig:justX-1}
	\begin{tikzpicture}[is bn]
		\node[dpad0] (1) at (-0.4,.85){$\var 1$};
		\node[dpad0] (X) at (0.4,.85){$X$};
		\draw[arr1] (1)  -- (X);
		\draw[fill=white!70!black]  (0,0) circle (\vsize) ++(-90:.22) node[label=below:\tiny$X$]{};
		\node at (-0.6,0.35){};
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.35){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture} \\[0.5em]
	\refstepcounter{subfigure}\label{subfig:justX-2}
	\begin{tikzpicture}
		\node[dpad0] (1) at  (-0.45,.85){$\var 1$};
		\node[dpad0] (X) at  (0.45,.85){$X$};
		\draw[arr1] (1) to[bend left=20] (X);
		\draw[arr1] (1) to[bend right=20] (X);
		\draw[fill=red!50!black] (0,0) circle (\vsize) ++(-90:.22) node[label=below:\tiny$X$]{};
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.35){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
\end{tabular}%}
\hspace{\spacerlength}\vrule\hspace{\spacerlength}
	%% EXAMPLE: X  Y
	% \adjustbox{valign=b}{
	\begin{tabular}{c}
	\refstepcounter{subfigure}\label{subfig:justXY}
	\begin{tikzpicture}[]  
		% \node[dpad0] (1) at (0,2){$\var 1$};
		\node[dpad0] (X) at (-0.45,.85){$X$};
		\node[dpad0] (Y) at (0.45,.85){$Y$};
		% \draw[arr] (1) to[] (X);
		% \draw[arr] (1) to[] (Y);
		\path[fill=green!50!black] (-0.2,0) circle (\vsize) ++(-110:.23) node[label=below:\tiny$X$]{};
		\path[fill=green!50!black] (0.2,0) circle (\vsize) ++(-70:.23) node[label=below:\tiny$Y$]{};
		\begin{scope}
			\clip (-0.2,0) circle (\vsize);
			\clip (0.2,0) circle (\vsize);
			\fill[green!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-0.2,0) circle (\vsize);
			% \draw[ultra thick,white] (0.2,0) circle (\vsize);
		\end{scope}
		\draw (-0.2,0) circle (\vsize);
		\draw (0.2,0) circle (\vsize);
%		\useasboundingbox (current bounding box);
		\node at (-0.8, 0.4){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}\\[0.5em]
	%% EXAMPLE: X -> Y
	\refstepcounter{subfigure}\label{subfig:XtoY}
	\begin{tikzpicture}[]
		% \node[dpad0] (1) at (0,2){$\var 1$};
		\node[dpad0] (X) at (-0.45,0.85){$X$};
		\node[dpad0] (Y) at (0.45,0.85){$Y$};
		\draw[arr1] (X) to[] (Y);
		% \draw[arr] (1) to[] (Y);
		\path[fill=green!50!black] (-0.2,0) circle (\vsize) ++(-110:.23) node[label=below:\tiny$X$]{};
		\path[fill=white!70!black] (0.2,0) circle (\vsize) ++(-70:.23) node[label=below:\tiny$Y$]{};
		\begin{scope}
			\clip (-0.2,0) circle (\vsize);
			\clip (0.2,0) circle (\vsize);
			\fill[green!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-0.2,0) circle (\vsize);
			% \draw[ultra thick,white] (0.2,0) circle (\vsize);
		\end{scope}
		\draw (-0.2,0) circle (\vsize);
		\draw (0.2,0) circle (\vsize);
%		\useasboundingbox (current bounding box);
		\node at (-0.8, 0.4){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
\end{tabular}%}
% \hspace{\spacerlength}
\begin{tabular}{c}
	%% EXAMPLE: X <-> Y
	\refstepcounter{subfigure}\label{subfig:XY-cycle}
	\begin{tikzpicture}[center base]
		% \node[dpad0] (1) at (0,2){$\var 1$};
		\node[dpad0] (X) at (-0.45,0.85){$X$};
		\node[dpad0] (Y) at (0.45,0.85){$Y$};
		\draw[arr1] (X) to[bend left] (Y);
		\draw[arr1] (Y) to[bend left] (X);
		\draw[fill=white!70!black] (-0.2,0) circle (\vsize) ++(-110:.25) node[label=below:\tiny$X$]{};
		\draw[fill=white!70!black] (0.2,0) circle (\vsize) ++(-70:.25) node[label=below:\tiny$Y$]{};
		\begin{scope}
			\clip (-0.2,0) circle (\vsize);
			\clip (0.2,0) circle (\vsize);
			\fill[green!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-0.2,0) circle (\vsize);
			% \draw[ultra thick,white] (0.2,0) circle (\vsize);
		\end{scope}
		\draw (-0.2,0) circle (\vsize);
		\draw (0.2,0) circle (\vsize);
%		\useasboundingbox (current bounding box.south west) rectangle (current bounding box.north east);
		\node at (-0.85, 0.4){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}\\[2.5em]
% \hspace{\spacerlength}%% EXAMPLE: 1 -> Y;1->X
\refstepcounter{subfigure}\label{subfig:XYindep}
	\begin{tikzpicture}[center base, is bn] 
		\node[dpad0] (1) at (0,0.75){$\var 1$};
		\node[dpad0] (X) at (-0.7,0.95){$X$};
		\node[dpad0] (Y) at (0.7,0.95){$Y$};
		\draw[arr0] (1) to[] (X);
		\draw[arr0] (1) to[] (Y);
		\draw[fill=white!70!black] (-0.2,0) circle (\vsize) ++(-110:.23) node[label=below:\tiny$X$]{};
		\draw[fill=white!70!black] (0.2,0) circle (\vsize) ++(-70:.23) node[label=below:\tiny$Y$]{};
		\begin{scope}
			\clip (-0.2,0) circle (\vsize);
			\clip (0.2,0) circle (\vsize);
			\fill[red!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-0.2,0) circle (\vsize);
		% \draw[ultra thick,white] (0.2,0) circle (\vsize);					
		\end{scope}
		\draw (-0.2,0) circle (\vsize);
		\draw (0.2,0) circle (\vsize);
%		\useasboundingbox (current bounding box.south west) rectangle (current bounding box.north east);
		\node at (-0.88, 0.4){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
\end{tabular}
\hspace{\spacerlength}
	 %% EXAMPLE: 1 -> X -> Y
	 \refstepcounter{subfigure}\label{subfig:1XY}
	\begin{tikzpicture}[center base, is bn]
		\node[dpad0] (1) at (0.15,2){$\var 1$};
		\node[dpad0] (X) at (-0.45,1.4){$X$};
		\node[dpad0] (Y) at (0.35,1){$Y$};
		\draw[arr0] (1) to[] (X);
		\draw[arr1] (X) to[] (Y);
		\path[fill=white!70!black] (-0.2,0) circle (\vsize) ++(-110:.23) node[label=below:\tiny$X$]{};
		\path[fill=white!70!black] (0.2,0) circle (\vsize) ++(-70:.23) node[label=below:\tiny$Y$]{};
		\begin{scope}
			\clip (-0.2,0) circle (\vsize);
			\clip (0.2,0) circle (\vsize);
			% \fill[red!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-0.2,0) circle (\vsize);
			% \draw[ultra thick,white] (0.2,0) circle (\vsize);					\end{scope}
		\end{scope}
		\draw (-0.2,0) circle (\vsize);
		\draw (0.2,0) circle (\vsize);
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.6){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
\hspace{\spacerlength}\hspace{2.5pt}\vrule\hspace{2.5pt}\hspace{\spacerlength}
	%% EXAMPLE: 1 -> X -> Y -> Z
	 \refstepcounter{subfigure}\label{subfig:1XYZ}
	\begin{tikzpicture}[center base,is bn]
		\node[dpad0] (1) at (-0.5,2.3){$\var1$};
		\node[dpad0] (X) at (-0.5,1.5){$X$};
		\node[dpad0] (Y) at (0.35,1.25){$Y$};
		\node[dpad0] (Z) at (0.25,2.25){$Z$};subfiglabelcolor
		\draw[arr1] (1) to (X);
		\draw[arr1] (X) to[] (Y);
		\draw[arr2] (Y) to[] (Z);
		\path[fill=white!70!black] (210:0.22) circle (\vsize) ++(-130:.25) node[label=below:\tiny$X$]{};
		\path[fill=white!70!black] (-30:0.22) circle (\vsize) ++(-50:.25) node[label=below:\tiny$Y$]{};
		\path[fill=white!70!black] (90:0.22) circle (\vsize) ++(40:.29) node[label=above:\tiny$Z$]{};
		\begin{scope}
			\clip (90:0.22) circle (\vsize);
			\clip (210:0.22) circle (\vsize);
			\fill[red!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
			% \draw[ultra thick,white] (90:0.2) circle (\vsize);	
			\clip (-30:0.22) circle (\vsize);
			\fill[white!70!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (-30:0.2) circle (\vsize);
			% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
			% \draw[ultra thick,white] (90:0.2) circle (\vsize);
		\end{scope}
		\begin{scope}
			\draw[] (-30:0.22) circle (\vsize);
			\draw[] (210:0.22) circle (\vsize);		
			\draw[] (90:0.22) circle (\vsize);
		\end{scope}
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.7){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
	\hspace{3pt}
\hspace{\spacerlength}%\vrule\hspace{\spacerlength}
	%% EXAMPLE: X -> Y -> Z -> X
	\refstepcounter{subfigure}\label{subfig:XYZ-cycle}
	\begin{tikzpicture}[center base] 
		% \node[dpad0] (1) at (-0.5,2.3){$\var1$};
		\node[dpad0] (X) at (-0.5,1.75){$X$};
		\node[dpad0] (Y) at (0.35,1.25){$Y$};
		\node[dpad0] (Z) at (0.25,2.25){$Z$};
		% \draw[arr0] (1) to (X);
		\draw[arr1] (X) to[bend right=25] (Y);
		\draw[arr1] (Y) to[bend right=25] (Z);
		\draw[arr1] (Z) to[bend right=25] (X);
		%option: -- either X -> Y -> Z -> X, or <-> Y <-> Z <-> X. For the latter, uncomment the 6 lines below and comment out the next 3.
		% \draw[arr1] (Z) to[bend left=5] (Y);
		% \draw[arr1] (Y) to[bend left=5] (X);
		% \draw[arr1] (X) to[bend left=5] (Z);
		% \draw[fill=red!50!black] (210:0.22) circle (\vsize) ++(-130:.27) node[label=below:\tiny$X$]{};
		% \draw[fill=red!50!black] (-30:0.22) circle (\vsize) ++(-50:.27) node[label=below:\tiny$Y$]{};
		% \draw[fill=red!50!black] (90:0.22) circle (\vsize) ++(140:.31) node[label=above:\tiny$Z$]{};

		% grey filling for one covering.
		\draw[fill=white!70!black] (210:0.22) circle (\vsize) ++(-130:.27) node[label=below:\tiny$X$]{};
		\draw[fill=white!70!black] (-30:0.22) circle (\vsize) ++(-50:.27) node[label=below:\tiny$Y$]{};
		\draw[fill=white!70!black] (90:0.22) circle (\vsize) ++(40:.31) node[label=above:\tiny$Z$]{};

		\begin{scope}
			\clip (-30:0.22) circle (\vsize);
			\clip (210:0.22) circle (\vsize);
			% \fill[white!70!black] (-1,-1) rectangle (3,3);
			\clip (90:0.22) circle (\vsize);
			\fill[green!50!black] (-1,-1) rectangle (3,3);
		\end{scope}
		\begin{scope}
			\draw[] (-30:0.22) circle (\vsize);
			\draw[] (210:0.22) circle (\vsize);		
			\draw[] (90:0.22) circle (\vsize);
		\end{scope}
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.7){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}
\hspace{3pt}
\hspace{\spacerlength}%\vrule\hspace{\spacerlength}
	%% EXAMPLE: X -> Y <- Z
	\refstepcounter{subfigure}\label{subfig:XZtoY}
	\begin{tikzpicture}[center base] 
		% \node[dpad0] (1) at (-0.5,2.3){$\var1$};
		\node[dpad0] (X) at (-0.45,1.9){$X$};
		\node[dpad0] (Y) at (0.3,1.25){$Y$};
		\node[dpad0] (Z) at (0.4,2.15){$Z$};
		% \draw[arr0] (1) to (X);
		\draw[arr0] (X) to[] (Y);
		\draw[arr1] (Z) to[] (Y);
		\path[fill=green!50!black] (210:0.22) circle (\vsize) ++(-130:.25) node[label=below:\tiny$X$]{};
		\path[fill=red!50!black] (-30:0.22) circle (\vsize) ++(-50:.25) node[label=below:\tiny$Y$]{};
		\path[fill=green!50!black] (90:0.22) circle (\vsize) ++(40:.29) node[label=above:\tiny$Z$]{};
		\begin{scope}
			\clip (-30:0.22) circle (\vsize);
			\clip (90:0.22) circle (\vsize);
			\fill[white!70!black] (-1,-1) rectangle (3,3);
		\end{scope}
		\begin{scope}
			\clip (-30:0.22) circle (\vsize);
			\clip (210:0.22) circle (\vsize);
			\fill[white!70!black] (-1,-1) rectangle (3,3);

			\clip (90:0.22) circle (\vsize);
			\fill[green!50!black] (-1,-1) rectangle (3,3);
			% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
			% \draw[ultra thick,white] (90:0.2) circle (\vsize);	
			% \draw[ultra thick,white] (-30:0.2) circle (\vsize);
			% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
			% \draw[ultra thick,white] (90:0.2) circle (\vsize);
		\end{scope}
		\draw[] (-30:0.22) circle (\vsize);
		\draw[] (210:0.22) circle (\vsize);		
		\draw[] (90:0.22) circle (\vsize);
%		\useasboundingbox (current bounding box);
		\node at (-0.7, 0.7){\slshape\color{subfiglabelcolor}\thesubfigure};
	\end{tikzpicture}~
	\hspace{\spacerlength}%\vrule\hspace{\spacerlength}
		%% EXAMPLE: X <-> Y <-> Z
		\refstepcounter{subfigure}\label{subfig:XYZ-bichain}
		\begin{tikzpicture}[center base] 
			% \node[dpad0] (1) at (0.1,2.4){$\var1$};
			\node[dpad0] (X) at (-0.3,1.2){$X$};
			\node[dpad0] (Y) at (0.3,1.9){$Y$};
			\node[dpad0] (Z) at (-0.35,2.5){$Z$};
			% \draw[arr1] (1) to (X);
			% \draw[arr1] (1) to (Y);
			\draw[arr1] (X) to[bend right=15] (Y);
			\draw[arr1] (Y) to[bend right=15] (X);
			\draw[arr1] (Y) to[bend right=15] (Z);
			\draw[arr1] (Z) to[bend right=15] (Y);
			\path[fill=white!70!black] (210:0.22) circle (\vsize) ++(-130:.25) node[label=below:\tiny$X$]{};
			\path[fill=red!50!black] (-30:0.22) circle (\vsize) ++(-50:.25) node[label=below:\tiny$Y$]{};
			\path[fill=white!70!black] (90:0.22) circle (\vsize) ++(40:.29) node[label=above:\tiny$Z$]{};
			\begin{scope}
				\clip (-30:0.22) circle (\vsize);
				\clip (90:0.22) circle (\vsize);
				\fill[white!70!black] (-1,-1) rectangle (3,3);
			\end{scope}
			\begin{scope}
				\clip (90:0.22) circle (\vsize);
				\clip (210:0.22) circle (\vsize);
				\fill[red!50!black] (-1,-1) rectangle (3,3);
			\end{scope}
			\begin{scope}
				\clip (-30:0.22) circle (\vsize);
				\clip (210:0.22) circle (\vsize);
				\fill[white!70!black] (-1,-1) rectangle (3,3);

				\clip (90:0.22) circle (\vsize);
				\fill[green!50!black] (-1,-1) rectangle (3,3);
				% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
				% \draw[ultra thick,white] (90:0.2) circle (\vsize);	
				% \draw[ultra thick,white] (-30:0.2) circle (\vsize);
				% \draw[ultra thick,white] (210:0.2) circle (\vsize);		
				% \draw[ultra thick,white] (90:0.2) circle (\vsize);
			\end{scope}
			\draw[] (-30:0.22) circle (\vsize);
			\draw[] (210:0.22) circle (\vsize);		
			\draw[] (90:0.22) circle (\vsize);
%			\useasboundingbox (current bounding box);
			\node at (-0.7, 0.7){\slshape\color{subfiglabelcolor}\thesubfigure};
		\end{tikzpicture}
		}
}
\addtocounter{figure}{-1} %undo the thing I did to make subfigs work
\caption{\itshape Illustrations of example graph information
	  functions $\{ \IDef{G_i} \}$, drawn underneath their
	  associated multigraphs $\{ G_i\}$. Each circle represents a
	  variable; an area in the intersection of circles $\{C_j\}$
	  but outside of circles $\{D_k\}$ corresponds to information
	  that is shared between all $C_j$'s, but not in any
	  $D_k$. Variation of a candidate distribution $\mu$ in a
	  green area makes its qualitative fit better (according to
	  $\IDef{}$), while variation in a red area makes its
	  qualitative fit worse; grey is neutral. Only the boxed
	  structures in blue, whose graph information functions can be
	  seen as assertions of (conditional) independence, are
	  expressible as BNs.} 

\label{fig:info-diagram}
\end{figure}

The examples here are in reference to \Cref{fig:info-diagram}.
Subfigures \ref{subfig:justX-0}, \ref{subfig:justX-1}, and \ref{subfig:justX-2} show that adding edges makes distriutions more deterministic. 
As each edge $\ed LXY$ corresponds to an assertion about the ability to determine $Y$ from $X$, this should make some sense.
In particular, \ref{subfig:justX-2} can be justified by the fact that if you can determine X from two different random draws, the draws probably did not have much randomness in them. Thus we can qualitatively encode a double-headed arrow as two arrows, further justifying the notation.
	%oli11: note that it does not matter for the semantics, because failing to meet the constraint imposed by a double-headed arrow will give infinite cost anyway, for any edge, as \beta > 0.
%	
Without any edges (e.g., \ref{subfig:justX-0},\ref{subfig:justXY}), the $G$-information rewards distributions with the most uncertainty. Each additional edge adds a penalty for a crescent, as when we move from \ref{subfig:justXY} to \ref{subfig:XtoY} to \ref{subfig:XY-cycle}.
%
Some graphs (\Cref{subfig:justX-1,subfig:1XY}) are \emph{universal}, in that every distribution gets the same score (so that score must be zero, beause this is the score a degenerate distribution gets). Such a graph has a structure such that \emph{any} distribution can be precisely encoded by the process in (b). 
%	
The $G$-information can also indicate independencies and conditional independencies, illustrated respectively in \ref{subfig:XYindep} and \ref{subfig:1XYZ}.

So far all of the behaviors we have seen have been instances of entropy maximization / minimization, or independencies, but $G$-information captres more: for instance, if $G$ has cycles, as in \ref{subfig:XY-cycle} or \ref{subfig:XYZ-cycle}, the $G$-information prioritizes shared information between all variables. 

In more complicated examples, where both penalties and rewards exist, we argue that the $G$-information still implicitly captures the qualitative structure. In \ref{subfig:XYZ-bichain}, $X$ and $Y$ determine one another, and $Z$ and $Y$ determine one another. It is clear that $X$ and $Z$ should be indpenedent given $Y$; it can also be argued that $Y$ should not have any randomness of its own (otherwise the draws from $X$ or $Z$ would likey not match one another) and that this structure suggests co-variation of all three variables.
\end{example}
	
\begin{remark}
	We have also considered the variant, where we use $\E_{x \sim \mu_X} \H (\bp (x))$ in place of $\H_\mu(Y \mid X)$ to define the $G$-information deficit, as in 
	\begin{equation}
		\IDef{\dg M}^{\mat p}(\mu) := \sum\alle \H_\mu(Y\mid X) - \H_\mu
		\label{eqn:alt-extra}
	\end{equation}

	This has the benefit of having a linear first term and enjoying strong convexity for all values of $\gamma$. However, it is a more complex and less qualitatively separated. For distributions $\mu \in \Inc_{\dg M}$, these quantities are the same; therefore, the difference lies exclusively in the way it scores distributions that are already inconsistent with the edges.
	We can use this fact to give us a stronger maximum-entropy theorem for Bayesian Networks than has previously be given in \cite{williamson2000}.
\end{remark}

	

$\Inc({\dg M}, \mu)$ and $\IDef{\dg M},\mu)$ give us two measures of compatibility between ${\dg M}$ and a distribution $\mu$. We take the score of interest to be their sum, with the trade-off
controlled by a parameter $\gamma \ge 0$:
\begin{equation}
	\bbr{\dg M}_\gamma(\mu).
	:= \Inc_{\dg M}(\mu) + \gamma \IDef{\dg M}(\mu)
	\label{eqn:full-score}
\end{equation}
%joe9*: I think the proposition and the following sentence are
%worth adding 

%joe10
%        The following just make precise that the scoring semantics
The following just makes precise that the scoring semantics
generalizes the first semantics.
% \begin{prop}[restate=prop:sd-is-zeroset]\label{prop:sd-is-zeroset}
\begthm{prop}{prop:sd-is-zeroset}
For all PDGs $\dg M$, we have that $\SD{\dg M} = \{ \mu : \bbr{\dg
	M}_0(\mu) = 0\}$. 
\end{prop}

%%BEGIN_FOLD
  \subsection{PDGs As Unique Distributions}\label{sec:uniq-dist-semantics}
%		
		% shows that PDGS are 
Before we provide an interpretation of a PDG as a probability distribution, we stress that this distribution does \emph{not} capture all of the important information in the PDG---for example, a PDG can represent inconsistent knowledge states. Still, by giving a distribution, we enable comparisons with other graphical models. In the process, we will discover 
that PDGs are a surprisingly flexible tool for articulating distributions themselves. All we need to do is select the minimizers of our loss function.
We thus define 
		
\begin{defn}[Optimal Distributions]
	\begin{equation}
		\bbr{\dg M}_\gamma^* = \arg\min_{\mu \in
			\Delta\V(\dg M)} \bbr{\dg M}_\gamma(\mu).
	\end{equation}   
\end{defn}

In general, $\bbr{\dg M}_\gamma^*$ does not give a unique distribution.  But if $\gamma$ is sufficiently small, then it does:

\begthm{prop}{prop:sem3}%\begin{prop}
	If $\dg M$ is a PDG and $0 < \gamma \leq \min_L \beta_L^{\dg M}$, then $\bbr{\dg M}_\gamma^*$ is a singleton. 
\end{prop}
		
We are especially interested in the case where $\gamma$ is small; this amounts to emphasizing the accuracy of the probability distribution as a description of probabilistic information, rather than the independence structure of the PDG.  This is what was going on in all the examples in the introduction.  This motivates us to consider what happens as $\gamma$ goes to 0.  If $S_\gamma$ is a set of probability distributions for all $\gamma \in [0,1]$, we define $\lim_{\gamma \rightarrow 0} S_\gamma$ to consist of all distributions $\mu$ such that there is a sequence $(\gamma_i, \mu_i)_{i \in \mathbb N}$ with $\gamma_i \to 0$ and $\mu_i \to \mu$ such that $\mu_i \in S_{\gamma_i}$ for all $i$. It can be further shown that

\begthm{prop}{prop:limit-uniq}
	For all $\dg M$, $\lim_{\gamma\to0}\bbr{\dg M}_\gamma^*$ is a singleton. 
\end{prop} 

Let $\bbr{{\dg M}}^*$ be the unique element of $\smash{\lim\limits_{\gamma \rightarrow 0}} \bbr{{\dg M}}_\gamma^*$. 
The semantics has an important property: 

\begthm{prop}{prop:consist}
	$\bbr{\dg M}^* \in \bbr{\dg M}_0^*$, so if $\dg M$ is consistent,
	then $\bbr{\dg M}^* \in \SD{\dg  M}$.
\end{prop}
	
In contrast with the other two semantics, $\bbr{\dg M_1 \cup
\dg M_2}^*$ cannot be eaily calcualted from $\bbr{\dg M_1}_\gamma^*$ and
$\bbr{\dg M_2}_\gamma^*$. We will see that it is nonetheless effectively the semantics used by other graphical models.

\begin{defn}
	Let $\bbr{M}'(\mu) := \Inc{\dg M}(\mu) + \IDef{\dg M}^{\mat p}(\mu)$ be the altered version of the information definition.
\end{defn}
\begin{prop}\label{prop:u-convex}
$\bbr{\dg M}'_\gamma(\mu)$ is $\gamma$-strongly convex.% in $\mu$.
\end{prop}
\begin{proof}
	$\Inc_{\dg M}( \mu)$ is convex in $\mu$
	(\Cref{thm:inc-convex}), and $\gamma\sum\alle \E_{x\sim \mu_X}
	\H(\bp(x))$ is linear in $\mu$.  
	Negative entropy is $1$-strongly convex
	(\Cref{prop:neg-ent-convex}), so $- \gamma \H(\mu)$ is $\gamma$-strongly convex.
	The sum of a $\gamma$-strongly convex, linear, and
	convex functions must be $\gamma$-strongly convex. 
	%		, and strongly so when the coefficient on $-\H$ ($\gamma$) is positive. 
	%(see \cite{Rockafellar1970ConvexA})
\end{proof}

%END_FOLD

	\subsection{Lax PDGs}
	\begin{defn}[PDG]\label{def:PDG}
		A (lax) PDG is a tuple $\pdgvars[]$ where
		\begin{description}[nosep]
			\item[$\N$]~is a finite collection of nodes, which are identified with variables
			\item[$\Ed$]~is a collection of directed edges (arrows), each with a source, target, and a (possibly empty) label.
			\item[$\V$]~associates each node $N \in \N$ with a set $\V(N)$,
			representing the values that the variable $N$ can take. 
			\item[$\mathbf p$] associates, for each edge $L = (X,Y, \ell) \in \Ed$ and $x \in \V(X)$ a distribution $\bp(x)$ on $Y$, whenever $\beta_L > 0$.
			\item[$\beta$]~associates to each edge $L$, a number in $[0,\infty]$, indicating certainty in the conditional distribution $\bp(Y \mid X)$ 
			\item[$\alpha$]~associates to each edge $L$, a number in $[0,1]$, indicating degree of belief that $L$ holds causally.
		\end{description}
		\vspace{-1.4em}
	\end{defn}
	

	
	\section{Generalized Semantics}
	Let $d$ be a distribution over $\Ed^{\dg M}$. 
	Choose a link $\ed LXY \in \Ed$, and let $\mathbf Z_L := \N^\dg M \setminus \{X, Y\}$ be the set of all other variables, so that a joint setting is characterized by a value $(x,y,\mat z)$.
	
	The link $L$ and its associated cpt $\bp$ can be extended to a transformer $\tau_L: \Delta \V(\dg M) \to \Delta \V(\dg M)$ on joint distributions in a couple of ways, such as:
\[
	\begin{aligned}
		\tau(\mu)(x,y,\mat z) :&= \mu(x,y,\mat z)\frac{\bp(y \mid x)}{\mu(y\mid x)} \\
		&= \mu(x)\; \bp(y \mid x)\; \mu(\mat z \mid x,y) 
	\end{aligned}\qquad\text{and}\qquad
	\begin{aligned}
		\tau(\mu)(x,y,\mat z) :&= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x, \mathbf z)} \\
		&= \mu(x,\mat z)\; \bp(y \mid x)
	\end{aligned}\\
\]
	I have yet to discover which of these things is correct. 
%	\begin{align*}
%		 \tau(\mu)(x,y,\mat z) &:= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x)} 
%			 &  \tau(\mu)(x,y,\mat z) &:= \mu(x,y,\mat z) \frac{\bp(y \mid x)}{\mu(y\mid x, \mathbf z)} \\
% 			&= \mu(x)\; \bp(y \mid x)\; \mu(\mat z \mid x,y) 
%			 &   &= \mu(x,\mat z)\; \bp(y \mid x)
%	\end{align*}
%	

		
	
	
\end{document}
