Recall our discussion of life on an unknown planet.
The probabilists in us might not be willing to so easily give up the notion that this data ought to define a probability distribution, at least implicitly. There are other, more general graphical models after all.
	% We presumably already have a prior over everything, and in the event that we d
	%Bayesian networks are insufficient to represent this epistemic state is because the state is not a distribution and hence invalid? 
Maybe there's something simple we can do to turn it into one? It turns out that we can (almost always) simultaneously get a distribution and commit to preserving the relative ratios of the specified probabilities within the links, while also more clearly exposing our independence assumptions. 		
%specified to be probability distribution. If we had had a prior, perhaps we could have ``conditioned'' it on the new probability information that our colleague gave us.%
%\footnote{
%	``Conditioned'' gets quotation marks because the statement is not an event --- it is not information about the actual world $(S,C,L,W)$, but a meta-statement about probabilities. A probabilist can now avoid this by incorporating the things that a colleague can say into their mental model, and conditioning on the things that are said, which are now truly events. There are two issues with this. First, thsi runs into the problem that you already needed to have all worlds modeled in your head. Even if you allow yourself to form new concepts and worlds as the come to attention,  
%	}
 %	Together with the perspective that a Bayesian Network really encodes independence, this line of thought might cause one to think that the problem was a restrictive class of graphical model. Perhaps really we wanted a factor graph:



----------------------------------------


 The procedure for converting to a BN is simple: we simply take every node's incoming arrows, and insert the product of its parents as a node before it. With this procedure, if a node $N$ has just one parent $P$, we replace the subgraph [$P \to N$] with [$P \to N \stackrel\sim\rightleftarrows N$], which is redundant so we don't draw this. If a node had zero parents (i.e., the BN just gives it a probability distribution not dependent on anything), we insert the product of zero things, i.e., the singleton node $\mathsf 1$ (with $\mathcal V(\mathsf 1) = \{\star \}$), and set $\Pr(N \mid \star) = \Pr(N)$. 
 	
 	This sound much more complicated than it is. Consider the example below, where the left is a BN, and the right is the corresponding \modelname.


	We have effectively changed two things: first, visually encoded the probability distribution of $A$ as the arrow $1 \to A$ (which we are now allowed to omit; sometimes you don't want priors on things, such as your own actions). Second, we have combined the two arrows $B \to D$ and $C \to D$ into a single one, $B \times C \to D$. Though certainly more verbose, this is arguably visually clearer if want to follow arrows: you cannot compute $D$ from $B$; you need both $B$ and $C$.

----------------------------------------------

Analogy: constantly making sure the document has the right formatting. Ultimately the formatting may help but making sure it's corretly formatted after every character you type is `taxing and restrictive'. Fixing your habits allows you to make bigger modifications to the document, and consolidate the cost of fixing the formatting.
