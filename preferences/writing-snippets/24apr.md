THings I want to capture / Application Cases / Examples
	- Beleifs Change
		- So preferences are cached optimization preferences. At top there is the true optimization objective, and everything else is cached, and a property of SGD in high dimensional spaces, that it has some memory of previous optima. At the bottom, there are actions: the current policy.

			For the Pref Change thesis, this implies the full stack is (top part mine): 
															
														Good Empirical Practice
																↓
														Sound Updating Rules
																↓
															Belifs + 
				Objective    ⟶    Priority Base    ⟶   (Explicit or Kernel) Preferences   ⟶   Actions
									 
								|------------------------------------------------|
											Agent refers to the computation hhere
											
			
			
			SO if your beliefs change, your preferences should change. Examples:
				- I believe that eating meat is good for the environment, and I think it's a good thing to eat beef. I prefer beef to tofu. Then I update my beliefs to say the reverse, and so my previously-consistent preferences are no longer consistent
					"be good to the environment" spawned the "eat meat" preference via certain reasons. This entire chain of preferences fails to hold when the reason falls, and because of the formation order, we should change the more 
		
		
	- Individual Music Taste Change
	- Cultural Revolution
	- Art History 
	- Value Capture
