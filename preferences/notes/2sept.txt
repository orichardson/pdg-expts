
Some Examples / Hooks into socially relevant problems.

SOCIALLY RELEVANT TARGET EXAMPLES
	problem: the right to be forgotten
		having less information corresponds to fewer links in the graph (?). You can identify links as problematic for external reasons and change them (this is preference change). 

	problem: eugenics, biotic enhancements, etc.
		part of the reason this exists is the constant optimization, fixed preferences, inability to imagine different ones. Solutions include ``empathy'' (i.e., communication to change preferences locally)

	problem: destruction of the amazon / global warming
		People can't see the root causes of things (e.g., automation, the economic effect of purchasing habits, etc). Prices are suppose to reflect them but sometimes they borrow from the future; the sense in which there is any optimality is merely 
		Also, the utility and consumption habits of real people can change: economics generally does not deal with vegetarianism, for instance: where preferences are changed due to inconsistencies.

	Radical Markets' problem: monopoly!
		From the perspective of a utilitarian overmind, every decision should be made on a case-by-case basis, and the very notion of protected ``laws'' is problematic, leaving open potential for exploitation. However, using congitive boundedness we might be able to argue that the laws are simpler


ANALYSIS
	All of these problems can be regarded as market failures. WHat kinds of market failure can I address? Do I create new modes of market failure, that I've failed to identify? (Probably, but as long as I offer a way of resolving them, I might be ok.)
	
	Experiences are samples from a utility function, but the co-domain is a very expressive context --- no two experiences are in the same context, and among other things, the context includes the decision making criteria (e.g., utility function, if that's what's being used). So you can think of even evolving preferences on lower level issues happ
	
	The intuition is: you have experiences (samples), you learn represenations for both the world and the best things that make you happy. THere are also logical and world constraints, which cause there to be more inconsistencies. To resolve them, you can

	 - construct higher-level preferences that resolve them (but the resolutions have to be nice somehow; maximum entropy is nice)
	 - 
	


MORE TOY EXAMPLES
	Vegetarianism. You once eat meat, and it's most of what you care about. You optimize for eating maximum amount of meat which involves making friends with people (made of meat). You don't eat them because they'll help you get more meat down the line. Disease kills all livestalk, and you realize you're not going to survive until people can replace them. Do you eat your friends? No. We can model this with value capture.
	
	Is it a shitty optimizer who failed an objective to eat meat? Yes. Worry: this exposes itself to manipulation (humans are value-manipulatable; do we want AI overlords who are?)
	
	
	
	You learn it's slaughtered animals, and can no longer enjoy it with this new information. 
	


--------------

From walking
	* The "generalization" to ℝ-nodes where edges are just ∂X / ∂Y, like regulation networks / neural networks (kinda?), etc. are just the case where the variables are unary and foreground only. 
	  	Question: where is the non-linearity for NNs?
			> as regularizatioon: want to spread out value, high entropy (maybe hacky)
			> from finitude of states and boundedness of probability distributions (not sure if this exactly works out but [0,1] probability means impact is bounded anyway... ) 
			> bad state representation. If you need a non-linearity you need at least three states, obviously. We can then fit with splines and kernel methods.

	* A value resolution need not bottom out all the way at utilities. You can take a "greatest lower bound", and any node which absorbs all of the causal edges from a given node effectively determines its utility. Categorically, this is a co-limit (definitely from an ordering perspective, unsure about in general but it looks right). 

	* More evidence that preferences go backwards: decisions are easy if there's one effect and many causes for each node (causal merging, but pref branching), but requires weighing and resoultion if there are many effects and one cause for every node (causal branching, but pref merging)


	* Inconsistency is WAY too strong; we preference revise far before that, just when it's unlikely to be consistent. This is part of the entropy term, hopefully. If you have a big enough hypothesis space (VC dimension) you can always make new distinctions and shatter all examples, but this is undesirable for overfitting reasons. 
		Question: From a Bayesian perspective, what is the prior here?


----------
Walking 2

	STANDARD RELEVANT RESULTS THAT HAVE IMPLICATIONS FOR ME BUT ARE NOT NEW:
		* We can expand self transition matrices to graphs. It's just a point-wise splitting of the node into many nodes. Fixed points are solutions to consistency. For example, a preference matrix, converted to a proabalistic transition matrix, converges to have some weight on local optima, more if the cone beneath it is greater.
	
		* Game trees, in particular, can also be encoded this way. Computing expected utility via backwards propogation of preferences is backwards induction, and corresponds to the Bellman Equation in an MDP.
  
  
 	* Nontransitivity of distributions exist in the "beating other distributions on avg" sense, but to really lay that out you need additional assumptions, and it's not clear this can induce non-transitive preferences.
	
	* Arrow's Theorem says there are problems using orders and no probability. Gibbard (Satterthweite) theorem says that if deterministic, there's no way to avoid strategic voting, which seems to mean there's some instability somewhere. Fortunately we can just use probabilities for everything and avoid them.

	* General way that preferences change: abstraction or diffusion to a different domain, followed by a belief change.
