\documentclass{article}

\input{prefs-commands.tex}
\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{A Simpler Explanation}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}

\begin{document}
	This document is intended to be clear, simple, and at a high level. % It exists as a safe haven away from most of the math and generality I want to later think about, aside from some footnotes and a section where I list some connections I suspect exist. 
	I will use no numbers, prove nothing here, and just make some arguments and provide examples.
	
	\section{A Very Brief Description}
	This is an attempt at a model of joint preference and belief revision. It allows for representational flexibility, so that agents need not always have a perfectly consistent picture of reality, and is intended to capture phenomena such as learning preferences from data and revising them when they're inconsistent, while also disallowing value changes which would be objectionable from an agent's current perspective.
	
	There are two components involved: 
	\begin{enumerate}[nosep]
		\item Representation: An account of an agent's values as distributed across a network, whose nodes are preferences on small sets of alternatives, and whose edges are beliefs about the impact of one choice on another.
		\item Dynamics: A formulation of how to revise preferences due to inconsistencies that may arise as beliefs change, and the representation shifts due to new concepts.
	\end{enumerate}
		

	\section{Why This Is Worth Doing}
	\vspace{-1em}
	\textit{A few short motivations from several perspectives}
	
	\subsection*{[Ethos of Agency] The separation between values and tools isn't clean}
	The distinction between the things that you care about (e.g., utilities, goals, preferences, objectives), and the tools you have to make these things come about (e.g., beliefs, reasons, plans, optimization, inference) plays a large role in technical accounts of agency%
	\footnote{Beliefs should be considered optimization power: they you believe that action $A$ will have effect 1, an action $B$ will have effect 2, and a preference between the 1 and 2}. Generally the two are considered separately, and also people tend to fix preferences%
	\footnote{In the context of optimization, this corresponds to the practice of writing down an objective and finding extrema; in the context of decision theory, it is finding the best actions to satisfy some preferences}. This is nice because the fixed preferences provide a clean way to evaluate the quality of decisions, and criteria for success are a very important part of science.
	
	Unfortunately, human preferences and beliefs often depend on one another, and in more complex settings the line is much more blurred:
%	Subjectively, though it is hard to imagine having one without the other, and it seems like there might be G\"odelian knots if you take the separation seriously enough --- 
	\begin{itemize}[nosep]
		\item When you play a video game, are you trying just so you can win? If so, why do you care about winning? It buys you very little socially, and costs time and money. Or does the process of optimizing for this arbitrary goal itself have value? Does this make entertaining yourself fundamentally irrational?
%		\item In the process of achieving enlightenment, you go about removing your desires. But it seems like being without desires is in fact a desire--- why where you attempting to do this, anyway? 
%		\item Similarly,  care about is truth
		\item It is common to have beliefs about preferences: (`I think I like cheese', 'I believe freedom is bad'), and also preferences over beliefs (`Believing true things is good', `I want to know calculus')
		\item These can be nested quite deeply without anyone thinking too hard: If I ask if you like cheddar, you now believe that I want to know whether you like cheese --- note that this is a belief about a desire for a piece of knowledge about a preference, spanning multiple people.

		\item On a small scale, we think of a loss function as an optimization objective, and the algorithm (say, stochastic gradient descent) as optimization power. One can use both of these together to form a neural network, which is thought of as being just a tool, with no

		\item Complete agents can be used in either way: people, who have desires and experience emotions, can be used as optimization power with an external objective (e.g., hiring employees), but can also as the ultimate source of value (e.g., running an organization because it will help people)
	\end{itemize}
	The point is that even we could combine any model of generic optimization, with any model of value, the interactions between them would certainly be woven together, and their dynamics would be intertwined.
	
	%%
	{\color{green!30!black}This is why we model the dynamics of beliefs and preferences together, and why in some cases we will be able to represent a belief as a preference or vice versa. \footnote{the categorical interpretation, where each node is a category, the big graph forms a 2-category, and beliefs are value-preserving functors, supports the observation that the distinction between preferences and beliefs isn't a fundamental feature the objects themselves but rather how they're used--- here the arrow category here gives us preferences over beliefs. Similarly, quotient objects represented as arrows, just as utilities can be identified with beliefs about utilities.}}
	
	\subsection*{[Modeling Accuracy] Humans change their preferences}
	When you're born you have no conception of what foods or professions or ideals are good; all you have is your own evolutionarily programmed pleasure and pain. Not only does this feeling get more sophisticated in a way that seems to depend on the environment, but also later on in life, people willingly sacrifice their experiential pleasure for other things they care about. All of this stands even if we didn't see change in the more obvious toy cases: foods, activities, and luxury goods. Such changes do in fact occur, and do not detract from a person's ability to be conceptualized as a coherent agent; often they contribute to a person's character instead.
	
	The standard models do not account for any of this, and for many good reasons --- but dynamic preferences are necessary to capture a great deal of human behavior, and are much more important in a world where value is informational and changes quickly. Things like memes, fashion, games, conversations, lofty ideals, and art---things that standard economic theory has had a lot of difficulty ascribing value to---certainly play a bigger economic role than they have in the past, and arguably move faster as well.
	%is learning a new piece of information something that has value?
	
	%there is some overlap between conditioned preferences and dynamic preferences.
	{\color{green!30!black}This is why we model changes in preferences, and acquisition from only a very limited set of base values. We have some reasons to suspect that in most cases preferences generated this way will be roughly convergent, which if true would (1) provide an additional explanation (beyond empathy and genetics) for why humans end up sharing a lot of values, and (2) allay some concerns about misaligned AIs that are constructed in this way}
	
	\subsection*{[Computational Tractability] Computational Boundedness}

	The standard picture of decision theory requires keeping around preferences and beliefs about all possible things that are relevant to any decisions you make. To fully describe a general agent's values, then, you need to specify a preference ordering over all possible histories of settings of observable features. This is wildly intractable, and also annoyingly depends on what ``possible'' means, which is part of the agent's internal beliefs\footnote{Or alternatively, if you're going to do the work beforehand as a modeler, has to change when humanity discovers new features of the universe}. This is why people in practice restrict the scope of an agent to only a few modeler-chosen variables, assume that they only encounter one kind of decision, specify objectives in a compressed syntactic form.
	
	But even these more tractable restricted approximations we use do not degrade gracefully: in order to make any decisions at all you have to do expected utility calculations (which could be very expensive depending on how clear the impact of your decision on the world is), and there's certainly no clear way of making use of partial computations under time pressure.
	
	{\color{green!30!black} By keeping components of preferences around redundantly in many different forms, not only can we immediately re-use them for recent decisions we've made in a pinch, but also likely reduce the complexity of adding new nodes. This is because we can split the computation into meaningful chunks (one for each preference domain (node) we can connect a decision to), and computing impact on a few nodes of your choice is going to be much faster than computing the total impact on the space of all things that are observable}
	
	\subsection*{[Value Modesty] A second brittleness}
	Classical AI systems can do complex tasks that even modern ML systems struggle with. However, they have a debilitating flaw: any malformed input a designer has not anticipated causes the entire calculation to be wrong in salvageable ways. It's coded without an outside view of the explicit purpose: there's no possibility that the designer was \textit{wrong} in the algorithm specification, and there's no reasonable inference to make instead even we acknowledged it was possible she was.
	
	Incorporating probabilities and specifying objectives instead of algorithms has helped this problem enormously: by using a ton of redundant, noisy, examples of the algorithm working correctly, we can be reasonably resistant to malformed input. Unfortunately, these systems are not perfect either: they are often biased and sometimes cheat. We face a different kind of brittleness here: what if we forget about a second order effect and slightly mis-specify an objective function? What if we run it on a different input distribution? ML systems won't crash, but they will confidently display  wrong answers. The system doesn't think there's any possibility that its \textit{objective is wrong}, and even if it acknowledged this possibility, it's not clear there's anything to be done at this point.
	
	{\color{green!30!black}In analogy to the solution that statistical ML provided to the problem of brittle classical AI, the solution would seem to be a noisy, redundant encoding of the objective function in conjunction with a meta-objective: in our case, consistency. The diffusion of preferences (which we will also refer to as value capture) can also be thought of as a source of uncertainty about your values even if you didn't start with any: if there are multiple objectives consistent with everything you've seen, you acquire a preference for both of them.}

	Of course, humans are also often uncertain about their values (and experience value capture) in addition to their beliefs and the appropriate choice of actions.
%	Agents are allowed to have uncertainty in what they believe
	
%	nd so the synthetic agents we design, influenced by the standard picture and designed with particular use cases in mind, 


	
	\subsection*{[Theoretical Unity] Reductions to other theories and algorithms}
%	There are many different representations of preference theories 
	Although the aim in the rest of this document is to ignore generality and aim for descriptively simple, the generality is a genuine mathematical motivation for this theory: it can be viewed as a number of different things. I will briefly mention some of them that I'm excited about here, and then focus on simple things for the rest of the document.
	\begin{itemize}[nosep]
		\item Our model reduces to standard expected utility calculations in the degenerate case where the agent has preferences over possible histories of worlds, the representation of a world never changes, and the agent is cognitively unbounded
		\item It has a categorical interpretation which has some features I'd like to explore: in particular, meta-preferences seem to be related to higher order structure, a total utility function is like a limit, a total probability function is like a co-limit, preference-preserving beliefs are functorial, and the nodes already form a 2-category, whose objects are themselves enriched flat categories.
		\item There also a natural interpretation of this as an artificial neural network. If the underlying graph is a DAG with one sink, then it is a feed-forward network for calculating expected utility. In most interesting cases, it will have recurrent connections and no special output.
		\item If we throw out the preference information, it can be converted to a Baysian Network with a simple transformation
		\item If the nodes are all propositions with a preference for truth, and we add some explicit rules for creating and deleting nodes, it becomes an inductive theorem prover.
		\item The preference propagation is a bit like broadcasting on a network (of physical machines) to compute path lengths. This can be done Dijkstra / Distributed Bellman Ford \footnote{depending on whether we take right or left powers of matices} and can be computed by taking iterative powers of a giant matrix over the tropical semi-ring. Funnily enough, this is exactly how you compute the transitive closure of preference matrices on a small scale (except with a different semi-ring), which lends more credibility to the higher order categorical interpretation.
	\end{itemize}

	The possibility of bridging these many disparate fields makes the abstraction feel much more universal, and the ability to think about preference change in any of these terms could make it easy to identify analogs of non-trivial facts in other fields.
	
	% AI safety
	
%	There are many ways of framing this.
%		real preferences change. 
%		Dual brittleness: certainty in preferences
%		Computational boundedness
	

	\section{Longer Informal Description and Intuition}
	
	\subsection{Representation}
	We will consider preferences on small sets of alternatives (nodes), together with a graph of beliefs, about how one preference impacts another. For example, the nodes could include: the set of ice cream flavors, a collection of memorable experiences, freedom and control, and anything else that could conceivably be considered an exhaustive set of alternatives for a choice. On these sets, we require preferences of some form; while I would like to ultimately propose that they be thought of as semiring matrices, we will stick to utilities for each alternative in the examples here.
	
	In addition, we also need beliefs about the impact of $A$ on $B$. While we do not want to commit to one of these in general yet, conditional probabilities will do the trick to explain the examples which follow. A belief about how $A$ impacts $B$ then, for the remainder of this document, is a family of probability distributions over $B$, one for each alternative $a \in A$ --- an object which looks and acts like the conditional distribution $\Pr(B \mid A)$, with which it will intentionally be confused.
	
	The structure of a representation with both pieces might look something like this:
	\begin{center}
	\begin{tikzpicture}
		\node[bpt={d1|$d_1$}] at (0,1){};
		\node[tpt={d2|$d_2$}, below=1.5em of d1] {};
		\draw[arr,->, thin, gray] (d1) to[bend left=30](d2);
		\node[Dom={$\sf D$ (D) around \lab{d1}\lab{d2}}] {};
		
		
		\node[bpt={c1|$c_1$}] at (4,0){};
		\node[bpt={c2|$c_2$}, below=1.8em of c1] {};
		\node[bpt={c3|$c_3$}, below=1.8em of {c2}] {};
		\draw[arr,->, thin, gray] (c1) to[bend left=50](c2);
		\draw[arr,->, thin, gray] (c3) to[bend left=50](c1);
		\node[Dom={$\sf C$ (C) around \lab{c1}\lab{c3}}] {};
		
		\node[bpt={w1|$w_1$}] at (-4,0){};
		\node[bpt={w2|$w_2$}, below=1.8em of w1] {};
		\node[bpt={w3|$w_3$}, below=1.8em of w2] {};
		\draw[arr,->, thin, gray] (w2) to[bend left=50](w3);
		\draw[arr,->, thin, gray] (w1) to[bend left=50](w2);
		\node[Dom={$\sf W$ (W) around \lab{w1}\lab{w3}}] {};
		
		
		\foreach [evaluate=\x as \y using \x/2-1.75] \x in {0, 1, ...,7} {
			%					\pgfmathmacro{\tmp}{-1 + 0.2*\x}
			\node[bpt={e\x | $e_\x$}] (e\x) at (\y,-3) {};
		}
		%nonsense
		\draw[arr,->, thin, gray] (e0) to[bend right=30](e6);
		\draw[arr,->, thin, gray] (e6) to[bend left=20](e2);
		\draw[arr,->, thin, gray] (e2) to[bend left=20](e4);
		\draw[arr,->, thin, gray] (e4) to[bend left=10](e7);
		\draw[arr,->, thin, gray] (e7) to[bend left=40](e5);
		\draw[arr,->, thin, gray] (e5) to[bend left=20](e1);
		%end nonsense
		\coordinate (Q) at (0,-3.5);
		\node[bDom={$\sf E$ (E) around \lab{e0}\lab{e7}(Q)}] {};
		
		\draw[arr] (D) -- (C);
		\draw[arr] (D) -- (W);
		\draw[arr] (E) -- (C);
%		\draw[arr, dashed] (D) to[bend left] (E);
		\draw[arr] (E) -- (D);
	\end{tikzpicture}
	\end{center}
	There are alternatives in each of the domains, preferences among the alternatives (we draw $w_1 \to w_2$ to communicate that $w_1 \leqc w_2$, i.e., that $w_2$ has higher utility than $w_1$), and links (conditional probability distributions, whose values are not drawn here) between them. 
	
	\subsubsection{Representing Utilities, Probabilities}
	While I will explain more details of the correspondence between this presentation and other standard ones in section \ref{sec:emulation}, the examples make more sense if 
	
	Suppose the only thing in my model is a single variable $A$, which can take on values $a_1, a_2, \ldots$. A preference on the alternatives often takes the form of an order, and if the order is complete and transitive, then it can be represented as a utility function. We can capture this explicitly with our model: think of utility as a separate preference domain $\sf U \cong (\mathbb R, \leq)$, whose elements are the real numbers, with a preference given by the usual ordering. Now a choice of $A$ has an impact on what happens in $\sf U$, and if we know that each choice of $A$ gives us a deterministic utility, then this impact is actually just a function, and in particular can be represented as a degenerate probability distribution $\Pr( \mathsf U \mid A)$, which is just a function from $A$ to $\mathbb R$.
	
	\begin{center}
	\begin{tikzpicture}
		\node[dpadded] (A) at (0,0){$\sf A$};
		\node[dpadded] (U) at (5,0){$\mathbb R, \leq$};
		\draw[arr](A) to node[above]{$U : A \to \mathbb R $} (U);
	\end{tikzpicture}
	\end{center}
	
	In this sense, we can think of utility functions as a way of representing preferences on $A$ as a belief about how $A$ impacts a standard universal preference domain of ``goodness''
	
	\todo{Refactor}{\color{gray}
	
	So why is the standard to use the real numbers instead of some other ordered set? The biggest reason is that uncertainties need to trade off against value, and we use probabilities to track uncertainty. For example, suppose I think $a_1 \leqc a_2 \leqc a_3$. This is enough information to make choices between subsets of the values of $a$, but is not enough to know whether I prefer an $\alpha$ chance at $a_1$ and $(1-\alpha)$ chance at $a_3$, over a sure outcome of $a_2$. This is unsurprising --- after all, bets are different things altogether. In fact, we can also put the space of bets into our diagram to clarify what's going on here:
	
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (0,0){$\sf A$};
			\node[dpadded] (BA) at (-3, 0){${B}[\sf A]$};
			
			\draw [arr] (BA) to (A);
		\end{tikzpicture}
	\end{center}
	
	
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (0,0){$\sf A$};
			\node[dpadded] (1) at (-3,0){$\mathsf 1$};
			\draw[arr](1) to node[above]{$\Pr(A)$} (A);
		\end{tikzpicture}
	\end{center}

	By using only t
	}
	
	These two are dual in some sense. There are two features of a conditional probability distribution: the dependence on the input, and the distribution over the output. Intuitively, one of these choices is choice made by the agent, and the other one by the environment. A utility function uses only the dependence on the input, so it can be seen as a vector $u: A \to \mathbb R$; a probability distribution makes use only of the distribution over outputs, and can be represented as a vector 
	
	
	\todo{restructure: move to end}
	Think of the conditional probability $\Pr(B \mid A)$ as a right-stochastic matrix $P$: $P_{ij}$, 
	
%	if we consider all possible settings of the variables to be alternatives of a single preference domain $\cal W$, then we can 
	
	
	
	\subsubsection{Benefits of this Representation}
	It allows us:
	\begin{itemize}[nosep]
		\item to represent inconsistent and redundant preferences
		\item to introduce new choices, and to modify or delete preference information without needing to globally re-compile a joint preference
		\item to emulate other preference descriptions (section \ref{sec:emulation})
		\item to store preferences about common choices so they can be re-used and nudged without fully recomputing everything for each decision
	\end{itemize}
	Of course, the real reason for thinking about them this way is because it facilitates change of preferences:
%	It does not fully specify a joint preference on all of the nodes, but neither does a CP net, which only defines a partial order on the product of all variables. I argue that this complete preference on all possible outcomes is not a feature we need to represent, because we will never have a choice between them all. The only reason to do so is so that we can compute expected utility.
	
%	While we no longer always have an explicit answer to the question ``is this world better than that one'' (a choice we rarely need to make), we still have a decision making apparatus, and in the case where there's no inconsistency, we're still implicitly defining a joint utility function.
	
%	Moreover, maintaining it as more domains are added to the picture becomes very expensive.
	

	
	\subsection{Dynamics}
	
	Now, given a representation, we can compute some measure of inconsistency, and then try to move preferences (or even beliefs) around so as to reduce the total inconsistency. This simple procedure has a number of interesting consequences.
	% reasonable utility functions from strange starting places
	% value capture
	
	\subsubsection{Preference Formation}
	Suppose you're trying to decide what your preference for $X$ is: a standard procedure that humans rely on is the compilation of a (weighted) list of pros and cons. In other words, you examine the impact of a decision in $X$ on other things you care about, say variables $A, B, C\ldots$. Maybe $x_1$ makes $a_1$ more likely, which is a good thing, also $b_3$ and $c_2$ which are both bad. On the other hand, maybe $x_2$ doesn't impact $A$ as much, but makes $b_1$ and $c_1$ more likely, which is ideal, making $x_2$ better than $x_1$. Now, we have external reason to prefer $x_2$ to $x_1$ --- which means that there's a preference conflict if we were initially indifferent between the alternatives in $X$! By revising our preferences in $X$ to reflect $x_2 \geqc x_1$, we have reduced the conflict. 

	\begin{center}
		\begin{tikzpicture}
			\def\N{2}
			\foreach [evaluate=\x as \y using (\x-\N/2)/1.5+1] \x in {1, ..., \N} {
				\node[bpt={x\x | $x_\x$}] at (\y,2) {};
			}
			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[Dom={$\sf X$ (X) around \lab{x1}\lab{x\N}}] {};
			
			%A
			\def\N{2}
			\foreach [evaluate=\x as \y using (\x-\N/2)/1.5-3] \x in {1, ..., \N} {
				\node[bpt={a\x | $a_\x$}] at (\y,0) {};
			}
			\draw[arr,->, thin, gray] (a2) to[bend left=50](a1);
			\node[bDom={$\sf A$ (A) around \lab{a1}\lab{a\N}}] {};
			
			%B
			\def\N{3}
			\foreach [evaluate=\x as \y using (\x-\N/2)/1.5+1] \x in {1, ..., \N} {
				\node[bpt={b\x | $b_\x$}] at (\y,0) {};
			}
			\draw[arr,->, thin, gray] (b3) to[bend left=30](b1);
			\draw[arr,->, thin, gray] (b2) to[bend right=10](b1);
			\node[bDom={$\sf B$ (B) around \lab{b1}\lab{b\N}}] {};
			
			%C
			\def\N{2}
			\foreach [evaluate=\x as \y using (\x-\N/2)/1.5+ 5] \x in {1, ..., \N} {
				\node[bpt={c\x | $c_\x$}] at (\y,0) {};
			}
			\draw[arr,->, thin, gray] (c2) to[bend left=50](c1);
			\node[bDom={$\sf C$ (C) around \lab{c1}\lab{c\N}}] {};
			
			\draw[arr] (X) -- (A);
			\draw[arr] (X) -- (B);
			\draw[arr] (X) -- (C);
		\end{tikzpicture}
	\end{center}

	Note what we have done: we have a link for how a choice in $X$ impacts downstream things, and so existing preferences flow backwards against the flow of causality, to form a preference on $X$. This is the standard, deductive way of making decisions. To see why  more clearly, consider the diagram before, except where we represent
	
	\begin{center}
	\end{center}
	
%	This is somehow the most natural, and computationally easy way to compute 
	
	\subsubsection{Preference Learning}
	
	There is also 
	
	
%	One way of representing this is as a diffusion of computation across the network
	
%	Intuitively, a 
	
		
	% axioms are cycles
	% topology of the graph 
	
	
	\section{Examples}
	
	
	\subsection{Framing Problems}
	Since inconsistency is driving preference changes in this formulation, we will start with the case where clearly this is what's going on. 
	%
	Suppose you prefer $a_1$ over $a_2$ (for variable $A$) and $b_2$ over $b_1$ (for variable $B$). Later, you come to believe that a choice of $a_1$ is really logically equivalent to $b_1$, and $a_2$ to $b_2$; in fact, $A$ and $B$ were the same variable. This happens all the time for humans, if $A$ and $B$ were given different descriptions. For example,
	
	\begin{quotation}
		\it\small
		You believe that the rich should not get a larger federal tax exemption (than the poor do) for having children.
		At the same time, you believe that if the default number of children were 2, and people paid a surcharge to the government for foregoing them, that the rich should pay a larger surcharge (than the poor do) for this. 
		However, the default number of children is not a feature of the world, and the same amount of money changes hands in both cases; the two preferences are logically in conflict.
		
%		You later come to realize that these preferences are logically in conflict; you then update your beliefs about both of them, causing yourself to be less sure, until the more entrenched one wins, and your viewpoint on the other issue has swapped.
	\end{quotation}
	
	This creates a conflict--- it is impossible to have all four of
	\[ \Big\{\text{consistency},\qquad a_1 \geqc a_2,\qquad b_1 \leqc b_2,\qquad\text{the belief that}~(a_1 \equiv b_1) \land( a_2 \equiv b_2)\Big\}\]
	
	Before we go any further, notice that there's not even an obvious, natural representation of this conundrum in terms of classical decision theory: if utilities are over outcomes, and the two descriptions pick out the same set of outcomes (without changing their relative probabilities), then you will never be in a situation like this. Let's try to salvage this from the classical point of view in a few ways.
	%Let's try a few. 
	
	\subsubsection{}
	One possible patch is to have utilities over \textit{descriptions} of worlds, dependent on more than just the worlds they pick out. Now, this ``conflict'' from before is not problematic: you prefer $a_1$ to $a_2$, $b_2$ over $b_1$. 
	
	We can use this to represent the state of the world, but now:
	\begin{itemize}%[nosep]
		\item Computation of expected utility now requires priors over descriptions of worlds, an object which is even more astronomically complex than a prior over worlds. %In addition to being large, these probabilities seems problematic in other ways: a change in the distribution of language you use might now change your beliefs 
		
		\item Crucially, this account does not denounce, let alone provide any mechanism for resolving the disagreement, and therefore we sacrifice all of the standard rationality guarantees, such as resistance to dutch booking%
			\footnote{If you have utilities $u_1, u_2, v_1, v_2$ for $a_1, a_2, b_1, b_2$, respectively, then you would be willing to pay $u_1 + v_2$ for a bet of $a_1$ or $b_2$ (which has probability 1), and $u_2 + v_1$ for a bet of $a_2$ or $b_1$, which again is probability 1. Since the difference in utility differs between the $a$ and $b$ descriptions, $u_1 - u_2 \neq v_1 - v_2$, and so  $u_1 + v_2 \neq u_2 + v_1$, and a bookie can make unbounded money off of you by selling you one bet and buying the other.}.
		Decisions consistent with this picture may have nothing to do with the world may be entirely based on the number of parentheses in the formula.
%		\item The premises, that we prefer $a_1$ to $a_2$ and $b_2$ to $b_1$, are 
	\end{itemize}

	Clearly, this is a flimsy model as it stands, but it's not clear how to fix it. If we add independence of description as an axiom, we're back to our original representation problem. 
	
	\subsubsection{}
	Another tactic we compatible with the classical decision theory perspective\footnote{the model described here actually corresponds to an influence diagram with two decision and two value nodes}, this time much closer to our own formulation, is to define utility by additively separable components, each arising from a variable (we have once again implicitly given ourselves utilities over descriptions of the world), relying on belief revision to effectively make some worlds impossible.
	
	The set of possible worlds which we have utilities over is once again all possible assignments to variables, and hence includes ``impossible possible worlds'', such as the one where $a_1$ and $b_2$ are simultaneously true. By belief updating, the likelihood of these worlds can be driven to zero, and when we say ``$a_1 \geqc a_2$'', we really mean that the variable $A$ has an additively separable component of our total utility function (resp. for $B$), and it is merely an unfortunate fact of life that good things are attached to bad ones. If the two preferences carry the same weight, then as the possibility of achieving impossible worlds vanish, the two given preferences annihilate one another and we are left indifferent, as far as the total utility function is concerned.
	
	This seems in many ways better, because the tension is resolved, it can happen continuously, the agent is only vulnerable to dutch booking as long as its beliefs are inaccurate, and we get a temporal reason for why the agent's choices change. In other ways, it is less satisfying:
	\begin{itemize}%[nosep]
		\item In order to define a classical agent, a modeler must specify a utility function, and in this case means that each additively separable component must have been there in the beginning. In particular, this means the modeler decides on the set of variables\footnote{If instead the modeler only described a way of generating these utilities, so that the set of variables could change, then there must be some additional structure to make these decisions, making the picture immediately very non-classical --- classical utilities do not depend on an agent's beliefs or other mental features}, and so agents can only be in this situation insofar as the modeler has given them representational redundancy: it is impossible for an agent to experience conflict in this way if the variables in fact determine unique states of the world.
		
		\item There can never be a time when the agent holds both the contradictory preferences, and also the belief that they are contradictory, even if belief updates happen slowly.		
		
		\item You can never truly change your opinion on either issue: you will always want $a_1$ over $a_2$ and $b_2$ over $b_1$ --- even if you now see how they are exactly the same decision.
		%By contrast, humans seem to take both into account and then decide on a joint picture of how the decision should be made.
		
		\item Because we define a fixed utility function, the two conflicting components on the same concept could not have been the result of some dynamics mechanism; they were in fact put there (explicitly) by the modeler. 
	\end{itemize}
	
	More briefly, our complaint is that the components of the utility function didn't actually change, and that the set of variables is necessarily fixed.
	
	\subsubsection{In terms of preference networks}
	
	We propose a slight modification of the previous model, where preferences actually flow along the beliefs that link them: In our case, this means we consider what it would look like to make a decision about variable $B$ looking only at what we care about in $A$. At this point, we should be a bit more concrete. 

	To avoid numbers and keep this as simple as possible, let's further 
	
	Suppose our preference on ($a_1 \geqc a_2$) has an image of 
	\begin{center}
	\begin{tikzpicture}
		\node[dpadded] (A) {$A$};
		\node[dpadded, right=3 of A] (B) {$B$};
		\draw[->, thick] (A.10) -- (B.170);
		\draw[->, thick] (B.190) -- (A.350);
	\end{tikzpicture}
	\end{center}
	
	
	
	% rather than being aggregated at the end, we conceptualize utility as through beliefs. For instance \todo{chipfiring}
	
	
	\subsection{Learning from Experience}
	
	We can use the same technique of reducing conflict to also 
	
%	\subsection{
	\section{Ties To Existing Models}
	
	\subsection{Emulating Other Structures}\label{sec:emulation}
	
	\subsection{Important Differences}
	
	\subsubsection{Departure from BNs and CP Nets}
	
	Although this representation looks like a Bayseian Network, and in fact any BN can be encoded in this way, the two formulations differ at face value in several important ways:
	\begin{enumerate}
		\item We encode preference information in each domains, 
		
		
		\item The existence of two arrows into $\sf C$ in the diagram above does not mean that there is a distribution $\Pr({\sf C \mid D, E})$, but rather two different ones $\Pr\sf( C \mid E)$ and $\Pr\sf(C \mid D)$. It is possible to recover this meaning if we are allowed to introduce new product nodes:
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (D) at (0,0) {$\sf D$};
			\node[dpadded] (E) at (0,-2) {$\sf E$};
			\node[dpadded] (DE) at (2.5,-1) {$\sf D \times E$};
			\node[dpadded] (C) at (5,-1) {$\sf C$};
			
			\draw[arr] (DE) -- (D);
			\draw[arr] (DE) -- (E);
			\draw[arr] (DE) -- (C);
			\end{tikzpicture}
		\end{center}
		
		\item Similarly, a node without any parents has a probability distribution on it in a BN; this is not the case for us (think of this as the zero-arity product, making this a special case of the above). Once again, this can be represented by explicitly adding an edge, from the singleton preference domain:
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {$\sf 1$};
			\node[dpadded] (D) at (4,0) {$\sf D$};
			
			
			\draw[arr] (1) to node[above]{$\Pr(D)$} (D);
			\end{tikzpicture}
		\end{center}
		
	\end{enumerate}
	As a result, a model like this does not always represent a factorization of the joint distribution on the product of the variables. In some sense, it does represent a factorization of joint preferences on the variables, but not in a way analogous to the BN approach (this is what CP Nets do).
	
	While it is true that I have implicitly defined a utility function, 
	
	
	\subsubsection{Static description in terms of Influence Diagrams}
	Our rounded nodes function somewhat like a hybrid of the three nodes used in influence diagrams: aleatory variables (circles), decisions (squares), and values (octagons). \todo{}
	
	\begin{center}
		\begin{tikzpicture}[baseline=(current bounding box.center)]
		\node[dpadded] (D) at (0,0) {$\sf D$};
		\node[dpadded] (C) at (2,0) {$\sf C$};
		\node[dpadded] (E) at (1,-1.6) {$\sf E$};
		\draw[arr] (D) -- (C); \draw[arr] (E) -- (C); \draw[arr] (E) -- (D);
		\end{tikzpicture}
		$\quad\stackrel{?}{\cong}\quad$
		\begin{tikzpicture}[lesspad/.style={inner sep=0.6em,outer sep=0.2em},baseline=(current bounding box.center)]
		\node[dpadded, square, lesspad] (dD) at (0,3) {$\sf D$};
		\node[dpadded, circle, lesspad] (aD) at (-0.8,1.5) {$\sf D$};
		\node[dpadded, octagon, lesspad] (vD) at (-1.6,0) {$\sf D$};
		\draw[arr] (dD) -- (aD); \draw[arr] (aD) -- (vD);
		
		\node[dpadded, square, lesspad] (dC) at (2,3.0) {$\sf C$};
		\node[dpadded, circle, lesspad] (aC) at (2.8,1.5) {$\sf C$};
		\node[dpadded, octagon, lesspad] (vC) at (3.6,0) {$\sf C$};
		\draw[arr] (dC) -- (aC); \draw[arr] (aC) -- (vC);
		
		\node[dpadded, square, lesspad] (dE) at (-0.5,-1) {$\sf E$};
		\node[dpadded, circle, lesspad] (aE) at (1,-1) {$\sf E$};
		\node[dpadded, octagon, lesspad] (vE) at (2.5,-1) {$\sf E$};
		\draw[arr] (dE) -- (aE); \draw[arr] (aE) -- (vE);
		
		\draw[arr] (dD) -- (aC);
		\draw[arr] (dE) -- (aD);
		\draw[arr] (dE) -- (aC);
		
		%		\draw[arr] (DE) -- (D);
		%		\draw[arr] (DE) -- (E);
		%		\draw[arr] (DE) -- (C);
		\end{tikzpicture}
	\end{center}
	
	
\end{document}