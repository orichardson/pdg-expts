\documentclass{article}

\input{prefs-commands.tex}
\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{A Simpler Explanation}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}

\begin{document}
	This document is intended to be clear, simple, and at a high level. It exists as a safe haven away from most of the math and generality I want to later think about, aside from some footnotes and a section where I list some connections I suspect exist. I will prove nothing here, and just make some arguments and provide examples.
	
	\section{A Very Brief Description}
	This is an attempt at a model of joint preference and belief revision. It allows for representational flexibility, so that agents need not always have a perfectly consistent picture of reality, and is intended to capture phenomena such as learning preferences from data and revising them when they're inconsistent, while also disallowing value changes which would be objectionable from an agent's current perspective.
	
	This is done by representing preferences in a distributed way across a network, whose nodes are preferences on small sets of alternatives, and whose edges are beliefs about the impact of one choice on another.
	
	

	\section{Why This Is Worth Doing}
	\vspace{-1em}
	\textit{A few short motivations from several perspectives}
	
	\subsection*{[Ethos of Agency] The separation between values and tools isn't clean}
	The distinction between the things that you care about (e.g., utilities, goals, preferences, objectives), and the tools you have to make these things come about (e.g., beliefs, reasons, plans, optimization, inference) plays a large role in technical accounts of agency%
	\footnote{Beliefs should be considered optimization power: they you believe that action $A$ will have effect 1, an action $B$ will have effect 2, and a preference between the 1 and 2}. Generally the two are considered separately, and also people tend to fix preferences%
	\footnote{In the context of optimization, this corresponds to the practice of writing down an objective and finding extrema; in the context of decision theory, it is finding the best actions to satisfy some preferences}. This is nice because the preferences provide a metric of quality on the other front; these criteria for success are a very important part of science.
	
	As useful as this is, preferences and beliefs often depend on one another, and in more complex, human settings, the line is much more blurred:
%	Subjectively, though it is hard to imagine having one without the other, and it seems like there might be G\"odelian knots if you take the separation seriously enough --- 
	\begin{itemize}[nosep]
		\item When you play a video game, are you trying just so you can win? If so, why do you care about winning? It buys you very little socially, and costs time and money. Or does the process of optimizing for this arbitrary goal itself have value?
%		\item In the process of achieving enlightenment, you go about removing your desires. But it seems like being without desires is in fact a desire--- why where you attempting to do this, anyway? 
%		\item Similarly,  care about is truth
		\item It is common to have beliefs about preferences: (`I think I like cheese', 'I believe freedom is bad'), and also preferences over beliefs (`Believing true things is good', `I want to know calculus')
		\item These can be nested quite deeply without anyone thinking too hard: If I ask if you like cheddar, you now believe that I want to know whether you like cheese --- note that this is a belief about a desire for a piece of knowledge about a preference, spanning multiple people.

		\item On a small scale, we think of a loss function as an optimization objective, and the algorithm (say, stochastic gradient descent) as optimization power. One can use both of these together to form a neural network, which is thought of as being just a tool, with no

		\item Complete agents can be used in either way: people, who have desires and experience emotions, can be used as optimization power with an external objective (e.g., hiring employees), but can also as the ultimate source of value (e.g., running an organization because it will help people)
	\end{itemize}
	The point is that even we could combine any model of generic optimization, with any model of value, the interactions between them would certainly be woven together, and their dynamics would be intertwined.
	
	%%
	{\color{green!30!black}This is why we model the dynamics of beliefs and preferences together, and why in some cases we will be able to represent a belief as a preference or vice versa. \footnote{the categorical interpretation, where each node is a category, the big graph forms a 2-category, and beliefs are value-preserving functors, supports the observation that the distinction between preferences and beliefs isn't a fundamental feature the objects themselves but rather how they're used--- here the arrow category here gives us preferences over beliefs. Similarly, quotient objects represented as arrows, just as utilities can be identified with beliefs about utilities}}
	
	\subsection*{[Modeling Accuracy] Humans change their preferences}
	When you're born you have no conception of what foods or professions or ideals are good; all you have is your own evolutionarily programmed pleasure and pain. Not only does this feeling get more sophisticated in a way that seems to depend on the environment, but also later on in life, people willingly sacrifice their experiential pleasure for other things they care about. All of this stands even if we didn't see change in the more obvious toy cases: foods, activities, and luxury goods. Such changes do in fact occur, and do not detract from a person's ability to be conceptualized as a coherent agent; often they contribute to a person's character instead.
	
	The standard models do not account for any of this, and for many good reasons --- but dynamic preferences are necessary to capture a great deal of human behavior, and are much more important in a world where value is informational and changes quickly. Things like memes, fashion, games, conversations, lofty ideals, and art---things that standard economic theory has had a lot of difficulty ascribing value to---certainly play a bigger economic role than they have in the past, and arguably move faster as well.
	%is learning a new piece of information something that has value?
	
	%there is some overlap between conditioned preferences and dynamic preferences.
	{\color{green!30!black}This is why we model changes in preferences, and acquisition from only a very limited set of base values. We have some reasons to suspect that in most cases preferences generated this way will be roughly convergent, which if true would (1) provide an additional explanation (beyond empathy and genetics) for why humans end up sharing a lot of values, and (2) allay some concerns about misaligned AIs that are constructed in this way}
	
	\subsection*{[Computational Tractability] Computational Boundedness}

	The standard picture of decision theory requires keeping around preferences and beliefs about all possible things that are relevant to any decisions you make. To fully describe a general agent's values, then, you need to specify a preference ordering over all possible histories of settings of observable features. This is wildly intractable, and also annoyingly depends on what ``possible'' means, which is part of the agent's internal beliefs\footnote{Or alternatively, if you're going to do the work beforehand as a modeler, has to change when humanity discovers new features of the universe}. This is why people in practice restrict the scope of an agent to only a few modeler-chosen variables, assume that they only encounter one kind of decision, specify objectives in a compressed syntactic form.
	
	But even these more tractable restricted approximations we use do not degrade gracefully: in order to make any decisions at all you have to do expected utility calculations (which could be very expensive depending on how clear the impact of your decision on the world is), and there's certainly no clear way of making use of partial computations under time pressure.
	
	{\color{green!30!black} By keeping components of preferences around redundantly in many different forms, not only can we immediately re-use them for recent decisions we've made in a pinch, but also likely reduce the complexity of adding new nodes. This is because we can split the computation into meaningful chunks (one for each preference domain (node) we can connect a decision to), and computing impact on a few nodes of your choice is going to be much faster than computing the total impact on the space of all things that are observable}
	
	\subsection*{[Value Modesty] A second brittleness}
	Classical AI systems can do complex tasks that even modern ML systems struggle with. However, they have a debilitating flaw: any malformed input a designer has not anticipated causes the entire calculation to be wrong in salvageable ways. It's coded without an outside view of the explicit purpose: there's no possibility that the designer was \textit{wrong} in the algorithm specification, and there's no reasonable inference to make instead even we acknowledged it was possible she was.
	
	Incorporating probabilities and specifying objectives instead of algorithms has helped this problem enormously: by using a ton of redundant, noisy, examples of the algorithm working correctly, we can be reasonably resistant to malformed input. Unfortunately, these systems are not perfect either: they are incredibly biased and sometimes cheat. We face a different kind of brittleness: what if we slightly mis-specify an objective function because we forgot about a second order effect? Or what if we run it on a slightly different kind of input? Modern ML systems won't break as far as the algorithm is concerned, but they also will confidently display the wrong answers. The system doesn't think there's any possibility that its \textit{objective is wrong}, and even if it acknowledged this possibility, it's not clear there's anything to be done at this point.
	
	{\color{green!30!black}In analogy to the solution that statistical ML provided to the problem of brittle classical AI, the solution would seem to be a noisy, redundant encoding of the objective function in conjunction with a meta-objective: in our case, consistency. The diffusion of preferences (which we will also refer to as value capture) can also be thought of as a source of uncertainty about your values even if you didn't start with any: if there are multiple objectives consistent with everything you've seen, you acquire a preference for both of them.}

	Of course, humans are also often uncertain about their values (and experience value capture) in addition to their beliefs and the appropriate choice of actions.
%	Agents are allowed to have uncertainty in what they believe
	
%	nd so the synthetic agents we design, influenced by the standard picture and designed with particular use cases in mind, 


	
	\subsection*{[Theoretical Unity] Reductions to other theories and explicit links}
%	There are many different representations of preference theories 
	Although the aim in the rest of this document is to ignore generality and aim for descriptively simple, the generality is a genuine mathematical motivation for this theory: it can be viewed as a number of different things. I will briefly mention some of them that I'm excited about here, and then focus on simple things for the rest of the document.
	\begin{itemize}[nosep]
		\item The model reduces to standard expected utility calculations in the degenerate case where the agent starts with preferences over worlds, the representation of a world never changes, and the agent is cognitively unbounded
		\item It has a categorical interpretation which has some features I'd like to explore: in particular, meta-preferences seem to be related to higher order structure, a total utility function is like a limit, a total probability function is like a co-limit, preference-preserving beliefs are functorial, and the nodes already form a 2-category, whose objects are themselves enriched flat categories.
		\item There also a natural interpretation of this as an artificial neural network. If the underlying graph is a DAG with one sink, then it is a feed-forward network for calculating expected utility. In most interesting cases, it will have recurrent connections and no special output.
		\item If we throw out the preference information, it can be converted to a Baysian Network with a simple transformation
		\item If the nodes are all propositions with a preference for truth, and we add some explicit rules for creating and deleting nodes, it becomes an inductive theorem prover.
		\item The preference propagation is a bit like broadcasting on a network (of physical machines) to compute path lengths. This can be done Dijkstra / Distributed Bellman Ford \footnote{depending on whether we take right or left powers of matices} and can be computed by taking iterative powers of a giant matrix over the tropical semi-ring. Funnily enough, this is exactly how you compute the transitive closure of preference matrices on a small scale (except with a different semi-ring), which lends more credibility to the higher order categorical interpretation.
	\end{itemize}

	The possibility of bridging these many disparate fields makes the abstraction feel much more universal, and the ability to think about preference change in any of these terms could make it easy to identify analogs of non-trivial facts in other fields.
	
	% AI safety
	
%	There are many ways of framing this.
%		real preferences change. 
%		Dual brittleness: certainty in preferences
%		Computational boundedness
	

	

	
	\section{Simple Examples}
	
	
\end{document}