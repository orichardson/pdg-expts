\documentclass{book}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb, amsthm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\E}{\mathbb E}

\usepackage{parskip}

\begin{document}
	\chapter{Examples}
	
	\section{Wrong Variables.}
	Suppose worlds are parameterized by three variables $A, B, C$, each of which can take on two values: variable $A$ can take on either $a$ or $\bar a$, $B$ can take on $b$ or $\bar b$, and $C$ is either $c$ or $\bar c$.
	
	Suppose further that the original agent cannot observe these variables directly, but rather observes variables $X$ and $Y$, which are logically defined as 
	\begin{align*}
		X &:= A \land B \\
		Y &:= B \lor C
	\end{align*}
	
	If I originally have a preference order
	\[  \]
	
	\section{New Constraint}
	
	
	
%	You, however, not being able to observe the world, have preferences for $C := A \otimes B$, for $D := B \otimes C$, and also for $E := C \otimes A$, amounting to a choice of exactly one out of every pair of real variable. All three conjunctions are, of course, unsatisfiable.
	

	\chapter{Unification}
%	If $S$ is a set of states, and $O$ is the set of outcomes, 
	
%	let $\mathcal L$
	
	% By Savage, we know that we don't need to assume the existence of a probability distribution (over states or outcomes); one can derrive this from the axioms
	
	\section{The Emotion / Reason Dichotomy}
	Morals, preferences, goals, utilities, rewards/punishments, and desires all have something in common: the behavior that characterizes them is an optimization. They answer a question about \emph{why} something was done, in a way that is compatible with planning and rational, well-thought-out behavior. If one sees a person repeatedly doing something, say walking their dog, it is reasonable to conclude that either they hold that this is a morally good thing to do, have a preference or goal / sub-goal of walking their dog, enjoy it, or have times when they want to do it. Moreover, these are considered explanations of \emph{why} behavior happens. They are descriptions of the things that agents optimize.
	
	The second feature they share is a subjectivity: anyone can have any preferences or utilities or rewards (perhaps subject to certain constraints if you don't want to be manipulated, want to behave robustly in the presence of adversaries, and so on). Once the space has been constrained, having different preferences is merely... a matter of preference. The theories we use for modeling agents do not take into account the mechanisms by which one might obtain preferences, which 
	
	More egregiously still, standard utility / reward maximization picture has nothing to say about the possibility of these quantities changing over time.
	
	
	
	All of this is to be contrasted with two other concepts:

	\begin{enumerate}
		\item Empirical analysis which determines that some behavior (as opposed to another one) is occurring. 
		\item Theories of belief, rationality, and \emph{how} to optimize.
	\end{enumerate}
	
	
	
	All of these have been postulated as reasons why people do things, and more generally, as theories about ways to shape behavior of agents. 
	
	
	
	The general idea is to consider two descriptions of motivation equivalent if they necessarily result in indistinguishable behavior. 
	
	% Related: opinions --- beliefs + judgement
	% 	Emotions --- attitudes towards things [mroe general]
	
	\subsection{Utilities and Preferences}
	
	To simplify things, economists and decision theorists start with the case when the set of possibilities is small and finite. If $O$ is the set of things one could choose between, i.e., the set of outcomes, then we can formalize a preference as a pre-order $(O, \preccurlyeq)$ on $O$.
	
	\begin{align*}
		\forall x \in O.&~x \preccurlyeq x \tag{Reflexivity}\\
		\forall x,y,z \in O.&~(x\preccurlyeq y)~\land~( y \preccurlyeq z) \Rightarrow~(x \preccurlyeq z) \tag{Transitivity}
	\end{align*}
	
	Rather than dealing with the partial order directly, we might like to have an embedding into something we have more intuition for, such as natural numbers or a continuous space. The problem with this, of course, is that the space may have some structure which is incompatible with the partial order. 
	
	of these results in a ranking, 
	
	However, there is often a lot more structure on outcomes
	
	
	

	
	\subsection{Utilities and Rewards}
	
	Both utilities and rewards are real-valued functions from something in the world to a one-dimensional notion of `good-ness' represented by $\mathbb R$, and hence are sometimes thought of as equivalent; here we will do some of the work to explore in what sense they might be equivalent, and the sorts of issues that might come up if conflating the two without any thought.
	
	Utilities are over outcomes, so a utility function $U : Z \to \mathbb R$ must be
	
	\[ \pi^*(x) = \argmax_{y: Y} \E_{z : Z} \Big[ U(z)~\Big|~ y,x\Big] \]
	
	\subsubsection{Determinism}
	If $\cal W$ is the set of all possible world states, and $\tau: \cal W \to W$ is a deterministic function describing the evolution of the world, then having a preference over futures starting at $w_0$ and consistent with $\tau$ is meaningless, as there is only one such sequence of worlds. Therefore, pure determinism only makes sense with agents with imperfect information. 
	
	Let $X$ be the space of world representations for an agent, and let $\eta: \mathcal W \to X$ be some function, thought of as perception, which generates a world representation. This gives us an equivalence class $[w] := \eta^{-1}(\eta(w))$ of information sets of the agent, which may not be stable under $\tau$, and so it may be the case that $w \underset\eta\sim w'$ but not $\tau(w) \underset\eta\sim \tau(w')$
	
	
	
	The fact that one gets more information over time suggests that the optimal policy, even with infinite computation, in general will change with additional samples. After two steps, the policy becomes
	
	\begin{align*}
		\pi^*(\eta\circ\tau (x)) &= \argmax_{y: Y} \E_{z : Z} \Big[ U(z)~\Big|~ y,\eta\tau x \Big]  \\
			&= \argmax_{y: Y} \int\limits_{\mathrm dZ} \Pr\Big[z~\Big|~\left(Y^{(t)} = y \right) \land \left(X^{(t)}= \eta \circ \tau (x)\right) \land \left(X^{(t-1)} = x \right) \Big] U(z)
	\end{align*}
		
	
	\subsubsection{Nondeterminism}
	Once again, suppose $\cal W$ is the set of possible worlds, with now $\tau: \mathcal W \to 2^{\mathcal W}$, a non-deterministic version of the transition function in the deterministic setting from before.
	
	In this more general case, we can still relate the two. To begin, suppose that we have a complete preferences over possible futures. 
	
	
	\section{Fields in which a related problem has been addressed}
	
%	\subsection{Selective Pressure}
	\subsection{Art History} % Model literally changes in peoples' aesthetic preferences
	\subsection{Pedagogy} % Representation Changes.
	\subsection{}
	
	
	\section{Applications}
	
	\subsection{Content Recommendation}
	The way that these assumptions of static preferences manifest themselves in content recommendation systems is the
	
	\subsection{AI Safety}
	\subsection{Better Inverse Reinforcement Learning}
	\subsection{Robotics: Life-long Learning}
	\subsection{Meta Ethics}
	
	\section{As a Learning Problem}
	
	
\end{document}