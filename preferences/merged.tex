\documentclass{article}

\input{prefs-commands.tex}

\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{Merged: Dynamic Beliefs and Preferences}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}


\begin{document}
	\maketitle
	
%	\section{}

	\section*{A Quick Overview}
	This project is a model of joint preference and belief revision. The core representation we suggest, the marginal constraint graph, is more flexible than the standard ones, as agents need not always have a perfectly consistent picture of reality, intermediate representations have meanings, and we can capture phenomena such as learning preferences from data and revising them in various places. In special cases, we recover standard results, including expected utility calculations, belief updating by conditioning.
	
	We can roughly separate the work into two pieces:
	\begin{enumerate}%[nosep]
		\item Representation: the sufficiency of our new representation as a unification of existing results. In particular, we think of an agent's values as distributed across a network of concepts, whose nodes are preferences on small sets of alternatives, and whose edges are beliefs about the impact of one variable on another.
		\item Dynamics: A formulation of how to revise the representation in light of new information or additional computation. We will show how can be dealt with through inconsistency, illustrate some options for regularization, and ultimately factor out these as general meta-preferences. 
	\end{enumerate}
	
	\part{Motivations}
%	This work an be motivated in many different ways; I imagine the most compelling one to depend on who you are and what you already care about. In any individual paper I plan to only include one of these, but until the audience has been narrowed, I want to keep all of them around. %to remind myself which directions I want to push, and to record selling points outside of the best publishing path.

	\begin{center}
	\begin{tikzpicture}
		\node[ellipse,draw,align=center] (R) at (4,-3) {World\\ Representation};
		\node[ellipse,draw] (B) at (3,1) {Beliefs};
		\node[ellipse,draw] (V) at (3,-1) {Values};
		\node[ellipse,draw] (A) at (5.5,0) {Actions};
		
		\draw[arr,-] (B) -- (4,0) -- (V);
		\draw[arr, shorten <=0] (4,0) -- (A);
 	\end{tikzpicture}
	\end{center}

	\section{Good Reasons Not To Maximize Utility}
	\begin{enumerate}
		\item \textbf{No utility function}. Maybe your preferences are not representable in this way --- maybe they violate the vNM axioms, or you just don't have anything that resembles an ordered set. Maybe your intrinsic utility is stochastic.
		
		\item \textbf{No complete prior.} It is possible that you're not a Bayesian, and don't have (access to) a probability distribution on all salient features of your world\footnote{In fact, once we add enough representational complexity to our world, this is impossible to have; see section \ref{sec:impossible-prior}}.  
%		Of course, if you accept Savage's axioms, then you act as though you have both a probability distribution over states and a utility function
		
		\item \textbf{Thinking takes time.} (this constrains the complexity of your utility and probability information, or at least requires them to be in a form that admits fast computations)
		
		\item \textbf{Internal Actions.} In addition to the external actions, there are also invisible mental ones: nobody has complete control over what they do in all respects at all times. Emotionally preparing yourself to talk to your boss, warming up your fingers to play piano, and intentionally committing a phone number to memory are all examples of mental actions, which are rarely modeled. More generally, for any interface with the world that a model prescribes, there will be extra 
		
		 %you may not know exactly what it is you have to do in orde to 
		\item \textbf{Different world representation.} 
%			You have a distorted perception of the world. 
		Models, both in probabalistic reasoning and decision theory, usually start by defining the set of possible states of the system. For a modal logician, this is the underlying set of a Kripke structure; for probability theorists, it's a set of outcomes. Everything works well if you set up a model in which the agents are aware of the structure you've set up, but simply don't know what world they're in --- but nothing precludes them from conceptualizing things in entirely different terms
			
		There are of course ways of putting together
		
		
		\item \textbf{Things Change over Time.} To 
	\end{enumerate}	

	Despite the fact that there has been work backing off of each of these assumptions, utility maximization is still ubiquitous (and taken for granted) in most primary applications, including consumption models and the construction of artificial agents. In section \ref{sec:optimization-bad} I argue that this is the source of some cultural problems. But part of the reason that many of the other techniques has not gained traction is that none is as clean and as useful as expected utility maximization, and to the best of my knowledge people try to isolate and tackle these issues individually. My framework provides a way of representing many of these issues effectively and cleanly, while also reducing to the expected utility computations when none of these issues apply. 


	\section{Desiderata}
	
	
%	Currently preferences are thought of as static objects, fixed as part of the structure and identity of an agent, independent of beliefs, complete, and over some fixed domain. This is clearly not at all how human preferences work, and I posit that it's not the right way to think of preference for synthetic agents either.
	
	Perhaps among other things, any model of preference dynamics that is to be used to build synthetic agents which have control over important decisions, should have the following properties: 
	\begin{enumerate}[noitemsep]
		\item Provide an answer to the `value loading' problem: show how you can learn ``reasonable'' preferences by interacting with the world
		\item Reduce to static models for some parameter settings
		\item Behave reasonably when combined with changes of perspective 
		\item Be resistant to standard challenges to irrationality, such as dutch book arguments
		\item Have weak safety guarantees: an agent should not eagerly adopt preferences which are totally in conflict with its current ones
	\end{enumerate}
	
	%	We might also require that a theory be useful for explaining human behavior. We expect ours to be able to explain:
	Because the view of preferences we adopt here is different from the standard ones in economics (in particular, it lends itself naturally to incorporating boundedness), we have a hope of explaining some behavior which was previously classified irrational, as an optimal in some sense. The following psychological effects lend themselves to explanation:
	\begin{enumerate}[noitemsep]
		\item \textbf{Value Capture.} You really care about $X$ (say, learning math), which is vague and hard to measure, so you come up with some metric $Y$ (say, scores on undergrad math exams). Over time, optimizing for $Y$ will cause you to optimize less effectively for $X$ and assign intrinsic value to $Y$ (getting good exam scores becomes valuable, independent of whether you learn math\footnote{Social signaling plays into this, but this occurs also in cases where people try to hide the signal: constant grade checking, pokemon go addiction, etc.}). Related to Goodhart and Cambell's laws.
		\item \textbf{Framing Effects.} It is well-documented that the style of presentation, even for logically equivalent scenarios, can have a significant impact on a person's choice. But if we formalize these as the same outcome, there's no way for utility-maximizing agent to behave this way.
		\item \textbf{Connoisseur Effects.} Someone who listens to a lot of rap music has more nuanced, complicated preferences on rap music than someone who has not heard as much. Similarly, people work to develop palates for wine, and ``all Indian food tastes the same'' is an insult, indicating a shallow experience with the cuisine. There are two technical aspects: first, additional experiences increase preference complexity.
		
		\item \textbf{Adaptive Preferences.} Even things we find objectionable are normalized over time, and people often change to prefer things they're used to, even if they are initially opposed. This is sometimes thought of as a prioritization of safety, and is maybe best thought of as a thought-saving feature: the things you're used to have gotten you this far already. 
		
%		\item \textbf{}
		
		\item \textbf{Novelty Effects (anti-adaptive preferences).} 
		
	\end{enumerate}

	\section{Problems with the Standard Representation}
	\subsection{Fixed World Representation}
	
	The set of things which are even considered possible is contentious enough that we don't want to build it (and actually the correct one) canonically into an agent's picture of the world. In some sense only the current world is possible, as far as anyone can be sure. Moreover, no matter how you chose to represent the world, an agent's picture of what's possible can change over time, which means the modeler needs to write down beforehand everything that could possibly happen. We talk more about this in section \ref{sec:cat-worlds}
	
	\subsubsection{World Explosion} \label{sec:world-explosion}
	
	\section{The Big Picture: Why This is Important}
	\begin{enumerate}
		\item Subjective representations of the world
		\item Joint dynamics
		\item 
	\end{enumerate}

	\subsection{A Misguided Ethos of Agency}
	The distinction between the things that you care about (e.g., utilities, goals, preferences, objectives), and the tools you have for understanding and affecting the world (e.g., beliefs, reasons, plans, optimization, inference) plays a large role in technical accounts of agency%
	\footnote{Beliefs should be considered optimization power: they you believe that action $A$ will have effect 1, an action $B$ will have effect 2, and a preference between the 1 and 2}.
	Generally the two are considered separately, and also people tend to fix preferences%
	\footnote{In the context of optimization, this corresponds to the practice of writing down an objective and finding extrema; in the context of decision theory, it is finding the best actions to satisfy some preferences}. 
	%
	This is nice because the fixed preferences provide a clean way to evaluate the quality of decisions, and criteria for success are a very important part of science. It is clear why modeling things in this way caught on so quickly and durably --- it is simple, prescribes tractable computations, is expressive enough to capture any behavior over fixed time periods, if you make modeling choices in the right ways. 
	
	This heavy dependence on modeling assumptions is an underappreciated shortcoming, and freedom to chose the representation gives modelers a lot of power to over-fit to their desired outcomes. 
	
	
	In real life, human preferences and beliefs often depend on one another, and in more complex settings the line is much more blurred:
	%	Subjectively, though it is hard to imagine having one without the other, and it seems like there might be G\"odelian knots if you take the separation seriously enough --- 
	\begin{itemize}
		\item When you play a video game, are you trying just so you can win? If so, why do you care about winning? Decision theorists often take this as primitive, but it can also be considered in a larger picture, which decision theorists also purport to model. It buys you very little socially, and costs time and money. Or does the process of optimizing for this arbitrary goal itself have value? Does this make entertaining yourself fundamentally irrational?
		%		\item In the process of achieving enlightenment, you go about removing your desires. But it seems like being without desires is in fact a desire--- why where you attempting to do this, anyway? 
		%		\item Similarly,  care about is truth
		\item It is common for humans to have beliefs about preferences: (`I think I like cheese', 'I believe freedom is bad'), and also preferences over beliefs (`Believing true things is good', `I want to know calculus') --- but the clear separation in standard decision theory makes this difficult to understand. 
		
		These can be nested quite deeply without anyone thinking too hard: If I ask if you like cheddar, you now believe that I want to know whether you like cheese --- note that this is a belief about a desire for a piece of knowledge about a preference, spanning multiple people.
		
		\item On a small scale, we think of a loss function as an optimization objective, and the algorithm (say, stochastic gradient descent) as optimization power. One can use both of these together to form a neural network, which is thought of as being just a tool, with no
		
		\item Complete agents can be used in either way: people, who have desires and experience emotions, can be used as optimization power with an external objective (e.g., hiring employees), but can also as the ultimate source of value (e.g., running an organization because it will help people)
	\end{itemize}
	This is an argument against the orthogonality hypothesis \cite{orthogonality}, and similar lines of thought. It seems clear that if we could combine any picture of truth, with any model of value, and any optimization procedure, which look remotely like those that humans have, the interactions between them would certainly be woven together, and their dynamics would be intertwined.
		
	%%
	{\color{green!30!black}This is why we model the dynamics of beliefs and preferences together, and why in some cases we will be able to represent a belief as a preference or vice versa%
		\footnote{the categorical interpretation, where each node is a category, the big graph forms a 2-category, and beliefs are value-preserving functors, supports the observation that the distinction between preferences and beliefs isn't a fundamental feature the objects themselves but rather how they're used--- here the arrow category here gives us preferences over beliefs. Similarly, quotient objects represented as arrows, just as utilities can be identified with beliefs about utilities.}.
	The way we've laid things out, the interaction between the two resembles a duality.
	}
	
	\subsection{Human Preferences Change}
	When you're born you have no conception of what foods or professions or ideals are good; all you have is your own evolutionarily programmed pleasure and pain. Not only does this feeling get more sophisticated in a way that seems to depend on the environment, but also later on in life, people willingly sacrifice their experiential pleasure for other things they care about. All of this stands even if we didn't see change in the more obvious toy cases: foods, activities, and luxury goods. Such changes do in fact occur, and do not detract from a person's ability to be conceptualized as a coherent agent; often they contribute to a person's character instead.
	
	The standard models do not account for any of this, and for many good reasons --- but dynamic preferences are necessary to capture a great deal of human behavior, and are much more important in a world where value is informational and changes quickly. Things like memes, fashion, games, conversations, lofty ideals, and art---things that standard economic theory has had a lot of difficulty ascribing value to---certainly play a bigger economic role than they have in the past, and arguably move faster as well.
	%is learning a new piece of information something that has value?
	
	%there is some overlap between conditioned preferences and dynamic preferences.
	{\color{green!30!black}This is why we model changes in preferences, and acquisition from only a very limited set of base values. We have some reasons to suspect that in most cases preferences generated this way will be similar, which if true would (1) provide an additional explanation (beyond empathy and genetics) for why humans end up sharing a lot of values, and (2) allay some concerns about misaligned AIs that are constructed in this way}
	
	\subsection{Computational Tractability}
	
	The standard picture of decision theory requires keeping around preferences and beliefs about all possible things that are relevant to any decisions you make. \todo{Is there an edit here to be made about small vs large worlds?} To fully describe a general agent's values, then, you need to specify a preference ordering over all possible histories of settings of observable features. This is wildly intractable, and also annoyingly depends on what ``possible'' means, which is part of the agent's internal beliefs\footnote{Or alternatively, if you're going to do the work beforehand as a modeler, has to change when humanity discovers new features of the universe}. This is why people in practice restrict the scope of an agent to only a few modeler-chosen variables, assume that they only encounter one kind of decision, specify objectives in a compressed syntactic form.
	
	But even these more tractable restricted approximations we use do not degrade gracefully: in order to make any decisions at all you have to do expected utility calculations (which could be very expensive depending on how clear the impact of your decision on the world is), and there's certainly no clear way of making use of partial computations under time pressure.
	
	{\color{green!30!black} By keeping parts of preferences around in many different forms, attached to different contexts, not only can we immediately re-use them for recent decisions we've made in a pinch, but also reduce the complexity of adding new nodes. This is because we can split the computation into meaningful chunks (one for each preference domain (node) we can connect a decision to).}
	
	\todo{mention related work on tractability: BN's, Markov Networks, Markov Networks}
	
	\subsection{Value Modesty}
	
	
	
	%% bad, unclear paragraph
	In the case that their designers had in mind, hand-crafted classical AI systems can perform tasks that are very difficult to impossible to do with ML, such as \todo{}. However, they have a debilitating flaw: any malformed input a designer has not anticipated causes the entire calculation to be wrong in salvageable ways. It's coded without an outside view of the explicit purpose: there's no possibility that the designer was \textit{wrong} in the algorithm specification, and there's no reasonable inference to make instead even we acknowledged it was possible she was.
	
	Incorporating probabilities and specifying objectives instead of algorithms has helped this problem enormously: by using a ton of redundant, noisy, examples of the algorithm working correctly, we can be reasonably resistant to malformed input. Unfortunately, these systems are not perfect either: they are often biased and sometimes cheat. We face a different kind of brittleness here: what if we forget about a second order effect and slightly mis-specify an objective function? What if we run it on a different input distribution? ML systems won't crash, but they will confidently display  wrong answers. The system doesn't think there's any possibility that its \textit{objective is wrong}, and even if it acknowledged this possibility, it's not clear there's anything to be done at this point.
	
	{\color{green!30!black}In analogy to the solution that statistical ML provided to the problem of brittle classical AI, the solution would seem to be a noisy, redundant encoding of the objective function in conjunction with a meta-objective: in our case, consistency. The diffusion of preferences (which we will also refer to as value capture) can also be thought of as a source of uncertainty about your values even if you didn't start with any: if there are multiple objectives consistent with everything you've seen, you acquire a preference for both of them.}
	
	Of course, humans are also often uncertain about their values (and experience value capture) in addition to their beliefs and the appropriate choice of actions.
	%	Agents are allowed to have uncertainty in what they believe
	
	%	nd so the synthetic agents we design, influenced by the standard picture and designed with particular use cases in mind, 
	
	
	
	\subsection*{[Theoretical Unity] Reductions to other theories and algorithms}
	%	There are many different representations of preference theories 
	The generality is a genuine mathematical motivation for this theory: it can be viewed as a number of different things. I will briefly mention some of them that I'm excited about here, and then focus on simple things for the rest of the document.
	\begin{itemize}[nosep]
		\item Our model reduces to standard expected utility calculations in the degenerate case where the agent has preferences over possible histories of worlds, the representation of a world never changes, and the agent is cognitively unbounded
		\item It has a categorical interpretation which has some features I'd like to explore: in particular, meta-preferences seem to be related to higher order structure, a total utility function is like a limit, a total probability function is like a co-limit, preference-preserving beliefs are functorial, and the nodes already form a 2-category, whose objects are themselves enriched flat categories.
		\item There also a natural interpretation of this as an artificial neural network. If the underlying graph is a DAG with one sink, then it is a feed-forward network for calculating expected utility. In most interesting cases, it will have recurrent connections and no special output.
		\item For agents that have priors, the beliefs encoded in this way can be converted to a Baysian Network with a simple transformation
		%		\item If the nodes are all propositions with a preference for truth, and we add some explicit rules for creating and deleting nodes, it becomes an inductive theorem prover.
		\item The preference propagation is a bit like broadcasting on a network (of physical machines) to compute path lengths. This can be done Dijkstra / Distributed Bellman Ford \footnote{depending on whether we take right or left powers of matices} and can be computed by taking iterative powers of a giant matrix over the tropical semi-ring. Funnily enough, this is exactly how you compute the transitive closure of preference matrices on a small scale (except with a different semi-ring), which lends more credibility to the higher order categorical interpretation.
	\end{itemize}
	
	The possibility of bridging these many disparate fields makes the abstraction feel much more universal, and the ability to think about preference change in any of these terms could make it easy to identify analogs of non-trivial facts in other fields.
	
%	These are features that we would like to explain in humans, but needn't necessarily be essential .

%	\section{A Temporal Extension}
%	
%	Many issues with decision theory can be attributed to a failure to account for the passage of time. 
%	One popular example of this is the ubiquitous assumption of cognitive unboundedness: in a static world, where everything is frozen except your own thoughts, you can afford to do decision theory exactly by computing expected utility. You have only one decision to make
%	
%	Still, cognitive boundedness is only one example. When we move to a dynamic setting, most classical results no longer apply, though people often like to pretend that they do. 
%	\begin{enumerate}
%		\item There's no reason to 
%	\end{enumerate}
%	
%	If your beliefs
	
	
%	\section{Better Models of Humans}


	\clearpage
	\part{Marginal Constraint Graphs}
	The basic tool that we will use to solve these problems is a graphical representation of (a piece of) an agent's mental space, which we call a marginal constraint graph. These are diagrams which look and behave like Bayesian Networks in many cases.
	\begin{enumerate}
		\item We allow agents to add additional nodes and edges (form new concepts and connections), dynamically
		\item We interpret arrows individually as marginals, rather than collectively as a conditional probability table depending on all variables above
	\end{enumerate}
		
	
	
	MCGs generalize , and as such are 
	
, 
	
	\section*{Definitions and Semantics}
	\begin{defn*}[MCG]
		A marginal constraint graph $(\cal A, L)$ is a collection of variables $\cal A$, attached to each of which is some preference information: (this could be an order, utility, pairwise utility matrix, a supremum function), plus a collection of probabilistic links $\cal L$ between some (but not necessarily all) pairs of them, where $L: A\to B \in \cal L$ is a (sub) Markov kernel $A \to \Delta [B \cup \{\bullet\}]$\footnote{The additional ``phantom element'' $\bullet$ absorbs probability density that we don't want to equivocate on, allowing our model to capture partial families of conditional probabilities, by extension things like implication, and giving agents more tools to avoid inconsistency. This has the effect of making our links substochatic matrices/kernels rather than stochastic ones. However, if we restrict to beliefs which assign zero probability to $\bullet$, everything in the model works as before. See section \ref{sec:substochastic} for details} representing beliefs about how a setting of $A$ to $a \neq \bullet$ will impact the value of $B$. 
	\end{defn*}
	
	Variables can be thought of equivalently as:
	\begin{itemize}[nosep]
		\item random variables $X$ which can take on values $\{x_i\}$ (or possibly none, which we denote $\bullet$)
		\item sets $X$ with elements $\{x_i\}$
		\item partitions of the universe of outcomes (which is not fixed) into disjoint (but possibly not exhaustive) events. Disjointness here is very weak and can be forced by adding tagging outcomes with additional data, analogous to a disjoint union.
	\end{itemize}
	
%	In will focus on models where the preference information is encoded as a special utility domain $U$, with links from other variables, but there 
	%While there may be some propositions tying this case to utilities or preference orders, we will avoid talking explicitly about the more general setting of preference matrices and choice functions out for now.
	
	
	
	We can now give a more formal definition:
	\begin{defn}\label{def:mcg}
		A marginal constraint graph (MCG) is a tuple 
		\[ \left(\mathcal N : \mathbf{FinSet},~~\mathcal L : 2^{\cal N \times N},~~ \langle\mathcal S, \Sigma\rangle : \mathcal N \to \mathbf{MeasSet}, ~~\mathbf p : \prod_{(A,B): \mathcal L} \left[ \Big. \mathcal S_A \times \Sigma_B \to [0,1]\right] \right) \]
		where $p(L)$ is a Markov kernel, i.e., for every $L[A,B] : \mathcal L$, and $a \in \mathcal S_A$, $\mathbf p_L[a \mid \cdot~]$ is a probability distribution on $(\mathcal S_B, \Sigma_B)$, and $\mathbf p_L[~\cdot \mid B]$ is $\Sigma_A$-measurable for every $B \in \Sigma_B$.
	\end{defn}
		

%	\part{Intuitions }

	\section{Emulation: Bayesian Networks}
	\label{sec:convert2bn}
	The semantics of a Baysian network ensure that there is no inconsistency: the arrows into a node taken together collectively determine a single well-defined probability distribution. Formally they consist of a set of nodes $\cal N$, and for each $N \in \cal N$, a set of parents $\mathrm{Par}(N)$, and a conditional probability distribution $\Pr( N \mid \mathrm{Par}(N))$, which is a distribution over the values of $N$ for each setting of every variable in $\mathrm{Par}(N)$. While each of our arrows can be interpreted by itself as a marginal, a collection of arrows into a single node must be taken together to have any meaning in a BN. 
	
	The procedure for converting to a BN is simple: we simply take every node's incoming arrows, and insert the product of its parents as a node before it. With this procedure, if a node $N$ has just one parent $P$, we replace $P \to N$ with $P \to N ~=~ N$, which is redundant so we don't draw this. If a node had zero parents (i.e., the BN just gives it a probability distribution not dependent on anything), we insert the product of zero things, i.e., the singleton node $1 = \{*\}$, a variable which only takes one value, and set $\Pr(N \mid *) = \Pr(N)$. 
	
	This sound much more complicated than it is. Consider the example below, where the left is a BN, the center is the corresponding MCG, and the right is a compact visual representation of the MCG in the case where $B \times C$ has no other edges attached to it directly.
	\begin{center}
		
		\begin{tikzcd}[center base, column sep=2.5em]
			& A \ar[dl]\ar[dr] \\
			B \ar[dr] && C \ar[dl]\\
			& D &
		\end{tikzcd}
		\hfil
		\begin{tikzcd}[center base, column sep=1em, row sep=1.5em, dpad]
			& \mathsf 1 \ar[d] &\\
			& A \ar[dl]\ar[dr ]% \ar[dd,dashed, gray] 
			\\
			B && C \\
			& B \times C \ar[ul, gray!70] \ar[ur, gray!70]\ar[d] & \\
			& D &
		\end{tikzcd}
	\hfil
	\begin{tikzpicture}[ center base]
		\matrix(m) [matrix of math nodes, column sep=2em, row sep=1.5em, commutative diagrams/every cell] {
			& \mathsf 1 &\\
			& A & \\
			B && C \\
			& {} & \\
			& D & \\};
		
		\draw[archain] (m-1-2) -- (m-2-2) -- (m-3-3)(m-2-2) -- (m-3-1) ;
		\draw[arr, -] (m-3-3) -- (m-4-2.center) -- (m-3-1);
		\draw[arr, shorten <=0] (m-4-2.center) -- (m-5-2);
	\end{tikzpicture}
	\end{center}
	\vspace{0.5em}
	
	We have effectively changed two things: first, visually encoded the probability distribution of $A$ as the arrow $1 \to A$ (which we are now allowed to omit; sometimes you don't want priors on things, such as your own actions). Second, we have combined the two arrows $B \to D$ and $C \to D$ into a single one, $B \times C \to D$. Though certainly more verbose, this is arguably visually clearer if want to follow arrows: you cannot compute $D$ from $B$; you need both $B$ and $C$.
	
	In order to fully get the joint representation given by the BN we would also need to make the final assumption that $B \CI C \mid A$. This is possible to do with an extra arrow, but this solution doe not scale well and clutters the diagram. Instead, we will leave the picture as it is, and tackle the independence in a weaker way.
	
	\begin{defn*}
		The \emph{center} of an MCG $(\cal A, L)$ is the set of joint distributions on $\cal A$ that come closest to satisfying $\cal L$, which are of maximum entropy.
	\end{defn*}
	
	This is an alternate way of capturing the conditional independence of variables that are not required by the model to be related, without ever conditioning on ancestors, which include products\footnote{to see why this is problematic, consider that the product of all variables can always be formed, and is the ancestor of all variables. If we condition on it, then everything is always conditionally independent, as we're looking at a degenerate distribution consisting of a single outcome.}.  In some sense this is the worst case outcome for an agent intending to narrow down possibilities: a distribution in the center requires the maximum amount of information to determine the state of the world, but at the same time cannot leverage this assumption to simplify things. %The principle of maximum entropy is well established\footnote{Despite this, utilities are still thought of as fixed, minimum entropy objects.} \todo{choose references}. W
	We also get:
	
	
	\begin{restatable}{theorem}{rthm:bn-maxent} \label{rthm:bn-maxent}
		The center of the MCG obtained by transforming a Bayesian Network as described above (i.e., by inserting an extra node $X := \prod_{P \in \mathrm{pa(A)}} P$ before every node, with the appropriate projections), is a singleton set consisting of exactly the probability distribution that the BN represents.
	\end{restatable}
	
	\begin{coro}
		Products (defined in section \ref{sec:belief-typing}) in the center of a model $(\cal A, L)$ have the usual uniqueness to make them categorical limits.
	\end{coro}
	
	We will clarify this definition and explain the connection to thermodynamics more carefully (section \ref{sec:thermo}) once we have a numerical definition of consistency. In the mean time, we continue to argue for this collection of marginals as a way of representing beliefs.
	
	\subsection{A More Formal Treatment}
	\begin{defn}
		A Baysian network (BN) is a tuple
		\[
		\mathcal B = \left(\mathcal N : \mathbf{FinSet}, ~~\mathrm{Par}: \mathcal N \to 2^{\mathcal N},~~ \mathcal S: \mathcal N \to \mathbf{FinSet},~~\Pr: \prod_{N : \mathcal N}  \left[ \mathcal S_N \times \left(\prod_{P : \mathrm{Par}(N)} \mathcal S_P\right)  \to [0,1] \right] \right)
		\]
		such that
		\begin{itemize}[nosep]
			\item the graph $\bigcup_{N, P \in \mathrm{Par}(N)}(N, P)$ is acyclic, i.e., there exists no cycle of nodes $N_0, N_1, \cdots, N_k = N_0$ in $\mathcal N^k$ such that $N_{i+1} \in \mathrm{Par}(N_i)$ for each $i \in \{0, 1, \cdots, k\}$.
			\item For all $N \in \mathcal N$, $\Pr(N)$ is a probability distribution on $\mathcal S_N$, i.e., 
			\[ \forall N\in \mathcal N.~\forall \vec{p} \in {\prod_{P : \mathrm{Par}(N)} \mathcal S_P}.~~ \sum_{n \in \mathcal S_{N}} \Pr_N(\vec{p}, n) = 1\]
		\end{itemize}
	\end{defn}


	\section{Emulation: Sets of Probability Distributions}
	\section{Emulation: Expected Utility}
	
	
	\section{Co-algebraic Structure}
	So far, we have not made a big deal out of our extra element $\bullet$ that we have included into every domain, but the fact that we have exempted ourselves from giving it a distribution turns out to dramatically increase the representational capacity of the model. Of course, we will be able to say less about this larger class of models, but many parts of the story will translate cleanly.
	
	\subsection{Bundling and Unbundling} 
	To begin, by relaxing the restriction on our links from stochastic to sub-stochastic matrices, we have gifted ourselves the ability to un-bundle and re-bundle variables back together. One should think of this as a way of building ``left-handed'' or inductive constructions, rather than ``right-handed'' or co-inductive ones. This will allow us to represent game trees and automata, as well as
	
	\begin{center}
		\todo{DIAGRAM 1}
		\begin{tikzpicture}
		
		\end{tikzpicture}
	\end{center}
	
	\subsection{Branching}
	


	\section{Reduction: Fragment of Factor Graphs}
	\section{Reduction: Game Trees and Automata}


	\section{Category Theory}

	We can also define these structures more compactly (and in my view, cleanly) with a categorical notation, perhaps shining some light on how the different definitions fit together, justify some design decisions, and import a giant mathematical toolbox which makes some features completely obvious. If you're sold already, you can skip next bit (section \ref{sec:cat-defense}) in which I attempt to give a more complete justification, and if you are not willing to parse abstract nonsense, you can skip this section entirely.
	
	\subsection{Why Use Category Theory?}\label{sec:cat-defense}
	

%	\subsubsection{Entropic 2-Category}
	\subsection{}
	We'll start with the fragment of 
	
	\begin{defn}
		A \emph{categorical MCG} is a diagram in $\bf Mark$ of shape $\cal A$ --- that is, a functor $\mathcal A \to \mathbf{Mark}$, where $\mathcal A$, thought of as attention, is (the category generated by) the graph whose nodes and edges are relevant features of the problem setup.
	\end{defn}
	
	This makes sense, as these topological graphs
	\subsection{Worlds.} \label{sec:cat-worlds}
	
	We can use this definition to 
	
	\begin{defn}
		A \emph{world object} of an MCG $G : \mathcal A \to \mathbf{Mark}$ is a weak limit of $G$ --- that is, a cone over $G$, for which there exists a (possibly non-unique) arrow 
	\end{defn}

	\begin{example}
		If $\mathcal A$ is a discrete category, with $n$ elements, then MCGs of shape $\mathcal A = \{\tilde X_1, \tilde X_2, \ldots, \tilde X_n\}$ is just a set of $n$ names (with identities), to be interpreted by $M$ as random variables (with their identities). If $M : \mathcal A \to \mathbf{Mark}$ is a (weak) final cone, as illustrated below.
		\[ \begin{tikzcd}
				&& Y  \ar[ddll, gray, "f_1"']\ar[ddl,gray, "f_2"description]\ar[ddrr,gray, "f_n"] \ar[d, dashed, "\exists!"]\\
				&& \mathcal W \ar[dll, "\pi_1"description]\ar[dl, "\pi_2"description]\ar[drr, "\pi_n"description] \\
				X_1 & X_2 & \cdots & & X_n
			\end{tikzcd} \]
			
		We claim that $\cal W$ is just the standard product of measurable spaces, and each $\pi$ is a projection. The reason for this can be seen information theoretically --- clearly the product of all of the variables with projections is a cone over the $M$, since there are no non-trivial arrows in $\cal A$ so no equations must be satisfied. %Also, any other cone has factors through it in the obvious way. 
		
		The only sticking point is the, possibility that it's not  %and each projection has to have entropy zero. 
	\end{example}

	This has some huge advantages over defining a world up front. First of all, we don't need to describe the set of all possible worlds globally and in a way that people can agree on. 
	
	We do not get the problem in \ref{sec:world-explosion}.
	
	% Explosion of possible worlds by unwrapping and then product...
	
	\begin{example}
		\todo{projections}
	\end{example}
	
	\begin{example}
		\todo{filtered limit}
	\end{example}

	\begin{conj}
		% colimit of diagram
	\end{conj}
	
	\subsection{Bundling and the Category of Elements}
	\subsection{Meta-links and Higher Structure}
	\subsection{Denotational Semantics}
	
	\part{Values on Marginal Constraint Graphs}
	
	The first strand of this project is a general representation of many ways that have been historically used to represent values: namely, utilities, goals, desires, preferences, choice functions, and natural extensions of these suggested by this representation, some of which are also well-studied. 
	
	\section{Setup}
	
	% There are two key insights / differences:
	% (1) allowing for a variety of scopes: you don't need to supply the data on all possible configurations at once.
	% (2) taking a categorical perspecitve, viewing these data as all special kinds of maps.
	
	The general idea is that to attach some additional preference data to each alternative in a domain
	
	\subsection{Definitions}
	

	
	\section{Reductions}
	
	The standard tool to represent values (and the ones that people are most familiar with) are orders, i.e., flat categories. 
	
	The simplest non-trivial order is:
	\begin{center}
		\raisebox{0.7em}{$\mathbb B = $}~\begin{tikzpicture}
			\node[pt=0] at (0,1){};
			\node[pt=1, right=1.2em of 0] {};
			\coordinate[below=0.3em of 0] (gutter);
			\node[right=0.6em of 0,anchor=center]{$\leqc$};
			\node[draw=gray, rounded corners=2, inner sep=0.5em, fit={\lab{0}\lab{1}(gutter)}] {}; 
		\end{tikzpicture}
	\end{center}
	
	\subsection{Desires}
	
	I want $\varphi$, is 
	
	One standard logical approach would be to give a Kripke model, so that a statment like $s \vDash W_{\alpha} \varphi$, for instance, would be true when agent $\alpha$ wants $\varphi$ in state $s$. In such a model, there is already machinery for talking about the set of all possible states (call it $\cal W$), and so effectively we have specified a function $W_\alpha \varphi: \mathcal W \to \mathbb B$, which assigns 1 to worlds where $\alpha$ wants $\varphi$ and 0 to the others.
	
	This is the behavior we would like to capture, but rather than use the set of possible worlds $\cal W$, we will adopt 
	
	
	\subsection{Utilities}
	
		
	
	A utility function $u : \Omega \to \mathbb R$ is a $\mathbb R$-value on the set of outcomes $\Omega$. 
	
	Effectively, 

	
	\subsection{Preferences}
	We can also consider orders which are not total; a discrete order encodes an inability to compare any of the options, lattices encode a possible inability to compare individual options, but provide a way to formalize the combination the best and worst features of two alternatives.
	
	
	An order is a function $\leq: \Omega \times \Omega \to \mathbbm 2$
	
	\part{Dynamics}
	
	\section{Consistency}
	Sometimes an MCG effectively, such as when 
%	
	
	\subsection{Pairwise Consistency}
	Many of our motivating examples can be thought of as a ``behavioral inconsistency'': you value one thing, but your actions are inconsistent with it. Maybe you value sleep, are aware that to get sleep you need to go to bed early, and yet still do something you know is less valuable instead, such as checking social media. Maybe you 

	Any example where you have to domains like this.
	
	\section{Reduction: Belief Updating}
	\subsection{Conditioning}
	\subsection{Jeffrey's Rule}
	\subsection{Pearl's Rule}
	
	\section{Abstraction}
	In addition to nodes which represent specific, observable variables that we want to ensure everyone is aware of, we are in the business of allowing agents to construct nodes of their own --- examples we've seen so far include product nodes, utility nodes, and other features of the world which become suddenly salient.
	
	Here we want to deal with abstraction nodes, which often arise by approximate factorization of arrows into more manageable pieces.

	\subsection{Compression}
	\subsection{Divorcing the Specific and General Cases}
	

	
	
	\begin{example}
		Suppose you are more likely to go into work if there's no rain, and you have some probability $p$ of there being rain. A simple linear model might look like this:
		
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {$1$};
			
			\node[bpt={r|$r$}, above right=0.75em and 4 of 1.east] {};
			\node[tpt={notr|$\lnot r$}, below=1.5em of r] {};
			\node[Dom={Rain (R) around \lab{r}\lab{notr}}] {};
			%		\node[dpadded, right=2 of 1] (R) {Rain};
			
			\node[bpt={l|home}, above right=0.75em and 5.5 of R.east] {};
			\node[tpt={notl|work}, below=1.5em of l] {};
			\node[Dom={Loc (L) around \lab{l}\lab{notl}}] {};
			%		\node[dpadded, right=2 of R] (L) {Loc};
			
			\draw[archain] (1) -- node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$*$}{$r$,$\lnot r$}	p & 1-p \end{idxmat}}}
			(R) --
			node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$r$,$\lnot r$}{home,work}	.6 & .4 \\ .1 & .9 \end{idxmat}}} (L);
			\end{tikzpicture}
		\end{center}
		
		Now, suppose it happens that it's actually raining right now --- what happens to the model? The standard answer is to condition, and allow the new, specific information to overwrite the general model. Doing this is a perfectly reasonable thing, but cements a meaning of the node ``Rain'' that we may not have intended --- it now means ``it is raining today'', rather than in general. This is not a problem for a human modeler, who presumably has fit the model to data somehow, and plans on copying it for each use. There are (at least) two downsides to this. First, we've really just abdicated responsibility to a trustworthy human who will keep track of the copying for us, and handle any sort of abstraction for us. Secondly, it is not clear how and when to update the model based on new experiences.
		
		In the meta-theory, this gives the human in charge an indexed family of variables:
		
		\[ \left\{~\begin{tikzpicture}[center base]

			\node[dpadded] (1) at (0,0) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain$_i$};
			\node[dpadded, right=2 of R] (L) {Loc$_i$};
			
			
			\draw[archain] (1) -- node[above]{$r_i$?} (R) -- node[above]{$q$} (L);
		\end{tikzpicture}~\right\}_{i \in \text{Experiences}} \]
		Of course, we can also internalize this to the agent's picture (and given that our formalism is entirely built around conditioning on variables, it seems silly not to) --- a picture that might look like this:
				
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0.5,-0.5) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain};
			\node[dpadded, right=2 of R] (L) {Loc};
			
			\foreach [evaluate=\x as \y using \x/2 + 2] \x in {0, 1, ...,4} {
				\node[bpt={e\x | $e_\x$}] (e\x) at (\y,-3) {};
			}
			\coordinate (Q) at (2,-3.2);
			\node[bDom={Experiences (E) around \lab{e0}\lab{e4}(Q)}] {};
			
			\draw[archain] (E) -- node[left]{$p_i$} (R) -- node[above]{$q$} (L);
			\draw[archain] (1) -- node[above]{$\tilde p$} (R);
			\end{tikzpicture}
		\end{center}
	
		We store concrete values of whether or not it rained (with possible uncertainty due to lack of memory) in the ``Experiences'' node. Note that the picture we've drawn below is not expressible with any standard graphical model, because has an arrow merge, which we interpret as a constraint, and so we get non-standard behavior --- our model can be inconsistent, for instance, if the general probability of rain $\tilde p = 0$ when it has rained. 
		
		We gain the additional benefit of 
		
		We can also play tricks by bundling and un-bundling this experience variable:
		
		%Compression
	\end{example}

	\section{Value Updating}
	In the case where values are represented as a utility 
	
	\section{Thermodynamic Analogy}
	
	
	\appendix
	\section{Category Theoretic Preliminaries}
	\todo{Minimal Category Theory needed to understand: categories, functors, diagrams, cones, limits}
	\subsection{Markov Category and Giry Monad}
	
	\subsection{sub-Markov Category}
	
	\section{More Arguments against the Standard Model} %todo section titles
	
	\subsection{Can't Have Priors on Everything}\label{sec:impossible-prior}
	
	
%	Just as one can get an order from a utility function, which 
		
	
	
\end{document}