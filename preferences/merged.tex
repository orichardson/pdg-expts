\documentclass{article}

\input{prefs-commands.tex}

\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{Merged: Dynamic Beliefs and Preferences}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}


\begin{document}
	\maketitle
	
%	\section{}

	\section*{A Quick Overview}
	In some sense, the goal of this project is to provide a compelling picture of preference revision --- but in doing so, we will paint a very different, generalized picture of decision theory. 
	
	The core representation we suggest, the marginal constraint graph, is more flexible than the standard ones, as agents need not always have a perfectly consistent picture of reality, intermediate representations have meanings, and we can capture phenomena such as learning preferences from data and revising them in various places. In special cases, we recover standard results, including expected utility calculations, belief updating by conditioning.
	
	We can roughly separate the work into two pieces:
	\begin{enumerate}%[nosep]
		\item Representation: the sufficiency of our new representation as a unification of existing results. In particular, we think of an agent's values as distributed across a network of concepts, whose nodes are preferences on small sets of alternatives, and whose edges are beliefs about the impact of one variable on another.
		\item Dynamics: A formulation of how to revise the representation in light of new information or additional computation. We will show how can be dealt with through inconsistency, illustrate some options for regularization, and ultimately factor out these as general meta-preferences. 
	\end{enumerate}

	\tableofcontents
	\clearpage
	\part{Motivations}
%	This work an be motivated in many different ways; I imagine the most compelling one to depend on who you are and what you already care about. In any individual paper I plan to only include one of these, but until the audience has been narrowed, I want to keep all of them around. %to remind myself which directions I want to push, and to record selling points outside of the best publishing path.

	\begin{ctikzpicture}
		\node[ellipse,draw,align=center] (R) at (4,-3) {World\\ Representation};
		\node[ellipse,draw] (B) at (3,1) {Beliefs};
		\node[ellipse,draw] (V) at (3,-1) {Values};
		\node[ellipse,draw] (A) at (5.5,0) {Actions};
		
		\draw[arr,-] (B) -- (4,0) -- (V);
		\draw[arr, shorten <=0] (4,0) -- (A);
 	\end{ctikzpicture}


	\section{Issues with Standard Issue Decision Theory}
	\subsection{Some Reasons not to Calculate Expected Utility}
	
	\begin{enumerate}
		\item \textbf{No utility function}. Maybe your preferences are not representable in this way --- maybe they violate the vNM axioms, or you just don't have anything that resembles an ordered set. Maybe your intrinsic utility is stochastic.
		
		\item \textbf{No complete prior.} It is possible that you're not a Bayesian, and don't have (access to) a probability distribution on all salient features of your world\footnote{In fact, once we add enough representational complexity to our world, this is impossible to have; see section \ref{sec:impossible-prior}}.  
		%		Of course, if you accept Savage's axioms, then you act as though you have both a probability distribution over states and a utility function
		
		\item \textbf{Thinking takes time.} (this constrains the complexity of your utility and probability information, or at least requires them to be in a form that admits fast computations)
		
		\item \textbf{Internal Actions.} In addition to the external actions, there are also invisible mental ones: nobody has complete control over what they do in all respects at all times. Emotionally preparing yourself to talk to your boss, warming up your fingers to play piano, and intentionally committing a phone number to memory are all examples of mental actions, which are rarely modeled. More generally, for any interface with the world that a model prescribes, there will be extra 
		
		%you may not know exactly what it is you have to do in orde to 
		\item \textbf{Different world representation.} 
		%			You have a distorted perception of the world. 
		Models, both in probabalistic reasoning and decision theory, usually start by defining the set of possible states of the system. For a modal logician, this is the underlying set of a Kripke structure; for probability theorists, it's a set of outcomes.This is fine if agents are aware of the structure you've set up, but simply don't know what world they're in --- but nothing precludes them from conceptualizing things in entirely different terms. 
		
		When the agent fails to obey expected utility for this reason, there are three ways to assign blame: it could be the agent's fault for not truly understanding what is possible, the modeler's fault for failing to capture the the agent's picture of the world.
		
		
		
		\item \textbf{Things Change over Time.}
	\end{enumerate}	
	
	Despite the fact that there has been work backing off of each of these assumptions, utility maximization is still ubiquitous (and taken for granted) in most primary applications, including consumption models and the construction of artificial agents. In section \ref{sec:optimization-bad} I argue that this is the source of some cultural problems. But part of the reason that many of the other techniques has not gained traction is that none is as clean and as useful as expected utility maximization, and to the best of my knowledge people try to isolate and tackle these issues individually. My framework provides a way of representing many of these issues effectively and cleanly, while also reducing to the expected utility computations when none of these issues apply. 
	
	
	\subsection{Fixed World Representation}
	
	The set of things which are even considered possible is contentious enough that we don't want to build it (and actually the correct one) canonically into an agent's picture of the world. In some sense only the current world is possible, as far as anyone can be sure. Moreover, no matter how you chose to represent the world, an agent's picture of what's possible can change over time, which means the modeler needs to write down beforehand everything that could possibly happen. We talk more about this in section \ref{sec:cat-worlds}
	
	\subsubsection{World Explosion} \label{sec:world-explosion}
	
	\section{The Big Picture: Why This is Important}
	\begin{enumerate}
		\item Subjective representations of the world
		\item Joint dynamics
		\item 
	\end{enumerate}

	\subsection{Fixing the Ethos of Agency}
	The distinction between the things that you care about (e.g., utilities, goals, preferences, objectives), and the tools you have for understanding and affecting the world (e.g., beliefs, reasons, plans, optimization, inference) plays a large role in technical accounts of agency%
	\footnote{Beliefs should be considered optimization power: they you believe that action $A$ will have effect 1, an action $B$ will have effect 2, and a preference between the 1 and 2}.
	Generally the two are considered separately, and also people tend to fix preferences%
	\footnote{In the context of optimization, this corresponds to the practice of writing down an objective and finding extrema; in the context of decision theory, it is finding the best actions to satisfy some preferences}. 
	%
	This is nice because the fixed preferences provide a clean way to evaluate the quality of decisions, and criteria for success are a very important part of science. It is clear why modeling things in this way caught on so quickly and durably --- it is simple, prescribes tractable computations, is expressive enough to capture any behavior over fixed time periods, if you make modeling choices in the right ways. 
	
	This heavy dependence on modeling assumptions is an underappreciated shortcoming, and freedom to chose the representation gives modelers a lot of power to over-fit to their desired outcomes. 
	
	
	In real life, human preferences and beliefs often depend on one another, and in more complex settings the line is much more blurred:
	%	Subjectively, though it is hard to imagine having one without the other, and it seems like there might be G\"odelian knots if you take the separation seriously enough --- 
	\begin{itemize}
		\item When you play a video game, are you trying just so you can win? If so, why do you care about winning? Decision theorists often take this as primitive, but it can also be considered in a larger picture, which decision theorists also purport to model. It buys you very little socially, and costs time and money. Or does the process of optimizing for this arbitrary goal itself have value? Does this make entertaining yourself fundamentally irrational?
		%		\item In the process of achieving enlightenment, you go about removing your desires. But it seems like being without desires is in fact a desire--- why where you attempting to do this, anyway? 
		%		\item Similarly,  care about is truth
		\item It is common for humans to have beliefs about preferences: (`I think I like cheese', 'I believe freedom is bad'), and also preferences over beliefs (`Believing true things is good', `I want to know calculus') --- but the clear separation in standard decision theory makes this difficult to understand. 
		
		These can be nested quite deeply without anyone thinking too hard: If I ask if you like cheddar, you now believe that I want to know whether you like cheese --- note that this is a belief about a desire for a piece of knowledge about a preference, spanning multiple people.
		
		\item On a small scale, we think of a loss function as an optimization objective, and the algorithm (say, stochastic gradient descent) as optimization power. One can use both of these together to form a neural network, which is thought of as being just a tool, with no
		
		\item Complete agents can be used in either way: people, who have desires and experience emotions, can be used as optimization power with an external objective (e.g., hiring employees), but can also as the ultimate source of value (e.g., running an organization because it will help people)
	\end{itemize}
	This is an argument against the orthogonality hypothesis \cite{orthogonality}, and similar lines of thought. It seems clear that if we could combine any picture of truth, with any model of value, and any optimization procedure, which look remotely like those that humans have, the interactions between them would certainly be woven together, and their dynamics would be intertwined.
		
	%%
	{\color{green!30!black}This is why we model the dynamics of beliefs and preferences together, and why in some cases we will be able to represent a belief as a preference or vice versa%
		\footnote{the categorical interpretation, where each node is a category, the big graph forms a 2-category, and beliefs are value-preserving functors, supports the observation that the distinction between preferences and beliefs isn't a fundamental feature the objects themselves but rather how they're used--- here the arrow category here gives us preferences over beliefs. Similarly, quotient objects represented as arrows, just as utilities can be identified with beliefs about utilities.}.
	The way we've laid things out, the interaction between the two resembles a duality.
	}
	
	\subsection{Better Models of Humans, whose Preferences Change}
	When you're born you have no conception of what foods or professions or ideals are good; all you have is your own evolutionarily programmed pleasure and pain. Not only does this feeling get more sophisticated in a way that seems to depend on the environment, but also later on in life, people willingly sacrifice their experiential pleasure for other things they care about. All of this stands even if we didn't see change in the more obvious toy cases: foods, activities, and luxury goods. Such changes do in fact occur, and do not detract from a person's ability to be conceptualized as a coherent agent; often they contribute to a person's character instead.
	
	The standard models do not account for any of this, and for many good reasons --- but dynamic preferences are necessary to capture a great deal of human behavior, and are much more important in a world where value is informational and changes quickly. Things like memes, fashion, games, conversations, lofty ideals, and art---things that standard economic theory has had a lot of difficulty ascribing value to---certainly play a bigger economic role than they have in the past, and arguably move faster as well.
	%is learning a new piece of information something that has value?
	
	%there is some overlap between conditioned preferences and dynamic preferences.
	\subsubsection{}
	{\color{green!30!black}This is why we model changes in preferences, and acquisition from only a very limited set of base values. We have some reasons to suspect that in most cases preferences generated this way will be similar, which if true would (1) provide an additional explanation (beyond empathy and genetics) for why humans end up sharing a lot of values, and (2) allay some concerns about misaligned AIs that are constructed in this way}
	
	\subsection{Computational Tractability}
	
	The standard picture of decision theory requires keeping around preferences and beliefs about all possible things that are relevant to any decisions you make. \todo{Is there an edit here to be made about small vs large worlds?} To fully describe a general agent's values, then, you need to specify a preference ordering over all possible histories of settings of observable features. This is wildly intractable, and also annoyingly depends on what ``possible'' means, which is part of the agent's internal beliefs\footnote{Or alternatively, if you're going to do the work beforehand as a modeler, has to change when humanity discovers new features of the universe}. This is why people in practice restrict the scope of an agent to only a few modeler-chosen variables, assume that they only encounter one kind of decision, specify objectives in a compressed syntactic form.
	
	But even these more tractable restricted approximations we use do not degrade gracefully: in order to make any decisions at all you have to do expected utility calculations (which could be very expensive depending on how clear the impact of your decision on the world is), and there's certainly no clear way of making use of partial computations under time pressure.
	
	{\color{green!30!black} By keeping parts of preferences around in many different forms, attached to different contexts, not only can we immediately re-use them for recent decisions we've made in a pinch, but also reduce the complexity of adding new nodes. This is because we can split the computation into meaningful chunks (one for each preference domain (node) we can connect a decision to).}
	
	\todo{mention related work on tractability: BN's, Markov Networks, Markov Networks}
	
	\subsection{The Value Of Inconsistency.}	
	
	%% bad, unclear paragraph
	There are still many tasks human-coded expert systems excel at, which are difficult for trained statistical systems. However, they are slowly losing their ground due to a debilitating shared flaw: any input that the designer has not anticipated causes the entire calculation to be wrong in un-salvageable ways. From the perspective of the expert system, though, there is nothing wrong. 
	
	The program is specified by \emph{behavior},  It's coded without an outside view of the explicit purpose: there's no possibility that the designer was \textit{wrong} in the algorithm specification, and there's no reasonable inference to make instead even we acknowledged it was possible she was.
	
	\textbf{The only way you can know if you're wrong is if there is enough redundancy to highlight inconsistency.}
	
	Incorporating probabilities and specifying objectives instead of algorithms has helped this problem enormously: by using a ton of redundant, noisy, examples of the algorithm working correctly, we can be reasonably resistant to malformed input. Unfortunately, these systems are not perfect either: they are often biased and sometimes cheat. We face a different kind of brittleness here: what if we forget about a second order effect and slightly mis-specify an objective function? What if we run it on a different input distribution? ML systems won't crash, but they will confidently display  wrong answers. The system doesn't think there's any possibility that its \textit{objective is wrong}, and even if it acknowledged this possibility, it's not clear there's anything to be done at this point.
	
	{\color{green!30!black}In analogy to the solution that statistical ML provided to the problem of brittle classical AI, the solution would seem to be a noisy, redundant encoding of the objective function in conjunction with a meta-objective: in our case, consistency. The diffusion of preferences (which we will also refer to as value capture) can also be thought of as a source of uncertainty about your values even if you didn't start with any: if there are multiple objectives consistent with everything you've seen, you acquire a preference for both of them.}
	
	Of course, humans are also often uncertain about their values (and experience value capture) in addition to their beliefs and the appropriate choice of actions.
	%	Agents are allowed to have uncertainty in what they believe
	
	%	nd so the synthetic agents we design, influenced by the standard picture and designed with particular use cases in mind, 
	
	
	
	\subsection*{[Theoretical Unity] Reductions to other theories and algorithms} \todo{Edit this}
	%	There are many different representations of preference theories 
	The generality is a genuine mathematical motivation for this theory: it can be viewed as a number of different things. I will briefly mention some of them that I'm excited about here, and then focus on simple things for the rest of the document.
	\begin{itemize}[nosep]
		\item Our model reduces to standard expected utility calculations in the degenerate case where the agent has preferences over possible histories of worlds, the representation of a world never changes, and the agent is cognitively unbounded
		\item It has a categorical interpretation which has some features I'd like to explore: in particular, meta-preferences seem to be related to higher order structure, a total utility function is like a limit, a total probability function is like a co-limit, preference-preserving beliefs are functorial, and the nodes already form a 2-category, whose objects are themselves enriched flat categories.
		\item There also a natural interpretation of this as an artificial neural network. If the underlying graph is a DAG with one sink, then it is a feed-forward network for calculating expected utility. In most interesting cases, it will have recurrent connections and no special output.
		\item For agents that have priors, the beliefs encoded in this way can be converted to a Baysian Network with a simple transformation
		%		\item If the nodes are all propositions with a preference for truth, and we add some explicit rules for creating and deleting nodes, it becomes an inductive theorem prover.
		\item The preference propagation is a bit like broadcasting on a network (of physical machines) to compute path lengths. This can be done Dijkstra / Distributed Bellman Ford \footnote{depending on whether we take right or left powers of matices} and can be computed by taking iterative powers of a giant matrix over the tropical semi-ring. Funnily enough, this is exactly how you compute the transitive closure of preference matrices on a small scale (except with a different semi-ring), which lends more credibility to the higher order categorical interpretation.
	\end{itemize}
	
	The possibility of bridging these many disparate fields makes the abstraction feel much more universal, and the ability to think about preference change in any of these terms could make it easy to identify analogs of non-trivial facts in other fields.
	
%	These are features that we would like to explain in humans, but needn't necessarily be essential .

%	\section{A Temporal Extension}
%	
%	Many issues with decision theory can be attributed to a failure to account for the passage of time. 
%	One popular example of this is the ubiquitous assumption of cognitive unboundedness: in a static world, where everything is frozen except your own thoughts, you can afford to do decision theory exactly by computing expected utility. You have only one decision to make
%	
%	Still, cognitive boundedness is only one example. When we move to a dynamic setting, most classical results no longer apply, though people often like to pretend that they do. 
%	\begin{enumerate}
%		\item There's no reason to 
%	\end{enumerate}
%	
%	If your beliefs
	
	
%	\section{Better Models of Humans}

	\section{Desiderata}


	%	Currently preferences are thought of as static objects, fixed as part of the structure and identity of an agent, independent of beliefs, complete, and over some fixed domain. This is clearly not at all how human preferences work, and I posit that it's not the right way to think of preference for synthetic agents either.
	
	Perhaps among other things, any model of preference dynamics that is to be used to build synthetic agents which have control over important decisions, should have the following properties: 
	\begin{enumerate}[noitemsep, label=\textbf{D\arabic*}.]
		\item Provide an answer to the `value loading' problem: show how you can learn ``reasonable'' preferences by interacting with the world
		\item Reduce to static models for some parameter settings
		\item Behave reasonably when combined with changes of perspective 
		\item Be resistant to standard challenges to irrationality, such as dutch book arguments
		\item Have weak safety guarantees: an agent should not eagerly adopt preferences which are totally in conflict with its current ones
	\end{enumerate}
	
	%	We might also require that a theory be useful for explaining human behavior. We expect ours to be able to explain:
	Because the view of preferences we adopt here is different from the standard ones in economics (in particular, it lends itself naturally to incorporating boundedness), we have a hope of explaining some behavior which was previously classified irrational, as an optimal in some sense. The following psychological effects lend themselves to explanation:
	\begin{enumerate}[noitemsep]
		\item \textbf{Value Capture.} You really care about $X$ (say, learning math), which is vague and hard to measure, so you come up with some metric $Y$ (say, scores on undergrad math exams). Over time, optimizing for $Y$ will cause you to optimize less effectively for $X$ and assign intrinsic value to $Y$ (getting good exam scores becomes valuable, independent of whether you learn math\footnote{Social signaling plays into this, but this occurs also in cases where people try to hide the signal: constant grade checking, pokemon go addiction, etc.}). Related to Goodhart and Cambell's laws.
		\item \textbf{Framing Effects.} It is well-documented that the style of presentation, even for logically equivalent scenarios, can have a significant impact on a person's choice. But if we formalize these as the same outcome, there's no way for utility-maximizing agent to behave this way.
		\item \textbf{Connoisseur Effects.} Someone who listens to a lot of rap music has more nuanced, complicated preferences on rap music than someone who has not heard as much. Similarly, people work to develop palates for wine, and ``all Indian food tastes the same'' is an insult, indicating a shallow experience with the cuisine. There are two technical aspects: first, additional experiences increase preference complexity.
		
		\item \textbf{Adaptive Preferences.} Even things we find objectionable are normalized over time, and people often change to prefer things they're used to, even if they are initially opposed. This is sometimes thought of as a prioritization of safety, and is maybe best thought of as a thought-saving feature: the things you're used to have gotten you this far already. 
		
		%		\item \textbf{}
		
		\item \textbf{Novelty Effects (anti-adaptive preferences).} 
		
	\end{enumerate}


	\clearpage
	\part{Probabilistic Constraint Graphs}
	The basic tool that we will use to solve these problems is a graphical representation of (a piece of) an agent's mental space, which we call a marginal constraint graph. Throughout this section, we will introduce them from many different perspectives, but They look a bit like Bayesian Networks, but are more general: rather than representing a single distribution, they represent (soft) constraints on distributions. 
	
	\begin{itemize}
		\item If the constraints pick out a single distribution, the PCG is a graphical factorization of that distribution. 
		\item If the PCG is under-constrained, there are many distributions consistent with it. We can use this to emulate weaker notions of uncertainty than probability distributions themselves. However, not all such distributions are equal: some may be simpler than others, or better match the symmetries of the constraints; we use entropy to distinguish between them.
		\item An over-constrained PCG is has internal inconsistencies, the resolution of which drives belief and preference change. Just as in the under-constrained case, not all resolutions are equal: some are more or less constrained 
	\end{itemize}
	
		

	\section{Uncertainty Examples \todo{fold into other sections}}
	\subsection{Coin Tosses}
	\begin{example}
		You're about to flip a coin. The result of this flip is the only thing you're thinking about, but you know nothing about the outcome, other than that it could land heads or tails. This (lack of) knowledge is the single node in an PCG:

		\begin{ctikzpicture}
			\node[bpt={h | $H$}] at (-0.3,0) {};
			\node[bpt={t | $T$}] at (0.3,0) {};
%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[bDom={Coin (C) around \lab{h}\lab{t}}] {};
		\end{ctikzpicture}
		
		One might think this is vacuous, but in fact this conveys two important things: that the agent is aware of and conceptualizes the coin flip (with the two outcomes), and a total uncertainty about what values they actually take and how they relate to anything else.
	\end{example}

	\begin{example}
		Now you start reasoning about this coin, and decide that the outcome of the flip is determined by some intrinsic feature of the coin: its bias; you'd be willing to assign a probability to the coin flip if you knew the bias. We illustrate this with the following diagram:
				
		\begin{ctikzpicture}
			
			\node[bpt={h | $H$}] at (-0.3,0) {};
			\node[bpt={t | $T$}] at (0.3,0) {};
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[bDom={Coin (C) around \lab{h}\lab{t}}] {};
			
			
			\node[dpadded, left=1 of C] (B) {$\sf Bias$};
			
			
			\draw[arr] (B) -- (C);
		\end{ctikzpicture}
	
		where Bias is some value in $[0,1]$ and the arrow $P$ is given by 
		\[ P : \mathsf{Bias} \to \Delta \{H, T\}, \qquad P(b)  =  \begin{cases}
		 	H & \text{with probability $b$} \\
		 	T & \text{with probability $1-b$}
		\end{cases} \]
		Bias is now an infinite random variable, and functions as a higher-order probability, which has been "internalized" to the picture. Doing this again may seem vacuous, as we have not gained any information about whether the coin will land heads or tails, but once again we get information about what the agent thinks is relevant. Putting this node in the space can make otherwise innocuous changes suddenly meaningful --- for instance, if Bias had only a finite selection of possibilities, a granularity perhaps imposed by the agent's way of representing numbers, then the agent has given themselves information about the coin through arbitrary choices.
	\end{example}


	\begin{example}
		We will take our coin example a bit further. Already in the previous case, the set of biases is consistent with some set of distributions. Suppose that our agent continues to think and realizes they consider some set of distributions over bias possible. Now that they are aware of it, we give it a name ($\sf S$), and put it into the picture:
		
		
		\begin{ctikzpicture}
			
			\node[bpt={h | $H$}] at (-0.3,0) {};
			\node[bpt={t | $T$}] at (0.3,0) {};
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[Dom={Coin (C) around \lab{h}\lab{t}}] {};
			
			
			\node[dpadded, left=1 of C] (B) {Bias};
			\node[dpadded, left=1 of B] (S) {$\sf S$};
			
			
			\draw[archain] (S) -- node[above]{$q$} (B) -- (C);
		\end{ctikzpicture}
		
		The arrow $q$ represents the conditional distribution $\mu_s$ on Bias, for $s \in S$. This example illustrates that not only can these PCGs represent sets of distributions with some external interpretation, but this can also be internalized to the PCG itself, and so the kind of inference that can be done externally with lower probabilities, for instance, can be done just by the the prescribed decision theory. 
		
		For example, if $S = \{ \mu_B : \mu_B(H) > 0.5\}$, i.e., you are certain that the probability of heads is at least 0.5 (maybe you know the company which makes it, which only weights the head side, but can do any amount), and you are trying to decide whether to bet that the coin lands heads or tails, you now have the following picture:
	
		\begin{ctikzpicture}
			
			\node[bpt={h | $H$}] at (-0.3,0) {};
			\node[bpt={t | $T$}] at (0.3,0) {};
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[Dom={Coin (C) around \lab{h}\lab{t}}] {};
			
			
			\node[dpadded, left=1 of C] (B) {Bias};
			\node[dpadded, left=1 of B] (S) {$\sf S$};
			\node[dpadded, below=0.8 of C] (Bet) {Bet};
			\node[dpadded, above right=0.4 and 1.9 of Bet.center] (money) {\$};
			
			
			\draw[archain] (S) -- node[above]{$q$} (B) -- (C);
			\mergearr{Bet}{C}{money};
		\end{ctikzpicture}
	
		From this, even though I have no prior on $S$, I know that no matter what distribution on bias is actualized, an even bet for heads nets more money than an even bet for tails. 
	\end{example}

	\begin{example}
		Finally, we can also add weights, which we can use to reason about higher order probability, although we need to be careful about normalization and our arrows will either need to be interpreted as non-standard measures, or we have to make some extra assumptions in the general case. Intuitively, the picture looks like this:
		
		\begin{ctikzpicture}

			
			\node[bpt={h | $H$}] at (-0.3,0) {};
			\node[bpt={t | $T$}] at (0.3,0) {};
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[Dom={Coin (C) around \lab{h}\lab{t}}] {};
			
			
			\node[dpadded, left=1 of C] (B) {Bias};
			\node[dpadded, left=1 of B] (S) {$\sf S$};
			\node[dpadded, left=1 of S] (1) {$\sf 1$};
			
			\draw[archain] (1) --node[above]{$p$} (S) -- node[above]{$q$} (B) --node[above]{$r$} (C);
		\end{ctikzpicture}
	 	Now we finally have grounded the chain so that it has the unique node $\sf 1$ as a root of a tree, and so by Proposition \ref{prop:prob-eq}, there is exactly one distribution consistent with it. We can see that the higher order probability is still represented, and modifications to the arrows which represent more abstract concepts do change the rest of the problem setup --- but at the end, we can use composition (which here is acting as the bind of our probability monad) to squash all of the information a probability.
	\end{example}

	
%	\subsection{Unknown Number of Balls}
	\subsection{Pardoned Prisoner}
	
	
	\begin{example}
		We take this example as well from the Reasoning about Uncertainty book. Suppose there are three prisoners, $A$, $B$, and $C$. One of them has been chosen at random to be pardoned. Prisoner $A$ asks the executioner which of the other two was executed. Just before she gets her answer, the agent's PCG looks like this:
		
		\begin{ctikzpicture}
			\node[dpadded] (1) at ( -2,0) {$\sf 1$};
			
			\node[rpt={a | $A$}] at (0,0.6) {};
			\node[rpt={b | $B$}] at (0,0) {};
			\node[rpt={c | $C$}] at (0,-0.6) {};
			
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[bDom={{Pardoned \\ Prisoner} (PP) around \lab{a}\lab{c}}] {};
			
			\node[rpt={b2 | $B$}] at (3, 0.6) {};
			\node[rpt={c2 | $C$}] at (3,-0) {};
			\node[bDom={{Revealed \\ Condemned \\ Prisoner} (CP) around \lab{b2}\lab{c2}}] {};
			
			\draw[archain] (1) -- (PP) -- (CP);
		\end{ctikzpicture}
	
		Note that in order to tie together the two nodes with an arrow, we need to either know the probability of each prisoner being pardoned for each choice of revealed condemned prisoner, or more naturally, the probability that the executioner reveals each prisoner, given the identity of the pardoned one --- that is, we need to model the protocol of the executioner. If we assume the executioner is truthful, we still need to know what happens when $A$ was the pardoned prisoner and the executioner has a choice. Suppose that in this case, $B$ is always revealed. This results, through composition, in a prior distribution over the revealed prisoner, with $\frac{2}{3}$ on $B$ and $\frac{1}{3}$ on $C$.
		
		
		Once we make our observation, we get a new arrow; suppose we hear that $B$ was revealed.
		
		\begin{ctikzpicture}
			\node[dpadded] (1) at ( -2,2) {$\sf 1$};
			
			\node[rpt={a | $A$}] at (0,0.6) {};
			\node[rpt={b | $B$}] at (0,0) {};
			\node[rpt={c | $C$}] at (0,-0.6) {};
			
			%			\draw[arr,->, thin, red, dashed] (x1) to[bend right=40](x2);
			\node[bDom={{Pardoned \\ Prisoner} (PP) around \lab{a}\lab{c}}] {};
			
			\node[rpt={b2 | $B$}] at (3, 0.6) {};
			\node[rpt={c2 | $C$}] at (3,-0) {};
			\node[bDom={{Revealed \\ Condemned \\ Prisoner} (CP) around \lab{b2}\lab{c2}}] {};
			
			\draw[archain] (1) -- (PP) -- (CP);
			\draw[arr, dashed] (1) to[bend left=10] (CP);
		\end{ctikzpicture}
		There is now an inconsistency: the two paths from 1 give different distributions on the revealed prisoner. There are many ways to ameliorate this:
		\begin{enumerate}[nosep]
			\item The agent can ignore what the executioner told her, because it conflicts with your a-priori distribution on the probability of what the executioner would say
			\item The agent can revise their view of the executioner's protocol upon hearing $B$, and now believe that the executioner would have told her this no matter who was pardoned.
			\item The agent can change their belief about how likely each prisoner is to have been pardoned. In this case, she can conclude that her chances of dying are now 1 in 2. Of course, if the executioner had said $C$, she would have had to conclude she was guaranteed to die.
		\end{enumerate}		
	\end{example}

	\begin{example}[Prisoners and subdistributions]
		Maybe we actually do not know the guard's protocol. We can still interpret the arrow as a sub-Markov transition / sub-stochastic matrix, where now we do not know what happens when $A$ is the pardoned prisoner. In this case, we still can compose the arrows, and obtain a sub-distribution $\Pr_*(B) = \frac{1}{3}$, $\Pr_*(C) = \frac{1}{3}$. Upon hearing the name of the revealed prisoner, we cannot update, and still have the same sub-distribution. 
	\end{example}

	
	
	
	\section{Definitions and Semantics}
	\begin{defn*}[PCG, semi-formal]\label{def:pcg-semiformal}
		A marginal constraint graph $(\cal N, L)$ is a collection of variables $\cal N$, attached to each of which is some preference information: (this could be an order, utility, pairwise utility matrix, a supremum function), plus a collection of probabilistic links $\cal L$ between some (but not necessarily all) pairs of them, where $L: A\to B \in \cal L$ is a (sub) Markov kernel $A \to \Delta [B \cup \{\bullet\}]$\footnote{The additional ``phantom element'' $\bullet$ absorbs probability density that we don't want to equivocate on, allowing our model to capture partial families of conditional probabilities, by extension things like implication, and giving agents more tools to avoid inconsistency. This has the effect of making our links substochatic matrices/kernels rather than stochastic ones. However, if we restrict to beliefs which assign zero probability to $\bullet$, everything in the model works as before. See section \ref{sec:substochastic} for details} representing beliefs about how a setting of $A$ to $a \neq \bullet$ will impact the value of $B$. 
	\end{defn*}
	
	Variables can be thought of equivalently as:
	\begin{itemize}[nosep]
		\item random variables $X$ which can take on values $\{x_i\}$ (or possibly none, which we denote $\bullet$)
		\item sets $X$ with elements $\{x_i\}$
		\item partitions of the universe of outcomes (which is not fixed) into disjoint (but possibly not exhaustive) events. Disjointness here is very weak and can be forced by adding tagging outcomes with additional data, analogous to a disjoint union.
	\end{itemize}
	
%	In will focus on models where the preference information is encoded as a special utility domain $U$, with links from other variables, but there 
	%While there may be some propositions tying this case to utilities or preference orders, we will avoid talking explicitly about the more general setting of preference matrices and choice functions out for now.
	
	\subsection{Interpretations}
	
%	\begin{enumerate}
%		\item A set of probability distributions
%		\item A weighted set of probability distributions. 
%	\end{enumerate}
	By analogy to the distinction between a qualitative and quantitative Bayesian Network, 	
	we can make a distinction between a sources of information in in an PCG. We can talk about the shape of the PCG --- topology of the graph itself, before any numbers have been included--- in addition to its interpretation in a probabilistic setting. One can imagine also interpreting them with other models of uncertainty.  I will also remark that the effects here are captured succinctly and with clearer relations to other interpretations (e.g., of databases, semantics, and diagrams), by viewing a PCG as a functor; see section \ref{sec:cat-defs} for details. 
	
	\subsubsection{Qualitative PCGs} The connectivity a BN (as well as of a Markov Network or factor graph) has something to say about independence --- that if two nodes are not connected, then they have to be independent, if we fix some separating variables. This is a very strong statement. Intuitively when building a graph like this, if I have not connected two nodes, it is because I don't really know how they interact, not because I'm certain that the probability distribution separates when we fix some middle node along a path.
	
	For us, an arrow $A \to B$ indicates that you'd be willing to bet on $B$ (and believe you'd be rational in doing so) if you knew the value of $A$. Unlike the other graphical models, such a structure does not rule out any joint distribution on the nodes, which makes them less useful for factoring a unique distribution. 
	
	Still, the connectivity alone is meaningful, as it determines:	
	
	\begin{itemize}[nosep]
		\item How over / under-constrained the agent's knowledge is
		\item What variables the agent thinks are most robustly predictive of which others
		\item Which beliefs are most redundantly held
	\end{itemize} 
	If we're also given access to the entrenchment weights on the arrows, we can use the connectivity to determine which beliefs are likely to change when observations are made. For instance, if observations are made on $C$ via Jeffrey's rule for each value of $A$, we can infer an edge $A \to C$ in the picture below:
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (150:1.2) {$A$};
			\node[dpadded] (B) at (30:1.2) {$B$};
			\node[dpadded] (C) at (-90:1.2) {$C$};
			\node[dpadded, right=1.2 of B] (D) {$D$};
			\draw[archain] (A) -- node[above]{$1$} (B) -- node[above]{${\epsilon}$} (D);
			\draw[arr] (B) -- node[right]{$\frac{1}{\epsilon}$} (C);
			\draw[arr, dashed, gray] (A) -- node[left]{} (C);
		\end{tikzpicture}
	\end{center}
	As a result of this observation, we can infer from these and the connectivity that the belief $A \to B$ will change, rather than the one from $B \to C$. The connectivity is important; note that the belief $B \to D$ will not change even though it is much more weakly held.

	
	\subsubsection{Quantitative PCGs}
	Filling in all of the numbers, we get a quantitative PCG the PCG gives us the object we've been talking about until now, with families of distributions on each edge. Fully interpreted PCG's 
	
	\subsubsection{Partially Quantitative PCGs}
	We can also interpret arrows part way. By giving ourselves access to the amount of uncertainty in the arrows (e.g., the entropy, or a whether or not a given arrow is deterministic), we can draw additional conclusions.
	
	
	We can also interpret only a subset of the arrows, which places (soft) constraints on the remaining arrows. 
	
	\subsection{Semantics}
	The semantics of a quantitative PCG 
	
	
	
	\subsection{Formalism}
	We can now give a more formal definition:
	\begin{defn}\label{def:pcg}
		A probabilistic constraint graph (PCG) is a tuple 
		\[ \left(\mathcal N : \mathbf{FinSet},~~\mathcal L : 2^{\cal N \times N},~~ \langle\mathcal S, \Sigma\rangle : \mathcal N \to \mathbf{MeasSet}, ~~\mathbf p : \prod_{(A,B): \mathcal L} \left[ \Big. \mathcal S_A \times \Sigma_B \to [0,1]\right] \right) \]
		where $p(L)$ is a Markov kernel, i.e., for every $L[A,B] : \mathcal L$, and $a \in \mathcal S_A$, $\mathbf p_L[a \mid \cdot~]$ is a probability distribution on $(\mathcal S_B, \Sigma_B)$, and $\mathbf p_L[~\cdot \mid B]$ is $\Sigma_A$-measurable for every $B \in \Sigma_B$.
	\end{defn}
		

%	\part{Intuitions }
	\section{Recovering the Standard Picture}
	
	\subsection{Probability Spaces}
	
	\subsection{Random Variables}
	
	\subsection{Preferences, and Utility}

	Suppose the only thing I conceptualize is a single variable $A$. A preference on the alternatives $a \in \mathcal S(A)$ often takes the form of an order. If it is a total order (and $|A| \leq |\mathbb R|$), then it can be represented as a utility function. We can capture this explicitly with our model: think of utility as a separate preference domain $\sf U \cong (\mathbb R, \leq)$, whose elements are the real numbers, with a preference given by the usual ordering. Now a choice of $A$ has an impact on what happens in $\sf U$, and if we know that each choice of $A$ gives us a deterministic utility, then this impact is actually just a function, and in particular can be represented as a degenerate probability distribution $\Pr( \mathsf U \mid A)$, which is just a function from $A$ to $\mathbb R$.
	
	\begin{center}
		\begin{tikzpicture}
		\node[dpadded] (A) at (0,0){$\sf A$};
		\node[dpadded] (U) at (5,0){$\mathbb R, \leq$};
		\draw[arr](A) to node[above]{$U_A : A \to \mathbb R $} (U);
		\end{tikzpicture}
	\end{center}
	
	In this sense, we can think of utility functions as a way of representing preferences on $A$ as a belief about how $A$ impacts a standard universal preference domain of ``goodness''. Similarly, if we have multiple variables, we could always just take their product all of their variables, and consider a conditional probability distribution from this to the utility preference domain $\mathsf U $.
	
	\begin{center}
		\begin{tikzpicture}
			\node[dpadded] (A) at (0,0){$\sf A_1 \times A_2 \times \cdots $};
			\node[dpadded] (U) at (5,0){$\mathbb R, \leq$};
			\draw[arr](A) to node[above]{$U$} (U);
		\end{tikzpicture}
	\end{center}

	

	\section{Relation to Bayesian Networks}
	
	\subsection{Differences}
	These are diagrams which look and behave like Bayesian Networks in many cases, but are more general. Here are some structural differences:
	\begin{itemize}[nosep]
		\item We interpret each arrow as a conditional probability table in isolation, rather than considering all arrows into a node together as a larger conditional probability from the cartesian product. This gives us a notion of composition, which we will defend later.
		\item Similarly (this is the zeroth order case of the above), we do not assign distributions to nodes without parents. 
		\item We make no explicit assumptions about independence (except insofar as the arrows encode this). We achieve separation in a weaker way, by giving preference to maximum entropy distributions.
	\end{itemize}
%	We will also use them differently: 	

%	Differences in use:
%	\begin{itemize}[nosep]
%		\item We allow agents to add additional nodes and edges (form new concepts and connections), dynamically, and in fact use this ability to add and remove nodes as tricks for representing things.
%	\end{itemize}
	
	\subsection{Converting BNs to PCGs}
	
	\label{sec:convert2bn}
	The semantics of a Baysian network ensure that there is no inconsistency: the arrows into a node taken together collectively determine a single well-defined probability distribution. Formally they consist of a set of nodes $\cal N$, and for each $N \in \cal N$, a set of parents $\mathrm{Par}(N)$, and a conditional probability distribution $\Pr( N \mid \mathrm{Par}(N))$, which is a distribution over the values of $N$ for each setting of every variable in $\mathrm{Par}(N)$. While each of our arrows can be interpreted by itself as a marginal, a collection of arrows into a single node must be taken together to have any meaning in a BN. 
	
	The procedure for converting to a BN is simple: we simply take every node's incoming arrows, and insert the product of its parents as a node before it. With this procedure, if a node $N$ has just one parent $P$, we replace $P \to N$ with $P \to N ~=~ N$, which is redundant so we don't draw this. If a node had zero parents (i.e., the BN just gives it a probability distribution not dependent on anything), we insert the product of zero things, i.e., the singleton node $1 = \{*\}$, a variable which only takes one value, and set $\Pr(N \mid *) = \Pr(N)$. 
	
	This sound much more complicated than it is. Consider the example below, where the left is a BN, the center is the corresponding PCG, and the right is a compact visual representation of the PCG in the case where $B \times C$ has no other edges attached to it directly.
	\begin{center}
		
		\begin{tikzcd}[center base, column sep=2em]
			& A \ar[dl]\ar[dr] \\
			B \ar[dr] && C \ar[dl]\\
			& D &
		\end{tikzcd}
		\hfil
		\begin{tikzcd}[center base, column sep=1em, row sep=1.8em, dpad={light pad}, every arrow/.style={arr}]
			& \mathsf 1 \ar[d] &\\
			& A \ar[dl]\ar[dr ]% \ar[dd,dashed, gray] 
			\\
			B && C \\
			& B \times C \ar[ul, gray!70] \ar[ur, gray!70]\ar[d] & \\
			& D &
		\end{tikzcd}
	\hfil
	\begin{tikzpicture}[ center base]
		\matrix(m) [matrix of math nodes, column sep=1.3em, row sep=1.5em, commutative diagrams/every cell] {
			& \mathsf 1 &\\
			& A & \\
			B && C \\
			& {} & \\
			& D & \\};
		
		\mergearr{m-3-3}{m-3-1}{m-5-2};
		\draw[archain] (m-1-2) -- (m-2-2) -- (m-3-3)(m-2-2) -- (m-3-1) ;
%		\draw[arr, -] (m-3-3) -- (m-4-2.center) -- (m-3-1);
%		\draw[arr, shorten <=0] (m-4-2.center) -- (m-5-2);
	\end{tikzpicture}
	\end{center}
	\vspace{0.5em}
	
	We have effectively changed two things: first, visually encoded the probability distribution of $A$ as the arrow $1 \to A$ (which we are now allowed to omit; sometimes you don't want priors on things, such as your own actions). Second, we have combined the two arrows $B \to D$ and $C \to D$ into a single one, $B \times C \to D$. Though certainly more verbose, this is arguably visually clearer if want to follow arrows: you cannot compute $D$ from $B$; you need both $B$ and $C$.
	
	In order to fully get the joint representation given by the BN we would also need to make the final assumption that $B \CI C \mid A$. This is possible to do with an extra arrow, but this solution doe not scale well and clutters the diagram. Instead, we will leave the picture as it is, and tackle the independence in a weaker way.
	
	\begin{defn*}
		The \emph{center} of an PCG $(\cal A, L)$ is the set of joint distributions on $\cal A$ that come closest to satisfying $\cal L$, which are of maximum entropy.
	\end{defn*}
	
	This is an alternate way of capturing the conditional independence of variables that are not required by the model to be related, without ever conditioning on ancestors, which include products\footnote{to see why this is problematic, consider that the product of all variables can always be formed, and is the ancestor of all variables. If we condition on it, then everything is always conditionally independent, as we're looking at a degenerate distribution consisting of a single outcome.}.  In some sense this is the worst case outcome for an agent intending to narrow down possibilities: a distribution in the center requires the maximum amount of information to determine the state of the world, but at the same time cannot leverage this assumption to simplify things. %The principle of maximum entropy is well established\footnote{Despite this, utilities are still thought of as fixed, minimum entropy objects.} \todo{choose references}. W
	We also get:
	
	
	\begin{restatable}{theorem}{rthm:bn-maxent} \label{rthm:bn-maxent}
		The center of the PCG obtained by transforming a Bayesian Network as described above (i.e., by inserting an extra node $X := \prod_{P \in \mathrm{pa(A)}} P$ before every node, with the appropriate projections), is a singleton set consisting of exactly the probability distribution that the BN represents.
	\end{restatable}
	

	
	We will clarify this definition and explain the connection to thermodynamics more carefully (section \ref{sec:thermo}) once we have a numerical definition of consistency. In the mean time, we continue to argue for this collection of marginals as a way of representing beliefs.
	
	\subsection{A More Formal Treatment}
	\begin{defn}
		A Baysian network (BN) is a tuple
		\[
		\mathcal B = \left(\mathcal N : \mathbf{FinSet}, ~~\mathrm{Par}: \mathcal N \to 2^{\mathcal N},~~ \mathcal S: \mathcal N \to \mathbf{FinSet},~~\Pr: \prod_{N : \mathcal N}  \left[ \mathcal S_N \times \left(\prod_{P : \mathrm{Par}(N)} \mathcal S_P\right)  \to [0,1] \right] \right)
		\]
		such that
		\begin{itemize}[nosep]
			\item the graph $\bigcup_{N, P \in \mathrm{Par}(N)}(N, P)$ is acyclic, i.e., there exists no cycle of nodes $N_0, N_1, \cdots, N_k = N_0$ in $\mathcal N^k$ such that $N_{i+1} \in \mathrm{Par}(N_i)$ for each $i \in \{0, 1, \cdots, k\}$.
			\item For all $N \in \mathcal N$, $\Pr(N)$ is a probability distribution on $\mathcal S_N$, i.e., 
			\[ \forall N\in \mathcal N.~\forall \vec{p} \in {\prod_{P : \mathrm{Par}(N)} \mathcal S_P}.~~ \sum_{n \in \mathcal S_{N}} \Pr_N(\vec{p}, n) = 1\]
		\end{itemize}
	\end{defn}

	\section{Arguments for PCGs}
	\subsection{The Possibility of Types and Embedded Logic}\label{sec:belief-typing}
	The classical picture also features a fixed set of variables. In addition to allowing new concepts to form for exogenous reasons, we would like to have inductive ways of introducing new ones logically, as combinations of the existing variables. Interpreting variables as types, whose possible assignments are terms, the syntax of which variables we can construct and what beliefs we have about the resulting picture is a type theory. Even leaving that aside, there are some obvious ways one might want to combine existing domains; the primary one we are concerned with here is products.
	
	In terms of beliefs, you might already have beliefs about how likely two events $A$ and $B$ are to occur, then suddenly wonder about how they interact. For instance, you may already have beliefs about how likely you are to leave your keys in the ignition, and also how often your car is dead, and then wonder if there's a connection. 
	In terms of preferences, you might think $a \geqc a'$ and $b \geqc b'$, but now wonder what you would do if you had to choose between $a b'$ and $a' b$. In both cases, you are slightly enlarging your picture to consider the relevant features that a classical agent would already have.
	
	For this reason, we introduce the ability to create a domain $A \times B$, if $A$ and $B$ are nodes in the picture, whose elements are the cartesian product of $A$ and $B$. %Moreover, this comes with projection links $\pi_A : A \times B \to \Delta(A)$, $\pi_B: A\times B \to \Delta B$ given by $\pi_A((a,b), a') = \delta_{a,a'}$ and $\pi_B((a,b), b') = \delta_{b,b'}$, as usual. Furthermore, we can construct a function $\langle f , g \rangle : X \to A \times B$ for any $f: X \to A$ and $g: X \to B$ that make the picture no more inconsistent.
	This is represented graphically as:
	\begin{center}
		\begin{tikzcd}
			%			&X\ar[dl,"f"']\ar[dr,"g"]\ar[d, "{\langle f, g \rangle}"description ]  \\
			A & A \times B \ar[l, "\pi_A"]\ar[r, "\pi_B"'] & B
		\end{tikzcd}
	\end{center}
	
	It may be worth noting that we can always construct $\langle f, g \rangle$, but uniqueness in general does not hold\footnote{so it is not (yet) a categorical product}. This is important because without any additional assumptions, knowing a distribution on $A$ and a distribution on $B$ does not allow you to infer a distribution on $A \times B$; there may be correlations. However, this is still a unique maximum entropy one, which assumes that the two are independent.
	
	\subsubsection{Products Variables vs Unions of Variable Sets}
	
	We could have achieved a similar thing by considering $\{A\}$ and $\{B\}$ as singleton sets of variables, and then adding $\{A,B\}$ to the picture. Here we would interpret as a set of random variable taking values in the range of the product of its elements possible values. The two accounts differ when there is an overlap between the sets. Should we be allowed to represent $A \times A$? 
	
	Taking unions does protect us from doing certain bad things: it would be a mistake to assign a distribution $\Pr(a, a') > 0$ for $ a \neq a'$, for instance --- but we're already in the business of allowing this kind of inconsistency --- in the picture below all the marginals are fine as far as the syntax is concerned, but there is no way to assign a probability distribution on all of the nodes.
	\begin{center}
		\begin{tikzcd}[dpad]
			&1 \ar[d, "p(A\times A)"] \\
			& A \times A \ar[ld]\ar[rd] & \\
			A \ar[rr,equal]&& A
		\end{tikzcd}
	\end{center}
	
	It can also be very useful to keep extra copies of $A$ around even if (part of) one already exists buried in another variable, because we can delay integration using this trick, as in example \ref{ex:bayesrule} for instance. The most compelling reason for me is that one might not know that you're looking at two copies of $A$; maybe they've been framed differently --- but still you should be able to take a product and get two things, rather than a union, which immediately leaks the truth of their equivalence to an agent.
	
	There is also a good argument that both should be allowed if the agent can unify variables, which we can leave for a future discussion.
	
	\subsubsection{Additional Types}
	\todo{Gestures towards embedding logics; equivalence between logic and type theory; in particular: conditionals, coproducts, negation, higher order nodes (beliefs about prefs, prefs about beliefs)}	

		\subsection{Human Beliefs as Marginals}\label{sec:human-belief-marginals}
	From a modeling perspective, the biggest reason for keeping track of marginals rather than joint probability distributions, is that it can represent lots of information tracked by people, that BNs regard as incomplete. As seen earlier, the major feature we admit that is prohibited in a Baysian Network is a merging of two paths, where neither is a projection.
	
	In our case, we also have the possibility of branching and merging. In our picture, a diagram $A \rightarrow C \leftarrow B$ represents two probability distributions $\Pr(C\mid A)$ and $\Pr(C \mid B)$.
	Having this kind of information is both common and not representable as a (single) probability distribution. Scientific studies never control for everything that is relevant, so you're left with marginals which may not tell the whole story.
	
	\begin{example}
		After reading a number of empirical studies, you come to believe that smokers have a 70\% chance of developing cancer, compared to 20\% for non-smokers. At the same time, you believe that those who use tanning beds have a 80\% chance of developing cancer, compared to 18 \% for those who do not use them. You have no information about how the two interact.
	\end{example}
	
	\begin{example}
		You are on a game show, and offered a choice between several levers $(A)$; your choice will determine how much money you receive. You are uncertain what each lever does, but you do have a vague intuition about the mechanism, giving you a distribution over amounts for each lever ($\Pr(C \mid A)$). You also had enough time to read statistics about how well people have done in the past $(\Pr(C))$. You do not have any information about what levers they've chosen though, nor do you have a complete joint probability $\Pr(A, C)$. In fact, having an accurate probability on $A$ alone would seem to undermine your agency.
	\end{example}
	
	
	So information of this form may not be entirely complete, be contradictory, or make it seem as though the choice is an illusion. This ``outside view'' is also important for constructing Newcomb's problem:
	
	\begin{example}[Newcomb]
		There are two boxes. Box 1 is clear and visibly contains \$1k; box 2 is opaque, and will contain \$1M if a predictor (which you know has been very accurate in the past) predicts you will leave box 1, taking just this box, and will contain nothing if the predictor predicts you will take the first box. You have to choose whether or not to take the visible box (there's no reason not to take box 2).
		
		In the diagram below, you and your past choices determine the prediction $P$, as given by the blue line, but you do not have access to this information. They also determine your action $A$ in some sense, which is the right dashed arrow, which you only know for certain after you make your decision. Because the two processes cannot exchange information (i.e., the predictor cannot decide after your action has been made, and you don't get to know the prediction), these two processes determine the process $\mathrm{Identity} \to A \times P$.
		\begin{center}
			\begin{tikzcd}[dpad]
					A \ar[drr, bend left=70, pink] & \text{Your Past}\ar[dr, blue, dashed]\ar[l, dashed]\ar[d,dotted, gray]\\
					 & A \times P \ar[lu]\ar[r]\ar[dl]\ar[d] & P \\ 
				\$\$ & \mathbbm{1}_{A\stackrel{?}{=}P} & \mathsf 1 \ar[l]
			\end{tikzcd}
		\end{center}
		Now, on the one hand, you have a logical picture of how an action and prediction together will impact whether the predictor is right (the arrow $A \times P \to A \stackrel{?}{=} P$). On the other hand, you also have an outside view of how likely the predictor is to be right (the arrow $1 \to A \stackrel{?}{=} P$). 
		
		We have not resolved the paradox; we have merely represented the beliefs in the setup, which make very little sense as a BN.
		%		More weirdness comes from the fact that $A$ and $P$ interact.
	\end{example}
	
	At least classically, beliefs are only half of the picture. Keeping around marginals is also more psychologically plausible for preferences.
%	\section{Emulation: Other Models Of Uncertainty}
	
	
	\section{Co-algebraic Structure}
	So far, we have not made a big deal out of our extra element $\bullet$ that we have included into every domain, but the fact that we have exempted ourselves from giving it a distribution turns out to dramatically increase the representational capacity of the model. Of course, we will be able to say less about this larger class of models, but many parts of the story will translate cleanly.
	
	\subsection{Bundling and Unbundling} 
	To begin, by relaxing the restriction on our links from stochastic to sub-stochastic matrices, we have gifted ourselves the ability to un-bundle and re-bundle variables back together. One should think of this as a way of building ``left-handed'' or inductive constructions, rather than ``right-handed'' or co-inductive ones. This will allow us to represent game trees and automata, as well as
	
	\begin{center}
		\todo{DIAGRAM 1}
		\begin{tikzpicture}
		
		\end{tikzpicture}
	\end{center}
	
	\subsection{Branching}
	


	\section{Reduction: Fragment of Factor Graphs}
	\section{Reduction: Game Trees and Automata}


	\section{Category Theory}

	We can also define these structures more compactly (and in my view, cleanly) with a categorical notation, perhaps shining some light on how the different definitions fit together, justify some design decisions, and import a giant mathematical toolbox which makes some features completely obvious. If you're sold already, you can skip next bit (section \ref{sec:cat-defense}) in which I attempt to give a more complete justification, and if you are not willing to parse abstract nonsense, you can skip this section entirely.
	
	\subsection{Why Use Category Theory?}\label{sec:cat-defense}
	

%	\subsubsection{Entropic 2-Category}
	\subsection{Categorical Definition}\label{sec:cat-defs}
	We'll start with the fragment of 
	
	\begin{defn}
		A \emph{categorical PCG} is a diagram in $\bf Mark$ of shape $\cal A$ --- that is, a functor $\mathcal A \to \mathbf{Mark}$, where $\mathcal A$, thought of as attention, is (the category generated by) the graph whose nodes and edges are relevant features of the problem setup.
	\end{defn}
	
	This makes sense, as these topological graphs
	\subsection{Limits } \label{sec:cat-worlds}
	
	We can use this definition to 
	
	\begin{defn}
		A \emph{world object} of an PCG $G : \mathcal A \to \mathbf{Mark}$ is a weak limit of $G$ --- that is, a cone over $G$, for which there exists a (possibly non-unique) arrow 
	\end{defn}

	\begin{example}
		If $\mathcal A$ is a discrete category, with $n$ elements, then PCGs of shape $\mathcal A = \{\tilde X_1, \tilde X_2, \ldots, \tilde X_n\}$ is just a set of $n$ names (with identities), to be interpreted by $M$ as random variables (with their identities). If $M : \mathcal A \to \mathbf{Mark}$ is a (weak) final cone, as illustrated below.
		\[ \begin{tikzcd}
				&& Y  \ar[ddll, gray, "f_1"']\ar[ddl,gray, "f_2"description]\ar[ddrr,gray, "f_n"] \ar[d, dashed, "\exists!"]\\
				&& \mathcal W \ar[dll, "\pi_1"description]\ar[dl, "\pi_2"description]\ar[drr, "\pi_n"description] \\
				X_1 & X_2 & \cdots & & X_n
			\end{tikzcd} \]
			
		We claim that $\cal W$ is just the standard product of measurable spaces, and each $\pi$ is a projection. The reason for this can be seen information theoretically --- clearly the product of all of the variables with projections is a cone over the $M$, since there are no non-trivial arrows in $\cal A$ so no equations must be satisfied. %Also, any other cone has factors through it in the obvious way. 
		
		The only sticking point is the, possibility that it's not  %and each projection has to have entropy zero. 
	\end{example}

	This has some huge advantages over defining a world up front. First of all, we don't need to describe the set of all possible worlds globally and in a way that people can agree on. 
	
	We do not get the problem in \ref{sec:world-explosion}.
	
	% Explosion of possible worlds by unwrapping and then product...
	
	\begin{example}
		\todo{projections}
	\end{example}
	
	\begin{example}
		\todo{filtered limit}
	\end{example}

	\begin{conj}
		% colimit of diagram
	\end{conj}
	
	\subsection{Bundling and the Category of Elements}
	\subsection{Meta-links and Higher Structure}
	\subsection{Denotational Semantics}
	
	\part{Values on Probabilistic Constraint Graphs}
	
	The first strand of this project is a general representation of many ways that have been historically used to represent values: namely, utilities, goals, desires, preferences, choice functions, and natural extensions of these suggested by this representation, some of which are also well-studied. 
	
	\section{Setup}
	
	% There are two key insights / differences:
	% (1) allowing for a variety of scopes: you don't need to supply the data on all possible configurations at once.
	% (2) taking a categorical perspecitve, viewing these data as all special kinds of maps.
	
	The general idea is that to attach some additional preference data to each alternative in a domain
	
	\subsection{Definitions}
	

	
	\section{Reductions}
	
	The standard tool to represent values (and the ones that people are most familiar with) are orders, i.e., flat categories. 
	
	The simplest non-trivial order is:
	\begin{center}
		\raisebox{0.7em}{$\mathbb B = $}~\begin{tikzpicture}
			\node[pt=0] at (0,1){};
			\node[pt=1, right=1.2em of 0] {};
			\coordinate[below=0.3em of 0] (gutter);
			\node[right=0.6em of 0,anchor=center]{$\leqc$};
			\node[draw=gray, rounded corners=2, inner sep=0.5em, fit={\lab{0}\lab{1}(gutter)}] {}; 
		\end{tikzpicture}
	\end{center}
	
	\subsection{Desires}
	
	I want $\varphi$, is 
	
	One standard logical approach would be to give a Kripke model, so that a statment like $s \vDash W_{\alpha} \varphi$, for instance, would be true when agent $\alpha$ wants $\varphi$ in state $s$. In such a model, there is already machinery for talking about the set of all possible states (call it $\cal W$), and so effectively we have specified a function $W_\alpha \varphi: \mathcal W \to \mathbb B$, which assigns 1 to worlds where $\alpha$ wants $\varphi$ and 0 to the others.
	
	This is the behavior we would like to capture, but rather than use the set of possible worlds $\cal W$, we will adopt 
	
	
	\subsection{Utilities}
	
		
	
	A utility function $u : \Omega \to \mathbb R$ is a $\mathbb R$-value on the set of outcomes $\Omega$. 
	
	Effectively, 

	
	\subsection{Preferences}
	We can also consider orders which are not total; a discrete order encodes an inability to compare any of the options, lattices encode a possible inability to compare individual options, but provide a way to formalize the combination the best and worst features of two alternatives.
	
	
	An order is a function $\leq: \Omega \times \Omega \to \mathbbm 2$
	
	\part{Dynamics}
	In addition to the representation itself, we want to prescribe . 
	In the case where an agent is Bayesian 
	
	\section{Consistency}
	Sometimes an PCG effectively, such as when 
%	
	
	\subsection{Pairwise Consistency}
	Many of our motivating examples can be thought of as a ``behavioral inconsistency'': you value one thing, but your actions are inconsistent with it. Maybe you value sleep, are aware that to get sleep you need to go to bed early, and yet still do something you know is less valuable instead, such as checking social media. Maybe you 

	Any example where you have to domains like this.
	
	\section{Reduction: Belief Updating}
	\subsection{Conditioning}
	\subsection{Jeffrey's Rule}
	\subsection{Pearl's Rule}
	
	\section{Abstraction}
	In addition to nodes which represent specific, observable variables that we want to ensure everyone is aware of, we are in the business of allowing agents to construct nodes of their own --- examples we've seen so far include product nodes, utility nodes, and other features of the world which become suddenly salient.
	
	Here we want to deal with abstraction nodes, which often arise by approximate factorization of arrows into more manageable pieces.

	\subsection{Compression}
	\subsection{Divorcing the Specific and General Cases}
	

	
	
	\begin{example}
		Suppose you are more likely to go into work if there's no rain, and you have some probability $p$ of there being rain. A simple linear model might look like this:
		
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {$1$};
			
			\node[bpt={r|$r$}, above right=0.75em and 4 of 1.east] {};
			\node[tpt={notr|$\lnot r$}, below=1.5em of r] {};
			\node[Dom={Rain (R) around \lab{r}\lab{notr}}] {};
			%		\node[dpadded, right=2 of 1] (R) {Rain};
			
			\node[bpt={l|home}, above right=0.75em and 5.5 of R.east] {};
			\node[tpt={notl|work}, below=1.5em of l] {};
			\node[Dom={Loc (L) around \lab{l}\lab{notl}}] {};
			%		\node[dpadded, right=2 of R] (L) {Loc};
			
			\draw[archain] (1) -- node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$*$}{$r$,$\lnot r$}	p & 1-p \end{idxmat}}}
			(R) --
			node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$r$,$\lnot r$}{home,work}	.6 & .4 \\ .1 & .9 \end{idxmat}}} (L);
			\end{tikzpicture}
		\end{center}
		
		Now, suppose it happens that it's actually raining right now --- what happens to the model? The standard answer is to condition, and allow the new, specific information to overwrite the general model. Doing this is a perfectly reasonable thing, but cements a meaning of the node ``Rain'' that we may not have intended --- it now means ``it is raining today'', rather than in general. This is not a problem for a human modeler, who presumably has fit the model to data somehow, and plans on copying it for each use. There are (at least) two downsides to this. First, we've really just abdicated responsibility to a trustworthy human who will keep track of the copying for us, and handle any sort of abstraction for us. Secondly, it is not clear how and when to update the model based on new experiences.
		
		In the meta-theory, this gives the human in charge an indexed family of variables:
		
		\[ \left\{~\begin{tikzpicture}[center base]

			\node[dpadded] (1) at (0,0) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain$_i$};
			\node[dpadded, right=2 of R] (L) {Loc$_i$};
			
			
			\draw[archain] (1) -- node[above]{$r_i$?} (R) -- node[above]{$q$} (L);
		\end{tikzpicture}~\right\}_{i \in \text{Experiences}} \]
		Of course, we can also internalize this to the agent's picture (and given that our formalism is entirely built around conditioning on variables, it seems silly not to) --- a picture that might look like this:
				
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0.5,-0.5) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain};
			\node[dpadded, right=2 of R] (L) {Loc};
			
			\foreach [evaluate=\x as \y using \x/2 + 2] \x in {0, 1, ...,4} {
				\node[bpt={e\x | $e_\x$}] (e\x) at (\y,-3) {};
			}
			\coordinate (Q) at (2,-3.2);
			\node[bDom={Experiences (E) around \lab{e0}\lab{e4}(Q)}] {};
			
			\draw[archain] (E) -- node[left]{$p_i$} (R) -- node[above]{$q$} (L);
			\draw[archain] (1) -- node[above]{$\tilde p$} (R);
			\end{tikzpicture}
		\end{center}
	
		We store concrete values of whether or not it rained (with possible uncertainty due to lack of memory) in the ``Experiences'' node. Note that the picture we've drawn below is not expressible with any standard graphical model, because has an arrow merge, which we interpret as a constraint, and so we get non-standard behavior --- our model can be inconsistent, for instance, if the general probability of rain $\tilde p = 0$ when it has rained. 
		
		We gain the additional benefit of 
		
		We can also play tricks by bundling and un-bundling this experience variable:
		
		%Compression
	\end{example}

	\section{Value Updating}
	In the case where values are represented as a utility 
	
	\section{Thermodynamic Analogy}
	
	

	\part*{Appendix}
	\appendix

	\section{Category Theoretic Preliminaries}
	\todo{Minimal Category Theory needed to understand: categories, functors, diagrams, cones, limits}
	\subsection{Markov Category and Giry Monad}
	
	\subsection{sub-Markov Category}
	
	\section{More Arguments against the Standard Model} %todo section titles
	
	\subsection{Can't Have Priors on Everything}\label{sec:impossible-prior}
	
	
%	Just as one can get an order from a utility function, which 
		
	
	
\end{document}