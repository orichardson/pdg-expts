\documentclass{article}

\input{prefs-commands.tex}
\addbibresource{../refs.bib}
\addbibresource{../maths.bib}

\title{Merged: Dynamic Preferences}
\author{Oliver Richardson  \texttt{oli@cs.cornell.edu}}


\begin{document}
	\maketitle
	
%	\section{}

	\section*{A Quick Overview}
	This project is a model of joint preference and belief revision. The core representation we suggest, the marginal constraint graph, is more flexible than the standard ones, as agents need not always have a perfectly consistent picture of reality, intermediate representations have meanings, and we can capture phenomena such as learning preferences from data and revising them in various places. In special cases, we recover standard results, including expected utility calculations, belief updating by conditioning.
	
	We can roughly separate the work into two pieces:
	\begin{enumerate}%[nosep]
		\item Representation: the sufficiency of our new representation as a unification of existing results. In particular, we think of an agent's values as distributed across a network of concepts, whose nodes are preferences on small sets of alternatives, and whose edges are beliefs about the impact of one variable on another.
		\item Dynamics: A formulation of how to revise the representation in light of new information or additional computation. We will show how can be dealt with through inconsistency, illustrate some options for regularization, and ultimately factor out these as general meta-preferences. 
	\end{enumerate}
	
	\part{Motivations}
%	This work an be motivated in many different ways; I imagine the most compelling one to depend on who you are and what you already care about. In any individual paper I plan to only include one of these, but until the audience has been narrowed, I want to keep all of them around. %to remind myself which directions I want to push, and to record selling points outside of the best publishing path.

	\begin{center}
	\begin{tikzpicture}
		\node[ellipse,draw,align=center] (R) at (4,-3) {World\\ Representation};
		\node[ellipse,draw] (B) at (3,1) {Beliefs};
		\node[ellipse,draw] (V) at (3,-1) {Values};
		\node[ellipse,draw] (A) at (5.5,0) {Actions};
		
		\draw[arr,-] (B) -- (4,0) -- (V);
		\draw[arr, shorten <=0] (4,0) -- (A);
 	\end{tikzpicture}
	\end{center}

	\section{Good Reasons Not To Maximize Utility}
	\begin{enumerate}
		\item \textbf{No utility function}. Maybe your preferences are not representable in this way --- maybe they violate the vNM axioms, or you just don't have anything that resembles an ordered set. Maybe your intrinsic utility is stochastic.
		
		\item \textbf{No complete prior.} It is possible that you're not a Bayesian, and don't have (access to) a probability distribution on all salient features of your world\footnote{In fact, once we add enough representational complexity to our world, this is impossible to have; see section \ref{sec:impossible-prior}}.  
%		Of course, if you accept Savage's axioms, then you act as though you have both a probability distribution over states and a utility function
		
		\item \textbf{Thinking takes time.} (this constrains the complexity of your utility and probability information, or at least requires them to be in a form that admits fast computations)
		
		\item \textbf{Internal Actions.} In addition to the external actions, there are also invisible mental ones: nobody has complete control over what they do in all respects at all times. Emotionally preparing yourself to talk to your boss, warming up your fingers to play piano, and intentionally committing a phone number to memory are all examples of mental actions, which are rarely modeled. More generally, for any interface with the world that a model prescribes, there will be extra 
		
		 %you may not know exactly what it is you have to do in orde to 
		\item \textbf{Different world representation.} 
%			You have a distorted perception of the world. 
		Models, both in probabalistic reasoning and decision theory, usually start by defining the set of possible states of the system. For a modal logician, this is the underlying set of a Kripke structure; for probability theorists, it's a set of outcomes. Everything works well if you set up a model in which the agents are aware of the structure you've set up, but simply don't know what world they're in --- but nothing precludes them from conceptualizing things in entirely different terms
			
		There are of course ways of putting together
		
		
		\item \textbf{Things Change over Time.} To 
	\end{enumerate}	

	Despite the fact that there has been work backing off of each of these assumptions, utility maximization is still ubiquitous (and taken for granted) in most primary applications, including consumption models and the construction of artificial agents. In section \ref{sec:optimization-bad} I argue that this is the source of some cultural problems. But part of the reason that many of the other techniques has not gained traction is that none is as clean and as useful as expected utility maximization, and to the best of my knowledge people try to isolate and tackle these issues individually. My framework provides a way of representing many of these issues effectively and cleanly, while also reducing to the expected utility computations when none of these issues apply. 


	\section{Desiderata}
	
	
%	Currently preferences are thought of as static objects, fixed as part of the structure and identity of an agent, independent of beliefs, complete, and over some fixed domain. This is clearly not at all how human preferences work, and I posit that it's not the right way to think of preference for synthetic agents either.
	
	Perhaps among other things, any model of preference dynamics that is to be used to build synthetic agents which have control over important decisions, should have the following properties: 
	\begin{enumerate}[noitemsep]
		\item Provide an answer to the `value loading' problem: show how you can learn ``reasonable'' preferences by interacting with the world
		\item Reduce to static models for some parameter settings
		\item Behave reasonably when combined with changes of perspective 
		\item Be resistant to standard challenges to irrationality, such as dutch book arguments
		\item Have weak safety guarantees: an agent should not eagerly adopt preferences which are totally in conflict with its current ones
	\end{enumerate}
	
	%	We might also require that a theory be useful for explaining human behavior. We expect ours to be able to explain:
	Because the view of preferences we adopt here is different from the standard ones in economics (in particular, it lends itself naturally to incorporating boundedness), we have a hope of explaining some behavior which was previously classified irrational, as an optimal in some sense. The following psychological effects lend themselves to explanation:
	\begin{enumerate}[noitemsep]
		\item \textbf{Value Capture.} You really care about $X$ (say, learning math), which is vague and hard to measure, so you come up with some metric $Y$ (say, scores on undergrad math exams). Over time, optimizing for $Y$ will cause you to optimize less effectively for $X$ and assign intrinsic value to $Y$ (getting good exam scores becomes valuable, independent of whether you learn math\footnote{Social signaling plays into this, but this occurs also in cases where people try to hide the signal: constant grade checking, pokemon go addiction, etc.}). Related to Goodhart and Cambell's laws.
		\item \textbf{Framing Effects.} It is well-documented that the style of presentation, even for logically equivalent scenarios, can have a significant impact on a person's choice. But if we formalize these as the same outcome, there's no way for utility-maximizing agent to behave this way.
		\item \textbf{Connoisseur Effects.} Someone who listens to a lot of rap music has more nuanced, complicated preferences on rap music than someone who has not heard as much. Similarly, people work to develop palates for wine, and ``all Indian food tastes the same'' is an insult, indicating a shallow experience with the cuisine. There are two technical aspects: first, additional experiences increase preference complexity.
		
		\item \textbf{Adaptive Preferences.} Even things we find objectionable are normalized over time, and people often change to prefer things they're used to, even if they are initially opposed. This is sometimes thought of as a prioritization of safety, and is maybe best thought of as a thought-saving feature: the things you're used to have gotten you this far already. 
		
		\item \textbf{}
		
		\item \textbf{Novelty Effects (anti-adaptive preferences).} 
		
	\end{enumerate}

	\section{Problems with the Standard Representation}
	\subsection{Fixed World Representation}
	
	The set of things which are even considered possible is contentious enough that we don't want to build it (and actually the correct one) canonically into an agent's picture of the world. In some sense only the current world is possible, as far as anyone can be sure. Moreover, no matter how you chose to represent the world, an agent's picture of what's possible can change over time, which means the modeler needs to write down beforehand everything that could possibly happen. We talk more about this in section \ref{sec:cat-worlds}
	
	\subsubsection{Possible World Explosion} \label{sec:world-explosion}
	
%	These are features that we would like to explain in humans, but needn't necessarily be essential .

%	\section{A Temporal Extension}
%	
%	Many issues with decision theory can be attributed to a failure to account for the passage of time. 
%	One popular example of this is the ubiquitous assumption of cognitive unboundedness: in a static world, where everything is frozen except your own thoughts, you can afford to do decision theory exactly by computing expected utility. You have only one decision to make
%	
%	Still, cognitive boundedness is only one example. When we move to a dynamic setting, most classical results no longer apply, though people often like to pretend that they do. 
%	\begin{enumerate}
%		\item There's no reason to 
%	\end{enumerate}
%	
%	If your beliefs
	
	
%	\section{Better Models of Humans}


	\clearpage
	\part{Marginal Constraint Graphs}
	The basic tool that we will use to solve these problems is a graphical representation of (a piece of) an agent's mental space, which we call a marginal constraint graph. The standard picture of worlds is a 
	
	We allow agents to add additional nodes and edges (form new concepts and connections)
	
	MCGs generalize , and as such are 
	
, these diagrams look and behave like Bayesian Networks
	
	\section*{Informal Definitions and Semantics}
	\begin{defn*}
		A marginal constraint graph $(\cal A, L)$ is a collection of variables $\cal A$, attached to each of which is some preference information: (this could be an order, utility, pairwise utility matrix, a supremum function), plus a collection of probabilistic links $\cal L$ between some (but not necessarily all) pairs of them, where $L: A\to B \in \cal L$ is a (sub) Markov kernel $A \to \Delta (B \cup \{\bullet\})$\footnote{The additional ``phantom element'' $\bullet$ absorbs probability density that we don't want to equivocate on, allowing our model to capture partial families of conditional probabilities, by extension things like implication, and giving agents more tools to avoid inconsistency. This has the effect of making our links substochatic matrices/kernels rather than stochastic ones. However, if we restrict to beliefs which assign zero probability to $\bullet$, everything in the model works as before. See section \ref{sec:substochastic} for details} representing beliefs about how a setting of $A$ to $a \neq \bullet$ will impact the value of $B$. 
	\end{defn*}
	
	Variables can be thought of equivalently as:
	\begin{itemize}[nosep]
		\item random variables $X$ which can take on values $\{x_i\}$ (or possibly none, which we denote $\bullet$)
		\item sets $X$ with elements $\{x_i\}$
		\item partitions of the universe of outcomes (which is not fixed) into disjoint (but possibly not exhaustive) events. Disjointness here is very weak and can be forced by adding tagging outcomes with additional data, analogous to a disjoint union.
	\end{itemize}
	
	In this document, we will focus on models where the preference information is encoded as a special utility domain $U$, with links from other variables. While there may be some propositions tying this case to utilities or preference orders, we will avoid talking explicitly about the more general setting of preference matrices and choice functions out for now.
	
		

%	\part{Intuitions }

	\section{Emulation: Bayesian Networks}
	\section{Emulation: Sets of Probability Distributions}
	\section{Emulation: Expected Utility}
	
	
	\section{Co-algebraic Structure}
	So far, we have not made a big deal out of our extra element $\bullet$ that we have included into every domain, but the fact that we have exempted ourselves from giving it a distribution turns out to dramatically increase the representational capacity of the model. Of course, we will be able to say less about this larger class of models, but many parts of the story will translate cleanly.
	
	\subsection{Bundling} 
	To begin, by relaxing the restriction on our links from stochastic to sub-stochastic matrices, we have gifted ourselves the ability to un-bundle and re-bundle variables back together. One should think of this as a way of building ``left-handed'' or inductive constructions, rather than ``right-handed'' or co-inductive ones. This will allow us to represent game trees and automata, as well as
	
	\begin{center}
		\todo{DIAGRAM 1}
		\begin{tikzpicture}
		
		\end{tikzpicture}
	\end{center}

	


	\section{Reduction: Fragment of Factor Graphs}
	\section{Reduction: Game Trees and Automata}
	\section{Formalism}
	\begin{defn}\label{def:mcg}
		A marginal constraint graph (MCG) is a tuple 
		\[ \left(\mathcal N : \mathbf{FinSet},~~\mathcal L : 2^{\cal N \times N},~~ \langle\mathcal S, \Sigma\rangle : \mathcal N \to \mathbf{MeasSet}, ~~\mathbf p : \prod_{(A,B): \mathcal L} \left[ \Big. \mathcal S_A \times \Sigma_B \to [0,1]\right] \right) \]
		where $p(L)$ is a Markov kernel, i.e., for every $L[A,B] : \mathcal L$, and $a \in \mathcal S_A$, $\mathbf p_L[a \mid \cdot~]$ is a probability distribution on $(\mathcal S_B, \Sigma_B)$, and $\mathbf p_L[~\cdot \mid B]$ is $\Sigma_A$-measurable for every $B \in \Sigma_B$.
	\end{defn}

	\section{Category Theory}

	We can also define these structures more compactly (and in my view, cleanly) with a categorical notation, perhaps shining some light on how the different definitions fit together, justify some design decisions, and import a giant mathematical toolbox which makes some features completely obvious. If you're sold already, you can skip next bit (section \ref{sec:cat-defense}) in which I attempt to give a more complete justification, and if you are not willing to parse abstract nonsense, you can skip this section entirely.
	
	\subsection{Why Use Category Theory?}\label{sec:cat-defense}
	

%	\subsubsection{Entropic 2-Category}
	\subsection{}
	We'll start with the fragment of 
	
	\begin{defn}
		A \emph{categorical MCG} is a diagram in $\bf Mark$ of shape $\cal A$ --- that is, a functor $\mathcal A \to \mathbf{Mark}$, where $\mathcal A$, thought of as attention, is (the category generated by) the graph whose nodes and edges are relevant features of the problem setup.
	\end{defn}
	
	This makes sense, as these topological graphs
	\subsection{Better Worlds.} \label{cat-worlds}
	

	
	We can use this definition to 
	
	\begin{defn}
		A \emph{world object} of an MCG $G : \mathcal A \to \mathbf{Mark}$ is a limit of $G$.
	\end{defn}

	\begin{example}
		If $\mathcal A$ is a discrete category, with $n$ elements, then MCGs of shape $\mathcal A = \{\tilde X_1, \tilde X_2, \ldots, \tilde X_n\}$ is just a set of $n$ names (with identities), to be interpreted by $M$ as random variables (with their identities). If $M : \mathcal A \to \mathbf{Mark}$ is a final cone, as illustrated below.
		\[ \begin{tikzcd}
				&& Y  \ar[ddll, gray, "f_1"']\ar[ddl,gray, "f_2"description]\ar[ddrr,gray, "f_n"] \ar[d, dashed, "\exists!"]\\
				&& \mathcal W \ar[dll, "\pi_1"description]\ar[dl, "\pi_2"description]\ar[drr, "\pi_n"description] \\
				X_1 & X_2 & \cdots & & X_n
			\end{tikzcd} \]
			
		We claim that $\cal W$ is just the standard product of measurable spaces, and each $\pi$ is a projection. The reason for this can be seen information theoretically --- clearly the product of all of the variables with projections is a cone over the $M$, since there are no non-trivial arrows in $\cal A$ so no equations must be satisfied. %Also, any other cone has factors through it in the obvious way. 
		
		The only sticking point is the, possibility that it's not  %and each projection has to have entropy zero. 
	\end{example}

	This has some huge advantages over defining a world up front. First of all, we don't need to describe the set of all possible worlds globally and in a way that people can agree on. 
	
	We do not get the problem in \ref{sec:world-explosion}.
	
	% Explosion of possible worlds by unwrapping and then product...
	
	\begin{example}
		\todo{projections}
	\end{example}
	
	\begin{example}
		\todo{filtered limit}
	\end{example}

	\begin{conj}
		% colimit of diagram
	\end{conj}
	
	\subsection{Bundling and the Category of Elements}

	
	\part{Values on Marginal Constraint Graphs}
	
	The first strand of this project is a general representation of many ways that have been historically used to represent values: namely, utilities, goals, desires, preferences, choice functions, and natural extensions of these suggested by this representation, some of which are also well-studied. 
	
	\section{Setup}
	
	% There are two key insights / differences:
	% (1) allowing for a variety of scopes: you don't need to supply the data on all possible configurations at once.
	% (2) taking a categorical perspecitve, viewing these data as all special kinds of maps.
	
	The general idea is that to attach some additional preference data to each alternative in a domain
	
	\subsection{Definitions}
	

	
	\section{Reductions}
	
	The standard tool to represent values (and the ones that people are most familiar with) are orders, i.e., flat categories. 
	
	The simplest non-trivial order is:
	\begin{center}
		\raisebox{0.7em}{$\mathbb B = $}~\begin{tikzpicture}
			\node[pt=0] at (0,1){};
			\node[pt=1, right=1.2em of 0] {};
			\coordinate[below=0.3em of 0] (gutter);
			\node[right=0.6em of 0,anchor=center]{$\leqc$};
			\node[draw=gray, rounded corners=2, inner sep=0.5em, fit={\lab{0}\lab{1}(gutter)}] {}; 
		\end{tikzpicture}
	\end{center}
	
	\subsection{Desires}
	
	I want $\varphi$, is 
	
	One standard logical approach would be to give a Kripke model, so that a statment like $s \vDash W_{\alpha} \varphi$, for instance, would be true when agent $\alpha$ wants $\varphi$ in state $s$. In such a model, there is already machinery for talking about the set of all possible states (call it $\cal W$), and so effectively we have specified a function $W_\alpha \varphi: \mathcal W \to \mathbb B$, which assigns 1 to worlds where $\alpha$ wants $\varphi$ and 0 to the others.
	
	This is the behavior we would like to capture, but rather than use the set of possible worlds $\cal W$, we will adopt 
	
	
	\subsection{Utilities}
	
		
	
	A utility function $u : \Omega \to \mathbb R$ is a $\mathbb R$-value on the set of outcomes $\Omega$. 
	
	Effectively, 

	
	\subsection{Preferences}
	We can also consider orders which are not total; a discrete order encodes an inability to compare any of the options, lattices encode a possible inability to compare individual options, but provide a way to formalize the combination the best and worst features of two alternatives.
	
	
	An order is a function $\leq: \Omega \times \Omega \to \mathbbm 2$
	
	\part{Dynamics}
	
	\section{Consistency}
	\subsection{Pairwise Consistency}
	Many of our motivating examples can be thought of as a ``behavioral inconsistency'': you value one thing, but your actions are inconsistent with it. Maybe you value sleep, are aware that to get sleep you need to go to bed early, and yet still do something you know is less valuable instead, such as checking social media. Maybe you 

	Any example where you have to domains like this.
	
	\section{Reduction: Belief Updating}
	\subsection{Conditioning}
	\subsection{Jeffrey's Rule}
	\subsection{Pearl's Rule}
	
	\section{Abstraction}
	In addition to nodes which represent specific, observable variables that we want to ensure everyone is aware of, we are in the business of allowing agents to construct nodes of their own --- examples we've seen so far include products, utility nodes, 
	
	Here we want to deal with abstraction nodes, which often arise by approximate factorization of arrows into more manageable pieces.
	
	\subsection{Divorcing the Specific and General Cases}
	

	
	
	\begin{example}
		Suppose you are more likely to go into work if there's no rain, and you have some probability $p$ of there being rain. A simple linear model might look like this:
		
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0,0) {$1$};
			
			\node[bpt={r|$r$}, above right=0.75em and 4 of 1.east] {};
			\node[tpt={notr|$\lnot r$}, below=1.5em of r] {};
			\node[Dom={Rain (R) around \lab{r}\lab{notr}}] {};
			%		\node[dpadded, right=2 of 1] (R) {Rain};
			
			\node[bpt={l|home}, above right=0.75em and 5.5 of R.east] {};
			\node[tpt={notl|work}, below=1.5em of l] {};
			\node[Dom={Loc (L) around \lab{l}\lab{notl}}] {};
			%		\node[dpadded, right=2 of R] (L) {Loc};
			
			\draw[archain] (1) -- node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$*$}{$r$,$\lnot r$}	p & 1-p \end{idxmat}}}
			(R) --
			node[above, fill=white, fill opacity=0.8]{\raisebox{2em}{\begin{idxmat}{$r$,$\lnot r$}{home,work}	.6 & .4 \\ .1 & .9 \end{idxmat}}} (L);
			\end{tikzpicture}
		\end{center}
		
		Now, suppose it happens that it's actually raining right now --- what happens to the model? The standard answer is to condition, and allow the new, specific information to overwrite the general model. Doing this is a perfectly reasonable thing, but cements a meaning of the node ``Rain'' that we may not have intended --- it now means ``it is raining today'', rather than in general. This is not a problem for a human modeler, who presumably has fit the model to data somehow, and plans on copying it for each use. There are (at least) two downsides to this. First, we've really just abdicated responsibility to a trustworthy human who will keep track of the copying for us, and handle any sort of abstraction for us. Secondly, it is not clear how and when to update the model based on new experiences.
		
		In the meta-theory, this gives the human in charge an indexed family of variables:
		
		\[ \left\{~\begin{tikzpicture}[center base]

			\node[dpadded] (1) at (0,0) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain$_i$};
			\node[dpadded, right=2 of R] (L) {Loc$_i$};
			
			
			\draw[archain] (1) -- node[above]{$r_i$?} (R) -- node[above]{$q$} (L);
		\end{tikzpicture}~\right\}_{i \in \text{Experiences}} \]
		Of course, we can also internalize this to the agent's picture (and given that our formalism is entirely built around conditioning on variables, it seems silly not to) --- a picture that might look like this:
				
		\begin{center}
			\begin{tikzpicture}
			\node[dpadded] (1) at (0.5,-0.5) {$1$};
			\node[dpadded, right=2 of 1] (R) {Rain};
			\node[dpadded, right=2 of R] (L) {Loc};
			
			\foreach [evaluate=\x as \y using \x/2 + 2] \x in {0, 1, ...,4} {
				\node[bpt={e\x | $e_\x$}] (e\x) at (\y,-3) {};
			}
			\coordinate (Q) at (2,-3.2);
			\node[bDom={Experiences (E) around \lab{e0}\lab{e4}(Q)}] {};
			
			\draw[archain] (E) -- node[left]{$p_i$} (R) -- node[above]{$q$} (L);
			\draw[archain] (1) -- node[above]{$\tilde p$} (R);
			\end{tikzpicture}
		\end{center}
	
		We store concrete values of whether or not it rained (with possible uncertainty due to lack of memory) in the ``Experiences'' node. Note that the picture we've drawn below is not expressible with any standard graphical model, because has an arrow merge, which we interpret as a constraint, and so we get non-standard behavior --- our model can be inconsistent, for instance, if the general probability of rain $\tilde p = 0$ when it has rained. 
		
		We gain the additional benefit of 
		
		We can also play tricks by bundling and un-bundling this experience variable:
	\end{example}
	
	
	\appendix
	\section{Category Theoretic Preliminaries}
	\subsection{Markov Category and Giry Monad}
	\subsection{sub-Markov Category}
	
	\section{More Arguments against the Standard Model} %todo section titles
	
	\subsection{Can't Have Priors on Everything}\label{sec:impossible-prior}
	
	
%	Just as one can get an order from a utility function, which 
		
	
	
\end{document}