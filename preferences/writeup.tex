\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb, amsthm}
\usepackage[backend=biber,
	style=alphabetic,
	%	citestyle=authoryear,
	natbib=true,
	url=true, 
	doi=true]{biblatex}

\addbibresource{../refs.bib}

\usepackage{microtype}
\usepackage{tikz}

\usepackage{color}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage[colorlinks=true, citecolor=deepgreen]{hyperref}

\newcommand\geqc{\succcurlyeq}
\newcommand\leqc{\preccurlyeq}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\usepackage{parskip}
\usepackage{enumitem}


\begin{document}
	\section{What I want to do}
	I'm trying to get a theory of preference dynamics.
	
	Currently preferences are thought of as static objects, fixed as part of the structure and identity of an agent, independent of beliefs, complete, and over some fixed domain. This is clearly not at all how human preferences work, and I posit that it's not the right way to think of preference for other agents either.
	
	A good model of preference dynamics should:
	\begin{enumerate}[noitemsep]
		\item An answer to the `value loading' problem: show how you can acquire ``reasonable'' preferences by interacting with the world
		\item Reduce to static models for some parameter settings
		\item Behave reasonably when combined with changes of perspective 
		\item Be resistant to dutch booking
		\item Have weak safety guarantees: an agent should not eagerly adopt preferences which are totally in conflict with its current ones
	\end{enumerate}

	We might also require that a theory be useful for explaining human behavior. We expect ours to be able to explain:
	\begin{enumerate}[noitemsep]
		\item Value Capture
		\item Framing Effects
		\item Preference Conflict Resolution
		\item The Conniseur effect
%		\item Cultural Revolution
	\end{enumerate}
	
	\section{Examples and Use Cases}
	
	\subsection{Serving Content}
	Since humans' preferences are not always dynamic, it is a mistake to design recommender systems as though they were. The problem of designing a recommender system is almost always framed as the task of deciding what content to display to what user at a single point in time, based on information about the content and the user, which hides potentially dynamic elements of preferences. Such a model can still accurately predict that a user's preferences have changed by looking at updated user data and considering the new user an entirely different person, but the approach of fixing this issue by trying to get more recent feature data, necessarily lags behind that source of data. While in principle it is may be possible to predict where a person's tastes are likely to go in the future, this kind of thinking mirrors only the recent past.
	
	Similarly, issues with recommender systems zeroing in on a particular niche preference set, and serving content only similar to the content the user has seen before. However, sometimes the similarity of the content is actually a detriment. For instance, if you've already watched an instructional video on integration by parts, you are less likely to click on other such videos, rather than more likely --- despite content similarities.
	
	\subsection{Framing Problems}
	It is well documented that people exhibit different preferences depending on the context and manner in which the preferences are elicited, and particularly the framing of the question. This fact is a big problem for preference theories which only model outcomes of the world, as they cannot distinguish between differing framings of the same logical scenario. 
	
	\subsection{Learning Preferences from Samples}
	There is also the question of where we got our preferences in the first place. While it might be reasonable to think that a person can be born with \textit{a propensity to like video games}, it's crazy to think that people are born with actual preferences about video games. Archimedes probably did not have preferences about what video games are good, but also it would be weird if he were unable to form any.
	
	A static theory of preferences fails to provide an account of how preferences form in the first place, making them useless for understanding reasons for motivations: at the end (or if you're a logically omniscient being that economists model, at the beginning), preferences just boil down to unquestionable axioms, that could have been anything. Instead, we would like to be able to provide an account of how preferences are be formed over new concepts, implicitly providing some restrictions on the kinds of preferences it is possible to have for a given environment. By doing so, we can get a tighter bound on what agents can actually value, giving us both more predictive power, and reducing the search space for trying to infer them, in inverse reinforcement learning, content recommendation, and so forth.
	
	\subsection{Gamification}
	Because static EU utilities assume that there is no 
	
	A particular effect that we would like to model is \textit{value capture}---
	
	\subsection{Specification of Robot Preferences}
	The problem with simply giving an agent an objective to optimize, is that people are really bad at understanding what optimizing any given objective actually means until they see it. In practice, people mis-specify objectives all the time, and are often mistakenly under the impression that they've incentivized the agent to do something slightly different. There are even numerous fairy tales and parables cautioning against this, which can be summed up with the aphorism ``be careful what you wish for''
	
	People don't have to be all that careful what they wish for around their friends and family, however, because they do not express their desires in such clear terms as objective functions, and the wishes they express are not taken literally, but rather in context with the other preferences this person has expressed, and the social context. A theory of preference dynamics would allow for both partial specification and overspecification, which may be a step towards corrigibility.
	
	\section{Formalism}
	We assume that each time $t$, we have a collection of domains $\mathcal D_t$, each of which admits an ordering. 
	
	\section{Revisited Examples}
	
	\subsection{Serving Content}
	\subsection{Framing Problems}
	\subsection{Learning from Samples}	
	\subsection{Gamification}
	\subsection{Specification of Robot Preferences}
			
	\section{CP Net Problems}
	The CP semantics are in many ways appealing: the dominance of $a$ over $\bar a$ independent of context seems to be a really nice way of capturing the utility brought by $a$ independent of the context of everything else. However, CP Nets are not well-behaved under changes of perspective, and lead to strange results if you think in terms of expected utility. The primary issue is that all of the variables are assumed to be independent, so that the space $\cal W$ of possible worlds is just the product of all of the individual variables:
	
	\[ \mathcal W \cong \prod_{X: \mathcal X} \Omega_X \] 
	
	Depending on how you think about it, there's always an injection one direction--- a world results in a setting of all of the variables:
	\[ \mathcal W \to \prod_{X: \mathcal X} \Omega_X \] 
	but even this may not be in keeping with the way we might want to use these models: some variables might not be relevant or well-defined in all contexts. For instance, people use choice of food as an example variable all the time: the variable represents my selection of dinner at restaurant $X$. But such a variable does not mean anything if I decide not to eat, or if I go to restaurant $Y$ which serves different items. One way to fix this might be to add a special ``not applicable'' value in the range of each variable like this, but now we've exacerbated our first problem even further: what does the world where ALL variables are set to ``not applicable'' look like? This failure to have a natural correspondence causes a number of problems:
	
	\subsection{Incompatibility with Expected Utility}
	
	It is possible to have 
	
	\subsection{Causes problems for Logical and Causal Structure}
	
\end{document}