\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb, amsthm}
\usepackage[backend=biber,
	style=alphabetic,
	%	citestyle=authoryear,
	natbib=true,
	url=true, 
	doi=true]{biblatex}

\addbibresource{../refs.bib}

\usepackage{microtype}
\usepackage{tikz}

\usepackage{color}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage[colorlinks=true, citecolor=deepgreen]{hyperref}

\newcommand\geqc{\succcurlyeq}
\newcommand\leqc{\preccurlyeq}

\setlength{\skip\footins}{1cm}
\setlength{\footnotesep}{0.4cm}

\usepackage{parskip}
\usepackage{enumitem}

\newtheorem*{remark}{Remark}


\begin{document}
	\section{What I want to do}
	I'm trying to get a theory of preference dynamics.
	
	Currently preferences are thought of as static objects, fixed as part of the structure and identity of an agent, independent of beliefs, complete, and over some fixed domain. This is clearly not at all how human preferences work, and I posit that it's not the right way to think of preference for synthetic agents either.
	
	Perhaps among other things, desiderata for a model of preference dynamics include:
	\begin{enumerate}[noitemsep]
		\item Provide an answer to the `value loading' problem: show how you can acquire ``reasonable'' preferences by interacting with the world
		\item Reduce to static models for some parameter settings
		\item Behave reasonably when combined with changes of perspective 
		\item Be resistant to standard challenges to rationality, such as dutch booking
		\item Have weak safety guarantees: an agent should not eagerly adopt preferences which are totally in conflict with its current ones
	\end{enumerate}
	
%	We might also require that a theory be useful for explaining human behavior. We expect ours to be able to explain:
	Because the view of preferences we adopt here is different from the standard ones in economics (in particular, it lends itself naturally to incorporating boundedness), we have a hope of explaining some behavior which was previously classified irrational, as an optimal in some sense. The following psychological effects lend themselves to explanation:
	\begin{enumerate}[noitemsep]
		\item Value Capture in Gamification
		\item Framing Effects
		\item The Conniseur Effect (adaptive preferences)
		\item The Novelty Effect (anti-adaptive preferences)
%		\item Cultural Revolution
	\end{enumerate}
	
%	\section{Why This is Useful}
	
	\section{Examples and Use Cases}
	
	\subsection{Serving Content}
	Since humans' preferences are not always dynamic, it is a mistake to design recommender systems as though they were. The problem of designing a recommender system is almost always framed as the task of deciding what content to display to what user at a single point in time, based on information about the content and the user, which hides potentially dynamic elements of preferences. Such a model can still accurately predict that a user's preferences have changed by looking at updated user data and considering the new user an entirely different person, but the approach of fixing this issue by trying to get more recent feature data, necessarily lags behind that source of data. While in principle it is may be possible to predict where a person's tastes are likely to go in the future, this kind of thinking mirrors only the recent past.
	
	Similarly, issues with recommender systems zeroing in on a particular niche preference set, and serving content only similar to the content the user has seen before. However, sometimes the similarity of the content is actually a detriment. For instance, if you've already watched an instructional video on integration by parts, you are less likely to click on other such videos, rather than more likely --- despite (and indeed because of) similarities between the two videos. While current recommender systems do 
	
	\subsection{Framing Problems}
	It is well documented that people exhibit different preferences depending on the context and manner in which the preferences are elicited, and particularly the framing of the question. This fact is a problem for preference theories which only model world states, as they cannot distinguish between differing framings of the same logical scenario. Nonetheless, humans reliably exhibit biases dependent on the way in which a problem is framed, and perhaps reasonably so: it takes a lot of cognitive effort to see exactly what the consequences of a choice look like, language is ambiguous, and different representations of a scenario almost always highlight different salient features.
	
	\subsection{Learning Preferences from Samples}
	There is also the question of where we got our preferences in the first place. While it might be reasonable to think that a person can be born with \textit{a propensity to like video games}, it's crazy to think that people are born with actual preferences about video games. Archimedes probably did not have preferences about what video games are good, but also it would be weird if he were unable to form any.
	
	A static theory of preferences fails to provide an account of how preferences form in the first place, making them useless for understanding reasons for motivations: at the end (or if you're a logically omniscient being that economists model, at the beginning), preferences just boil down to unquestionable axioms, that could have been anything. Instead, we would like to be able to provide an account of how preferences are be formed over new concepts, implicitly providing some restrictions on the kinds of preferences it is possible to have for a given environment. By doing so, we can get a tighter bound on what agents can actually value, giving us both more predictive power, and reducing the search space for trying to infer them, in inverse reinforcement learning, content recommendation, and so forth.
	
	\subsection{Gamification}
	One particularly objectionable feature of static preference models is that they depend on 
	
	For instance, in an expected utility setting, I can say that playing a game $g$ give me 5 utils (maybe in context). Maybe I can even break this down, and say that the storyline gave me 3, while the competitive aspects are collectively responsible for 2. 
	
	But the weird feature of this is that story telling and 
	
	A particular effect that we would like to model is \textit{value capture}---
	
	\subsection{Specification of Robot Preferences}
	The problem with simply giving an agent an objective to optimize, is that people are really bad at understanding what optimizing any given objective actually means until they see it. In practice, people mis-specify objectives all the time, and are often mistakenly under the impression that they've incentivized the agent to do something slightly different. There are even numerous fairy tales and parables cautioning against this, which can be summed up with the aphorism ``be careful what you wish for''
	
	People don't have to be all that careful what they wish for around their friends and family, however, because they do not express their desires in such clear terms as objective functions, and the wishes they express are not taken literally, but rather in context with the other preferences this person has expressed, and the social context. A theory of preference dynamics would allow for both partial specification and overspecification, which may be a step towards corrigibility.
	
	\section{Some Intuition about the Framework}
	We think of preferences over most things as actually being cached computations of how to optimize other, previously held preferences, filtered through some beliefs about the world. For instance, 
	
	\begin{itemize}[nosep]
		\item A taste for ice cream can be thought of as a cached version of the evolutionary preference for staying alive, seen through the lens of food choice: perhaps a cached preference computation from times when calories were scarce.
		\item A preference for green apples over red ones might be the cached computation from the last time you were faced with a similar choice, incorporating factors such as usefulness in the kitchen, sourness, color preference, and so forth. It is not atomic, but rather a cached view of other preferences through the apple lens.
		\item An affinity towards the political ideal freedom can be thought of as a cached view of your preferences over governments you've read about or experienced, seen through the lens of one particular feature: how much freedom the government offers.
		\item Vegetarianism can be thought of as a cached preference for environmentalism and reduction of animal suffering, filtered through the lens of food choice.
		\item Habits can be thought of as cached versions of your preferences (nutritional, entertainment, intellectual, structural, etc.), filtered through the lens of stimulus-response functions
	\end{itemize}
	
	With this in mind, our picture of preferences encodes a bunch of redundant information, in many different settings, and the picture is informed by the ways that the world can cause one setting to have a bearing on other related ones. 
	
	Note that it is possible to de-couple a preference from the original reasons for forming it: the first and last examples above are good illustrations. A sweet-tooth is no longer the best way to ensure evolutionary success now due to environmental distributional shift, but we retain this preference anyway. In the last example, people who became vegetarian purely for environmental and animal rights reasons, will do better on both fronts eating meat for a day in exchange for a friend abstaining for two --- and yet people are often hesitant to make this trade, because the preference acquires.
	
	If we were both cognitively unbounded and certain about what we cared about, we would not need to keep around preference domains for anything else for very long. We could just re-compute everything we needed from scratch from only our single true preference domain, which was precise enough to exactly capture the one thing we care about --- this corresponds to the classical view of static preferences.
	
	
	\subsection{Dynamics}
	The feature that drives preference changes, in this framework, is cognitive dissonance between conflicting preferences, which is why the static picture is static.
	
	%%%%%%%%%%%%%%%%%%
	There are two equivalent ways of thinking about how preference change works here: identity, and conflict.
	
	The first is by 
	

	\section{Formalism}
	\subsection{Representation}
	At each point in time, a representation of the agent's state $(\mathcal D_t, \mathcal B_t, W_t)$ consists of a collection of domains $\mathcal D_t$, each of which has a preference ordering, a collection of beliefs connecting some subset of pairs of domains together, and a weight $W_t : \mathcal D \to [-1,1]$ on each domain.
	
	There are multiple ways of nailing down the formalism further by explicating the ordering requirements from the domains, and structure of the beliefs\footnote{For example, we could also have used joint probabilities and undirected models to get a Markov Network, or possibly even used a causal model, where the structural equations are edges, and the nodes are variables. However, this would require a different consistency metric.}; as a first pass, we will assume only that the ordering $\leq_D$ on each domain $D \in \mathcal D_t$ is transitive and reflexive (making it a pre-order), and give the beliefs the structure of a conditional probability distribution:
	
	\[  \mathcal B_t \subseteq \prod_{A, B \in \mathcal D_t} \Big( A \to \Delta B \Big) \] 
	
	i.e., for some subset of pairs of domains $\{ (A_i, B_i) \}_i$, we have a probability distribution $\Delta B_i$ over $B_i$ associated each element of $A_i$. We will write $B [X \to Y]$ or $\Pr(Y | X)$ to denote the conditional distribution $B \in \mathcal B$ over $Y$ whose co-domain is $X$.
	
	
	\begin{remark}
		Fixing a time $t$, if we were to forget the orderings and treat the domains as sets, and the edges formed by $\mathcal B_t$ are acyclic, then $(\mathcal D_t, \mathcal B_t)$ forms a Bayesian Network.
	\end{remark}
 
 	
 	
 	\subsection{Dynamics}
	
 	Now, for the dynamic piece. We define the consistency of a link $B : X \to \Delta Y$ as: 	
 	\[ \zeta\big(B[X \to Y]\big) =  \sigma \left( \left(1- \frac{|W_X - W_Y|}{W_X + W_Y}\right) \sum_{x,x' \in X}\sum_{y,y' \in Y} X(x,x') Y(y,y') B(x)(y) B(x')(y') \right) \]
 	where $\sigma : \mathbb R \to [-1, 1]$ is an activation function such as sigmoid or $\tanh$, $B(y)(x)$ is the probability mass on $y$ in the distribution $B(x)$, and for a domain $D$, $D(d, d')$ is a signed preference indicator, defined as 
 	\[ D(d, d') := \begin{cases}
 		1 & d \prec_D d' \\
 		-1 & d \succ_D d' \\
 		0 & \text{otherwise}
 	\end{cases} \]
 	 	
 	We can also sum across the entire graph to get a measure for the whole model:
 	\[ \zeta(\mathcal D, \mathcal B, W) = \sum_{B[X\to Y] \in \mathcal B}~ \zeta(B) \]
 	
 	
 	\begin{remark}
 		If $\mathcal D$ consists of only a dingle domain $D$, with the identity distribution $\mathcal B = {B(x,y) = \delta_{x,y}: D \to D}$, then $\mathcal D$ is consistent, i.e., $\zeta(\mathcal D, \mathcal B, W) = 1$.
 	\end{remark}
 	
 	Finally, the update rule is just backpropagation of the consistency through the parameters: for each parameter 
 	\[ p \in \bigcup_{B[X\to Y] \in \mathcal B}  \Bigg( \{ X(x,x') | x, x' \in X\} \cup \{ Y(y, y') | y,y' \in Y\} \cup \{ W_X, W_y \} \cup \{B(x)(y) | x \in X, y \in Y\}  \Bigg) \]

	\[ \frac{\partial \zeta(B) }{\partial B(x)(y) } \]
	
	\subsection{Explanation of Update Rule}
	Suppose 
	
		
	\section{Revisited Examples}
%	Now that we have some formalism,
	
	\subsection{Framing Problems}
	Suppose there are two framings of a decision problem, one between the alternatives $X$ and the other between the alternatives $Y$. At first, suppose you do not know how $X$ and $Y$ relate to each other, but are instead separately given hypothetical scenarios in which you are asked to choose in both cases. You decide that:	
	\[ x \prec x' \qquad\text{and}\qquad y \succ y' \]
	You are reasonably confident about both of these decision, but you have more confidence in your preference on $X$, because you've had to make the choice in this context before $(W_X = 0.95, W_Y=0.8)$. Upon looking at the situation more closely, however, you discover that there is actually a logical equivalence between $x$ and $y$, and between $x'$ and $y'$ (denote this bijection $f : X\to Y$). Your preferences are now in conflict, and we calculate
	\begin{align*}
		\zeta(f) &= \sigma \left( \left(1- \frac{0.15}{1.75}\right) \sum_{a,a' \in X}\sum_{b,b' \in Y} X(a,a') Y(b,b') B(a)(b) B(a')(b') \right) \\
		&= \sigma \left( 0.914 \sum_{a,a' \in X} X(a,a') Y(f(a), f(a'))  \right) \\
		&= \sigma~ \bigg( 0.914 \Big(  X(x,x') Y(f(x), f(x')) + X(x',x) Y(f(x'), f(x))  \Big) \bigg) \\
		&= \sigma~ \bigg( 0.914 \Big(  1 \cdot Y(y, y') + (-1) \cdot Y(y', y)  \Big) \bigg) \\
		&= \sigma (-1.83)
	\end{align*}
	
	
	\subsection{Learning from Samples}	
	\subsection{Serving Content}
	
	In this simple example, we will consider 5 domains: Song Instances ($I$), 
	
	\subsection{Gamification}
	\subsection{Specification of Robot Preferences}
	
	\section{Plausible Theoretical Results to Chase}
	
	\begin{itemize}
		\item If we fix a maximum representation size (number of bits, comparisons, or the like), then the optimal strategy (relative to the ``true'' experience domain, which we cannot represent forever) in terms of preference fidelity is to utilize a large number of domains, rather than just a few big ones. Reasons: combinatorial explosion of orderings for big sets; generalization to unseen examples.
		
	\end{itemize}
	
	\section{Relation to Other Models}
			
	\subsection{CP Net Problems}
	The CP semantics are in many ways appealing: the dominance of $a$ over $\bar a$ independent of context seems to be a really nice way of capturing the utility brought by $a$ independent of the context of everything else. However, CP Nets are not well-behaved under changes of perspective, and lead to strange results if you think in terms of expected utility. The primary issue is that all of the variables are assumed to be independent, so that the space $\cal W$ of possible worlds is just the product of all of the individual variables:
	
	\[ \mathcal W \cong \prod_{X: \mathcal X} \Omega_X \] 
	
	Depending on how you think about it, there's always an injection one direction--- a world results in a setting of all of the variables:
	\[ \mathcal W \to \prod_{X: \mathcal X} \Omega_X \] 
	but even this may not be in keeping with the way we might want to use these models: some variables might not be relevant or well-defined in all contexts. For instance, people use choice of food as an example variable all the time: the variable represents my selection of dinner at restaurant $X$. But such a variable does not mean anything if I decide not to eat, or if I go to restaurant $Y$ which serves different items. One way to fix this might be to add a special ``not applicable'' value in the range of each variable like this, but now we've exacerbated our first problem even further: what does the world where ALL variables are set to ``not applicable'' look like? This failure to have a natural correspondence causes a number of problems:
	
	\subsubsection{Incompatibility with Expected Utility}
	
	Suppose that I'm choosing from 
	
	\subsubsection{Causes problems for Logical and Causal Structure}
	
\end{document}